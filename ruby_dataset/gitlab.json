[
  {
    "raw_code": "def self.start(queues, env: :development, directory: Dir.pwd, concurrency: 20, timeout: DEFAULT_SOFT_TIMEOUT_SECONDS, dryrun: false)\n      queues.map.with_index do |pair, index|\n        start_sidekiq(pair, env: env,\n          directory: directory,\n          concurrency: concurrency,\n          worker_id: index,\n          timeout: timeout,\n          dryrun: dryrun)\n      end",
    "comment": "Starts Sidekiq workers for the pairs of processes.  Example:  start([ ['foo'], ['bar', 'baz'] ], :production)  This would start two Sidekiq processes: one processing \"foo\", and one processing \"bar\" and \"baz\". Each one is placed in its own process group.  queues - An Array containing Arrays. Each sub Array should specify the queues to use for a single process.  directory - The directory of the Rails application.  Returns an Array containing the waiter threads (from Process.detach) of the started processes.",
    "label": "",
    "id": "1"
  },
  {
    "raw_code": "def self.start_sidekiq(queues, env:, directory:, concurrency:, worker_id:, timeout:, dryrun:)\n      counts = count_by_queue(queues)\n\n      cmd = %w[bundle exec sidekiq]\n      cmd << \"-c#{concurrency}\"\n      cmd << \"-e#{env}\"\n      cmd << \"-t#{timeout}\"\n      cmd << \"-gqueues:#{proc_details(counts)}\"\n      cmd << \"-r#{directory}\"\n\n      counts.each do |queue, count|\n        cmd << \"-q#{queue},#{count}\"\n      end",
    "comment": "Starts a Sidekiq process that processes _only_ the given queues.  Returns the PID of the started process.",
    "label": "",
    "id": "2"
  },
  {
    "raw_code": "def configure\n      if block_given?\n        yield(Configuration.default)\n      else\n        Configuration.default\n      end",
    "comment": "Customize default settings for the SDK using block. ErrorTrackingOpenAPI.configure do |config| config.username = \"xxx\" config.password = \"xxx\" end If no block given, return the default Configuration object.",
    "label": "",
    "id": "3"
  },
  {
    "raw_code": "def self.default\n      @@default ||= Configuration.new\n    end",
    "comment": "The default Configuration object.",
    "label": "",
    "id": "4"
  },
  {
    "raw_code": "def base_url(operation = nil)\n      index = server_operation_index.fetch(operation, server_index)\n      return \"#{scheme}://#{[host, base_path].join('/').gsub(/\\/+/, '/')}\".sub(/\\/+\\z/, '') if index == nil\n\n      server_url(index, server_operation_variables.fetch(operation, server_variables), operation_server_settings[operation])\n    end",
    "comment": "Returns base URL for specified operation based on server settings",
    "label": "",
    "id": "5"
  },
  {
    "raw_code": "def api_key_with_prefix(param_name, param_alias = nil)\n      key = @api_key[param_name]\n      key = @api_key.fetch(param_alias, key) unless param_alias.nil?\n      if @api_key_prefix[param_name]\n        \"#{@api_key_prefix[param_name]} #{key}\"\n      else\n        key\n      end",
    "comment": "Gets API key (with prefix if set). @param [String] param_name the parameter name of API key auth",
    "label": "",
    "id": "6"
  },
  {
    "raw_code": "def basic_auth_token\n      'Basic ' + [\"#{username}:#{password}\"].pack('m').delete(\"\\r\\n\")\n    end",
    "comment": "Gets Basic Auth token string",
    "label": "",
    "id": "7"
  },
  {
    "raw_code": "def auth_settings\n      {\n        'internalToken' =>\n          {\n            type: 'api_key',\n            in: 'header',\n            key: 'Gitlab-Error-Tracking-Token',\n            value: api_key_with_prefix('internalToken')\n          },\n      }\n    end",
    "comment": "Returns Auth Settings hash for api client.",
    "label": "",
    "id": "8"
  },
  {
    "raw_code": "def server_settings\n      [\n        {\n          url: \"https://localhost/errortracking/api/v1\",\n          description: \"No description provided\",\n        },\n        {\n          url: \"http://localhost/errortracking/api/v1\",\n          description: \"No description provided\",\n        }\n      ]\n    end",
    "comment": "Returns an array of Server setting",
    "label": "",
    "id": "9"
  },
  {
    "raw_code": "def server_url(index, variables = {}, servers = nil)\n      servers = server_settings if servers == nil\n\n      # check array index out of bound\n      if (index < 0 || index >= servers.size)\n        fail ArgumentError, \"Invalid index #{index} when selecting the server. Must be less than #{servers.size}\"\n      end",
    "comment": "Returns URL based on server settings  @param index array index of the server settings @param variables hash of variable and the corresponding value",
    "label": "",
    "id": "10"
  },
  {
    "raw_code": "def initialize(arg = nil)\n      if arg.is_a? Hash\n        if arg.key?(:message) || arg.key?('message')\n          super(arg[:message] || arg['message'])\n        else\n          super arg\n        end",
    "comment": "Usage examples: ApiError.new ApiError.new(\"message\") ApiError.new(:code => 500, :response_headers => {}, :response_body => \"\") ApiError.new(:code => 404, :message => \"Not Found\")",
    "label": "",
    "id": "11"
  },
  {
    "raw_code": "def to_s\n      message\n    end",
    "comment": "Override to_s to display a friendly error message",
    "label": "",
    "id": "12"
  },
  {
    "raw_code": "def initialize(config = Configuration.default)\n      @config = config\n      @user_agent = \"OpenAPI-Generator/#{VERSION}/ruby\"\n      @default_headers = {\n        'Content-Type' => 'application/json',\n        'User-Agent' => @user_agent\n      }\n    end",
    "comment": "Initializes the ApiClient @option config [Configuration] Configuration for initializing the object, default to Configuration.default",
    "label": "",
    "id": "13"
  },
  {
    "raw_code": "def call_api(http_method, path, opts = {})\n      request = build_request(http_method, path, opts)\n      response = request.run\n\n      if @config.debugging\n        @config.logger.debug \"HTTP response body ~BEGIN~\\n#{response.body}\\n~END~\\n\"\n      end",
    "comment": "Call an API with given options.  @return [Array<(Object, Integer, Hash)>] an array of 3 elements: the data deserialized from response body (could be nil), response status code and response headers.",
    "label": "",
    "id": "14"
  },
  {
    "raw_code": "def build_request(http_method, path, opts = {})\n      url = build_request_url(path, opts)\n      http_method = http_method.to_sym.downcase\n\n      header_params = @default_headers.merge(opts[:header_params] || {})\n      query_params = opts[:query_params] || {}\n      form_params = opts[:form_params] || {}\n      follow_location = opts[:follow_location] || true\n\n      update_params_for_auth! header_params, query_params, opts[:auth_names]\n\n      # set ssl_verifyhosts option based on @config.verify_ssl_host (true/false)\n      _verify_ssl_host = @config.verify_ssl_host ? 2 : 0\n\n      req_opts = {\n        :method => http_method,\n        :headers => header_params,\n        :params => query_params,\n        :params_encoding => @config.params_encoding,\n        :timeout => @config.timeout,\n        :ssl_verifypeer => @config.verify_ssl,\n        :ssl_verifyhost => _verify_ssl_host,\n        :sslcert => @config.cert_file,\n        :sslkey => @config.key_file,\n        :verbose => @config.debugging,\n        :followlocation => follow_location\n      }\n\n      # set custom cert, if provided\n      req_opts[:cainfo] = @config.ssl_ca_cert if @config.ssl_ca_cert\n\n      if [:post, :patch, :put, :delete].include?(http_method)\n        req_body = build_request_body(header_params, form_params, opts[:body])\n        req_opts.update :body => req_body\n        if @config.debugging\n          @config.logger.debug \"HTTP request body param ~BEGIN~\\n#{req_body}\\n~END~\\n\"\n        end",
    "comment": "Builds the HTTP request  @param [String] http_method HTTP method/verb (e.g. POST) @param [String] path URL path (e.g. /account/new) @option opts [Hash] :header_params Header parameters @option opts [Hash] :query_params Query parameters @option opts [Hash] :form_params Query parameters @option opts [Object] :body HTTP body (JSON/XML) @return [Typhoeus::Request] A Typhoeus Request",
    "label": "",
    "id": "15"
  },
  {
    "raw_code": "def build_request_body(header_params, form_params, body)\n      # http form\n      if header_params['Content-Type'] == 'application/x-www-form-urlencoded' ||\n          header_params['Content-Type'] == 'multipart/form-data'\n        data = {}\n        form_params.each do |key, value|\n          case value\n          when ::File, ::Array, nil\n            # let typhoeus handle File, Array and nil parameters\n            data[key] = value\n          else\n            data[key] = value.to_s\n          end",
    "comment": "Builds the HTTP request body  @param [Hash] header_params Header parameters @param [Hash] form_params Query parameters @param [Object] body HTTP body (JSON/XML) @return [String] HTTP body data in the form of string",
    "label": "",
    "id": "16"
  },
  {
    "raw_code": "def download_file(request)\n      tempfile = nil\n      encoding = nil\n      request.on_headers do |response|\n        content_disposition = response.headers['Content-Disposition']\n        if content_disposition && content_disposition =~ /filename=/i\n          filename = content_disposition[/filename=['\"]?([^'\"\\s]+)['\"]?/, 1]\n          prefix = sanitize_filename(filename)\n        else\n          prefix = 'download-'\n        end",
    "comment": "Save response body into a file in (the defined) temporary folder, using the filename from the \"Content-Disposition\" header if provided, otherwise a random filename. The response body is written to the file in chunks in order to handle files which size is larger than maximum Ruby String or even larger than the maximum memory a Ruby process can use.  @see Configuration#temp_folder_path",
    "label": "",
    "id": "17"
  },
  {
    "raw_code": "def json_mime?(mime)\n      (mime == '*/*') || !(mime =~ /Application\\/.*json(?!p)(;.*)?/i).nil?\n    end",
    "comment": "Check if the given MIME is a JSON MIME. JSON MIME examples: application/json application/json; charset=UTF8 APPLICATION/JSON */* @param [String] mime MIME @return [Boolean] True if the MIME is application/json",
    "label": "",
    "id": "18"
  },
  {
    "raw_code": "def deserialize(response, return_type)\n      body = response.body\n\n      # handle file downloading - return the File instance processed in request callbacks\n      # note that response body is empty when the file is written in chunks in request on_body callback\n      return @tempfile if return_type == 'File'\n\n      return nil if body.nil? || body.empty?\n\n      # return response body directly for String return type\n      return body if return_type == 'String'\n\n      # ensuring a default content type\n      content_type = response.headers['Content-Type'] || 'application/json'\n\n      fail \"Content-Type is not supported: #{content_type}\" unless json_mime?(content_type)\n\n      begin\n        data = JSON.parse(\"[#{body}]\", :symbolize_names => true)[0]\n      rescue JSON::ParserError => e\n        if %w(String Date Time).include?(return_type)\n          data = body\n        else\n          raise e\n        end",
    "comment": "Deserialize the response to the given return type.  @param [Response] response HTTP response @param [String] return_type some examples: \"User\", \"Array<User>\", \"Hash<String, Integer>\"",
    "label": "",
    "id": "19"
  },
  {
    "raw_code": "def convert_to_type(data, return_type)\n      return nil if data.nil?\n      case return_type\n      when 'String'\n        data.to_s\n      when 'Integer'\n        data.to_i\n      when 'Float'\n        data.to_f\n      when 'Boolean'\n        data == true\n      when 'Time'\n        # parse date time (expecting ISO 8601 format)\n        Time.parse data\n      when 'Date'\n        # parse date time (expecting ISO 8601 format)\n        Date.parse data\n      when 'Object'\n        # generic object (usually a Hash), return directly\n        data\n      when /\\AArray<(.+)>\\z/\n        # e.g. Array<Pet>\n        sub_type = $1\n        data.map { |item| convert_to_type(item, sub_type) }\n      when /\\AHash\\<String, (.+)\\>\\z/\n        # e.g. Hash<String, Integer>\n        sub_type = $1\n        {}.tap do |hash|\n          data.each { |k, v| hash[k] = convert_to_type(v, sub_type) }\n        end",
    "comment": "Convert data to the given return type. @param [Object] data Data to be converted @param [String] return_type Return type @return [Mixed] Data in a particular type",
    "label": "",
    "id": "20"
  },
  {
    "raw_code": "def sanitize_filename(filename)\n      filename.gsub(/.*[\\/\\\\]/, '')\n    end",
    "comment": "Sanitize filename by removing path. e.g. ../../sun.gif becomes sun.gif  @param [String] filename the filename to be sanitized @return [String] the sanitized filename",
    "label": "",
    "id": "21"
  },
  {
    "raw_code": "def update_params_for_auth!(header_params, query_params, auth_names)\n      Array(auth_names).each do |auth_name|\n        auth_setting = @config.auth_settings[auth_name]\n        next unless auth_setting\n        case auth_setting[:in]\n        when 'header' then header_params[auth_setting[:key]] = auth_setting[:value]\n        when 'query'  then query_params[auth_setting[:key]] = auth_setting[:value]\n        else fail ArgumentError, 'Authentication token must be in `query` or `header`'\n        end",
    "comment": "Update header and query params based on authentication settings.  @param [Hash] header_params Header parameters @param [Hash] query_params Query parameters @param [String] auth_names Authentication scheme name",
    "label": "",
    "id": "22"
  },
  {
    "raw_code": "def user_agent=(user_agent)\n      @user_agent = user_agent\n      @default_headers['User-Agent'] = @user_agent\n    end",
    "comment": "Sets user agent in HTTP header  @param [String] user_agent User agent (e.g. openapi-generator/ruby/1.0.0)",
    "label": "",
    "id": "23"
  },
  {
    "raw_code": "def select_header_accept(accepts)\n      return nil if accepts.nil? || accepts.empty?\n      # use JSON when present, otherwise use all of the provided\n      json_accept = accepts.find { |s| json_mime?(s) }\n      json_accept || accepts.join(',')\n    end",
    "comment": "Return Accept header based on an array of accepts provided. @param [Array] accepts array for Accept @return [String] the Accept header (e.g. application/json)",
    "label": "",
    "id": "24"
  },
  {
    "raw_code": "def select_header_content_type(content_types)\n      # return nil by default\n      return if content_types.nil? || content_types.empty?\n      # use JSON when present, otherwise use the first one\n      json_content_type = content_types.find { |s| json_mime?(s) }\n      json_content_type || content_types.first\n    end",
    "comment": "Return Content-Type header based on an array of content types provided. @param [Array] content_types array for Content-Type @return [String] the Content-Type header  (e.g. application/json)",
    "label": "",
    "id": "25"
  },
  {
    "raw_code": "def object_to_http_body(model)\n      return model if model.nil? || model.is_a?(String)\n      local_body = nil\n      if model.is_a?(Array)\n        local_body = model.map { |m| object_to_hash(m) }\n      else\n        local_body = object_to_hash(model)\n      end",
    "comment": "Convert object (array, hash, object, etc) to JSON string. @param [Object] model object to be converted into JSON string @return [String] JSON string representation of the object",
    "label": "",
    "id": "26"
  },
  {
    "raw_code": "def object_to_hash(obj)\n      if obj.respond_to?(:to_hash)\n        obj.to_hash\n      else\n        obj\n      end",
    "comment": "Convert object(non-array) to hash. @param [Object] obj object to be converted into JSON string @return [String] JSON string representation of the object",
    "label": "",
    "id": "27"
  },
  {
    "raw_code": "def build_collection_param(param, collection_format)\n      case collection_format\n      when :csv\n        param.join(',')\n      when :ssv\n        param.join(' ')\n      when :tsv\n        param.join(\"\\t\")\n      when :pipes\n        param.join('|')\n      when :multi\n        # return the array directly as typhoeus will handle it as expected\n        param\n      else\n        fail \"unknown collection format: #{collection_format.inspect}\"\n      end",
    "comment": "Build parameter value according to the given collection format. @param [String] collection_format one of :csv, :ssv, :tsv, :pipes and :multi",
    "label": "",
    "id": "28"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'status' => :'status',\n        :'updated_by_id' => :'updated_by_id'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "29"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "30"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'status' => :'String',\n        :'updated_by_id' => :'Integer'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "31"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "32"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::ErrorUpdatePayload` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "33"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "34"
  },
  {
    "raw_code": "def valid?\n      status_validator = EnumAttributeValidator.new('String', [\"unresolved\", \"resolved\", \"ignored\"])\n      return false unless status_validator.valid?(@status)\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "35"
  },
  {
    "raw_code": "def status=(status)\n      validator = EnumAttributeValidator.new('String', [\"unresolved\", \"resolved\", \"ignored\"])\n      unless validator.valid?(status)\n        fail ArgumentError, \"invalid value for \\\"status\\\", must be one of #{validator.allowable_values}.\"\n      end",
    "comment": "Custom attribute writer method checking allowed values (enum). @param [Object] status Object to be assigned",
    "label": "",
    "id": "36"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          status == o.status &&\n          updated_by_id == o.updated_by_id\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "37"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "38"
  },
  {
    "raw_code": "def hash\n      [status, updated_by_id].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "39"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "40"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "41"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "42"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "43"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "44"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "45"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "46"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'project_id' => :'projectId',\n        :'event_id' => :'eventId',\n        :'timestamp' => :'timestamp',\n        :'level' => :'level',\n        :'message' => :'message',\n        :'release' => :'release',\n        :'environment' => :'environment',\n        :'platform' => :'platform'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "47"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "48"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'project_id' => :'Integer',\n        :'event_id' => :'String',\n        :'timestamp' => :'Time',\n        :'level' => :'String',\n        :'message' => :'String',\n        :'release' => :'String',\n        :'environment' => :'String',\n        :'platform' => :'String'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "49"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "50"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::MessageEvent` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "51"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "52"
  },
  {
    "raw_code": "def valid?\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "53"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          project_id == o.project_id &&\n          event_id == o.event_id &&\n          timestamp == o.timestamp &&\n          level == o.level &&\n          message == o.message &&\n          release == o.release &&\n          environment == o.environment &&\n          platform == o.platform\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "54"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "55"
  },
  {
    "raw_code": "def hash\n      [project_id, event_id, timestamp, level, message, release, environment, platform].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "56"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "57"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "58"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "59"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "60"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "61"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "62"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "63"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'id' => :'id',\n        :'name' => :'name',\n        :'slug' => :'slug'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "64"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "65"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'id' => :'String',\n        :'name' => :'String',\n        :'slug' => :'String'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "66"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "67"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::Project` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "68"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "69"
  },
  {
    "raw_code": "def valid?\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "70"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          id == o.id &&\n          name == o.name &&\n          slug == o.slug\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "71"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "72"
  },
  {
    "raw_code": "def hash\n      [id, name, slug].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "73"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "74"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "75"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "76"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "77"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "78"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "79"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "80"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'fingerprint' => :'fingerprint',\n        :'project_id' => :'project_id',\n        :'name' => :'name',\n        :'description' => :'description',\n        :'actor' => :'actor',\n        :'event_count' => :'event_count',\n        :'approximated_user_count' => :'approximated_user_count',\n        :'last_seen_at' => :'last_seen_at',\n        :'first_seen_at' => :'first_seen_at',\n        :'status' => :'status',\n        :'stats' => :'stats'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "81"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "82"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'fingerprint' => :'Integer',\n        :'project_id' => :'Integer',\n        :'name' => :'String',\n        :'description' => :'String',\n        :'actor' => :'String',\n        :'event_count' => :'Integer',\n        :'approximated_user_count' => :'Integer',\n        :'last_seen_at' => :'Time',\n        :'first_seen_at' => :'Time',\n        :'status' => :'String',\n        :'stats' => :'ErrorStats'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "83"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "84"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::Error` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "85"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "86"
  },
  {
    "raw_code": "def valid?\n      status_validator = EnumAttributeValidator.new('String', [\"unresolved\", \"resolved\", \"ignored\"])\n      return false unless status_validator.valid?(@status)\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "87"
  },
  {
    "raw_code": "def status=(status)\n      validator = EnumAttributeValidator.new('String', [\"unresolved\", \"resolved\", \"ignored\"])\n      unless validator.valid?(status)\n        fail ArgumentError, \"invalid value for \\\"status\\\", must be one of #{validator.allowable_values}.\"\n      end",
    "comment": "Custom attribute writer method checking allowed values (enum). @param [Object] status Object to be assigned",
    "label": "",
    "id": "88"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          fingerprint == o.fingerprint &&\n          project_id == o.project_id &&\n          name == o.name &&\n          description == o.description &&\n          actor == o.actor &&\n          event_count == o.event_count &&\n          approximated_user_count == o.approximated_user_count &&\n          last_seen_at == o.last_seen_at &&\n          first_seen_at == o.first_seen_at &&\n          status == o.status &&\n          stats == o.stats\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "89"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "90"
  },
  {
    "raw_code": "def hash\n      [fingerprint, project_id, name, description, actor, event_count, approximated_user_count, last_seen_at, first_seen_at, status, stats].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "91"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "92"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "93"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "94"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "95"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "96"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "97"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "98"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'fingerprint' => :'fingerprint',\n        :'project_id' => :'projectId',\n        :'payload' => :'payload',\n        :'name' => :'name',\n        :'description' => :'description',\n        :'actor' => :'actor',\n        :'environment' => :'environment',\n        :'platform' => :'platform'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "99"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "100"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'fingerprint' => :'Integer',\n        :'project_id' => :'Integer',\n        :'payload' => :'String',\n        :'name' => :'String',\n        :'description' => :'String',\n        :'actor' => :'String',\n        :'environment' => :'String',\n        :'platform' => :'String'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "101"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "102"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::ErrorEvent` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "103"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "104"
  },
  {
    "raw_code": "def valid?\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "105"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          fingerprint == o.fingerprint &&\n          project_id == o.project_id &&\n          payload == o.payload &&\n          name == o.name &&\n          description == o.description &&\n          actor == o.actor &&\n          environment == o.environment &&\n          platform == o.platform\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "106"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "107"
  },
  {
    "raw_code": "def hash\n      [fingerprint, project_id, payload, name, description, actor, environment, platform].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "108"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "109"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "110"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "111"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "112"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "113"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "114"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "115"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'id' => :'id',\n        :'project' => :'project',\n        :'title' => :'title',\n        :'actor' => :'actor',\n        :'count' => :'count',\n        :'user_count' => :'userCount',\n        :'last_seen' => :'lastSeen',\n        :'first_seen' => :'firstSeen',\n        :'status' => :'status'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "116"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "117"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'id' => :'String',\n        :'project' => :'Project',\n        :'title' => :'String',\n        :'actor' => :'String',\n        :'count' => :'String',\n        :'user_count' => :'Integer',\n        :'last_seen' => :'Time',\n        :'first_seen' => :'Time',\n        :'status' => :'String'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "118"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "119"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::ErrorV2` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "120"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "121"
  },
  {
    "raw_code": "def valid?\n      status_validator = EnumAttributeValidator.new('String', [\"unresolved\", \"resolved\", \"ignored\"])\n      return false unless status_validator.valid?(@status)\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "122"
  },
  {
    "raw_code": "def status=(status)\n      validator = EnumAttributeValidator.new('String', [\"unresolved\", \"resolved\", \"ignored\"])\n      unless validator.valid?(status)\n        fail ArgumentError, \"invalid value for \\\"status\\\", must be one of #{validator.allowable_values}.\"\n      end",
    "comment": "Custom attribute writer method checking allowed values (enum). @param [Object] status Object to be assigned",
    "label": "",
    "id": "123"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          id == o.id &&\n          project == o.project &&\n          title == o.title &&\n          actor == o.actor &&\n          count == o.count &&\n          user_count == o.user_count &&\n          last_seen == o.last_seen &&\n          first_seen == o.first_seen &&\n          status == o.status\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "124"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "125"
  },
  {
    "raw_code": "def hash\n      [id, project, title, actor, count, user_count, last_seen, first_seen, status].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "126"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "127"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "128"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "129"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "130"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "131"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "132"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "133"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'start' => :'start',\n        :'_end' => :'end',\n        :'interval' => :'interval',\n        :'group' => :'group'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "134"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "135"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'start' => :'String',\n        :'_end' => :'String',\n        :'interval' => :'Array<String>',\n        :'group' => :'Array<StatsObjectGroupInner>'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "136"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "137"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::StatsObject` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "138"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "139"
  },
  {
    "raw_code": "def valid?\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "140"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          start == o.start &&\n          _end == o._end &&\n          interval == o.interval &&\n          group == o.group\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "141"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "142"
  },
  {
    "raw_code": "def hash\n      [start, _end, interval, group].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "143"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "144"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "145"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "146"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "147"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "148"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "149"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "150"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'by' => :'by',\n        :'totals' => :'totals',\n        :'series' => :'series'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "151"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "152"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'by' => :'Hash<String, Object>',\n        :'totals' => :'Hash<String, Object>',\n        :'series' => :'Hash<String, Object>'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "153"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "154"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::StatsObjectGroupInner` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "155"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "156"
  },
  {
    "raw_code": "def valid?\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "157"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          by == o.by &&\n          totals == o.totals &&\n          series == o.series\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "158"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "159"
  },
  {
    "raw_code": "def hash\n      [by, totals, series].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "160"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "161"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "162"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "163"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "164"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "165"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "166"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "167"
  },
  {
    "raw_code": "def self.attribute_map\n      {\n        :'frequency' => :'frequency'\n      }\n    end",
    "comment": "Attribute mapping from ruby-style variable name to JSON key.",
    "label": "",
    "id": "168"
  },
  {
    "raw_code": "def self.acceptable_attributes\n      attribute_map.values\n    end",
    "comment": "Returns all the JSON keys this model knows about",
    "label": "",
    "id": "169"
  },
  {
    "raw_code": "def self.openapi_types\n      {\n        :'frequency' => :'Object'\n      }\n    end",
    "comment": "Attribute type mapping.",
    "label": "",
    "id": "170"
  },
  {
    "raw_code": "def self.openapi_nullable\n      Set.new([\n      ])\n    end",
    "comment": "List of attributes with nullable: true",
    "label": "",
    "id": "171"
  },
  {
    "raw_code": "def initialize(attributes = {})\n      if (!attributes.is_a?(Hash))\n        fail ArgumentError, \"The input argument (attributes) must be a hash in `ErrorTrackingOpenAPI::ErrorStats` initialize method\"\n      end",
    "comment": "Initializes the object @param [Hash] attributes Model attributes in the form of hash",
    "label": "",
    "id": "172"
  },
  {
    "raw_code": "def list_invalid_properties\n      invalid_properties = Array.new\n      invalid_properties\n    end",
    "comment": "Show invalid properties with the reasons. Usually used together with valid? @return Array for valid properties with the reasons",
    "label": "",
    "id": "173"
  },
  {
    "raw_code": "def valid?\n      true\n    end",
    "comment": "Check to see if the all the properties in the model are valid @return true if the model is valid",
    "label": "",
    "id": "174"
  },
  {
    "raw_code": "def ==(o)\n      return true if self.equal?(o)\n      self.class == o.class &&\n          frequency == o.frequency\n    end",
    "comment": "Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "175"
  },
  {
    "raw_code": "def eql?(o)\n      self == o\n    end",
    "comment": "@see the `==` method @param [Object] Object to be compared",
    "label": "",
    "id": "176"
  },
  {
    "raw_code": "def hash\n      [frequency].hash\n    end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code",
    "label": "",
    "id": "177"
  },
  {
    "raw_code": "def self.build_from_hash(attributes)\n      new.build_from_hash(attributes)\n    end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "178"
  },
  {
    "raw_code": "def build_from_hash(attributes)\n      return nil unless attributes.is_a?(Hash)\n      attributes = attributes.transform_keys(&:to_sym)\n      self.class.openapi_types.each_pair do |key, type|\n        if attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)\n          self.send(\"#{key}=\", nil)\n        elsif type =~ /\\AArray<(.*)>/i\n          # check to ensure the input is an array given that the attribute\n          # is documented as an array but the input is not\n          if attributes[self.class.attribute_map[key]].is_a?(Array)\n            self.send(\"#{key}=\", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })\n          end",
    "comment": "Builds the object from hash @param [Hash] attributes Model attributes in the form of hash @return [Object] Returns the model itself",
    "label": "",
    "id": "179"
  },
  {
    "raw_code": "def _deserialize(type, value)\n      case type.to_sym\n      when :Time\n        Time.parse(value)\n      when :Date\n        Date.parse(value)\n      when :String\n        value.to_s\n      when :Integer\n        value.to_i\n      when :Float\n        value.to_f\n      when :Boolean\n        if value.to_s =~ /\\A(true|t|yes|y|1)\\z/i\n          true\n        else\n          false\n        end",
    "comment": "Deserializes the data based on type @param string type Data type @param string value Value to be deserialized @return [Object] Deserialized data",
    "label": "",
    "id": "180"
  },
  {
    "raw_code": "def to_s\n      to_hash.to_s\n    end",
    "comment": "Returns the string representation of the object @return [String] String presentation of the object",
    "label": "",
    "id": "181"
  },
  {
    "raw_code": "def to_body\n      to_hash\n    end",
    "comment": "to_body is an alias to to_hash (backward compatibility) @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "182"
  },
  {
    "raw_code": "def to_hash\n      hash = {}\n      self.class.attribute_map.each_pair do |attr, param|\n        value = self.send(attr)\n        if value.nil?\n          is_nullable = self.class.openapi_nullable.include?(attr)\n          next if !is_nullable || (is_nullable && !instance_variable_defined?(:\"@#{attr}\"))\n        end",
    "comment": "Returns the object in the form of hash @return [Hash] Returns the object in the form of hash",
    "label": "",
    "id": "183"
  },
  {
    "raw_code": "def _to_hash(value)\n      if value.is_a?(Array)\n        value.compact.map { |v| _to_hash(v) }\n      elsif value.is_a?(Hash)\n        {}.tap do |hash|\n          value.each { |k, v| hash[k] = _to_hash(v) }\n        end",
    "comment": "Outputs non-array value in the form of hash For object, use to_hash. Otherwise, just return the value @param [Object] value Any valid value @return [Hash] Returns the value in the form of hash",
    "label": "",
    "id": "184"
  },
  {
    "raw_code": "def get_error(project_id, fingerprint, opts = {})\n      data, _status_code, _headers = get_error_with_http_info(project_id, fingerprint, opts)\n      data\n    end",
    "comment": "Get information about the error @param project_id [Integer] ID of the project where the error was created @param fingerprint [Integer] ID of the error that needs to be updated deleted @param [Hash] opts the optional parameters @return [Error]",
    "label": "",
    "id": "185"
  },
  {
    "raw_code": "def get_error_with_http_info(project_id, fingerprint, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: ErrorsApi.get_error ...'\n      end",
    "comment": "Get information about the error @param project_id [Integer] ID of the project where the error was created @param fingerprint [Integer] ID of the error that needs to be updated deleted @param [Hash] opts the optional parameters @return [Array<(Error, Integer, Hash)>] Error data, response status code and response headers",
    "label": "",
    "id": "186"
  },
  {
    "raw_code": "def list_errors(project_id, opts = {})\n      data, _status_code, _headers = list_errors_with_http_info(project_id, opts)\n      data\n    end",
    "comment": "List of errors @param project_id [Integer] ID of the project where the error was created @param [Hash] opts the optional parameters @option opts [String] :sort  (default to 'last_seen_desc') @option opts [String] :status  (default to 'unresolved') @option opts [String] :query @option opts [String] :cursor Base64 encoded information for pagination @option opts [Integer] :limit Number of entries to return (default to 20) @option opts [String] :stats_period  (default to '24h') @option opts [String] :query_period  (default to '30d') @return [Array<Error>]",
    "label": "",
    "id": "187"
  },
  {
    "raw_code": "def list_errors_with_http_info(project_id, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: ErrorsApi.list_errors ...'\n      end",
    "comment": "List of errors @param project_id [Integer] ID of the project where the error was created @param [Hash] opts the optional parameters @option opts [String] :sort  (default to 'last_seen_desc') @option opts [String] :status  (default to 'unresolved') @option opts [String] :query @option opts [String] :cursor Base64 encoded information for pagination @option opts [Integer] :limit Number of entries to return (default to 20) @option opts [String] :stats_period  (default to '24h') @option opts [String] :query_period  (default to '30d') @return [Array<(Array<Error>, Integer, Hash)>] Array<Error> data, response status code and response headers",
    "label": "",
    "id": "188"
  },
  {
    "raw_code": "def list_events(project_id, fingerprint, opts = {})\n      data, _status_code, _headers = list_events_with_http_info(project_id, fingerprint, opts)\n      data\n    end",
    "comment": "Get information about the events related to the error @param project_id [Integer] ID of the project where the error was created @param fingerprint [Integer] ID of the error within the project @param [Hash] opts the optional parameters @option opts [String] :sort  (default to 'occurred_at_asc') @option opts [String] :cursor Base64 encoded information for pagination @option opts [Integer] :limit Number of entries to return (default to 20) @return [Array<ErrorEvent>]",
    "label": "",
    "id": "189"
  },
  {
    "raw_code": "def list_events_with_http_info(project_id, fingerprint, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: ErrorsApi.list_events ...'\n      end",
    "comment": "Get information about the events related to the error @param project_id [Integer] ID of the project where the error was created @param fingerprint [Integer] ID of the error within the project @param [Hash] opts the optional parameters @option opts [String] :sort  (default to 'occurred_at_asc') @option opts [String] :cursor Base64 encoded information for pagination @option opts [Integer] :limit Number of entries to return (default to 20) @return [Array<(Array<ErrorEvent>, Integer, Hash)>] Array<ErrorEvent> data, response status code and response headers",
    "label": "",
    "id": "190"
  },
  {
    "raw_code": "def update_error(project_id, fingerprint, body, opts = {})\n      data, _status_code, _headers = update_error_with_http_info(project_id, fingerprint, body, opts)\n      data\n    end",
    "comment": "Update the status of the error @param project_id [Integer] ID of the project where the error was created @param fingerprint [Integer] ID of the error that needs to be updated deleted @param body [ErrorUpdatePayload] Error update object with the new values @param [Hash] opts the optional parameters @return [Error]",
    "label": "",
    "id": "191"
  },
  {
    "raw_code": "def update_error_with_http_info(project_id, fingerprint, body, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: ErrorsApi.update_error ...'\n      end",
    "comment": "Update the status of the error @param project_id [Integer] ID of the project where the error was created @param fingerprint [Integer] ID of the error that needs to be updated deleted @param body [ErrorUpdatePayload] Error update object with the new values @param [Hash] opts the optional parameters @return [Array<(Error, Integer, Hash)>] Error data, response status code and response headers",
    "label": "",
    "id": "192"
  },
  {
    "raw_code": "def list_messages(project_id, opts = {})\n      data, _status_code, _headers = list_messages_with_http_info(project_id, opts)\n      data\n    end",
    "comment": "List of messages @param project_id [Integer] ID of the project where the message was created @param [Hash] opts the optional parameters @option opts [Integer] :limit Number of entries to return (default to 20) @return [Array<MessageEvent>]",
    "label": "",
    "id": "193"
  },
  {
    "raw_code": "def list_messages_with_http_info(project_id, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: MessagesApi.list_messages ...'\n      end",
    "comment": "List of messages @param project_id [Integer] ID of the project where the message was created @param [Hash] opts the optional parameters @option opts [Integer] :limit Number of entries to return (default to 20) @return [Array<(Array<MessageEvent>, Integer, Hash)>] Array<MessageEvent> data, response status code and response headers",
    "label": "",
    "id": "194"
  },
  {
    "raw_code": "def delete_project(id, opts = {})\n      delete_project_with_http_info(id, opts)\n      nil\n    end",
    "comment": "Deletes all project related data. Mostly for testing purposes and later for production to clean updeleted projects. @param id [Integer] ID of the project @param [Hash] opts the optional parameters @return [nil]",
    "label": "",
    "id": "195"
  },
  {
    "raw_code": "def delete_project_with_http_info(id, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: ProjectsApi.delete_project ...'\n      end",
    "comment": "Deletes all project related data. Mostly for testing purposes and later for production to clean updeleted projects. @param id [Integer] ID of the project @param [Hash] opts the optional parameters @return [Array<(nil, Integer, Hash)>] nil, response status code and response headers",
    "label": "",
    "id": "196"
  },
  {
    "raw_code": "def list_events(project_id, fingerprint, opts = {})\n      data, _status_code, _headers = list_events_with_http_info(project_id, fingerprint, opts)\n      data\n    end",
    "comment": "Get information about the events related to the error @param project_id [Integer] ID of the project where the error was created @param fingerprint [Integer] ID of the error within the project @param [Hash] opts the optional parameters @option opts [String] :sort  (default to 'occurred_at_asc') @option opts [String] :cursor Base64 encoded information for pagination @option opts [Integer] :limit Number of entries to return (default to 20) @return [Array<ErrorEvent>]",
    "label": "",
    "id": "197"
  },
  {
    "raw_code": "def list_events_with_http_info(project_id, fingerprint, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: EventsApi.list_events ...'\n      end",
    "comment": "Get information about the events related to the error @param project_id [Integer] ID of the project where the error was created @param fingerprint [Integer] ID of the error within the project @param [Hash] opts the optional parameters @option opts [String] :sort  (default to 'occurred_at_asc') @option opts [String] :cursor Base64 encoded information for pagination @option opts [Integer] :limit Number of entries to return (default to 20) @return [Array<(Array<ErrorEvent>, Integer, Hash)>] Array<ErrorEvent> data, response status code and response headers",
    "label": "",
    "id": "198"
  },
  {
    "raw_code": "def projects_api_project_id_envelope_post(project_id, opts = {})\n      data, _status_code, _headers = projects_api_project_id_envelope_post_with_http_info(project_id, opts)\n      data\n    end",
    "comment": "Ingestion endpoint for error events sent from client SDKs @param project_id [Integer] ID of the project where the error was created @param [Hash] opts the optional parameters @return [ErrorEvent]",
    "label": "",
    "id": "199"
  },
  {
    "raw_code": "def projects_api_project_id_envelope_post_with_http_info(project_id, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: EventsApi.projects_api_project_id_envelope_post ...'\n      end",
    "comment": "Ingestion endpoint for error events sent from client SDKs @param project_id [Integer] ID of the project where the error was created @param [Hash] opts the optional parameters @return [Array<(ErrorEvent, Integer, Hash)>] ErrorEvent data, response status code and response headers",
    "label": "",
    "id": "200"
  },
  {
    "raw_code": "def projects_api_project_id_store_post(project_id, opts = {})\n      data, _status_code, _headers = projects_api_project_id_store_post_with_http_info(project_id, opts)\n      data\n    end",
    "comment": "Ingestion endpoint for error events sent from client SDKs @param project_id [Integer] ID of the project where the error was created @param [Hash] opts the optional parameters @return [ErrorEvent]",
    "label": "",
    "id": "201"
  },
  {
    "raw_code": "def projects_api_project_id_store_post_with_http_info(project_id, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: EventsApi.projects_api_project_id_store_post ...'\n      end",
    "comment": "Ingestion endpoint for error events sent from client SDKs @param project_id [Integer] ID of the project where the error was created @param [Hash] opts the optional parameters @return [Array<(ErrorEvent, Integer, Hash)>] ErrorEvent data, response status code and response headers",
    "label": "",
    "id": "202"
  },
  {
    "raw_code": "def get_stats_v2(group_id, opts = {})\n      data, _status_code, _headers = get_stats_v2_with_http_info(group_id, opts)\n      data\n    end",
    "comment": "Stats of events received for the group @param group_id [Integer] ID of the group @param [Hash] opts the optional parameters @return [Array<StatsObject>]",
    "label": "",
    "id": "203"
  },
  {
    "raw_code": "def get_stats_v2_with_http_info(group_id, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: ErrorsV2Api.get_stats_v2 ...'\n      end",
    "comment": "Stats of events received for the group @param group_id [Integer] ID of the group @param [Hash] opts the optional parameters @return [Array<(Array<StatsObject>, Integer, Hash)>] Array<StatsObject> data, response status code and response headers",
    "label": "",
    "id": "204"
  },
  {
    "raw_code": "def list_errors_v2(project, group_id, opts = {})\n      data, _status_code, _headers = list_errors_v2_with_http_info(project, group_id, opts)\n      data\n    end",
    "comment": "List of errors(V2) @param project [Array<Integer>] ID of the project where the error was created @param group_id [Integer] ID of the group @param [Hash] opts the optional parameters @option opts [String] :status  (default to 'unresolved') @option opts [String] :query @option opts [String] :start Optional start of the stat period in format 2006-01-02T15:04:05 @option opts [String] :_end Optional end of the stat period in format 2006-01-02T15:04:05 @option opts [String] :environment @option opts [Integer] :limit Number of entries to return (default to 20) @option opts [String] :sort Optional sorting column of the entries (default to 'date') @return [Array<ErrorV2>]",
    "label": "",
    "id": "205"
  },
  {
    "raw_code": "def list_errors_v2_with_http_info(project, group_id, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: ErrorsV2Api.list_errors_v2 ...'\n      end",
    "comment": "List of errors(V2) @param project [Array<Integer>] ID of the project where the error was created @param group_id [Integer] ID of the group @param [Hash] opts the optional parameters @option opts [String] :status  (default to 'unresolved') @option opts [String] :query @option opts [String] :start Optional start of the stat period in format 2006-01-02T15:04:05 @option opts [String] :_end Optional end of the stat period in format 2006-01-02T15:04:05 @option opts [String] :environment @option opts [Integer] :limit Number of entries to return (default to 20) @option opts [String] :sort Optional sorting column of the entries (default to 'date') @return [Array<(Array<ErrorV2>, Integer, Hash)>] Array<ErrorV2> data, response status code and response headers",
    "label": "",
    "id": "206"
  },
  {
    "raw_code": "def list_projects(group_id, opts = {})\n      data, _status_code, _headers = list_projects_with_http_info(group_id, opts)\n      data\n    end",
    "comment": "List of projects @param group_id [Integer] ID of the group @param [Hash] opts the optional parameters @return [Array<Project>]",
    "label": "",
    "id": "207"
  },
  {
    "raw_code": "def list_projects_with_http_info(group_id, opts = {})\n      if @api_client.config.debugging\n        @api_client.config.logger.debug 'Calling API: ErrorsV2Api.list_projects ...'\n      end",
    "comment": "List of projects @param group_id [Integer] ID of the group @param [Hash] opts the optional parameters @return [Array<(Array<Project>, Integer, Hash)>] Array<Project> data, response status code and response headers",
    "label": "",
    "id": "208"
  },
  {
    "raw_code": "def self.start(argv)\n        # Set a custom process name\n        update_process_title!\n\n        Gitlab::Backup::Cli::Runner.start(argv)\n      end",
    "comment": "Entrypoint for the application Run any initialization logic from here",
    "label": "",
    "id": "209"
  },
  {
    "raw_code": "def initialize(\n          context:,\n          backup_bucket: nil,\n          wait_for_completion: nil,\n          registry_bucket: nil,\n          service_account_file: nil)\n          @context = context\n          @metadata = build_metadata\n          @workdir = create_temporary_workdir!\n          @archive_directory = context.backup_basedir.join(metadata.backup_id)\n          super(\n            backup_bucket: backup_bucket,\n            wait_for_completion: wait_for_completion,\n            registry_bucket: registry_bucket,\n            service_account_file: service_account_file\n          )\n        end",
    "comment": "@param [Gitlab::Backup::Cli::SourceContext, Context::OmnibusContext] context",
    "label": "",
    "id": "210"
  },
  {
    "raw_code": "def release!\n          FileUtils.rm_rf(workdir)\n        end",
    "comment": "At the end of a successful backup, call this to release temporary resources",
    "label": "",
    "id": "211"
  },
  {
    "raw_code": "def write_metadata!\n          return if metadata.write!(workdir)\n\n          raise Gitlab::Backup::Cli::Error, 'Failed to write metadata to disk'\n        end",
    "comment": "Write the backup_information.json data to disk",
    "label": "",
    "id": "212"
  },
  {
    "raw_code": "def create_temporary_workdir!\n          # Ensure base directory exists\n          FileUtils.mkdir_p(context.backup_basedir)\n\n          Pathname(Dir.mktmpdir('backup', context.backup_basedir))\n        end",
    "comment": "@return [Pathname] temporary directory",
    "label": "",
    "id": "213"
  },
  {
    "raw_code": "def self.find_executable(binary)\n          executable_file = proc { |name| next name if File.file?(name) && File.executable?(name) }\n\n          # Retrieve PATH from ENV or use a fallback\n          path = ENV['PATH']&.split(File::PATH_SEPARATOR) || %w[/usr/local/bin /usr/bin /bin]\n\n          # check binary against each PATH\n          path.each do |dir|\n            file = File.expand_path(binary, dir)\n\n            return file if executable_file.call(file)\n          end",
    "comment": "Search on PATH or default locations for provided binary and return its fullpath  @param [String] binary name @return [String|False] full path to the binary file",
    "label": "",
    "id": "214"
  },
  {
    "raw_code": "def self.executable_exist?(name)\n          !!find_executable(name)\n        end",
    "comment": "Check whether provided binary name exists on PATH or default locations  @param [String] binary name @return [Boolean] whether binary exists",
    "label": "",
    "id": "215"
  },
  {
    "raw_code": "def info(content)\n            self.puts(\"#{ICONS[:info]} #{content}\")\n          end",
    "comment": "Display info content on a new line with default formatting (stdout)  @param [String] content",
    "label": "",
    "id": "216"
  },
  {
    "raw_code": "def print_tag(status)\n            output =\n              case status\n              when :success\n                Rainbow(\"[DONE]\\n\").green\n              when :failure\n                Rainbow(\"[FAILED]\\n\").red\n              when :skipped\n                Rainbow(\"[SKIPPED]\\n\").yellow\n              else\n                raise ArgumentError, \"State must be one of the following: #{STATES.join(', ')}\"\n              end",
    "comment": "Prints a success/failure tag with default formatting and colors  @param [Symbol] status is one of the symbols defined in STATES",
    "label": "",
    "id": "217"
  },
  {
    "raw_code": "def warning(content)\n            self.puts(\"#{ICONS[:warning]} #{content}\", stderr: true)\n          end",
    "comment": "Display warning content on a new line with default formatting (stderr)  @param [String] content",
    "label": "",
    "id": "218"
  },
  {
    "raw_code": "def error(content)\n            self.puts(\"#{ICONS[:error]} #{content}\", stderr: true)\n          end",
    "comment": "Display error content on a new line with default formatting (stderr)  @param [String] content",
    "label": "",
    "id": "219"
  },
  {
    "raw_code": "def puts(content, stderr: false)\n            stderr ? warn(timestamp_format(content)) : $stdout.puts(timestamp_format(content))\n          end",
    "comment": "Display provided content in a new line with a timestamp  @param [String] content the content to be displayed @param [Boolean] stderr when true outputs to stderr instead of stdout",
    "label": "",
    "id": "220"
  },
  {
    "raw_code": "def print(content, timestamp: false, stderr: false)\n            output = stderr ? $stderr : $stdout\n            output.print(timestamp ? timestamp_format(content) : content)\n          end",
    "comment": "Display provided content in the current line with option to use a timestamp  @param [String] content the content to be displayed @param [Boolean] timestamp when true prepends content with a timestamp format",
    "label": "",
    "id": "221"
  },
  {
    "raw_code": "def flush!(stderr: false)\n            stderr ? $stderr.flush : $stdout.flush\n          end",
    "comment": "Forces output to flush  @param [Boolean] stderr",
    "label": "",
    "id": "222"
  },
  {
    "raw_code": "def initialize(\n          context:,\n          backup_id: nil,\n          backup_bucket: nil,\n          wait_for_completion: nil,\n          registry_bucket: nil,\n          service_account_file: nil\n        )\n          @context = context\n          @backup_id = backup_id\n          @workdir = create_temporary_workdir!\n          @archive_directory = context.backup_basedir.join(backup_id)\n\n          @metadata = nil\n          super(\n            backup_bucket: backup_bucket,\n            wait_for_completion: wait_for_completion,\n            registry_bucket: registry_bucket,\n            service_account_file: service_account_file\n          )\n        end",
    "comment": "@param [Context::SourceContext|Context::OmnibusContext] context @param [String] backup_id",
    "label": "",
    "id": "223"
  },
  {
    "raw_code": "def release!\n          FileUtils.rm_rf(workdir)\n        end",
    "comment": "At the end of a successful restore, call this to release temporary resources",
    "label": "",
    "id": "224"
  },
  {
    "raw_code": "def create_temporary_workdir!\n          # Ensure base directory exists\n          # KYLE - does this need to exist? Maybe for tests?\n          FileUtils.mkdir_p(context.backup_basedir)\n\n          Pathname(Dir.mktmpdir('restore', context.backup_basedir))\n        end",
    "comment": "@return [Pathname] temporary directory",
    "label": "",
    "id": "225"
  },
  {
    "raw_code": "def initialize(*cmd_args, chdir: Gitlab::Backup::Cli.root, env: {})\n            @cmd_args = cmd_args.freeze\n            @chdir = chdir\n            @env = env.freeze\n          end",
    "comment": "@example Usage Shell::Command.new('echo', 'Some amazing output').capture @param [Array<String>] cmd_args a list of command and its arguments to run @param [String|Pathname] chdir a path where Shell::Command should run from @param [Hash<String,String>] env a hash containing key=>values to be set as ENV when running the command",
    "label": "",
    "id": "226"
  },
  {
    "raw_code": "def cmd_args(with_env: false)\n            if with_env && env.any?\n              # When providing cmd_args to `Open3.pipeline`, the env needs to be the first element of the array.\n              #\n              # While `Open3.capture3` accepts an empty hash as a valid parameter, it doesn't work with\n              # `Open3.pipeline`, so we modify the returned array only when the env hash is not empty.\n              @cmd_args.dup.prepend(env)\n            else\n              @cmd_args.dup\n            end",
    "comment": "List of command arguments  @param [Boolean] with_env whether to include env hash in the returned list @return [Array<Hash|String>]",
    "label": "",
    "id": "227"
  },
  {
    "raw_code": "def capture\n            start = Time.now\n            stdout, stderr, status = Open3.capture3(env, *cmd_args, chdir: chdir)\n            duration = Time.now - start\n\n            Result.new(stdout: stdout, stderr: stderr, status: status, duration: duration)\n          end",
    "comment": "Execute a process and return its output and status  @return [Command::Result] Captured output from executing a process",
    "label": "",
    "id": "228"
  },
  {
    "raw_code": "def capture_each\n            start = Time.now\n\n            stdout = +''\n            stderr = +''\n            status = Open3.popen3(env, *cmd_args, chdir: chdir) do |i, o, e, wait_thread|\n              i.close_write\n\n              io_threads = []\n              io_threads << Thread.new do\n                until o.eof?\n                  o.gets.tap do |output|\n                    stdout << output\n                    yield :stdout, output.chomp\n                  end",
    "comment": "Execute a process and intercept its captured output line by line  @example Usage Shell::Command.new('echo', 'Some amazing output').capture_each { |stream, output| puts output } @yieldparam [Symbol] stream type (either :stdout or :stderr) @yieldparam [String] output @return [Command::Result] -- Captured output from executing a process",
    "label": "",
    "id": "229"
  },
  {
    "raw_code": "def run_single_pipeline!(input: nil, output: nil)\n            start = Time.now\n            input = typecast_input!(input)\n            output = typecast_output!(output)\n\n            # Open3 writes on `err_write` and we receive from `err_read`\n            err_read, err_write = IO.pipe\n\n            # Pipeline accepts custom {Process.spawn} options\n            # stderr capture is always performed, stdin and stdout redirection\n            # are performed only when either `input` or `output` are present\n            options = { err: err_write } # redirect stderr to IO pipe\n            options[:in] = input if input # redirect stdin\n            options[:out] = output if output # redirect stdout\n\n            status_list = Open3.pipeline(cmd_args(with_env: true), chdir: chdir, **options)\n            duration = Time.now - start\n\n            err_write.close # close the pipe before reading\n            stderr = err_read.read\n            err_read.close # close after reading to avoid leaking file descriptors\n\n            SinglePipelineResult.new(stderr: stderr, status: status_list[0], duration: duration)\n          end",
    "comment": "Run single command in pipeline mode with optional input or output redirection  @param [IO|String|Array] input stdin redirection @param [IO|String|Array] output stdout redirection @return [Command::SinglePipelineResult]",
    "label": "",
    "id": "230"
  },
  {
    "raw_code": "def typecast_output!(output)\n            case output\n            # Matches an array with a path followed by 'read,write' flag and permissions bit\n            # E.g. [Pathname.new('/tmp/file'), 'w', 0o600] OR\n            # Matches an array with a path followed by bit based flag and permissions bit\n            # E.g. [Pathname.new('/tmp/file'), File::WRONLY|File::CREAT, 0o600]\n            in [Pathname, String, Integer] | [Pathname, Integer, Integer]\n              output[0] = output[0].to_s\n              output\n            in Pathname\n              output.to_s\n            else\n              output\n            end",
    "comment": "When providing an output to Open3.pipeline / spawn, we can't pass a Pathname. It needs to be converted to a string, otherwise we get an error",
    "label": "",
    "id": "231"
  },
  {
    "raw_code": "def initialize(*shell_commands)\n            @shell_commands = shell_commands\n          end",
    "comment": "@param [Array<Shell::Command>] shell_commands list of commands",
    "label": "",
    "id": "232"
  },
  {
    "raw_code": "def run!(input: nil, output: nil)\n            start = Time.now\n            input = typecast_input!(input)\n            output = typecast_output!(output)\n\n            # Open3 writes on `err_write` and we receive from `err_read`\n            err_read, err_write = IO.pipe\n\n            # Pipeline accepts custom {Process.spawn} options\n            # stderr capture is always performed, stdin and stdout redirection\n            # are performed only when either `input` or `output` are present\n            options = { err: err_write } # redirect stderr to IO pipe\n            options[:in] = input if input # redirect stdin\n            options[:out] = output if output # redirect stdout\n\n            status_list = Open3.pipeline(*build_command_list, **options)\n            duration = Time.now - start\n\n            err_write.close # close the pipe before reading\n            stderr = err_read.read\n            err_read.close # close after reading to avoid leaking file descriptors\n\n            Result.new(\n              stderr: stderr,\n              status_list: status_list,\n              duration: duration\n            )\n          end",
    "comment": "Run commands in pipeline with optional input or output redirection  @param [IO|String|Array] input stdin redirection @param [IO|String|Array] output stdout redirection @return [Pipeline::Result]",
    "label": "",
    "id": "233"
  },
  {
    "raw_code": "def build_command_list\n            @shell_commands.map { |command| command.cmd_args(with_env: true) }\n          end",
    "comment": "Returns an array of arrays that contains the expanded command args with their env hashes when available  The output is intended to be used directly by Open3.pipeline  @return [Array<Array<Hash,String>>]",
    "label": "",
    "id": "234"
  },
  {
    "raw_code": "def version\n            version = Shell::Command.new(cmd, '--version').capture.stdout.dup\n            version.force_encoding('locale').split(\"\\n\").first\n          end",
    "comment": "Returns the version of tar command available  @return [String] the first line of `--version` output",
    "label": "",
    "id": "235"
  },
  {
    "raw_code": "def pack_cmd(archive_file:, target_directory:, target:, excludes: [])\n            tar_args = []\n            tar_args += build_exclude_patterns(*DEFAULT_EXCLUDES)\n            tar_args += build_exclude_targets(*excludes)\n            tar_args += %W[\n              --directory=#{target_directory}\n              --create\n              --file=#{archive_file}\n            ]\n\n            # Ensure single target or multiple targets are converted to string before adding to args,\n            # to avoid type conversion errors with Pathname\n            tar_args += Array(target).map(&:to_s)\n\n            Shell::Command.new(cmd, *tar_args)\n          end",
    "comment": "Tar's Shell::Command that can be used directly or combined with a Shell::Pipeline  @param [String|Pathname] archive_file the archive file with full path (used by tar's --file option) @param [String|Pathname] target_directory the path to evaluate targets (used by tar's --directory option) @param [String|Pathname|Array] target what will be packed into the archive @param [Array<String>] excludes targets that will be excluded from the backup @return [Gitlab::Backup::Cli::Shell::Command]",
    "label": "",
    "id": "236"
  },
  {
    "raw_code": "def extract_cmd(archive_file:, target_directory:)\n            tar_args = %W[\n              --unlink-first\n              --recursive-unlink\n              --directory=#{target_directory}\n              --extract\n              --file=#{archive_file}\n            ]\n\n            Shell::Command.new(cmd, *tar_args)\n          end",
    "comment": "@param [Object] archive_file @param [Object] target_directory @return [Gitlab::Backup::Cli::Shell::Command]",
    "label": "",
    "id": "237"
  },
  {
    "raw_code": "def initialize(database_name:, snapshot_id: nil, schemas: [], env: {})\n            @database_name = database_name\n            @snapshot_id = snapshot_id\n            @schemas = schemas\n            @env = env\n          end",
    "comment": "@param [String] database_name @param [String] snapshot_id the snapshot id to use when creating a database dump @param [Array<String>] schemas @param [Hash<String,String>] env",
    "label": "",
    "id": "238"
  },
  {
    "raw_code": "def spawn(output:)\n            Process.spawn(env, 'pg_dump', *cmd_args, out: output)\n          end",
    "comment": "Spawn a pg_dump process and assign a given output IO  @param [IO] output the output IO",
    "label": "",
    "id": "239"
  },
  {
    "raw_code": "def cmd_args\n            args = [\"--clean\"] # Pass '--clean' to include 'DROP TABLE' statements in the DB dump.\n            args << '--if-exists'\n            args << \"--snapshot=#{snapshot_id}\" if snapshot_id\n\n            schemas.each do |schema|\n              args << '-n'\n              args << schema\n            end",
    "comment": "Returns a list of arguments used by the pg_dump command  @return [Array<String (frozen)>]",
    "label": "",
    "id": "240"
  },
  {
    "raw_code": "def initialize(*tasks, chdir: Gitlab::Backup::Cli.root)\n            @tasks = tasks\n            @chdir = chdir\n          end",
    "comment": "@param [Array<String>] *tasks a list of tasks to be executed @param [String|Pathname] chdir a path where rake tasks are run from",
    "label": "",
    "id": "241"
  },
  {
    "raw_code": "def execute\n            Bundler.with_original_env do\n              @result = Shell::Command.new(*rake_command, chdir: chdir).capture\n            end",
    "comment": "Execute the rake task and return its execution result status  @return [self]",
    "label": "",
    "id": "242"
  },
  {
    "raw_code": "def capture_each(&block)\n            Bundler.with_original_env do\n              @result = Shell::Command.new(*rake_command, chdir: chdir).capture_each(&block)\n            end",
    "comment": "Execute the rake task and intercept its output line by line including a final result status  @example Usage Rake.new('some:task').capture_each { |stream, output| puts output if stream == :stdout } @yield |stream, output| Return output from :stdout or :stderr stream line by line @yieldparam [Symbol] stream type (either :stdout or :stderr) @yieldparam [String] output content @return [Gitlab::Backup::Cli::Command::Result] -- Captured output from executing a process",
    "label": "",
    "id": "243"
  },
  {
    "raw_code": "def success?\n            @result&.status&.success? || false\n          end",
    "comment": "Return whether the execution was a success or not  @return [Boolean] whether the execution was a success",
    "label": "",
    "id": "244"
  },
  {
    "raw_code": "def output\n            @result&.stdout || ''\n          end",
    "comment": "Return the captured rake output  @return [String] stdout content",
    "label": "",
    "id": "245"
  },
  {
    "raw_code": "def stderr\n            @result&.stderr || ''\n          end",
    "comment": "Return the captured error content  @return [String] stdout content",
    "label": "",
    "id": "246"
  },
  {
    "raw_code": "def duration\n            @result&.duration || 0.0\n          end",
    "comment": "Return the captured execution duration  @return [Float] execution duration",
    "label": "",
    "id": "247"
  },
  {
    "raw_code": "def rake_command\n            %w[bundle exec rake] + tasks\n          end",
    "comment": "Return a list of commands necessary to execute `rake`  @return [Array<String (frozen)>] array of commands to be used by Shellout",
    "label": "",
    "id": "248"
  },
  {
    "raw_code": "def schedule_backup_job(storage, relative_path, gl_project_path, always_create)\n            json_job = {\n              storage_name: storage,\n              relative_path: relative_path,\n              gl_project_path: gl_project_path,\n              always_create: always_create\n            }.to_json\n\n            @input_stream.puts(json_job)\n          end",
    "comment": "Schedule a new backup job through a non-blocking JSON based pipe protocol  @see https://gitlab.com/gitlab-org/gitaly/-/blob/master/doc/gitaly-backup.md",
    "label": "",
    "id": "249"
  },
  {
    "raw_code": "def default_cert_dir\n            ENV.fetch('SSL_CERT_DIR', OpenSSL::X509::DEFAULT_CERT_DIR)\n          end",
    "comment": "These variables will be moved to a config file via https://gitlab.com/gitlab-org/gitlab/-/issues/500437",
    "label": "",
    "id": "250"
  },
  {
    "raw_code": "def initialize(configuration)\n            @configuration = configuration\n            @snapshot_id = nil\n            @connection_name = configuration.name\n          end",
    "comment": "@param [ActiveRecord::DatabaseConfigurations::HashConfig] configuration",
    "label": "",
    "id": "251"
  },
  {
    "raw_code": "def pg_env_variables\n            return @pg_env_variables if defined? @pg_env_variables\n\n            @pg_env_variables = {}\n\n            DATABASE_ENV_VARIABLES.each do |config_key, env_variable_name|\n              value = connection_params[config_key].to_s.presence\n\n              next unless value\n\n              @pg_env_variables[env_variable_name] = value\n            end",
    "comment": "Database connection params and credentials as PG ENV variables  @return [Hash<String => String>]",
    "label": "",
    "id": "252"
  },
  {
    "raw_code": "def connection_params\n            @connection_params ||= configuration.configuration_hash.dup\n          end",
    "comment": "Return the connection params from `database.yml`  @return [Hash<Symbol => String>]",
    "label": "",
    "id": "253"
  },
  {
    "raw_code": "def connection\n            @connection ||= connection_pool.connection\n          end",
    "comment": "Connection associated with current database  @return [ActiveRecord::ConnectionAdapters::PostgreSQLAdapter] connection",
    "label": "",
    "id": "254"
  },
  {
    "raw_code": "def connection_pool\n            @connection_pool ||= connection_base_model.establish_connection(configuration)\n          end",
    "comment": "@return [ActiveRecord::ConnectionAdapters::ConnectionPool] connection",
    "label": "",
    "id": "255"
  },
  {
    "raw_code": "def connection_base_model\n            klass_name = configuration.name.camelize\n\n            if self.class.const_defined?(klass_name.to_sym, false)\n              return \"#{self.class.name}::#{klass_name}\".constantize\n            end",
    "comment": "Creates a new class inheriting from ApplicationRecord to hold the connection pool  It relies on the connection name to have a unique class that is ties to the connection params This is necessary to allow for multiple connection pools to exist at the same time",
    "label": "",
    "id": "256"
  },
  {
    "raw_code": "def each\n            return enum_for(__method__) unless block_given?\n\n            entries.each do |item|\n              yield(item)\n            end",
    "comment": "Iterator for configured databases  @return [Enumerator, Array<Database>]",
    "label": "",
    "id": "257"
  },
  {
    "raw_code": "def entries\n            return @entries if defined?(@entries)\n\n            @entries = database_configurations.map do |config|\n              Database.new(config)\n            end",
    "comment": "All unique configured databases (excluding hidden/partitions)",
    "label": "",
    "id": "258"
  },
  {
    "raw_code": "def main_database\n            each do |database|\n              return database if database.connection_name == 'main'\n            end",
    "comment": "@return [Database]",
    "label": "",
    "id": "259"
  },
  {
    "raw_code": "def database_configurations\n            return @database_configurations if defined?(@database_configurations)\n\n            config_yaml = load_from_database_yaml!\n            ActiveRecord::Base.configurations = config_yaml\n\n            @database_configurations = ActiveRecord::Base.configurations\n                                                         .configs_for(env_name: context.env, include_hidden: false)\n          end",
    "comment": "Return ActiveRecord parsed database configurations object  @return [ActiveRecord::DatabaseConfigurations]",
    "label": "",
    "id": "260"
  },
  {
    "raw_code": "def serialize_value(type:, value:)\n            return value if value.nil?\n\n            case type\n            when :string then serialize_string(value)\n            when :time then serialize_time(value)\n            when :integer then serialize_integer(value)\n            else\n              raise ArgumentError, \"Unknown data type key #{type.inspect} provided when serializing backup metadata\"\n            end",
    "comment": "Given a metadata value, prepare and format the value as a JSON primitive type before serializing  @param [Symbol] type @param [String|Time|Integer] value @return [Integer, String] the converted JSON primitive value",
    "label": "",
    "id": "261"
  },
  {
    "raw_code": "def serialize_integer(value)\n            return value if value.nil?\n\n            value.to_i\n          end",
    "comment": "Serialize the integer value  @return [Integer]",
    "label": "",
    "id": "262"
  },
  {
    "raw_code": "def serialize_string(value)\n            value.to_s\n          end",
    "comment": "Serialize the String value  @return [String]",
    "label": "",
    "id": "263"
  },
  {
    "raw_code": "def serialize_time(value)\n            return value if value.nil?\n            raise ArgumentError unless value.is_a?(Time)\n\n            # ensures string values and nil are properly cast to Time objects\n            value.iso8601\n          end",
    "comment": "Serialize the Time value using ISO8601 format  @return [String]",
    "label": "",
    "id": "264"
  },
  {
    "raw_code": "def parse_value(type:, value:)\n            return value if value.nil?\n\n            case type\n            when :string then parse_string(value)\n            when :time then parse_time(value)\n            when :integer then parse_integer(value)\n            else\n              raise ArgumentError, \"Unknown data type key #{type.inspect} provided when parsing backup metadata\"\n            end",
    "comment": "Given a JSON primitive +value+ loaded from a file, cast it to the expected class as specified by +type+  @param [Symbol] type @param [Object] value @return [Object] the parsed and converted value",
    "label": "",
    "id": "265"
  },
  {
    "raw_code": "def parse_string(value)\n            value.to_s\n          end",
    "comment": "@param [String|Integer|Object] value",
    "label": "",
    "id": "266"
  },
  {
    "raw_code": "def self.build(gitlab_version:)\n            created_at = Time.current\n            backup_id = \"#{created_at.strftime('%s_%Y_%m_%d_')}#{gitlab_version}\"\n\n            new(\n              backup_id: backup_id,\n              created_at: created_at,\n              gitlab_version: gitlab_version\n            )\n          end",
    "comment": "Build a new BackupMetadata with defaults  @param [String] gitlab_version",
    "label": "",
    "id": "267"
  },
  {
    "raw_code": "def self.load!(basepath)\n            basepath = Pathname(basepath) unless basepath.is_a? Pathname\n\n            json_file = basepath.join(METADATA_FILENAME)\n            json = JSON.parse(File.read(json_file), JSON_PARSE_OPTIONS)\n            deserializer = Gitlab::Backup::Cli::Metadata::Deserializer\n\n            parsed_fields = {}\n            METADATA_SCHEMA.each do |attribute_name, type|\n              stored_value = json[attribute_name]\n              parsed_value = deserializer.parse_value(type: type, value: stored_value)\n\n              parsed_fields[attribute_name] = parsed_value\n            end",
    "comment": "Load the metadata from the JSON file stored in the given basepath  @param [String|Pathname] basepath @return [Gitlab::Backup::Cli::Metadata::BackupMetadata, nil]",
    "label": "",
    "id": "268"
  },
  {
    "raw_code": "def to_hash\n            serializer = Gitlab::Backup::Cli::Metadata::Serializer\n\n            METADATA_SCHEMA.each_with_object({}) do |(attribute_name, type), output|\n              # fetch attribute value dynamically\n              # rubocop:disable GitlabSecurity/PublicSend - we cant use read_attribute here, methods are already limited\n              value = public_send(attribute_name)\n              # rubocop:enable GitlabSecurity/PublicSend\n              serialized_value = serializer.serialize_value(type: type, value: value)\n\n              output[attribute_name] = serialized_value\n            end",
    "comment": "Expose the information that will be part of the Metadata JSON file",
    "label": "",
    "id": "269"
  },
  {
    "raw_code": "def write!(basepath)\n            basepath = Pathname(basepath) unless basepath.is_a? Pathname\n\n            json_file = basepath.join(METADATA_FILENAME)\n            json = JSON.pretty_generate(to_hash)\n\n            json_file.open(File::RDWR | File::CREAT, METADATA_FILE_MODE) do |file|\n              file.write(json)\n            end",
    "comment": "Write the metadata to a JSON file in the given basepath  @param [String|Pathname] basepath @return [Boolean] whether successfully wrote to the disk",
    "label": "",
    "id": "270"
  },
  {
    "raw_code": "def dump(path)\n            raise NotImplementedError\n          end",
    "comment": "dump task backup to `path`  @param [String] path fully qualified backup task destination",
    "label": "",
    "id": "271"
  },
  {
    "raw_code": "def restore(path)\n            raise NotImplementedError\n          end",
    "comment": "restore task backup from `path`",
    "label": "",
    "id": "272"
  },
  {
    "raw_code": "def initialize(context, storage_path, excludes: [])\n            super(context)\n\n            @storage_path = storage_path\n            @excludes = excludes\n          end",
    "comment": "@param [String] storage_path @param [Array] excludes",
    "label": "",
    "id": "273"
  },
  {
    "raw_code": "def initialize(source)\n            @source = source\n            @config = nil\n\n            load!\n          end",
    "comment": "@param [String|Pathname] source",
    "label": "",
    "id": "274"
  },
  {
    "raw_code": "def ci_builds_path\n            path = gitlab_config.dig(env, 'gitlab_ci', 'builds_path') || DEFAULT_CI_BUILDS_PATH\n\n            absolute_path(path)\n          end",
    "comment": "CI Builds basepath",
    "label": "",
    "id": "275"
  },
  {
    "raw_code": "def ci_job_artifacts_path\n            path = gitlab_config.dig(env, 'artifacts', 'path') ||\n              gitlab_config.dig(env, 'artifacts', 'storage_path') ||\n              gitlab_shared_path.join(DEFAULT_JOBS_ARTIFACTS_PATH)\n\n            absolute_path(path)\n          end",
    "comment": "Job Artifacts basepath",
    "label": "",
    "id": "276"
  },
  {
    "raw_code": "def ci_secure_files_path\n            path = gitlab_config.dig(env, 'ci_secure_files', 'storage_path') ||\n              gitlab_shared_path.join(DEFAULT_SECURE_FILES_PATH)\n\n            absolute_path(path)\n          end",
    "comment": "CI Secure Files basepath",
    "label": "",
    "id": "277"
  },
  {
    "raw_code": "def ci_lfs_path\n            path = gitlab_config.dig(env, 'lfs', 'storage_path') ||\n              gitlab_shared_path.join(DEFAULT_CI_LFS_PATH)\n\n            absolute_path(path)\n          end",
    "comment": "CI LFS basepath",
    "label": "",
    "id": "278"
  },
  {
    "raw_code": "def packages_path\n            path = gitlab_config.dig(env, 'packages', 'storage_path') ||\n              gitlab_shared_path.join(DEFAULT_PACKAGES)\n\n            absolute_path(path)\n          end",
    "comment": "Packages basepath",
    "label": "",
    "id": "279"
  },
  {
    "raw_code": "def pages_path\n            path = gitlab_config.dig(env, 'pages', 'path') ||\n              gitlab_shared_path.join(DEFAULT_PAGES)\n\n            absolute_path(path)\n          end",
    "comment": "GitLab Pages basepath",
    "label": "",
    "id": "280"
  },
  {
    "raw_code": "def registry_path\n            path = gitlab_config.dig(env, 'registry', 'path') ||\n              gitlab_shared_path.join(DEFAULT_REGISTRY_PATH)\n\n            absolute_path(path)\n          end",
    "comment": "Registry basepath",
    "label": "",
    "id": "281"
  },
  {
    "raw_code": "def terraform_state_path\n            path = gitlab_config.dig(env, 'terraform_state', 'storage_path') ||\n              gitlab_shared_path.join(DEFAULT_TERRAFORM_STATE_PATH)\n\n            absolute_path(path)\n          end",
    "comment": "Terraform State basepath",
    "label": "",
    "id": "282"
  },
  {
    "raw_code": "def upload_path\n            path = gitlab_config.dig(env, 'uploads', 'storage_path') ||\n              gitlab_basepath.join(DEFAULT_UPLOADS_PATH)\n\n            absolute_path(path).join('uploads')\n          end",
    "comment": "Upload basepath",
    "label": "",
    "id": "283"
  },
  {
    "raw_code": "def gitlab_basepath\n            return Pathname.new(GITLAB_PATH) if GITLAB_PATH\n\n            raise ::Gitlab::Backup::Cli::Error, 'GITLAB_PATH is missing'\n          end",
    "comment": "Return the GitLab base directory @return [Pathname]",
    "label": "",
    "id": "284"
  },
  {
    "raw_code": "def gitlab_shared_path\n            shared_path = gitlab_config.dig(env, 'shared', 'path') || DEFAULT_SHARED_PATH\n\n            Pathname(shared_path)\n          end",
    "comment": "Return the shared path used as a fallback base location to each blob type We use this to determine the storage location when everything else fails @return [Pathname]",
    "label": "",
    "id": "285"
  },
  {
    "raw_code": "def absolute_path(path)\n            # Joins with gitlab_basepath when relative, otherwise return full path\n            Pathname(File.expand_path(path, gitlab_basepath))\n          end",
    "comment": "Return a fullpath for a given path  When the path is already a full one return itself as a Pathname otherwise uses gitlab_basepath as its base @param [String|Pathname] path @return [Pathname]",
    "label": "",
    "id": "286"
  },
  {
    "raw_code": "def self.available?\n            ENV.key?(OMNIBUS_CONFIG_ENV) && omnibus_config_filepath.exist?\n          end",
    "comment": "Is the tool running in an Omnibus installation?  @return [Boolean]",
    "label": "",
    "id": "287"
  },
  {
    "raw_code": "def self.omnibus_config_filepath\n            unless ENV.key?(OMNIBUS_CONFIG_ENV)\n              raise ::Gitlab::Backup::Cli::Error, \"#{OMNIBUS_CONFIG_ENV} is not defined\"\n            end",
    "comment": "@return [Pathname|Nillable]",
    "label": "",
    "id": "288"
  },
  {
    "raw_code": "def base_dir\n            \"#{@prefix}/#{disk_hash[0..1]}/#{disk_hash[2..3]}\" if disk_hash\n          end",
    "comment": "Base directory  @return [String] directory where repository is stored",
    "label": "",
    "id": "289"
  },
  {
    "raw_code": "def disk_path\n            \"#{base_dir}/#{disk_hash}\" if disk_hash\n          end",
    "comment": "Disk path is used to build repository path on disk  @return [String] combination of base_dir and the repository own name without `.git`, `.wiki.git`, or any other extension",
    "label": "",
    "id": "290"
  },
  {
    "raw_code": "def disk_hash\n            @disk_hash ||= OpenSSL::Digest::SHA256.hexdigest(container.id.to_s) if container.id\n          end",
    "comment": "Generates the hash for the repository path and name on disk If you need to refer to the repository on disk, use the `#disk_path`",
    "label": "",
    "id": "291"
  },
  {
    "raw_code": "def self.id\n            raise NotImplementedError\n          end",
    "comment": "Identifier used as parameter in the CLI to skip from executing",
    "label": "",
    "id": "292"
  },
  {
    "raw_code": "def backup!(backup_path)\n            backup_output = backup_path.join(destination_path)\n\n            # During test, we ensure storage exists so we can run against `RAILS_ENV=test` environment\n            FileUtils.mkdir_p(storage_path) if context&.env&.test? && respond_to?(:storage_path, true)\n\n            target.dump(backup_output)\n          end",
    "comment": "Initiate a backup  @param [Pathname] backup_path a path where to store the backups",
    "label": "",
    "id": "293"
  },
  {
    "raw_code": "def id = self.class.id\n\n          # Name of the task used for logging.\n          def human_name\n            raise NotImplementedError\n          end\n\n          # Where the task should put its backup file/dir\n          def destination_path\n            raise NotImplementedError\n          end\n\n          # Path to remove after a successful backup, uses #destination_path when not specified\n          def cleanup_path\n            destination_path\n          end\n\n          # `true` if the destination might not exist on a successful backup\n          def destination_optional\n            false\n          end\n\n          # `true` if the task can be used\n          def enabled\n            true\n          end\n\n          def enabled?\n            enabled\n          end\n\n          def config\n            return context.config(id) if context\n\n            Output.warning(\"No context passed to derive configuration from.\")\n            nil\n          end\n\n          def asynchronous?\n            target.asynchronous? || false\n          end\n\n          def wait_until_done!\n            target.wait_until_done!\n          end\n\n          def target\n            return @target unless @target.nil?\n\n            @target ||= local\n          end\n\n          private\n\n          # The target factory method\n          def local\n            raise NotImplementedError\n          end\n        end",
    "comment": "Key string that identifies the task",
    "label": "",
    "id": "294"
  },
  {
    "raw_code": "def cleanup!\n    dir_permissions = (File.stat(gitlab_basepath).mode & 0o777).to_s(8) # retrieve permissions in octal format)\n\n    FileUtils.rm_rf(gitlab_basepath) if dir_permissions == \"700\" # ensure it's a temporary dir before deleting\n  end",
    "comment": "Deletes the temporary folders",
    "label": "",
    "id": "295"
  },
  {
    "raw_code": "def spec_path\n    Pathname.new(__dir__).join('..').expand_path\n  end",
    "comment": "Specs basepath @return [Pathname]",
    "label": "",
    "id": "296"
  },
  {
    "raw_code": "def fixtures_path\n    spec_path.join('fixtures')\n  end",
    "comment": "Fixtures basepath @return [Pathname]",
    "label": "",
    "id": "297"
  },
  {
    "raw_code": "def temp_path\n    spec_path.join('..', 'tmp').expand_path\n  end",
    "comment": "Temporary folder basepath inside project @return [Pathname]",
    "label": "",
    "id": "298"
  },
  {
    "raw_code": "def all\n        new(type: :all)\n      end",
    "comment": "Class methods to start the chain",
    "label": "",
    "id": "299"
  },
  {
    "raw_code": "def migrations(versions_only: false)\n        migrations = @migrations.sort_by { |version, _| version }\n\n        migrations.map { |m| versions_only ? m.first : m.last }\n      end",
    "comment": "Returns all migrations sorted by version",
    "label": "",
    "id": "300"
  },
  {
    "raw_code": "def find_by_version(version)\n        @migrations[version.to_s]\n      end",
    "comment": "Find a specific migration by version",
    "label": "",
    "id": "301"
  },
  {
    "raw_code": "def find_version_by_class_name(class_name)\n        @migrations.find do |_version, klass|\n          klass.name&.demodulize == class_name\n        end&.first\n      end",
    "comment": "Find a version by class name (e.g., 'CreateCode')",
    "label": "",
    "id": "302"
  },
  {
    "raw_code": "def partition_names\n        Array.new(@number_of_partitions) do |i|\n          generate_partition_name(i)\n        end",
    "comment": "Returns list of all partition names for this collection",
    "label": "",
    "id": "303"
  },
  {
    "raw_code": "def collection_name\n        @name\n      end",
    "comment": "Returns the collection alias/view name",
    "label": "",
    "id": "304"
  },
  {
    "raw_code": "def generate_partition_name(index)\n        \"#{@name}#{ActiveContext.adapter.separator}#{index}\"\n      end",
    "comment": "Generates a specific partition name given an index",
    "label": "",
    "id": "305"
  },
  {
    "raw_code": "def fully_exists?(&partition_exists_check)\n        partition_names.all?(&partition_exists_check)\n      end",
    "comment": "Helps executors check if collection is fully set up",
    "label": "",
    "id": "306"
  },
  {
    "raw_code": "def each_partition\n        partition_names.each do |name|\n          yield name\n        end",
    "comment": "Iterator for operating on partitions",
    "label": "",
    "id": "307"
  },
  {
    "raw_code": "def self.transform(collection:, node:, user:)\n          new(collection: collection, user: user).process(node)\n        end",
    "comment": "Transforms a query node into Opensearch query DSL  @param node [ActiveContext::Query] The query node to transform @return [Hash] The Opensearch query DSL @example Processor.transform(ActiveContext::Query.filter(status: 'active'))",
    "label": "",
    "id": "308"
  },
  {
    "raw_code": "def process_knn(node)\n          knn_params = extract_knn_params(node)\n\n          query = build_bool_query(:should) do |queries|\n            queries << { knn: knn_params }\n          end",
    "comment": "Processes KNN query, combining with optional filter conditions  @param node [ActiveContext::Query] The KNN query node @return [Hash] KNN parameters at root level, with filter conditions nested inside KNN if present @example # Basic KNN: # => { knn: { field: 'embedding', ... } } # KNN with filter: # => { #      knn: { #        field: 'embedding', #        ..., #        filter: { bool: { must: [...] } } #      } #    }",
    "label": "",
    "id": "309"
  },
  {
    "raw_code": "def process_or_with_knn(node)\n          knn_child = find_knn_child(node)\n          query = build_or_conditions(node, knn_child)\n          knn_query = { knn: extract_knn_params(knn_child) }\n\n          if query.empty?\n            build_bool_query(:should) do |queries|\n              queries << knn_query\n            end",
    "comment": "Processes OR conditions that include a KNN query  @param node [ActiveContext::Query] The OR query node containing KNN @return [Hash] A combined structure with KNN at root level and other conditions under 'query' @example # For KNN OR filter: # => { #      knn: { field: 'embedding', ... }, #      query: { bool: { should: [...], minimum_should_match: 1 } } #    }",
    "label": "",
    "id": "310"
  },
  {
    "raw_code": "def extract_knn_params(node)\n          values = knn_node_values(node)\n\n          {\n            values[:field] => {\n              k: values[:k],\n              vector: values[:vector]\n            }\n          }\n        end",
    "comment": "Extracts KNN parameters from a node into the expected format  @param node [ActiveContext::Query] The KNN query node @return [Hash] The formatted KNN parameters @example # => { #      'embedding': { #        vector: [0.1, 0.2], #        k: 5 #      } #    }",
    "label": "",
    "id": "311"
  },
  {
    "raw_code": "def process(node)\n          case node.type\n          when :all     then process_all\n          when :filter  then process_filter(node.value)\n          when :prefix  then process_prefix(node.value)\n          when :or      then process_or(node)\n          when :and     then process_and(node.children)\n          when :knn     then process_knn(node)\n          when :limit   then process_limit(node)\n          else\n            raise ArgumentError, \"Unsupported node type: #{node.type}\"\n          end",
    "comment": "Processes a query node and returns the corresponding Elasticsearch query  @param node [ActiveContext::Query] The query node to process @return [Hash] The Elasticsearch query DSL @raise [ArgumentError] If the query type is not supported",
    "label": "",
    "id": "312"
  },
  {
    "raw_code": "def process_filter(conditions)\n          build_bool_query(:must) do |queries|\n            conditions.each do |field, value|\n              queries << (value.is_a?(Array) ? { terms: { field => value } } : { term: { field => value } })\n            end",
    "comment": "Processes filter conditions into term or terms queries  @param conditions [Hash] The filter conditions where keys are fields and values are the terms @return [Hash] A bool query with term/terms clauses in the must array @example Single value (term) process_filter(status: 'active') # => { query: { bool: { must: [{ term: { status: 'active' } }] } } } @example Array value (terms) process_filter(status: ['active', 'pending']) # => { query: { bool: { must: [{ terms: { status: ['active', 'pending'] } }] } } }",
    "label": "",
    "id": "313"
  },
  {
    "raw_code": "def process_prefix(conditions)\n          build_bool_query(:must) do |queries|\n            conditions.each do |field, value|\n              queries << { prefix: { field => value } }\n            end",
    "comment": "Processes prefix conditions into prefix queries  @param conditions [Hash] The prefix conditions where keys are fields and values are the prefixes @return [Hash] A bool query with prefix clauses in the must array @example process_prefix(name: 'test', path: 'foo/') # => { query: { bool: { must: [ #      { prefix: { name: 'test' } }, #      { prefix: { path: 'foo/' } } #    ] } } }",
    "label": "",
    "id": "314"
  },
  {
    "raw_code": "def process_or(node)\n          if contains_knn?(node)\n            process_or_with_knn(node)\n          else\n            process_simple_or(node.children)\n          end",
    "comment": "Processes OR queries, with special handling for KNN  @param node [ActiveContext::Query] The OR query node @return [Hash] Either: - A bool query with should clauses for simple OR conditions - A combined structure with KNN at root level and other conditions under 'query' for OR with KNN @see #process_simple_or @see #process_or_with_knn",
    "label": "",
    "id": "315"
  },
  {
    "raw_code": "def process_simple_or(children)\n          build_bool_query(:should, minimum_should_match: 1) do |queries|\n            children.each do |child|\n              queries << extract_query(process(child))\n            end",
    "comment": "Processes simple OR conditions (without KNN)  @param children [Array<ActiveContext::Query>] The child queries to OR together @return [Hash] A bool query with should clauses and minimum_should_match: 1 @example process_simple_or([filter_query, prefix_query]) # => { query: { bool: { #      should: [...], #      minimum_should_match: 1 #    } } }",
    "label": "",
    "id": "316"
  },
  {
    "raw_code": "def process_or_with_knn(_)\n          raise NotImplementedError\n        end",
    "comment": "Processes OR conditions that include a KNN query",
    "label": "",
    "id": "317"
  },
  {
    "raw_code": "def process_and(children)\n          build_bool_query(:must) do |queries|\n            children.each do |child|\n              queries << extract_query(process(child))\n            end",
    "comment": "Processes AND conditions  @param children [Array<ActiveContext::Query>] The child queries to AND together @return [Hash] A bool query with must clauses @example process_and([filter_query, prefix_query]) # => { query: { bool: { must: [...] } } }",
    "label": "",
    "id": "318"
  },
  {
    "raw_code": "def process_knn(_)\n          raise NotImplementedError\n        end",
    "comment": "Processes KNN query, combining with optional filter conditions",
    "label": "",
    "id": "319"
  },
  {
    "raw_code": "def process_limit(node)\n          child_query = process(node.children.first)\n          child_query.merge(size: node.value)\n        end",
    "comment": "Processes limit by adding size parameter  @param node [ActiveContext::Query] The limit query node @return [Hash] The query with size parameter added @example # With size 10: # => { query: {...}, size: 10 }",
    "label": "",
    "id": "320"
  },
  {
    "raw_code": "def contains_knn?(node)\n          node.children.any? { |child| child.type == :knn }\n        end",
    "comment": "Checks if node contains a KNN query  @param node [ActiveContext::Query] The query node to check @return [Boolean] true if any child is a KNN query",
    "label": "",
    "id": "321"
  },
  {
    "raw_code": "def find_knn_child(node)\n          node.children.find { |child| child.type == :knn }\n        end",
    "comment": "Finds the KNN child in a query node  @param node [ActiveContext::Query] The query node to search @return [ActiveContext::Query, nil] The KNN query node if found",
    "label": "",
    "id": "322"
  },
  {
    "raw_code": "def build_or_conditions(node, knn_child)\n          other_queries = node.children.reject { |child| child == knn_child }\n          return {} if other_queries.empty?\n\n          build_bool_query(:should, minimum_should_match: 1) do |queries|\n            other_queries.each { |child| queries << extract_query(process(child)) }\n          end",
    "comment": "Builds OR conditions excluding KNN query  @param node [ActiveContext::Query] The query node to process @param knn_child [ActiveContext::Query] The KNN child to exclude @return [Hash] A bool query with the remaining conditions",
    "label": "",
    "id": "323"
  },
  {
    "raw_code": "def build_bool_query(type, minimum_should_match: nil)\n          query = { bool: { type => [] } }\n          query[:bool][:minimum_should_match] = minimum_should_match if minimum_should_match\n\n          yield query[:bool][type]\n\n          { query: query }\n        end",
    "comment": "Helper to build bool queries consistently  @param type [:must, :should] The bool query type @param minimum_should_match [Integer, nil] Optional minimum matches for should clauses @yield [Array] Yields an array to add query clauses to @return [Hash] The constructed bool query",
    "label": "",
    "id": "324"
  },
  {
    "raw_code": "def extract_query(processed)\n          processed[:query]\n        end",
    "comment": "Safely extracts query part from processed result  @param processed [Hash] The processed query result @return [Hash] The query part",
    "label": "",
    "id": "325"
  },
  {
    "raw_code": "def bulk\n          results = []\n\n          results << client.bulk(body: index_operations.flatten, refresh: true) unless index_operations.empty?\n\n          build_delete_operations.each do |op|\n            results << client.delete_by_query(\n              index: op[:index],\n              body: op[:body]\n            )\n          end",
    "comment": "Executes upsert operations in one bulk request Create a single delete_by_query request for processing deletes",
    "label": "",
    "id": "326"
  },
  {
    "raw_code": "def build_index_operations(ref)\n          return unless [:upsert, :update].include?(ref.operation.to_sym)\n\n          ref.jsons.map do |hash|\n            add_index_operation([\n              { update: { _index: ref.partition, _id: hash[:unique_identifier], routing: ref.routing }.compact },\n              { doc: hash.except(:unique_identifier), doc_as_upsert: true }\n            ])\n          end",
    "comment": "Builds an upsert operation for every ref where operation is :upsert These operations will be processed in bulk",
    "label": "",
    "id": "327"
  },
  {
    "raw_code": "def build_delete_operations\n          delete_operations = []\n\n          refs.group_by(&:partition).each do |partition, partition_refs|\n            shoulds = []\n            ref_ids_to_delete = []\n\n            partition_refs.each do |ref|\n              case ref.operation.to_sym\n              when :upsert\n                shoulds << delete_with_version_query(ref)\n              when :update\n                # no-op\n              when :delete\n                ref_ids_to_delete << ref.identifier\n              else\n                raise StandardError, \"Operation #{ref.operation} is not supported\"\n              end",
    "comment": "Builds up a bool query containing multiple shoulds: A single terms query containing ids of refs where operation is :delete A bool query with a `filter` for the `ref_id` and `must_not` for the `ref_version` for :upsert refs This ensures we only delete old versions of the document",
    "label": "",
    "id": "328"
  },
  {
    "raw_code": "def process(_node)\n            raise NotImplementedError, \"#{self.class.name} must implement #process\"\n          end",
    "comment": "@abstract Implement #process in subclass to transform query nodes",
    "label": "",
    "id": "329"
  },
  {
    "raw_code": "def transform(collection:, node:, user:)\n            raise NotImplementedError, \"#{name} must implement .transform\"\n          end",
    "comment": "@abstract Implement .transform in subclass to handle query transformation",
    "label": "",
    "id": "330"
  },
  {
    "raw_code": "def add_ref(ref)\n          raise NotImplementedError\n        end",
    "comment": "Adds a reference to the refs array  @param ref [Object] The reference to add @return [Boolean] True if bulk processing should be forced, e.g., when a size threshold is reached",
    "label": "",
    "id": "331"
  },
  {
    "raw_code": "def empty?\n          raise NotImplementedError\n        end",
    "comment": "Checks if nothing should be processed  @return [Boolean] True if bulk processing should be skipped",
    "label": "",
    "id": "332"
  },
  {
    "raw_code": "def bulk\n          raise NotImplementedError\n        end",
    "comment": "Performs bulk processing on the refs array  @return [Object] The result of bulk processing",
    "label": "",
    "id": "333"
  },
  {
    "raw_code": "def process_bulk_errors(_result)\n          raise NotImplementedError\n        end",
    "comment": "Processes errors from bulk operation  @param result [Object] The result from the bulk operation @return [Array] Any failures that occurred during bulk processing",
    "label": "",
    "id": "334"
  },
  {
    "raw_code": "def reset\n          @refs = []\n          # also reset anything that builds up from the refs array\n        end",
    "comment": "Resets the adapter to a clean state",
    "label": "",
    "id": "335"
  },
  {
    "raw_code": "def self.transform(collection:, node:, user:)\n          new(collection: collection, user: user).process(node)\n        end",
    "comment": "Transforms a query node into Elasticsearch query DSL  @param node [ActiveContext::Query] The query node to transform @return [Hash] The Elasticsearch query DSL @example Processor.transform(ActiveContext::Query.filter(status: 'active'))",
    "label": "",
    "id": "336"
  },
  {
    "raw_code": "def process_knn(node)\n          knn_params = extract_knn_params(node)\n          base_query = node.children.any? ? process(node.children.first) : nil\n          knn_params[:filter] = extract_query(base_query) if base_query\n\n          { knn: knn_params }\n        end",
    "comment": "Processes KNN query, combining with optional filter conditions  @param node [ActiveContext::Query] The KNN query node @return [Hash] KNN parameters at root level, with filter conditions nested inside KNN if present @example # Basic KNN: # => { knn: { field: 'embedding', ... } } # KNN with filter: # => { #      knn: { #        field: 'embedding', #        ..., #        filter: { bool: { must: [...] } } #      } #    }",
    "label": "",
    "id": "337"
  },
  {
    "raw_code": "def process_or_with_knn(node)\n          knn_child = find_knn_child(node)\n          other_conditions = build_or_conditions(node, knn_child)\n          knn_params = extract_knn_params(knn_child)\n\n          other_conditions.empty? ? { knn: knn_params } : { knn: knn_params, query: extract_query(other_conditions) }\n        end",
    "comment": "Processes OR conditions that include a KNN query  @param node [ActiveContext::Query] The OR query node containing KNN @return [Hash] A combined structure with KNN at root level and other conditions under 'query' @example # For KNN OR filter: # => { #      knn: { field: 'embedding', ... }, #      query: { bool: { should: [...], minimum_should_match: 1 } } #    }",
    "label": "",
    "id": "338"
  },
  {
    "raw_code": "def extract_knn_params(node)\n          values = knn_node_values(node)\n\n          {\n            field: values[:field],\n            query_vector: values[:vector],\n            k: values[:k],\n            num_candidates: values[:k] * 10\n          }\n        end",
    "comment": "Extracts KNN parameters from a node into the expected format  @param node [ActiveContext::Query] The KNN query node @return [Hash] The formatted KNN parameters @example # => { #      field: 'embedding', #      query_vector: [0.1, 0.2], #      k: 5, #      num_candidates: 50 #    }",
    "label": "",
    "id": "339"
  },
  {
    "raw_code": "def with_raw_connection(&block)\n          handle_connection(raw_connection: true, &block)\n        end",
    "comment": "Provides raw PostgreSQL connection",
    "label": "",
    "id": "340"
  },
  {
    "raw_code": "def with_connection(&block)\n          handle_connection(raw_connection: false, &block)\n        end",
    "comment": "Provides Rails-wrapped connection for using ActiveRecord methods",
    "label": "",
    "id": "341"
  },
  {
    "raw_code": "def with_model_for(table_name)\n          model_class = Class.new(::ActiveRecord::Base) do\n            self.table_name = table_name\n\n            def self.name\n              \"ActiveContext::Model::#{table_name.classify}\"\n            end\n\n            def self.to_s\n              name\n            end\n          end",
    "comment": "Creates an ActiveRecord model for a specific table and yields it within the connection context @param table_name [String] The name of the table to create a model for @yield [Class] A dynamically created ActiveRecord model class with the correct connection",
    "label": "",
    "id": "342"
  },
  {
    "raw_code": "def ar_model_for(table_name)\n          klass = nil\n          with_model_for(table_name) do |model_class|\n            klass = model_class\n          end",
    "comment": "For backward compatibility and simpler queries",
    "label": "",
    "id": "343"
  },
  {
    "raw_code": "def perform_bulk_operation(operation_type, model, collection_name, operations)\n          data = operations.filter_map { |op| op[collection_name][operation_type] }\n\n          return data if data.empty?\n\n          case operation_type\n          when :upsert\n            upsert_data = prepare_upsert_data(data)\n            model.transaction do\n              upsert_data.each do |upsert_group|\n                model.upsert_all(\n                  upsert_group[:data],\n                  unique_by: upsert_group[:unique_by],\n                  update_only: upsert_group[:update_only_columns]\n                )\n              end",
    "comment": "rubocop:disable Rails/SkipsModelValidations -- bulk_upsert is more performant and we don't have validations",
    "label": "",
    "id": "344"
  },
  {
    "raw_code": "def prepare_upsert_data(data)\n          data.group_by(&:keys).map do |columns, grouped_data|\n            {\n              unique_by: [:id, :partition_id],\n              update_only_columns: columns - [:id, :partition_id],\n              data: grouped_data\n            }\n          end",
    "comment": "rubocop:enable Rails/SkipsModelValidations",
    "label": "",
    "id": "345"
  },
  {
    "raw_code": "def self.transform(collection:, node:, user:)\n          ActiveContext.adapter.client.with_model_for(collection.collection_name) do |model|\n            relation = new(collection: collection, model: model, user: user).process(node)\n            relation.to_sql\n          end",
    "comment": "Transforms a query node into a PostgreSQL query using ActiveRecord",
    "label": "",
    "id": "346"
  },
  {
    "raw_code": "def process(node)\n          case node.type\n          when :all    then process_all\n          when :filter then process_filter(node.value)\n          when :prefix then process_prefix(node.value)\n          when :and    then process_and(node.children)\n          when :or     then process_or(node.children)\n          when :knn    then process_knn(node)\n          when :limit  then process_limit(node)\n          else\n            raise ArgumentError, \"Unsupported node type: #{node.type}\"\n          end",
    "comment": "Processes a query node and returns the corresponding ActiveRecord relation",
    "label": "",
    "id": "347"
  },
  {
    "raw_code": "def initialize_documents!(ref, content_method, content_field)\n          return unless content_method && ref.respond_to?(content_method)\n\n          ref.documents << {} if ref.documents.empty?\n\n          ref.documents.each do |doc|\n            next if doc.key?(content_field)\n\n            doc[content_field] = ref.send(content_method) # rubocop: disable GitlabSecurity/PublicSend -- method is defined elsewhere\n          end",
    "comment": "Initializes the documents for a reference if they don't exist and populates the content field if a content_method is provided",
    "label": "",
    "id": "348"
  },
  {
    "raw_code": "def initialize; end\n\n        # Don't implement other required methods\n        def indexer_klass; end\n        def executor_klass; end\n      end\n\n      adapter = test_class.new\n      expect { adapter.client_klass }.to raise_error(NotImplementedError)\n    end\n  end",
    "comment": "Override initialize so it doesn't try to call the methods we're testing",
    "label": "",
    "id": "349"
  },
  {
    "raw_code": "def initialize; end\n\n        # Don't implement other required methods\n        def client_klass; end\n        def executor_klass; end\n      end\n\n      adapter = test_class.new\n      expect { adapter.indexer_klass }.to raise_error(NotImplementedError)\n    end\n  end",
    "comment": "Override initialize so it doesn't try to call the methods we're testing",
    "label": "",
    "id": "350"
  },
  {
    "raw_code": "def initialize; end\n\n        # Don't implement other required methods\n        def client_klass; end\n        def indexer_klass; end\n      end\n\n      adapter = test_class.new\n      expect { adapter.executor_klass }.to raise_error(NotImplementedError)\n    end\n  end",
    "comment": "Override initialize so it doesn't try to call the methods we're testing",
    "label": "",
    "id": "351"
  },
  {
    "raw_code": "def validate_file_checksum(checksum)\n        return true if Bundler.settings[:disable_checksum_validation]\n\n        source = @package.instance_variable_get(:@gem)\n\n        # Contary to upstream, we raise instead of silently returning\n        raise \"#{@package.inspect} does not have :@gem\" unless source\n        raise \"#{source.inspect} does not respond to :with_read_io\" unless source.respond_to?(:with_read_io)\n\n        digest =\n          if Gem::Version.new(Bundler::VERSION) >= Gem::Version.new(\"2.5.0\")\n            gem_checksum.digest\n          else\n            source.with_read_io do |io|\n              digest = SharedHelpers.digest(:SHA256).new\n              digest << io.read(16_384) until io.eof?\n              io.rewind\n              send(checksum_type(checksum), digest)\n            end",
    "comment": "Modified from https://github.com/rubygems/rubygems/blob/243173279e79a38f03e318eea8825d1c8824e119/bundler/lib/bundler/rubygems_gem_installer.rb#L116",
    "label": "",
    "id": "352"
  },
  {
    "raw_code": "def attempt_configure_connection\n        configure_connection\n      rescue Exception # Need to handle things such as Timeout::ExitException\n        disconnect!\n        raise\n      end",
    "comment": "Add the new method that wraps configure_connection with exception handling",
    "label": "",
    "id": "353"
  },
  {
    "raw_code": "def reconnect!(restore_transactions: false)\n        retries_available = connection_retries\n        deadline = retry_deadline && Process.clock_gettime(Process::CLOCK_MONOTONIC) + retry_deadline\n\n        @lock.synchronize do\n          reconnect\n\n          enable_lazy_transactions!\n          @raw_connection_dirty = false\n          @verified = true\n\n          reset_transaction(restore: restore_transactions) do\n            clear_cache!(new_connection: true)\n            attempt_configure_connection\n          end",
    "comment": "Disconnects from the database if already connected, and establishes a new connection with the database. Implementors should define private #reconnect instead.",
    "label": "",
    "id": "354"
  },
  {
    "raw_code": "def reset!\n        clear_cache!(new_connection: true)\n        reset_transaction\n        attempt_configure_connection\n      end",
    "comment": "Reset the state of this connection, directing the DBMS to clear transactions and other connection-related server-side state. Usually a database-dependent operation.  If a database driver or protocol does not support such a feature, implementors may alias this to #reconnect!. Otherwise, implementors should call super immediately after resetting the connection (and while still holding @lock).",
    "label": "",
    "id": "355"
  },
  {
    "raw_code": "def verify!\n        unless active?\n          @lock.synchronize do\n            if @unconfigured_connection\n              @raw_connection = @unconfigured_connection\n              @unconfigured_connection = nil\n              attempt_configure_connection\n              @verified = true\n              return\n            end",
    "comment": "Checks whether the connection to the database is still active (i.e. not stale). This is done under the hood by calling #active?. If the connection is no longer active, then this method will reconnect to the database.",
    "label": "",
    "id": "356"
  },
  {
    "raw_code": "def find_or_create_by(attributes, &block)\n          find_by(attributes) || create(attributes, &block)\n        end",
    "comment": "Adds patch for ActiveRecord `find_or_create_by`/`find_or_create_by!` methods so as to prevent it from opening subtransactions. Rails commit: https://github.com/rails/rails/commit/023a3eb3c046091a5d52027393a6d29d0576da01 Issue: https://gitlab.com/gitlab-org/gitlab/-/issues/439567",
    "label": "",
    "id": "357"
  },
  {
    "raw_code": "def scope_for(relation, owner = nil)\n            if scope.arity == 1 && owner.nil? && options.key?(:partition_foreign_key)\n              relation = relation.instance_exec(NO_OWNER, &scope)\n              if relation_includes_partition_id_condition?(relation)\n                relation.rewhere(relation.table[:partition_id].not_eq(nil))\n              else\n                relation\n              end",
    "comment": "We override the method to allow eager loading of partitioned records  For eager loading the owner is always nil and we supply a benign object that allows the scope to be evaluated with a query like `where partition_id between 1 and 100000` and transforms it into `where partition_id is not null` to ensure that no partition is left out by the query. This is safe because `partition_id` columns are defined as `not null` ",
    "label": "",
    "id": "358"
  },
  {
    "raw_code": "def read_attribute(attr_name)\n        send(attr_name)\n      end",
    "comment": "Mock AR methods",
    "label": "",
    "id": "359"
  },
  {
    "raw_code": "def reset\n        @static_item = nil\n        self\n      end",
    "comment": "Mock reset method",
    "label": "",
    "id": "360"
  },
  {
    "raw_code": "def name\n            @name ||= adapter.name\n          end",
    "comment": "Foreign key name should include the schema, as the same name could be used across different schemas  @example public.foreign_key_name",
    "label": "",
    "id": "361"
  },
  {
    "raw_code": "def name\n            @name ||= adapter.name\n          end",
    "comment": "Sequence should include the schema, as the same name could be used across different schemas  @example public.sequence_name",
    "label": "",
    "id": "362"
  },
  {
    "raw_code": "def owner\n            @owner ||= adapter.column_owner\n          end",
    "comment": "Fully qualified column reference (schema.table.column)",
    "label": "",
    "id": "363"
  },
  {
    "raw_code": "def type(node)\n            pg_type = parse_node(node.names.last)\n            type = PgTypes::TYPES.fetch(pg_type).dup\n            array_ext = '[]' if node.array_bounds.any?\n            precision_ext = \"(#{node.typmods.map { |typmod| parse_node(typmod) }.join(',')})\" if node.typmods.any?\n\n            if %w[timestamp timestamptz].include?(pg_type)\n              type.gsub!('timestamp', ['timestamp', precision_ext].compact.join)\n              precision_ext = nil\n            end",
    "comment": "Returns the node type  pg_type:: type alias, used internally by postgres, +int4+, +int8+, +bool+, +varchar+ type:: type name, like +integer+, +bigint+, +boolean+, +character varying+. array_ext:: adds the +[]+ extension for array types. precision_ext:: adds the precision, if have any, like +(255)+, +(6)+.  @info +timestamp+ and +timestamptz+ have a particular case when precision is defined. In this case, the order of the statement needs to be re-arranged from timestamp without time zone(6) to timestamp(6) without a time zone.",
    "label": "",
    "id": "364"
  },
  {
    "raw_code": "def parse_node(node)\n            return unless node\n\n            case node.node\n            when :constraint\n              parse_node(node.constraint.raw_expr)\n            when :partition_elem\n              node.partition_elem.name\n            when :func_call\n              \"#{parse_node(node.func_call.funcname.first)}()\"\n            when :a_const\n              parse_a_const(node.a_const)\n            when :type_cast\n              value = parse_node(node.type_cast.arg)\n              type = type(node.type_cast.type_name)\n              separator = MAPPINGS.key?(value) ? '' : \"::#{type}\"\n\n              [MAPPINGS.fetch(value, \"'#{value}'\"), separator].compact.join\n            else\n              get_value_from_key(node, key: node.node)\n            end",
    "comment": "Parses PGQuery nodes recursively  :constraint:: nodes that groups column default info :partition_elem:: node that store partition key info :func_cal:: nodes that stores functions, like +now()+ :a_const:: nodes that stores constant values, like +t+, +f+, +0.0.0.0+, +255+, +1.0+ :type_cast:: nodes that stores casting values, like +'name'::text+, +'0.0.0.0'::inet+ else:: extract node values in the last iteration of the recursion, like +int4+, +1.0+, +now+, +255+  @note boolean types types are mapped from +t+, +f+ to +true+, +false+",
    "label": "",
    "id": "365"
  },
  {
    "raw_code": "def statement\n            deparse_stmt[EXTRACT_REGEX].gsub(STATEMENT_REGEX, '(')\n          end",
    "comment": "PgQuery parses FK statements with an extra space in the referenced table column. This extra space needs to be removed.  @example REFERENCES ci_pipelines (id) => REFERENCES ci_pipelines(id)",
    "label": "",
    "id": "366"
  },
  {
    "raw_code": "def name\n            \"#{schema}.#{sequence_name}\"\n          end",
    "comment": "Fully qualified sequence name (schema.sequence_name)",
    "label": "",
    "id": "367"
  },
  {
    "raw_code": "def column_name\n            owner_column\n          end",
    "comment": "Just the column name",
    "label": "",
    "id": "368"
  },
  {
    "raw_code": "def column_owner\n            return unless owner_table && owner_column\n\n            \"#{column_schema}.#{owner_table}.#{owner_column}\"\n          end",
    "comment": "Fully qualified column reference (schema.table.column)",
    "label": "",
    "id": "369"
  },
  {
    "raw_code": "def schema\n            schema_name || owner_schema || 'public'\n          end",
    "comment": "Get the schema this sequence belongs to",
    "label": "",
    "id": "370"
  },
  {
    "raw_code": "def execute\n            extract_sequences\n            @sequences\n          end",
    "comment": "Returns a map of sequence name to sequence structure objects",
    "label": "",
    "id": "371"
  },
  {
    "raw_code": "def process_create_sequence(create_seq)\n            sequence_name = create_seq.sequence.relname\n            schema_name = resolve_schema_name(create_seq.sequence.schemaname)\n            full_name = \"#{schema_name}.#{sequence_name}\"\n\n            @sequences[full_name] = ::Gitlab::Schema::Validation::Adapters::SequenceStructureSqlAdapter.new(\n              sequence_name: sequence_name,\n              schema_name: schema_name\n            )\n          end",
    "comment": "Process CREATE SEQUENCE SQL queries. For example:  CREATE SEQUENCE web_hook_logs_id_seq",
    "label": "",
    "id": "372"
  },
  {
    "raw_code": "def process_alter_sequence(alter_seq)\n            sequence_schema = alter_seq.sequence.schemaname\n            sequence_schema = default_schema_name if sequence_schema == ''\n            sequence_name = alter_seq.sequence.relname\n\n            # Look for OWNED BY option\n            return unless alter_seq.options\n\n            owner_schema = default_schema_name\n            owner_table = nil\n            owner_column = nil\n\n            alter_seq.options.each do |option|\n              def_elem = option.def_elem\n\n              next unless def_elem.defname == 'owned_by'\n              next unless def_elem.arg && def_elem.arg.node == :list\n\n              owned_by_list = def_elem.arg.list.items\n\n              next unless owned_by_list.length >= 2\n\n              # Handle schema.table.column or table.column\n              if owned_by_list.length == 3\n                owner_schema = owned_by_list[0].string.sval\n                owner_table = owned_by_list[1].string.sval\n                owner_column = owned_by_list[2].string.sval\n              else\n                owner_table = owned_by_list[0].string.sval\n                owner_column = owned_by_list[1].string.sval\n              end",
    "comment": "Processes ALTER SEQUENCE SQL queries to extract column owner. For example:  ALTER SEQUENCE ai_code_suggestion_events_id_seq OWNED BY ai_code_suggestion_events.id;",
    "label": "",
    "id": "373"
  },
  {
    "raw_code": "def process_alter_table(alter_table)\n            table_name = alter_table.relation.relname\n            table_schema = resolve_schema_name(alter_table.relation.schemaname)\n\n            alter_table.cmds.each do |cmd|\n              alter_cmd = cmd.alter_table_cmd\n\n              # Look for SET DEFAULT nextval(...) commands\n              next unless alter_cmd.subtype == :AT_ColumnDefault\n\n              column_name = alter_cmd.name\n              sequence_name = extract_sequence_from_default(alter_cmd.def)\n              sequence_schema, sequence_name = process_sequence_name(sequence_name)\n              lookup_name = \"#{sequence_schema}.#{sequence_name}\"\n\n              # Update existing sequence or create new one\n              existing = @sequences[lookup_name]\n\n              @sequences[lookup_name] = ::Gitlab::Schema::Validation::Adapters::SequenceStructureSqlAdapter.new(\n                sequence_name: sequence_name,\n                schema_name: existing&.schema_name || table_schema,\n                owner_table: existing&.owner_table || table_name,\n                owner_column: existing&.owner_column || column_name,\n                owner_schema: existing&.owner_schema || table_schema\n              )\n            end",
    "comment": "Process ALTER TABLE commands to extract sequence owner. For example:  ALTER TABLE ONLY web_hook_logs ALTER COLUMN id SET DEFAULT nextval('web_hook_logs_id_seq'::regclass);",
    "label": "",
    "id": "374"
  },
  {
    "raw_code": "def fetch_sequences\n            # rubocop:disable Rails/SquishedSQLHeredocs\n            sql = <<~SQL\n              SELECT\n                c.relname AS sequence_name,\n                n.nspname AS schema,\n                pg_catalog.pg_get_userbyid(c.relowner) AS user_owner,\n                s.seqstart AS start_value,\n                s.seqincrement AS increment_by,\n                s.seqmin AS min_value,\n                s.seqmax AS max_value,\n                s.seqcycle AS cycle,\n                s.seqcache AS cache_size,\n                pg_catalog.obj_description(c.oid, 'pg_class') AS comment,\n                CASE\n                  WHEN d.refobjid IS NOT NULL THEN\n                    ref_class.relname || '.' || ref_attr.attname\n                  ELSE NULL\n                END AS owned_by_column\n              FROM pg_catalog.pg_class c\n              INNER JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\n              LEFT JOIN pg_catalog.pg_sequence s ON s.seqrelid = c.oid\n              LEFT JOIN pg_catalog.pg_depend d ON d.objid = c.oid\n                AND d.deptype = 'a'\n                AND d.classid = 'pg_class'::regclass\n              LEFT JOIN pg_catalog.pg_class ref_class ON ref_class.oid = d.refobjid\n              LEFT JOIN pg_catalog.pg_attribute ref_attr ON ref_attr.attrelid = d.refobjid\n                AND ref_attr.attnum = d.refobjsubid\n              WHERE c.relkind = 'S'\n                AND n.nspname IN ($1, $2)\n              ORDER BY c.relname, n.nspname\n            SQL\n            # rubocop:enable Rails/SquishedSQLHeredocs\n\n            connection.exec_query(sql, schemas).group_by { |seq| \"#{seq['schema']}.#{seq['sequence_name']}\" }\n          end",
    "comment": "Fetch all the sequences",
    "label": "",
    "id": "375"
  },
  {
    "raw_code": "def constraint_statements(constraint_type)\n            alter_table_statements(:AT_AddConstraint).filter do |stmt|\n              stmt.cmds.first.alter_table_cmd.def.constraint.contype == constraint_type\n            end",
    "comment": "Filter constraint statement nodes  @param constraint_type [Symbol] node type. One of CONSTR_PRIMARY, CONSTR_CHECK, CONSTR_EXCLUSION, CONSTR_UNIQUE or CONSTR_FOREIGN.",
    "label": "",
    "id": "376"
  },
  {
    "raw_code": "def alter_table_statements(subtype)\n            statements.filter_map do |statement|\n              node = statement.stmt.alter_table_stmt\n\n              next unless node\n\n              node if node.cmds.first.alter_table_cmd.subtype == subtype\n            end",
    "comment": "Filter alter table statement nodes  @param subtype [Symbol] node subtype +AT_AttachPartition+, +AT_ColumnDefault+ or +AT_AddConstraint+",
    "label": "",
    "id": "377"
  },
  {
    "raw_code": "def settings\n      {}\n    end",
    "comment": "This makes it compatible with Mail's `#deliver!` method https://github.com/mikel/mail/blob/22a7afc23f253319965bf9228a0a430eec94e06d/lib/mail/message.rb#L271",
    "label": "",
    "id": "378"
  },
  {
    "raw_code": "def hostname=(hostname)\n      super(@context.hostname_override || hostname)\n    end",
    "comment": "rubocop: disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "379"
  },
  {
    "raw_code": "def initialize(ip, port: nil)\n        @ip = ip\n        @port = port\n      end",
    "comment": "Argument ip should be an IPAddr object",
    "label": "",
    "id": "380"
  },
  {
    "raw_code": "def ip_include?(requested_ip)\n        return true if ip.include?(requested_ip)\n        return ip.include?(requested_ip.ipv4_mapped) if requested_ip.ipv4? && ip.ipv6?\n        return ip.ipv4_mapped.include?(requested_ip) if requested_ip.ipv6? && ip.ipv4?\n\n        false\n      end",
    "comment": "Prior to ipaddr v1.2.3, if the allow list were the IPv4 to IPv6 mapped address ::ffff:169.254.168.100 and the requested IP were 169.254.168.100 or ::ffff:169.254.168.100, the IP would be considered in the allow list. However, with https://github.com/ruby/ipaddr/pull/31, IPAddr#include? will only match if the IP versions are the same. This method preserves backwards compatibility if the versions differ by checking inclusion by coercing an IPv4 address to its IPv6 mapped address.",
    "label": "",
    "id": "381"
  },
  {
    "raw_code": "def perform_request(http_method, path, options, &block)\n          raise_if_options_are_invalid(options)\n          raise_if_blocked_by_silent_mode(http_method) if options.delete(:silent_mode_enabled)\n\n          log_info = options.delete(:extra_log_info)\n          async = options.delete(:async)\n\n          options_with_timeouts =\n            if !options.has_key?(:timeout)\n              options.with_defaults(DEFAULT_TIMEOUT_OPTIONS)\n            else\n              options\n            end",
    "comment": "TODO: This overwrites a method implemented by `HTTPParty` The calls to `get/...` will call this method instead of `httparty_perform_request`",
    "label": "",
    "id": "382"
  },
  {
    "raw_code": "def readuntil(terminator, ignore_eof = false, start_time = Process.clock_gettime(Process::CLOCK_MONOTONIC))\n        if NET_PROTOCOL_VERSION_0_2_0\n          offset = @rbuf_offset\n          begin\n            until idx = @rbuf.index(terminator, offset) # rubocop:disable Lint/AssignmentInCondition\n              if (elapsed = Process.clock_gettime(Process::CLOCK_MONOTONIC) - start_time) > HEADER_READ_TIMEOUT\n                raise Gitlab::HTTP_V2::HeaderReadTimeout,\n                  \"Request timed out after reading headers for #{elapsed} seconds\"\n              end",
    "comment": "rubocop: disable Style/RedundantReturn rubocop: disable Cop/LineBreakAfterGuardClauses rubocop: disable Layout/EmptyLineAfterGuardClause Original method: https://github.com/ruby/ruby/blob/cdb7d699d0641e8f081d590d06d07887ac09961f/lib/net/protocol.rb#L190-L200",
    "label": "",
    "id": "383"
  },
  {
    "raw_code": "def validate_url_with_proxy!(\n          url,\n          schemes:,\n          ports: [],\n          allow_localhost: false,\n          allow_local_network: true,\n          extra_allowed_uris: [],\n          ascii_only: false,\n          enforce_user: false,\n          enforce_sanitization: false,\n          deny_all_requests_except_allowed: false,\n          dns_rebind_protection: true,\n          outbound_local_requests_allowlist: []\n        )\n          # rubocop:enable Metrics/ParameterLists\n\n          return Result.new(nil, nil, true) if url.nil?\n\n          raise ArgumentError, 'The schemes is a required argument' if schemes.blank?\n\n          # Param url can be a string, URI or Addressable::URI\n          uri = parse_url(url)\n\n          validate_uri(\n            uri: uri,\n            schemes: schemes,\n            ports: ports,\n            enforce_sanitization: enforce_sanitization,\n            enforce_user: enforce_user,\n            ascii_only: ascii_only\n          )\n\n          unless deny_all_requests_except_allowed || dns_rebind_protection || !allow_local_network || !allow_localhost\n            return Result.new(uri, nil, true)\n          end",
    "comment": "Validates the given url according to the constraints specified by arguments.  ports - Raises error if the given URL port is not between given ports. allow_localhost - Raises error if URL resolves to a localhost IP address and argument is false. allow_local_network - Raises error if URL resolves to a link-local address and argument is false. extra_allowed_uris - Array of URI objects that are allowed in addition to hostname and IP constraints. This parameter is passed in this class when making the HTTP request. ascii_only - Raises error if URL has unicode characters and argument is true. enforce_user - Raises error if URL user doesn't start with alphanumeric characters and argument is true. enforce_sanitization - Raises error if URL includes any HTML/CSS/JS tags and argument is true. deny_all_requests_except_allowed - Raises error if URL is not in the allow list and argument is true. Can be Boolean or Proc. Defaults to instance app setting. dns_rebind_protection - Enforce DNS-rebinding attack protection. outbound_local_requests_allowlist - A list of trusted domains or IP addresses to which local requests are allowed when local requests for webhooks and integrations are disabled. This parameter is static and comes from the `outbound_local_requests_whitelist` application setting. # rubocop:disable Naming/InclusiveLanguage  Returns a Result object. rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "384"
  },
  {
    "raw_code": "def validate!(...)\n          result = validate_url_with_proxy!(...)\n          [result.uri, result.hostname]\n        end",
    "comment": "For backwards compatibility, Returns an array with [<uri>, <original-hostname>]. Issue for refactoring: https://gitlab.com/gitlab-org/gitlab/-/issues/410890",
    "label": "",
    "id": "385"
  },
  {
    "raw_code": "def enforce_uri_hostname(ip_address, uri, dns_rebind_protection, proxy_in_use)\n          unless dns_rebind_protection && ip_address && ip_address != uri.hostname\n            return Result.new(uri, nil, proxy_in_use)\n          end",
    "comment": "Returns the given URI with IP address as hostname and the original hostname respectively in an Array.  It checks whether the resolved IP address matches with the hostname. If not, it changes the hostname to the resolved IP address.  The original hostname is used to validate the SSL, given in that scenario we'll be making the request to the IP address, instead of using the hostname.",
    "label": "",
    "id": "386"
  },
  {
    "raw_code": "def get_address_info(uri)\n          Timeout.timeout(GETADDRINFO_TIMEOUT_SECONDS) do\n            Addrinfo.getaddrinfo(uri.hostname, get_port(uri), nil, :STREAM).map do |addr|\n              addr.ipv6_v4mapped? ? addr.ipv6_to_ipv4 : addr\n            end",
    "comment": "Returns addrinfo object for the URI.  @param uri [Addressable::URI]  @raise [Gitlab::HTTP_V2::UrlBlocker::BlockedUrlError, ArgumentError] raised if host is too long.  @return [Array<Addrinfo>]",
    "label": "",
    "id": "387"
  },
  {
    "raw_code": "def validate_deny_all_requests_except_allowed!(should_deny)\n          return unless deny_all_requests_except_allowed?(should_deny)\n\n          raise BlockedUrlError, \"Requests to hosts and IP addresses not on the Allow List are denied\"\n        end",
    "comment": "Raises a BlockedUrlError if the instance is configured to deny all requests.  This should only be called after allow list checks have been made.",
    "label": "",
    "id": "388"
  },
  {
    "raw_code": "def validate_limited_broadcast_address(addrs_info)\n          blocked_ips = [\"255.255.255.255\"]\n\n          return if (blocked_ips & addrs_info.map(&:ip_address)).empty?\n\n          raise BlockedUrlError, \"Requests to the limited broadcast address are not allowed\"\n        end",
    "comment": "Raises a BlockedUrlError if any IP in `addrs_info` is the limited broadcast address. https://datatracker.ietf.org/doc/html/rfc919#section-7",
    "label": "",
    "id": "389"
  },
  {
    "raw_code": "def internal?(uri)\n          check_uri(uri, Gitlab::HTTP_V2.configuration.allowed_internal_uris)\n        end",
    "comment": "Allow url from the GitLab instance itself but only for the configured hostname and ports",
    "label": "",
    "id": "390"
  },
  {
    "raw_code": "def connect\n      if use_ssl?\n        # reference early to load OpenSSL before connecting,\n        # as OpenSSL may take time to load.\n        @ssl_context = OpenSSL::SSL::SSLContext.new\n      end",
    "comment": "rubocop:disable Cop/LineBreakAroundConditionalBlock -- This is upstream code rubocop:disable Layout/ArgumentAlignment -- This is upstream code rubocop:disable Layout/AssignmentIndentation -- This is upstream code rubocop:disable Layout/LineEndStringConcatenationIndentation -- This is upstream code rubocop:disable Layout/MultilineOperationIndentation -- This is upstream code rubocop:disable Layout/SpaceInsideBlockBraces -- This is upstream code rubocop:disable Lint/UnusedBlockArgument -- This is upstream code rubocop:disable Metrics/AbcSize -- This is upstream code rubocop:disable Metrics/CyclomaticComplexity -- This is upstream code rubocop:disable Metrics/PerceivedComplexity -- This is upstream code rubocop:disable Naming/RescuedExceptionsVariableName -- This is upstream code rubocop:disable Style/AndOr -- This is upstream code rubocop:disable Style/BlockDelimiters -- This is upstream code rubocop:disable Style/EmptyLiteral -- This is upstream code rubocop:disable Style/IfUnlessModifier -- This is upstream code rubocop:disable Style/LineEndConcatenation -- This is upstream code rubocop:disable Style/MultilineIfThen -- This is upstream code rubocop:disable Style/Next -- This is upstream code rubocop:disable Style/RescueStandardError -- This is upstream code rubocop:disable Style/StringConcatenation -- This is upstream code",
    "label": "",
    "id": "391"
  },
  {
    "raw_code": "def self.each_response_header(sock)\n      start_time = Process.clock_gettime(Process::CLOCK_MONOTONIC)\n      key = value = nil\n      while true\n        uses_buffered_io = sock.is_a?(Gitlab::HTTP_V2::BufferedIo)\n\n        line = uses_buffered_io ? sock.readuntil(\"\\n\", true, start_time) : sock.readuntil(\"\\n\", true)\n        line = line.sub(/\\s{0,10}\\z/, '')\n        break if line.empty?\n        if line[0] == ?\\s or line[0] == ?\\t and value\n          # rubocop:disable Gitlab/NoCodeCoverageComment\n          # :nocov:\n          value << ' ' unless value.empty?\n          value << line.strip\n          # :nocov:\n          # rubocop:enable Gitlab/NoCodeCoverageComment\n        else\n          yield key, value if key\n          key, value = line.strip.split(/\\s{0,10}:\\s{0,10}/, 2)\n          raise Net::HTTPBadResponse, 'wrong header line format' if value.nil?\n        end",
    "comment": "rubocop: disable Cop/LineBreakAfterGuardClauses rubocop: disable Cop/LineBreakAroundConditionalBlock rubocop: disable Layout/EmptyLineAfterGuardClause rubocop: disable Style/AndOr rubocop: disable Style/CharacterLiteral rubocop: disable Style/InfiniteLoop Original method: https://github.com/ruby/ruby/blob/v2_7_5/lib/net/http/response.rb#L54-L69  Our changes: - Pass along the `start_time` to `Gitlab::HTTP_V2::BufferedIo`, so we can raise a timeout if reading the headers takes too long. - Limit the regexes to avoid ReDoS attacks.",
    "label": "",
    "id": "392"
  },
  {
    "raw_code": "def nil?\n    response.nil? || response.body.blank?\n  end",
    "comment": "Original method: https://github.com/jnunemaker/httparty/blob/v0.20.0/lib/httparty/response.rb#L83-L86 Related issue: https://github.com/jnunemaker/httparty/issues/568  We need to override this method because `Concurrent::Promise` calls `nil?` on the response when calling the `value` method. And the `value` calls `nil?`. https://github.com/ruby-concurrency/concurrent-ruby/blob/v1.2.2/lib/concurrent-ruby/concurrent/concern/dereferenceable.rb#L64",
    "label": "",
    "id": "393"
  },
  {
    "raw_code": "def stub_full_request(url, ip_address: IP_ADDRESS_STUB, port: 80, method: :get)\n      stub_dns(url, ip_address: ip_address, port: port)\n\n      url = stubbed_hostname(url, hostname: ip_address)\n      WebMock.stub_request(method, url)\n    end",
    "comment": "Fully stubs a request using WebMock class. This class also stubs the IP address the URL is translated to (DNS lookup).  It expects the final request to go to the `ip_address` instead the given url. That's primarily a DNS rebind attack prevention of Gitlab::HTTP (see: Gitlab::HTTP_V2::UrlBlocker). ",
    "label": "",
    "id": "394"
  },
  {
    "raw_code": "def append_path(host, path)\n      \"#{host.to_s.sub(%r{\\/+$}, '')}/#{remove_leading_slashes(path)}\" # rubocop:disable Style/RedundantRegexpEscape\n    end",
    "comment": "Append path to host, making sure there's one single / in between",
    "label": "",
    "id": "395"
  },
  {
    "raw_code": "def slugify(str, allow_dots: false)\n      pattern = allow_dots ? /[^a-z0-9.]/ : /[^a-z0-9]/\n      str.downcase\n        .gsub(pattern, '-')[0..62]\n        .gsub(/(\\A[-.]+|[-.]+\\z)/, '')\n    end",
    "comment": "A slugified version of the string, suitable for inclusion in URLs and domain names. Rules:  * Lowercased * Anything not matching [a-z0-9-] is replaced with a - * Conditionally allows dots [a-z0-9-.] * Maximum length is 63 bytes * First/Last Character is not a hyphen or a dot",
    "label": "",
    "id": "396"
  },
  {
    "raw_code": "def param_key(klass)\n      klass.name.underscore.tr('/', '_')\n    end",
    "comment": "Converts a class into the string representation of its name, following the ActiveModel naming convention https://api.rubyonrails.org/classes/ActiveModel/Naming.html#method-c-param_key param_key(Ci::SecureFile) returns \"ci_secure_file\"",
    "label": "",
    "id": "397"
  },
  {
    "raw_code": "def nlbr(str)\n      ActionView::Base.full_sanitizer.sanitize(+str, tags: []).gsub(/\\r?\\n/, '<br>').html_safe\n    end",
    "comment": "Converts newlines into HTML line break elements",
    "label": "",
    "id": "398"
  },
  {
    "raw_code": "def which(filename)\n      ENV['PATH']&.split(File::PATH_SEPARATOR)&.each do |path|\n        full_path = File.join(path, filename)\n        return full_path if File.executable?(full_path)\n      end",
    "comment": "Behaves like `which` on Linux machines: given PATH, try to resolve the given executable name to an absolute path, or return nil.  which('ruby') #=> /usr/bin/ruby",
    "label": "",
    "id": "399"
  },
  {
    "raw_code": "def ensure_array_from_string(string_or_array)\n      return string_or_array if string_or_array.is_a?(Array)\n\n      string_or_array.split(',').map(&:strip)\n    end",
    "comment": "Used in EE Accepts either an Array or a String and returns an array",
    "label": "",
    "id": "400"
  },
  {
    "raw_code": "def safe_downcase!(str)\n      if str.frozen?\n        str.downcase\n      else\n        str.downcase! || str\n      end",
    "comment": "A safe alternative to String#downcase!  This will make copies of frozen strings but downcase unfrozen strings in place, reducing allocations.",
    "label": "",
    "id": "401"
  },
  {
    "raw_code": "def parse_url(uri_string)\n      Addressable::URI.parse(uri_string)\n    rescue Addressable::URI::InvalidURIError, TypeError\n    end",
    "comment": "Converts a string to an Addressable::URI object. If the string is not a valid URI, it returns nil. Param uri_string should be a String object. This method returns an Addressable::URI object or nil.",
    "label": "",
    "id": "402"
  },
  {
    "raw_code": "def multiple_key_invert(hash)\n      hash.flat_map { |k, v| Array.wrap(v).zip([k].cycle) }\n        .group_by(&:first)\n        .transform_values { |kvs| kvs.map(&:last) }\n    end",
    "comment": "Invert a hash, collecting all keys that map to a given value in an array.  Unlike `Hash#invert`, where the last encountered pair wins, and which has the type `Hash[k, v] => Hash[v, k]`, `multiple_key_invert` does not lose any information, has the type `Hash[k, v] => Hash[v, Array[k]]`, and the original hash can always be reconstructed.  example:  multiple_key_invert({ a: 1, b: 2, c: 1 }) # => { 1 => [:a, :c], 2 => [:b] } ",
    "label": "",
    "id": "403"
  },
  {
    "raw_code": "def stable_sort_by(list)\n      list.sort_by.with_index { |x, idx| [yield(x), idx] }\n    end",
    "comment": "This sort is stable (see https://en.wikipedia.org/wiki/Sorting_algorithm#Stability) contrary to the bare Ruby sort_by method. Using just sort_by leads to instability across different platforms (e.g., x86_64-linux and x86_64-darwin18) which in turn leads to different sorting results for the equal elements across these platforms. This method uses a list item's original index position to break ties.",
    "label": "",
    "id": "404"
  },
  {
    "raw_code": "def valid_brackets?(string = '', allow_nested: true)\n      # remove everything except brackets\n      brackets = string.remove(/[^\\[\\]]/)\n\n      return true if brackets.empty?\n      # balanced counts check\n      return false if brackets.size.odd?\n\n      unless allow_nested\n        # nested brackets check\n        return false if brackets.include?('[[') || brackets.include?(']]') # rubocop:disable Style/SoleNestedConditional\n      end",
    "comment": "Check for valid brackets (`[` and `]`) in a string using this aspects: * open brackets count == closed brackets count * (optionally) reject nested brackets via `allow_nested: false` * open / close brackets coherence, eg. ][[] -> invalid",
    "label": "",
    "id": "405"
  },
  {
    "raw_code": "def restrict_within_concurrent_ruby\n      previous = Thread.current[:restrict_within_concurrent_ruby]\n      Thread.current[:restrict_within_concurrent_ruby] = true\n\n      yield\n    ensure\n      Thread.current[:restrict_within_concurrent_ruby] = previous\n    end",
    "comment": "Use this method to set the `restrict_within_concurrent_ruby` to `true` for the block. `raise_if_concurrent_ruby!` will use this flag to raise an error if it's set to `true`.",
    "label": "",
    "id": "406"
  },
  {
    "raw_code": "def allow_within_concurrent_ruby\n      previous = Thread.current[:restrict_within_concurrent_ruby]\n      Thread.current[:restrict_within_concurrent_ruby] = false\n\n      yield\n    ensure\n      Thread.current[:restrict_within_concurrent_ruby] = previous\n    end",
    "comment": "Use this method to disable the `restrict_within_concurrent_ruby` for the block. It is mainly used to prevent infinite loop when `ConcurrentRubyThreadIsUsedError` is rescued and sent to Sentry. More info: https://gitlab.com/gitlab-org/gitlab/-/issues/432145#note_1671305713",
    "label": "",
    "id": "407"
  },
  {
    "raw_code": "def raise_if_concurrent_ruby!(what)\n      return unless Thread.current[:restrict_within_concurrent_ruby]\n\n      raise ConcurrentRubyThreadIsUsedError, \"Cannot run '#{what}' if running from `Concurrent::Promise`.\"\n    end",
    "comment": "Running external methods can allocate I/O bound resources (like PostgreSQL connection or Gitaly) This is forbidden when running within a concurrent Ruby thread, for example `async` HTTP requests provided by the `gitlab-http` gem.",
    "label": "",
    "id": "408"
  },
  {
    "raw_code": "def to_rails_log_level(input, fallback = nil)\n      case input.to_s.downcase\n      when 'debug',   '0' then :debug\n      when 'info',    '1' then :info\n      when 'warn',    '2' then :warn\n      when 'error',   '3' then :error\n      when 'fatal',   '4' then :fatal\n      when 'unknown', '5' then :unknown\n      else\n        return :info unless fallback\n\n        # Normalize the fallback value, just in case\n        to_rails_log_level(fallback)\n      end",
    "comment": "Returns a valid Rails log_level, given a user input and an optional fallback  `config.log_level=` does NOT accept integers, but Ruby Loggers do.  @return [Symbol, nil]",
    "label": "",
    "id": "409"
  },
  {
    "raw_code": "def deep_sort_hashes(item)\n      case item\n      when Hash\n        item.keys.sort_by(&:to_s).each_with_object({}) do |key, sorted_hash|\n          value = item[key]\n          sorted_hash[key] = value.is_a?(Hash) || value.is_a?(Array) ? deep_sort_hashes(value) : value\n        end",
    "comment": "@param [Array, Hash] item - An Array or a Hash @return [Array, Hash]  Use this method to recursively sort any nested Hashes of a collection (an Array or Hash), with the sort order based on the keys of the nested Hashes.  - deeply nested Arrays are supported. For example: `[[[{b: 2, a: 1}]]]` will sort the nested Hash. - The order of any nested Arrays is preserved. For example: `[1, {b: 2, a: 1}, 3]` will sort the nested Hash, but preserve the order of the Array. - Any non-Array/non-Hash items are returned as-is without further recursion.",
    "label": "",
    "id": "410"
  },
  {
    "raw_code": "def uuid_v7(extra_timestamp_bits: 0) # rubocop:disable Metrics/AbcSize -- Ported as is\n      case (extra_timestamp_bits = Integer(extra_timestamp_bits))\n      when 0 # min timestamp precision\n        ms = Process.clock_gettime(Process::CLOCK_REALTIME, :millisecond)\n        rand = SecureRandom.random_bytes(10)\n        rand.setbyte(0, (rand.getbyte(0) & 0x0f) | 0x70) # version\n        rand.setbyte(2, (rand.getbyte(2) & 0x3f) | 0x80) # variant\n        format(\"%08x-%04x-%s\", (ms & 0x0000_ffff_ffff_0000) >> 16, (ms & 0x0000_0000_0000_ffff),\n          rand.unpack(\"H4H4H12\").join(\"-\"))\n\n      when 12 # max timestamp precision\n        ms, ns = Process.clock_gettime(Process::CLOCK_REALTIME, :nanosecond)\n                   .divmod(1_000_000)\n        extra_bits = ns * 4096 / 1_000_000\n        rand = SecureRandom.random_bytes(8)\n        rand.setbyte(0, (rand.getbyte(0) & 0x3f) | 0x80) # variant\n        format(\"%08x-%04x-7%03x-%s\", (ms & 0x0000_ffff_ffff_0000) >> 16, (ms & 0x0000_0000_0000_ffff), extra_bits,\n          rand.unpack(\"H4H12\").join(\"-\"))\n\n      when (0..12) # the generic version is slower than the special cases above\n        rand_a, rand_b1, rand_b2, rand_b3 = SecureRandom.random_bytes(10).unpack(\"nnnN\")\n        rand_mask_bits = 12 - extra_timestamp_bits\n        ms, ns = Process.clock_gettime(Process::CLOCK_REALTIME, :nanosecond)\n                   .divmod(1_000_000)\n        format(\"%08x-%04x-%04x-%04x-%04x%08x\",\n          (ms & 0x0000_ffff_ffff_0000) >> 16, (ms & 0x0000_0000_0000_ffff), 0x7000 |\n            ((ns * (1 << extra_timestamp_bits) / 1_000_000) << rand_mask_bits) |\n            (rand_a & ((1 << rand_mask_bits) - 1)), 0x8000 | (rand_b1 & 0x3fff), rand_b2, rand_b3)\n\n      else\n        raise ArgumentError, \"extra_timestamp_bits must be in 0..12, got: #{extra_timestamp_bits}\"\n      end",
    "comment": "Generate a random v7 UUID (Universally Unique IDentifier). Ported from ruby 3.3.0's https://github.com/ruby/securerandom/commit/34ed1a2ec35dc8f00ff69665b373cef7484c937f TODO remove when we support min ruby 3.3.0",
    "label": "",
    "id": "411"
  },
  {
    "raw_code": "def <=>(other)\n      return unless other.is_a? VersionInfo\n      return unless valid? && other.valid?\n\n      if other.major < @major\n        1\n      elsif @major < other.major\n        -1\n      elsif other.minor < @minor\n        1\n      elsif @minor < other.minor\n        -1\n      elsif other.patch < @patch\n        1\n      elsif @patch < other.patch\n        -1\n      elsif @suffix_s.empty? && other.suffix.present?\n        1\n      elsif other.suffix.empty? && @suffix_s.present?\n        -1\n      else\n        suffix <=> other.suffix\n      end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity rubocop:disable Metrics/PerceivedComplexity",
    "label": "",
    "id": "412"
  },
  {
    "raw_code": "def to_s\n      if valid?\n        \"%d.%d.%d%s\" % [@major, @minor, @patch, @suffix_s] # rubocop:disable Style/FormatString\n      else\n        'Unknown'\n      end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity rubocop:enable Metrics/PerceivedComplexity",
    "label": "",
    "id": "413"
  },
  {
    "raw_code": "def memory_usage_rss(pid: 'self')\n        results = { total: 0, anon: 0, file: 0 }\n\n        safe_yield_procfile(PROC_STATUS_PATH % pid) do |io|\n          io.each_line do |line|\n            if (value = parse_metric_value(line, RSS_TOTAL_PATTERN)) > 0\n              results[:total] = value.kilobytes\n            elsif (value = parse_metric_value(line, RSS_ANON_PATTERN)) > 0\n              results[:anon] = value.kilobytes\n            elsif (value = parse_metric_value(line, RSS_FILE_PATTERN)) > 0\n              results[:file] = value.kilobytes\n            end",
    "comment": "Returns the given process' RSS (resident set size) in bytes.",
    "label": "",
    "id": "414"
  },
  {
    "raw_code": "def memory_usage_uss_pss(pid: 'self')\n        sum_matches(PROC_SMAPS_ROLLUP_PATH % pid, uss: PRIVATE_PAGES_PATTERN, pss: PSS_PATTERN)\n          .transform_values(&:kilobytes)\n      end",
    "comment": "Returns the given process' USS/PSS (unique/proportional set size) in bytes.",
    "label": "",
    "id": "415"
  },
  {
    "raw_code": "def real_time(precision = :float_second)\n        Process.clock_gettime(Process::CLOCK_REALTIME, precision)\n      end",
    "comment": "Returns the current real time in a given precision.  Returns the time as a Float for precision = :float_second.",
    "label": "",
    "id": "416"
  },
  {
    "raw_code": "def monotonic_time\n        Process.clock_gettime(Process::CLOCK_MONOTONIC, :float_second)\n      end",
    "comment": "Returns the current monotonic clock time as seconds with microseconds precision.  Returns the time as a Float.",
    "label": "",
    "id": "417"
  },
  {
    "raw_code": "def process_runtime_elapsed_seconds\n        # Entry 22 (1-indexed) contains the process `starttime`, see:\n        # https://man7.org/linux/man-pages/man5/proc.5.html\n        #\n        # This value is a fixed timestamp in clock ticks.\n        # To obtain an elapsed time in seconds, we divide by the number\n        # of ticks per second and subtract from the system uptime.\n        start_time_ticks = proc_stat_entries[21].to_f\n        clock_ticks_per_second = Etc.sysconf(Etc::SC_CLK_TCK)\n        uptime - (start_time_ticks / clock_ticks_per_second)\n      end",
    "comment": "Returns the total time the current process has been running in seconds.",
    "label": "",
    "id": "418"
  },
  {
    "raw_code": "def sum_matches(proc_file, **patterns)\n        results = patterns.transform_values { 0 }\n\n        safe_yield_procfile(proc_file) do |io|\n          io.each_line do |line|\n            patterns.each do |metric, pattern|\n              results[metric] += parse_metric_value(line, pattern)\n            end",
    "comment": "Given a path to a file in /proc and a hash of (metric, pattern) pairs, sums up all values found for those patterns under the respective metric.",
    "label": "",
    "id": "419"
  },
  {
    "raw_code": "def uptime\n        Process.clock_gettime(Process::CLOCK_BOOTTIME)\n      rescue NameError\n        0\n      end",
    "comment": "Equivalent to reading /proc/uptime on Linux 2.6+.  Returns 0 if not supported, e.g. on Darwin.",
    "label": "",
    "id": "420"
  },
  {
    "raw_code": "def strong_memoize(name)\n        key = ivar(name)\n\n        if instance_variable_defined?(key)\n          instance_variable_get(key)\n        else\n          instance_variable_set(key, yield)\n        end",
    "comment": "Instead of writing patterns like this:  def trigger_from_token return @trigger if defined?(@trigger)  @trigger = Ci::Trigger.find_by_token(params[:token].to_s) end  We could write it like:  include Gitlab::Utils::StrongMemoize  def trigger_from_token Ci::Trigger.find_by_token(params[:token].to_s) end strong_memoize_attr :trigger_from_token  def enabled? Feature.enabled?(:some_feature) end strong_memoize_attr :enabled? ",
    "label": "",
    "id": "421"
  },
  {
    "raw_code": "def strong_memoize_with_expiration(name, expire_in)\n        key = ivar(name)\n        expiration_key = \"#{key}_expired_at\"\n\n        if instance_variable_defined?(expiration_key)\n          expire_at = instance_variable_get(expiration_key)\n          clear_memoization(name) if expire_at.past?\n        end",
    "comment": "Works the same way as \"strong_memoize\" but takes a second argument - expire_in. This allows invalidate the data after specified number of seconds",
    "label": "",
    "id": "422"
  },
  {
    "raw_code": "def ivar(name)\n        case name\n        when Symbol, String\n          :\"@#{name}\"\n        else\n          raise ArgumentError, \"Invalid type of '#{name}'\"\n        end",
    "comment": "Convert `\"name\"`/`:name` into `:@name`  Depending on a type ensure that there's a single memory allocation",
    "label": "",
    "id": "423"
  },
  {
    "raw_code": "def scheduled_stops\n        current_version_stops = SCHEDULED_STOPS.map do |minor|\n          Gitlab::VersionInfo.new(version_info.major, minor)\n        end",
    "comment": "We want this to return a list of scheduled stops for the current major version, and the first stop in the next major version.",
    "label": "",
    "id": "424"
  },
  {
    "raw_code": "def current_uuid7_time(extra_timestamp_bits: 0)\n      denominator = (1 << extra_timestamp_bits).to_r\n      Process.clock_gettime(Process::CLOCK_REALTIME, :nanosecond)\n        .then { |ns| ((ns / 1_000_000r) * denominator).floor / denominator }\n        .then { |ms| Time.at(ms / 1000r, in: \"+00:00\") }\n    end",
    "comment": "Helper method to get current time with precision based on extra_timestamp_bits",
    "label": "",
    "id": "425"
  },
  {
    "raw_code": "def get_uuid7_time(uuid, extra_timestamp_bits: 0)\n      denominator     = (1 << extra_timestamp_bits) * 1000r\n      extra_chars     = extra_timestamp_bits / 4\n      last_char_bits  = extra_timestamp_bits % 4\n      extra_chars    += 1 if last_char_bits != 0\n      timestamp_re    = /\\A(\\h{8})-(\\h{4})-7(\\h{#{extra_chars}})/\n      timestamp_chars = uuid.match(timestamp_re).captures.join\n      timestamp       = timestamp_chars.to_i(16)\n      timestamp     >>= 4 - last_char_bits unless last_char_bits == 0\n      timestamp /= denominator\n      Time.at(timestamp, in: \"+00:00\")\n    end",
    "comment": "Helper method to extract timestamp from UUID v7",
    "label": "",
    "id": "426"
  },
  {
    "raw_code": "def storage\n        store.store\n      end",
    "comment": "Access to the backing storage of the request store. This returns an object with `[]` and `[]=` methods that does not discard values.  This can be useful if storage is needed for a delimited purpose, and the forgetful nature of the null store is undesirable.",
    "label": "",
    "id": "427"
  },
  {
    "raw_code": "def write(key, value, _options = nil)\n        store.write(key, value)\n      end",
    "comment": "This method accept an options hash to be compatible with ActiveSupport::Cache::Store#write method. The options are not passed to the underlying cache implementation because RequestStore#write accepts only a key, and value params.",
    "label": "",
    "id": "428"
  },
  {
    "raw_code": "def self.new(\n    collection, header_to_value_hash, associations_to_preload = [], replace_newlines: false,\n    order_hint: :created_at)\n    CsvBuilder::Builder.new(\n      collection,\n      header_to_value_hash,\n      associations_to_preload,\n      replace_newlines: replace_newlines,\n      order_hint: order_hint\n    )\n  end",
    "comment": " * +collection+ - The data collection to be used * +header_to_value_hash+ - A hash of 'Column Heading' => 'value_method'. * +associations_to_preload+ - An array of records to preload with a batch of records. * +replace_newlines+ - default: false - If true, replaces newline characters with a literal \"\\n\" * +order_hint+ - default: :created_at - The column used to order the rows  The value method will be called once for each object in the collection, to determine the value for that row. It can either be the name of a method on the object, or a lamda to call passing in the object.",
    "label": "",
    "id": "429"
  },
  {
    "raw_code": "def render\n      Tempfile.create(['csv_builder_gzip', '.csv.gz']) do |tempfile|\n        Zlib::GzipWriter.open(tempfile.path) do |gz|\n          csv = CSV.new(gz)\n\n          write_csv csv, until_condition: -> {} # truncation must be handled outside of the CsvBuilder\n\n          csv.close\n        end",
    "comment": "Writes the CSV file compressed and yields the written tempfile and rows written.   Example: > CsvBuilder::Gzip.new(Issue, { title: -> (row) { row.title.upcase }, id: :id }).render do |tempfile, rows| >   puts tempfile.path >   puts `zcat #{tempfile.path}` >   puts rows > end",
    "label": "",
    "id": "430"
  },
  {
    "raw_code": "def render(truncate_after_bytes = nil)\n      Tempfile.open(['csv']) do |tempfile|\n        csv = CSV.new(tempfile)\n\n        write_csv csv, until_condition: -> do\n          truncate_after_bytes && tempfile.size > truncate_after_bytes\n        end",
    "comment": "Renders the csv to a string",
    "label": "",
    "id": "431"
  },
  {
    "raw_code": "def non_housekeeper_changes(\n        source_project_id:,\n        source_branch:,\n        target_branch:,\n        target_project_id:\n      )\n        existing_merge_request = get_existing_merge_request(\n          source_project_id: source_project_id,\n          source_branch: source_branch,\n          target_branch: target_branch,\n          target_project_id: target_project_id\n        )\n\n        return [] if existing_merge_request.nil?\n\n        merge_request_notes = get_merge_request_notes(\n          target_project_id: target_project_id,\n          iid: existing_merge_request['iid']\n        )\n\n        changes = Set.new\n\n        merge_request_notes.each do |note|\n          next false unless note[\"system\"]\n          next false if note[\"author\"][\"id\"] == current_user_id\n\n          match = match_system_note(note['body'])\n          changes << match if match\n        end",
    "comment": "This looks at the system notes of the merge request to detect if it has been updated by anyone other than the current housekeeper user. If it has then it assumes that they did this for a reason and we can skip updating this detail of the merge request. Otherwise we assume we should generate it again using the latest output.",
    "label": "",
    "id": "432"
  },
  {
    "raw_code": "def self.should_push_code?(change, push_when_approved)\n        return false if change.already_approved? && !push_when_approved\n\n        change.update_required?(:code)\n      end",
    "comment": "We do not want to push code if the MR already has approvals as it will reset the approvals. Also we do not push if someone else has added commits already.",
    "label": "",
    "id": "433"
  },
  {
    "raw_code": "def matches_filter_identifiers?(identifiers)\n        return true unless filter_identifiers\n\n        filter_identifiers.matches_filters?(identifiers)\n      end",
    "comment": "This method is used to filter for each change at more granular level. For example: - When we pass --filter-identifiers my_feature_flag, it should only create an MR specifically for my_feature_flag because we are having the feature flag name in the change.identifiers Since the filtering logic is also implemented in the `Runner` class, it is not required to use this in a keep, but it is recommended for keeps which have expensive computation to perform. This is because the Runner only gets the change object after the Keep has finished computing the change. This can be done in the `each_identified_change` method like: ``` next unless matches_filter_identifiers?(change.identifiers) ... do the computation heavy work and yield change object. ```",
    "label": "",
    "id": "434"
  },
  {
    "raw_code": "def each_identified_change\n        raise NotImplementedError, \"A Keep must implement each_identified_change method\"\n      end",
    "comment": "The each_identified_change method should search the codebase to find potential changes based on the specific intention of the keep. It only needs to construct and yield a Change object with `identifiers` and `context`. This method should NOT perform file modifications or other side effects. All actual changes should be implemented in the make_change method.  @yieldparam [Gitlab::Housekeeper::Change]",
    "label": "",
    "id": "435"
  },
  {
    "raw_code": "def make_change!(change)\n        raise NotImplementedError, \"A Keep must implement make_change method\"\n      end",
    "comment": "The make_change method performs the actual file modifications and prepares the final Change object. This method receives a Change object from each_identified_change and should: - Perform all file modifications and side effects - Set the final change details (title, description, changed_files, etc.) - Return the completed Change object, or nil if no changes should be made  @param [Gitlab::Housekeeper::Change] change The change object with context from each_change",
    "label": "",
    "id": "436"
  },
  {
    "raw_code": "def self.rubocop_autocorrect(files, logger: Logger.new(nil))\n        # Stop revealing RuboCop TODOs so RuboCop is only fixing material offenses.\n        env = { 'REVEAL_RUBOCOP_TODO' => nil }\n        cmd = ['rubocop', '--autocorrect', '--force-exclusion']\n\n        ::Gitlab::Housekeeper::Shell.execute(*cmd, *files, env: env)\n        true\n      rescue ::Gitlab::Housekeeper::Shell::Error => e\n        logger.warn(e.message)\n        false\n      end",
    "comment": "Run `rubocop --autocorrect --force-exclusion`.  RuboCop is run without revealed TODOs.",
    "label": "",
    "id": "437"
  },
  {
    "raw_code": "def create_change(\n    identifiers: %w[the identifier],\n    title: 'The change title',\n    description: 'The change description',\n    changed_files: ['change1.txt', 'change2.txt'],\n    labels: %w[some-label-1 some-label-2],\n    assignees: ['thegitlabassignee'],\n    reviewers: ['thegitlabreviewer'],\n    mr_web_url: nil,\n    non_housekeeper_changes: []\n  )\n\n    change = ::Gitlab::Housekeeper::Change.new\n    change.identifiers = identifiers\n    change.title = title\n    change.description = description\n    change.changed_files = changed_files\n    change.labels = labels\n    change.assignees = assignees\n    change.reviewers = reviewers\n    change.mr_web_url = mr_web_url\n    change.non_housekeeper_changes = non_housekeeper_changes\n\n    change\n  end",
    "comment": "rubocop: disable Metrics/ParameterLists",
    "label": "",
    "id": "438"
  },
  {
    "raw_code": "def stub_env(key_or_hash, value = nil)\n    init_stub unless env_stubbed?\n\n    if key_or_hash.is_a? Hash\n      key_or_hash.each do |key, value|\n        add_stubbed_value(key, ensure_env_type(value))\n      end",
    "comment": "Stub ENV variables  You can provide either a key and value as separate params or both in a Hash format  Keys and values will always be converted to String, to comply with how ENV behaves  @param key_or_hash [String, Hash<String,String>] @param value [String]",
    "label": "",
    "id": "439"
  },
  {
    "raw_code": "def build_gitlab_kerberos_url\n    [\n      kerberos_protocol,\n      \"://:@\",\n      gitlab.host,\n      \":#{kerberos_port}\",\n      gitlab.relative_url_root\n    ].join('')\n  end",
    "comment": "Curl expects username/password for authentication. However when using GSS-Negotiate not credentials should be needed. By inserting in the Kerberos dedicated URL \":@\", we give to curl an empty username and password and GSS auth goes ahead Known bug reported in http://sourceforge.net/p/curl/bugs/440/ and http://curl.haxx.se/docs/knownbugs.html",
    "label": "",
    "id": "440"
  },
  {
    "raw_code": "def verify_constant_array(modul, current, default)\n    values = default || []\n    unless current.nil?\n      values = []\n      current.each do |constant|\n        values.push(verify_constant(modul, constant, nil))\n      end",
    "comment": "check that values in `current` (string or integer) is a contant in `modul`.",
    "label": "",
    "id": "441"
  },
  {
    "raw_code": "def verify_constant(modul, current, default)\n    constant = modul.constants.find { |name| modul.const_get(name, false) == current }\n    value = constant.nil? ? default : modul.const_get(constant, false)\n    if current.is_a? String\n      value = begin\n        modul.const_get(current.upcase, false)\n      rescue StandardError\n        default\n      end",
    "comment": "check that `current` (string or integer) is a contant in `modul`.",
    "label": "",
    "id": "442"
  },
  {
    "raw_code": "def db_key_base_keys_truncated\n    db_key_base_keys.map do |key| # rubocop:disable Rails/Pluck -- No Rails context\n      key[0..31]\n    end",
    "comment": "Don't use this in new code, use db_key_base_keys_32_bytes instead!",
    "label": "",
    "id": "443"
  },
  {
    "raw_code": "def db_key_base_keys_32_bytes\n    db_key_base_keys.map do |key|\n      Gitlab::Utils.ensure_utf8_size(key, bytes: 32.bytes)\n    end",
    "comment": "Ruby 2.4+ requires passing in the exact required length for OpenSSL keys (https://github.com/ruby/ruby/commit/ce635262f53b760284d56bb1027baebaaec175d1). Previous versions quietly truncated the input.  Makes sure the key is exactly 32 bytes long, either by truncating or right-padding it with ASCII 0s. Use this when using :per_attribute_iv mode for attr_encrypted.",
    "label": "",
    "id": "444"
  },
  {
    "raw_code": "def db_key_base_keys\n    @db_key_base_keys ||= Array(Gitlab::Application.credentials.db_key_base).tap do |keys|\n      raise(MultipleDbKeyBaseError, \"Defining multiple `db_key_base` keys isn't supported yet.\") if keys.size > 1\n    end",
    "comment": "This should be used for :per_attribute_iv_and_salt mode. There is no need to truncate the key because the encryptor will use the salt to generate a hash of the password: https://github.com/attr-encrypted/encryptor/blob/c3a62c4a9e74686dd95e0548f9dc2a361fdc95d1/lib/encryptor.rb#L77",
    "label": "",
    "id": "445"
  },
  {
    "raw_code": "def build_sidekiq_routing_rules(rules)\n    return rules unless rules.nil? || rules&.empty?\n\n    [[Gitlab::SidekiqConfig::WorkerMatcher::WILDCARD_MATCH, 'default']]\n  end",
    "comment": "Route all jobs to 'default' queue. This setting is meant for self-managed instances use to keep things simple. See https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1491",
    "label": "",
    "id": "446"
  },
  {
    "raw_code": "def host(url)\n    url = url.downcase\n    url = \"http://#{url}\" unless url.start_with?('http')\n\n    # Get rid of the path so that we don't even have to encode it\n    url_without_path = url.sub(%r{(https?://[^/]+)/?.*}, '\\1')\n\n    URI.parse(url_without_path).host\n  end",
    "comment": "Extract the host part of the given +url+.",
    "label": "",
    "id": "447"
  },
  {
    "raw_code": "def cron_for_service_ping\n    # Set a default UUID for the case when the UUID hasn't been initialized.\n    uuid = Gitlab::CurrentSettings.uuid || GITLAB_INSTANCE_UUID_NOT_SET\n\n    minute = Digest::SHA256.hexdigest(uuid + 'minute').to_i(16) % 60\n    hour = Digest::SHA256.hexdigest(uuid + 'hour').to_i(16) % 24\n    day_of_week = Digest::SHA256.hexdigest(uuid).to_i(16) % 7\n\n    \"#{minute} #{hour} * * #{day_of_week}\"\n  end",
    "comment": "Runs at a consistent random time of day on a day of the week based on the instance UUID. This is to balance the load on the service receiving these pings. The sidekiq job handles temporary http failures.",
    "label": "",
    "id": "448"
  },
  {
    "raw_code": "def self.legacy_parse(object_store, object_store_type)\n    object_store ||= GitlabSettings::Options.build({})\n    object_store['enabled'] = false if object_store['enabled'].nil?\n    object_store['remote_directory'], object_store['bucket_prefix'] = split_bucket_prefix(\n      object_store['remote_directory']\n    )\n\n    object_store['direct_upload'] = true\n\n    object_store['proxy_download'] = false if object_store['proxy_download'].nil?\n    object_store['storage_options'] ||= {}\n\n    object_store\n  end",
    "comment": "Legacy parser",
    "label": "",
    "id": "449"
  },
  {
    "raw_code": "def parse!\n    return unless use_consolidated_settings?\n\n    main_config = settings['object_store']\n    common_config = main_config.slice('enabled', 'connection', 'proxy_download', 'storage_options')\n\n    # These are no longer configurable if common config is used\n    common_config['direct_upload'] = true\n    common_config['storage_options'] ||= {}\n\n    SUPPORTED_TYPES.each do |store_type|\n      overrides = main_config.dig('objects', store_type) || {}\n      target_config = common_config.merge(overrides.slice(*ALLOWED_OBJECT_STORE_OVERRIDES))\n      section = settings.try(store_type)\n\n      # Admins can selectively disable object storage for a specific\n      # type as an override in the consolidated settings.\n      next unless overrides.fetch('enabled', true)\n      next unless section\n\n      if section['enabled'] && target_config['bucket'].blank?\n        missing_bucket_for(store_type)\n        next\n      end",
    "comment": "This method converts the common object storage settings to the legacy, internal representation.  For example, with the folowing YAML:  object_store: enabled: true connection: provider: AWS aws_access_key_id: minio aws_secret_access_key: gdk-minio region: gdk endpoint: 'http://127.0.0.1:9000' path_style: true storage_options: server_side_encryption: AES256 proxy_download: true objects: artifacts: bucket: artifacts proxy_download: false lfs: bucket: lfs-objects  This method then will essentially call:  Settings.artifacts['object_store'] = { \"enabled\" => true, \"connection\" => { \"provider\" => \"AWS\", \"aws_access_key_id\" => \"minio\", \"aws_secret_access_key\" => \"gdk-minio\", \"region\" => \"gdk\", \"endpoint\" => \"http://127.0.0.1:9000\", \"path_style\" => true }, \"storage_options\" => { \"server_side_encryption\" => \"AES256\" }, \"direct_upload\" => true, \"proxy_download\" => false, \"remote_directory\" => \"artifacts\" }  Settings.lfs['object_store'] = { \"enabled\" => true, \"connection\" => { \"provider\" => \"AWS\", \"aws_access_key_id\" => \"minio\", \"aws_secret_access_key\" => \"gdk-minio\", \"region\" => \"gdk\", \"endpoint\" => \"http://127.0.0.1:9000\", \"path_style\" => true }, \"storage_options\" => { \"server_side_encryption\" => \"AES256\" }, \"direct_upload\" => true, \"proxy_download\" => true, \"remote_directory\" => \"lfs-objects\" }  Note that with the common config: 1. Only one object store credentials can now be used. This is necessary to limit configuration overhead when an object storage client (e.g. AWS S3) is used inside GitLab Workhorse. 2. However, a bucket has to be specified for each object type. Reusing buckets is not really supported, but we don't enforce that yet. 3. direct_upload cannot be configured anymore.",
    "label": "",
    "id": "450"
  },
  {
    "raw_code": "def use_consolidated_settings?\n    return false unless settings.dig('object_store', 'enabled')\n\n    connection = settings.dig('object_store', 'connection')\n\n    return false unless connection.present?\n\n    WORKHORSE_ACCELERATED_TYPES.each do |store|\n      section = settings.try(store)\n\n      next unless section\n      next unless section.dig('object_store', 'enabled')\n\n      section_connection = section.dig('object_store', 'connection')\n\n      # We can use consolidated settings if the main object store\n      # connection matches the section-specific connection. This makes\n      # it possible to automatically use consolidated settings as new\n      # settings (such as ci_secure_files) get promoted to a supported\n      # type. Omnibus defaults to an empty hash for the\n      # section-specific connection.\n      return false if section_connection.present? && section_connection.to_h != connection.to_h\n    end",
    "comment": "We only can use the common object storage settings if: 1. The common settings are defined 2. The legacy settings are not defined",
    "label": "",
    "id": "451"
  },
  {
    "raw_code": "def request_phase\n        authn_request = OneLogin::RubySaml::Authrequest.new\n\n        store_authn_request_id(authn_request)\n\n        with_settings do |settings|\n          redirect(authn_request.create(settings, additional_params_for_authn_request))\n        end",
    "comment": "NOTE: This method duplicates code from omniauth-saml so that we can access authn_request to store it See: https://github.com/omniauth/omniauth-saml/issues/172",
    "label": "",
    "id": "452"
  },
  {
    "raw_code": "def callback_url\n        full_host + callback_path\n      end",
    "comment": "NOTE: Overriding the callback_url method since in certain cases IDP doesn't return the correct ACS URL for us to validate See: https://gitlab.com/gitlab-org/gitlab/-/issues/491634",
    "label": "",
    "id": "453"
  },
  {
    "raw_code": "def override_omniauth(provider, controller, path_prefix = '/users/auth')\n  match \"#{path_prefix}/#{provider}/callback\",\n    to: \"#{controller}##{provider}\",\n    as: \"#{provider}_omniauth_callback\",\n    via: [:get, :post]\nend",
    "comment": "Allows individual providers to be directed to a chosen controller Call from inside devise_scope",
    "label": "",
    "id": "454"
  },
  {
    "raw_code": "def GET\n        if get_header(RACK_REQUEST_QUERY_STRING) == query_string\n          get_header(RACK_REQUEST_QUERY_HASH)\n        else\n          query_hash = parse_query(query_string, '&') # only allow ampersand here\n          set_header(RACK_REQUEST_QUERY_STRING, query_string)\n          set_header(RACK_REQUEST_QUERY_HASH, query_hash)\n        end",
    "comment": "rubocop: disable Naming/MethodName",
    "label": "",
    "id": "455"
  },
  {
    "raw_code": "def storage_validation_error(message)\n  raise \"#{message}. Please fix this in your gitlab.yml before starting GitLab.\"\nend",
    "comment": "frozen_string_literal: true",
    "label": "",
    "id": "456"
  },
  {
    "raw_code": "def recursive(*args)\n        @scope.with_values_ += args\n        @scope.recursive_value = true\n        @scope.extend(Gitlab::Database::ReadOnlyRelation)\n        @scope\n      end",
    "comment": "Returns a new relation expressing WITH RECURSIVE",
    "label": "",
    "id": "457"
  },
  {
    "raw_code": "def setting_provided?(key)\n      !settings[key].nil?\n    end",
    "comment": "`key` is said to be provided when `settings` has a non-nil value for `key`.",
    "label": "",
    "id": "458"
  },
  {
    "raw_code": "def smtp_starttls\n      return false if smtp_tls?\n\n      if setting_provided?(:enable_starttls) && settings[:enable_starttls]\n        # enable_starttls: provided and truthy\n        case settings[:enable_starttls]\n        when :auto then :auto\n        when :always then :always\n        else\n          :always\n        end",
    "comment": "Yields one of `:always`, `:auto` or `false` based on `enable_starttls` and `enable_starttls_auto` flags. Yields `false` when `smtp_tls?`.",
    "label": "",
    "id": "459"
  },
  {
    "raw_code": "def run?\n            return true unless klass < ActiveRecord::Base\n\n            super\n          end",
    "comment": "https://github.com/rails/rails/blob/v7.0.5/activerecord/lib/active_record/associations/preloader/association.rb#L114-L116",
    "label": "",
    "id": "460"
  },
  {
    "raw_code": "def preloaded_records\n            return [] unless klass < ActiveRecord::Base\n\n            super\n          end",
    "comment": "https://github.com/rails/rails/blob/v7.0.5/activerecord/lib/active_record/associations/preloader/association.rb#L137-L141",
    "label": "",
    "id": "461"
  },
  {
    "raw_code": "def target_classes\n            super.delete_if { |klass| !(klass < ActiveRecord::Base) }\n          end",
    "comment": "https://github.com/rails/rails/blob/v7.0.5/activerecord/lib/active_record/associations/preloader/branch.rb#L37-L45",
    "label": "",
    "id": "462"
  },
  {
    "raw_code": "def device_grant_attributes\n      {\n        application_id: client.id,\n        expires_in: configuration.device_code_expires_in,\n        scopes: scopes.to_s,\n        user_code: generate_user_code,\n        # This will result in a fallback to Default Organizzation\n        # OAuth device grant flow doesn't support other organizations yet\n        organization_id: Gitlab::Current::Organization.new.organization.id\n      }\n    end",
    "comment": "@return [Hash]",
    "label": "",
    "id": "463"
  },
  {
    "raw_code": "def keyset_paginate(cursor: nil, per_page: 20, keyset_order_options: {})\n    Gitlab::Pagination::Keyset::Paginator.new(scope: self.dup, cursor: cursor, per_page: per_page, keyset_order_options: keyset_order_options)\n  end",
    "comment": "This method loads the records for the requested page and returns a keyset paginator object.",
    "label": "",
    "id": "464"
  },
  {
    "raw_code": "def reverse_sql_order(order_query)\n    return super if order_query.empty?\n\n    keyset_order_values = []\n\n    order_query_without_keyset = order_query.flat_map do |o|\n      next o unless o.is_a?(Gitlab::Pagination::Keyset::Order)\n\n      keyset_order_values << o\n      KEYSET_ORDER_PLACEHOLDER\n    end",
    "comment": "This modifies `reverse_sql_order` so that it is aware of Gitlab::Pagination::Keyset::Order which can reverse order clauses with NULLS LAST because we provide it a `reversed_order_expression`. This allows us to use `#last` on these relations.  Overrides https://github.com/rails/rails/blob/v6.1.6.1/activerecord/lib/active_record/relation/query_methods.rb#L1331-L1358",
    "label": "",
    "id": "465"
  },
  {
    "raw_code": "def migrations\n        @migrations ||= migrations_unmemoized\n      end",
    "comment": "This method is called a large number of times per rspec example, and it reads + parses `db/migrate/*` each time. Memoizing it can save 0.5 seconds per spec.",
    "label": "",
    "id": "466"
  },
  {
    "raw_code": "def has_gitlab_patch_backtrace_marker?(backtrace)\n    match = GITLAB_BACKTRACE_REGEX.match(backtrace[2])\n\n    !!match && match[1] == __FILE__\n  end",
    "comment": "This function tells us whether the exception was generated by #next itself or by something in the Fiber that it invokes. If it's generated by #next, then the backtrace will have #gitlab_patch_backtrace_marker as the third item down the trace (since #gitlab_patch_backtrace_marker calls a block, which in turn calls #next.) If it's generated by the Fiber that #next invokes, then it won't contain this marker.",
    "label": "",
    "id": "467"
  },
  {
    "raw_code": "def backtrace_matches_caller?(backtrace)\n    backtrace[3..] == caller[1..]\n  end",
    "comment": "This function makes sure that the rest of the stack trace matches in order to avoid missing an exception that was generated by calling #next on another Enumerator inside the Fiber. This might miss some *very* contrived scenarios involving recursion, but exceptions don't provide Fiber information, so it's the best we can do.",
    "label": "",
    "id": "468"
  },
  {
    "raw_code": "def timestamps_with_timezone(**options)\n        options[:null] = false if options[:null].nil?\n\n        [:created_at, :updated_at].each do |column_name|\n          column(column_name, :datetime_with_timezone, **options)\n        end",
    "comment": "Appends columns `created_at` and `updated_at` to a table.  It is used in table creation like: create_table 'users' do |t| t.timestamps_with_timezone end",
    "label": "",
    "id": "469"
  },
  {
    "raw_code": "def datetime_with_timezone(column_name, **options)\n        column(column_name, :datetime_with_timezone, **options)\n      end",
    "comment": "Adds specified column with appropriate timestamp type  It is used in table creation like: create_table 'users' do |t| t.datetime_with_timezone :did_something_at end",
    "label": "",
    "id": "470"
  },
  {
    "raw_code": "def aliased_types(name, fallback)\n        fallback\n      end",
    "comment": "Disable timestamp alias to datetime",
    "label": "",
    "id": "471"
  },
  {
    "raw_code": "def self.use_encoder(encoder)\n        previous_encoder = json_encoder\n        self.json_encoder = encoder\n\n        result = yield\n\n        self.json_encoder = previous_encoder\n\n        result\n      end",
    "comment": "This method is used only to test that our encoder maintains compatibility with the default ActiveSupport encoder. See spec/lib/gitlab/json_spec.rb",
    "label": "",
    "id": "472"
  },
  {
    "raw_code": "def service_provider_constant(service_name, provider_name)\n      args = service_provider_search_args(service_name, provider_name)\n      Fog.const_get(args.first).const_get(*const_get_args(args.second))\n    rescue NameError # Try to find the constant from in an alternate location\n      Fog.const_get(args.second).const_get(*const_get_args(args.first))\n    end",
    "comment": "rubocop:disable Gitlab/ConstGetInheritFalse",
    "label": "",
    "id": "473"
  },
  {
    "raw_code": "def example_finished(notification)\n      # If available, use the spec location where (deeply nested) shared examples are used.\n      file, line_number = notification\n        .example\n        .metadata\n        .fetch(:shared_group_inclusion_backtrace)\n        .reverse\n        .lazy\n        .map { |bt| bt.formatted_inclusion_location&.split(':') }\n        .first\n\n      unless file && line_number\n        file = @current_example.file\n        line_number = @current_example.line_number\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables -- patching class which requires setting ivars",
    "label": "",
    "id": "474"
  },
  {
    "raw_code": "def total_count_with_limit(column_name = :all, options = {}) # :nodoc:\n      return @total_count if defined?(@total_count) && @total_count\n\n      # There are some cases that total count can be deduced from loaded records\n      if loaded?\n        # Total count has to be 0 if loaded records are 0\n        return @total_count = 0 if (current_page == 1) && @records.empty?\n        # Total count is calculable at the last page\n        return @total_count = ((current_page - 1) * limit_value) + @records.length if @records.any? && (@records.length < limit_value)\n      end",
    "comment": "This is a modified version of https://github.com/kaminari/kaminari/blob/c5186f5d9b7f23299d115408e62047447fd3189d/kaminari-activerecord/lib/kaminari/activerecord/active_record_relation_methods.rb#L17-L41 that limit the COUNT query to a configurable value to avoid query timeouts. The default limit value is 10,000 records rubocop: disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "475"
  },
  {
    "raw_code": "def length_validator_supports_dynamic_length_checks?(validator)\n  validator.options[:minimum].is_a?(Proc) &&\n    validator.options[:maximum].is_a?(Proc)\nend",
    "comment": "frozen_string_literal: true Discard the default Devise length validation from the `User` model. This needs to be discarded because the length validation provided by Devise does not support dynamically checking for min and max lengths. A new length validation has been added to the User model instead, to keep supporting dynamic password length validations, like: validates :password, length: { maximum: proc { password_length.max }, minimum: proc { password_length.min } }, allow_blank: true",
    "label": "",
    "id": "476"
  },
  {
    "raw_code": "def password_length_validator.validate(*_)\n    true\n  end",
    "comment": "discard the in-built length validator by always returning true",
    "label": "",
    "id": "477"
  },
  {
    "raw_code": "def module_include(base, mod)\n      MODULE_INCLUDE_MUTEX.synchronize do\n        old_methods = {}\n\n        mod.instance_methods.each do |sym|\n          old_methods[sym] = base.instance_method(sym) if base.method_defined?(sym)\n        end",
    "comment": "Internal: Inject into target module for the duration of the block.  mod - Module  Returns result of block.",
    "label": "",
    "id": "478"
  },
  {
    "raw_code": "def metrics_temp_dir\n  return unless Rails.env.development? || Rails.env.test?\n\n  if Gitlab::Runtime.sidekiq?\n    Rails.root.join('tmp/prometheus_multiproc_dir/sidekiq')\n  elsif Gitlab::Runtime.puma?\n    Rails.root.join('tmp/prometheus_multiproc_dir/puma')\n  else\n    Rails.root.join('tmp/prometheus_multiproc_dir')\n  end",
    "comment": "Keep separate directories for separate processes",
    "label": "",
    "id": "479"
  },
  {
    "raw_code": "def puma_dedicated_metrics_server?\n  Settings.monitoring.web_exporter.enabled\nend",
    "comment": "Whether a dedicated process should run that serves Rails application metrics, as opposed to using a Rails controller.",
    "label": "",
    "id": "480"
  },
  {
    "raw_code": "def active_job_railtie_callback?\n      callbacks = ActiveJob::Callbacks.singleton_class.__callbacks[:execute]\n\n      callbacks &&\n        callbacks.send(:chain).size == 1 &&\n        callbacks.first.kind == :around &&\n        callbacks.first.filter.is_a?(Proc) &&\n        callbacks.first.filter.source_location.first.ends_with?('lib/active_job/railtie.rb')\n    end",
    "comment": "We don't care about ActiveJob reloading the code in test env as we run jobs inline in test env. So in test, we remove this callback, which calls app.reloader.wrap, and ultimately calls FileUpdateChecker#updated? which is slow on macOS  https://github.com/rails/rails/blob/6-0-stable/activejob/lib/active_job/railtie.rb#L39-L46",
    "label": "",
    "id": "481"
  },
  {
    "raw_code": "def list_objects(bucket, options = {})\n          # rubocop: disable Style/PercentLiteralDelimiters -- this is an exact copy of the original method, just added match_glob here.\n          allowed_opts = %i(\n            delimiter\n            match_glob\n            max_results\n            page_token\n            prefix\n            projection\n            versions\n          )\n          # rubocop: enable Style/PercentLiteralDelimiters\n\n          @storage_json.list_objects(\n            bucket,\n            **options.select { |k, _| allowed_opts.include? k }\n          )\n        end",
    "comment": "This an identical copy of https://github.com/fog/fog-google/blob/v1.25.0/lib/fog/google/storage/storage_json/requests/list_objects.rb with just match_glob added to the allowed_opts",
    "label": "",
    "id": "482"
  },
  {
    "raw_code": "def respond_to_missing?(method_name, include_private = false)\n    return true if __sync!.respond_to?(method_name, false)\n\n    super\n  end",
    "comment": "In Ruby, `Array#flatten` recursively calls `#to_ary` on each element. If an element is a `BatchLoader` instance, and `to_ary` is not defined, Ruby will invoke `method_missing`, causing `BatchLoader` to delegate the call to its underlying `ActiveRecord` object.  However, since `ActiveRecord::Base` defines `to_ary` as a private method, the delegation attempt results in a `NoMethodError: private method `to_ary' called...  This ensures method_missing never calls public_send on a private method, avoiding crashes.",
    "label": "",
    "id": "483"
  },
  {
    "raw_code": "def find_routes(req)\n        path_info = req.path_info\n        routes = filter_routes(path_info).concat custom_routes.find_all { |r|\n          r.path.match?(path_info)\n        }\n\n        if req.head?\n          routes = match_head_routes(routes, req)\n        else\n          routes.select! { |r| r.matches?(req) }\n        end",
    "comment": "Besides the patch, this method is a duplicate for the original method defined in Rails: https://github.com/rails/rails/blob/v7.0.5/actionpack/lib/action_dispatch/journey/router.rb#L109-L132 See https://github.com/rails/rails/issues/47244",
    "label": "",
    "id": "484"
  },
  {
    "raw_code": "def attribute_instance_methods_as_symbols_available?\n          false\n        end",
    "comment": "Prevent attr_encrypted from defining virtual accessors for encryption data when the code and schema are out of sync. See this issue for more details: https://github.com/attr-encrypted/attr_encrypted/issues/332",
    "label": "",
    "id": "485"
  },
  {
    "raw_code": "def attr_encrypted(*attributes)\n    options = attributes.last.is_a?(Hash) ? attributes.pop : {}\n    options = attr_encrypted_default_options.dup.merge!(attr_encrypted_options).merge!(options)\n\n    options[:encode] = options[:default_encoding] if options[:encode] == true\n    options[:encode_iv] = options[:default_encoding] if options[:encode_iv] == true\n    options[:encode_salt] = options[:default_encoding] if options[:encode_salt] == true\n\n    attributes.each do |attribute|\n      encrypted_attribute_name = (options[:attribute] || [options[:prefix], attribute,\n        options[:suffix]].join).to_sym\n\n      if attribute_instance_methods_as_symbols_available?\n        instance_methods_as_symbols = attribute_instance_methods_as_symbols\n\n        attr_reader encrypted_attribute_name unless instance_methods_as_symbols.include?(encrypted_attribute_name)\n\n        unless instance_methods_as_symbols.include?(:\"#{encrypted_attribute_name}=\")\n          attr_writer encrypted_attribute_name\n        end",
    "comment": "rubocop:disable Metrics/AbcSize -- This is upstream code rubocop:disable Metrics/CyclomaticComplexity -- This is upstream code rubocop:disable Metrics/PerceivedComplexity -- This is upstream code",
    "label": "",
    "id": "486"
  },
  {
    "raw_code": "def deprecated(*args, version:, stack: 0)\n      super if Gitlab.dev_or_test_env?\n    end",
    "comment": "Disable all deprecations in non dev/test environments. ",
    "label": "",
    "id": "487"
  },
  {
    "raw_code": "def url(options = {})\n        if file.respond_to?(:url)\n          tmp_url = file.method(:url).arity == 0 ? file.url : file.url(options)\n\n          return tmp_url if tmp_url.present?\n        end",
    "comment": " === Parameters  [Hash] optional, the query params (only AWS)  === Returns  [String] the location where this file is accessible via a url ",
    "label": "",
    "id": "488"
  },
  {
    "raw_code": "def self.ensure_idempotency(key, ttl)\n    return if already_completed?(key)\n\n    result = yield\n\n    new(key, ttl).mark_as_completed!\n\n    result\n  end",
    "comment": "When code is wrapped with ensure_idempotency it won't be called again within the TTL(time to live) for the same key if the operation was successful completed  Example: IdempotencyCache.ensure_idempotency(\"tracking_cache:#{build_id}\", 5.hours) # idempotent within the TTL # wont run again if it ran successfully for that build within 5 hours track(params) end",
    "label": "",
    "id": "489"
  },
  {
    "raw_code": "def sanitize_filename(name)\n    name = name.tr(\"\\\\\", \"/\") # work-around for IE\n    name = ::File.basename(name)\n    name = name.gsub(CarrierWave::SanitizedFile.sanitize_regexp, \"_\")\n    name = \"_#{name}\" if /\\A\\.+\\z/.match?(name)\n    name = \"unnamed\" if name.empty?\n    name.mb_chars.to_s\n  end",
    "comment": "copy-pasted from CarrierWave::SanitizedFile",
    "label": "",
    "id": "490"
  },
  {
    "raw_code": "def self.run(component, checks = [])\n    executor = SimpleExecutor.new(component)\n\n    checks.each do |check|\n      executor << check\n    end",
    "comment": "Executes a bunch of checks for specified component  @param [String] component name of the component relative to the checks being executed @param [Array<BaseCheck>] checks classes of corresponding checks to be executed in the same order",
    "label": "",
    "id": "491"
  },
  {
    "raw_code": "def prepended(base)\n    base.definition.merge!(definition)\n  end",
    "comment": "This `prepended` hook will merge the enum definition of the prepended module into the base module to be used by `prepend_mod_with` helper method.",
    "label": "",
    "id": "492"
  },
  {
    "raw_code": "def translated_descriptions\n    definition.transform_values { |definition| _(definition[:description]) }\n  end",
    "comment": "Return list of dynamically translated descriptions.  It is required to define descriptions with `N_(...)`.  See https://github.com/grosser/fast_gettext#n_-and-nn_-make-dynamic-translations-available-to-the-parser",
    "label": "",
    "id": "493"
  },
  {
    "raw_code": "def definition\n    @definition.to_h\n  end",
    "comment": "We can use this method later to apply some sanity checks but for now, returning a Hash without any check is enough.",
    "label": "",
    "id": "494"
  },
  {
    "raw_code": "def self.connection\n      retrieve_connection\n    end",
    "comment": "Bypass the load balancer by restoring the default behavior of `connection` before the load balancer patches ActiveRecord::Base",
    "label": "",
    "id": "495"
  },
  {
    "raw_code": "def preload(names)\n      flipper.preload(names) # rubocop:disable CodeReuse/ActiveRecord -- This cop is not relevant in the Flipper context\n    end",
    "comment": "Preload the features with the given names.  names - An Array of String or Symbol names of the features.  https://github.com/flippercloud/flipper/blob/bf6a13f34fc7f45a597c3d66ec291f3e5855e830/lib/flipper/dsl.rb#L229",
    "label": "",
    "id": "496"
  },
  {
    "raw_code": "def enabled?(key, thing = nil, type: nil, default_enabled_if_undefined: nil)\n      thing = sanitized_thing(thing)\n\n      check_feature_flags_definition!(key, thing, type)\n\n      default_enabled = Feature::Definition.default_enabled?(key, default_enabled_if_undefined: default_enabled_if_undefined)\n      feature_value = current_feature_value(key, thing, default_enabled: default_enabled)\n\n      # If not yielded, then either recursion is happening, or the database does not exist yet, so use default_enabled.\n      feature_value = default_enabled if feature_value.nil?\n\n      # If we don't filter out this flag here we will enter an infinite loop\n      log_feature_flag_state(key, feature_value) if log_feature_flag_states?(key)\n\n      feature_value\n    end",
    "comment": "The default state of feature flag is read from `YAML`: 1. If feature flag does not have YAML it will fallback to `default_enabled: false` in production environment, but raise exception in development or tests. 2. The `default_enabled_if_undefined:` is tech debt related to Gitaly flags and should not be used outside of Gitaly's `lib/feature/gitaly.rb`",
    "label": "",
    "id": "497"
  },
  {
    "raw_code": "def register_feature_groups; end\n\n    def register_definitions\n      Feature::Definition.reload!\n    end\n\n    def register_hot_reloader\n      return unless check_feature_flags_definition?\n\n      Feature::Definition.register_hot_reloader!\n    end\n\n    def current_request\n      if Gitlab::SafeRequestStore.active?\n        Gitlab::SafeRequestStore[:flipper_request] ||= FlipperRequest.new\n      else\n        @flipper_request ||= FlipperRequest.new\n      end\n    end",
    "comment": "This method is called from config/initializers/0_inject_feature_flags.rb and can be used to register Flipper groups. See https://docs.gitlab.com/ee/development/feature_flags/  EE feature groups should go inside the ee/lib/ee/feature.rb version of this method.",
    "label": "",
    "id": "498"
  },
  {
    "raw_code": "def group_ids_for(feature_key)\n      FlipperGate.where(feature_key: feature_key)\n                 .pluck(:value)\n                 .select { |v| v.start_with?(\"Group:\") }\n                 .map { |v| v.sub(\"Group:\", \"\") }\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- rubocop doesn't recognize Flipper::Adapters::ActiveRecord::Gate as ActiveRecord.",
    "label": "",
    "id": "499"
  },
  {
    "raw_code": "def current_feature_value(key, thing, default_enabled:)\n      with_feature(key) do |feature|\n        if default_enabled && !Feature.persisted_name?(feature.name)\n          true\n        else\n          enabled = feature.enabled?(thing)\n\n          if enabled && !thing.nil?\n            opt_out = OptOut.new(thing)\n            feature.actors_value.exclude?(opt_out.flipper_id)\n          else\n            enabled\n          end",
    "comment": "Compute if thing is enabled, taking opt-out overrides into account Evaluate if `default enabled: false` or the feature has been persisted. `persisted_name?` can potentially generate DB queries and also checks for inclusion in an array of feature names (177 at last count), possibly reducing performance by half. So we only perform the `persisted` check if `default_enabled: true`",
    "label": "",
    "id": "500"
  },
  {
    "raw_code": "def with_feature(key)\n      feature = unsafe_get(key)\n      yield feature if feature.present?\n    ensure\n      pop_recursion_stack\n    end",
    "comment": "NOTE: it is not safe to call `Flipper::Feature#enabled?` outside the block",
    "label": "",
    "id": "501"
  },
  {
    "raw_code": "def feature_group\n      return unless params.key?(:feature_group)\n\n      Feature.group(params[:feature_group])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "502"
  },
  {
    "raw_code": "def users\n      return unless params.key?(:user)\n\n      params[:user].split(',').map do |arg|\n        UserFinder.new(arg).find_by_username || (raise UnknownTargetError, \"#{arg} is not found!\")\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "503"
  },
  {
    "raw_code": "def assign_ref_vars\n    ref_extractor = ExtractsRef::RefExtractor.new(repository_container, params.permit(:id, :ref, :path, :ref_type))\n    ref_extractor.extract!\n\n    @id = ref_extractor.id\n    @ref = ref_extractor.ref\n    @path = ref_extractor.path\n    @repo = ref_extractor.repo\n\n    if @ref.present?\n      @commit = ref_extractor.commit\n      @fully_qualified_ref = ref_extractor.fully_qualified_ref\n    end",
    "comment": "Extends the method to handle if there is no path and the ref doesn't exist in the repo, try to resolve the ref without an '.atom' suffix. If _that_ ref is found, set the request's format to Atom manually.  Automatically renders `not_found!` if a valid tree path could not be resolved (e.g., when a user inserts an invalid path or ref).  Automatically redirects to the current default branch if the ref matches a previous default branch that has subsequently been deleted.  Assignments are:  - @id     - A string representing the joined ref and path - @ref    - A string representing the ref (e.g., the branch, tag, or commit SHA) - @path   - A string representing the filesystem path - @commit - A Commit representing the commit from the given ref  If the :id parameter appears to be requesting a specific response format, that will be handled as well.",
    "label": "",
    "id": "504"
  },
  {
    "raw_code": "def redirect_renamed_default_branch?\n    false\n  end",
    "comment": "Override in controllers to determine which actions are subject to the redirect",
    "label": "",
    "id": "505"
  },
  {
    "raw_code": "def extract_ref_and_format(id)\n    return [id, nil] unless id.match?(/\\.(?:atom|json)$/)\n\n    id, _dot, format = id.rpartition('.')\n    ref = ref_names.find { |ref_name| id == ref_name }\n\n    raise InvalidPathError if ref.blank?\n\n    [ref, format.to_sym]\n  end",
    "comment": "If we have an ID of 'foo.atom' or 'foo.json', and the controller provides Atom, JSON, and HTML formats, then we have to check if the request was for the Atom version of the ID without the '.atom' suffix, the JSON version of the ID without the '.json' suffix, or the HTML version of the ID including the suffix. We only check this if the version including the suffix doesn't match, so it is possible to create a branch which has an unroutable Atom feed or JSON view.",
    "label": "",
    "id": "506"
  },
  {
    "raw_code": "def rectify_renamed_default_branch!\n    return unless redirect_renamed_default_branch?\n    return if @commit\n    return unless @id && @ref && repository_container.respond_to?(:previous_default_branch)\n    return unless repository_container.previous_default_branch == @ref\n    return unless request.get? || request.head?\n\n    flash[:notice] = _('The default branch for this project has been changed. Please update your bookmarks.')\n    redirect_to url_for(id: @id.sub(/\\A#{Regexp.escape(@ref)}/, repository_container.default_branch))\n\n    true\n  end",
    "comment": "For GET/HEAD requests, if the ref doesn't exist in the repository, check whether we're trying to access a renamed default branch. If we are, we can redirect to the current default branch instead of rendering a 404.",
    "label": "",
    "id": "507"
  },
  {
    "raw_code": "def self.next_rails?\n    Rails.gem_version >= Gem::Version.new(\"7.2\")\n  end",
    "comment": "This method will check your environment (e.g. `ENV['BUNDLE_GEMFILE]`) to determine whether your application is running with the next set of dependencies or the current set of dependencies.  @return [Boolean]",
    "label": "",
    "id": "508"
  },
  {
    "raw_code": "def self.render_and_post_process(text, context = {})\n    post_process(render(text, context), context)\n  end",
    "comment": "if you need to render markdown, then you probably need to post_process as well, such as removing references that the current user doesn't have permission to make",
    "label": "",
    "id": "509"
  },
  {
    "raw_code": "def primary_key\n      'id'\n    end",
    "comment": "Used by ActiveRecord's polymorphic association to set object_id",
    "label": "",
    "id": "510"
  },
  {
    "raw_code": "def base_class\n      self\n    end",
    "comment": "Used by ActiveRecord's polymorphic association to set object_type",
    "label": "",
    "id": "511"
  },
  {
    "raw_code": "def [](key)\n    send(key) if respond_to?(key) # rubocop:disable GitlabSecurity/PublicSend\n  end",
    "comment": "Used by AR for fetching attributes  Pass it along if we respond to it.",
    "label": "",
    "id": "512"
  },
  {
    "raw_code": "def self.raise_if_new_database_connection\n    return yield if Gitlab::Utils.to_boolean(ENV['SKIP_RAISE_ON_INITIALIZE_CONNECTIONS'])\n\n    previous_connection_counts =\n      ActiveRecord::Base.connection_handler.connection_pool_list(ApplicationRecord.current_role).map do |pool|\n        pool.connections.size\n      end",
    "comment": "Raises if new database connections established within the block  NOTE: this does not prevent existing connections that is already checked out from being used. You will need other means to prevent that such as by clearing all connections as implemented in the `:clear_active_connections_again` initializer for routes ",
    "label": "",
    "id": "513"
  },
  {
    "raw_code": "def in_operator_query_builder_params(array_data)\n    case filter\n    when ALL\n      in_operator_params(array_data: array_data)\n    when PUSH\n      # Here we need to add an order hint column to force the correct index usage.\n      # Without the order hint, the following conditions will use the `index_events_on_author_id_and_id`\n      # index which is not as efficient as the `index_events_for_followed_users` index.\n      # > target_type IS NULL AND action = 5 AND author_id = X ORDER BY id DESC\n      #\n      # The order hint adds an extra order by column which doesn't affect the result but forces the planner\n      # to use the correct index:\n      # > target_type IS NULL AND action = 5 AND author_id = X ORDER BY target_type DESC, id DESC\n      in_operator_params(\n        array_data: array_data,\n        scope: Event.where(target_type: nil).pushed_action,\n        order_hint_column: :target_type\n      )\n    when MERGED\n      in_operator_params(\n        array_data: array_data,\n        scope: Event.where(target_type: MergeRequest.to_s).merged_action\n      )\n    when COMMENTS\n      in_operator_params(\n        array_data: array_data,\n        scope: Event.commented_action,\n        in_column: :target_type,\n        in_values: [Note, *Note.descendants].map(&:name) # To make the query efficient we need to list all Note classes\n      )\n    when TEAM\n      in_operator_params(\n        array_data: array_data,\n        # TODO: Remove nil filter once backfill is complete https://gitlab.com/gitlab-org/gitlab/-/issues/565789\n        scope: Event.where(target_type: [nil, Event::TARGET_TYPES[:project].name]),\n        order_hint_column: :target_type,\n        in_column: :action,\n        in_values: Event.actions.values_at(*Event::TEAM_ACTIONS)\n      )\n    when ISSUE\n      in_operator_params(\n        array_data: array_data,\n        scope: Event.for_issue,\n        in_column: :action,\n        in_values: Event.actions.values_at(*Event::ISSUE_ACTIONS)\n      )\n    when WIKI\n      in_operator_params(\n        array_data: array_data,\n        scope: Event.for_wiki_page,\n        in_column: :action,\n        in_values: Event.actions.values_at(*Event::WIKI_ACTIONS)\n      )\n    when DESIGNS\n      in_operator_params(\n        array_data: array_data,\n        scope: Event.for_design,\n        in_column: :action,\n        in_values: Event.actions.values_at(*Event::DESIGN_ACTIONS)\n      )\n    else\n      in_operator_params(array_data: array_data)\n    end",
    "comment": "This method build specialized in-operator optimized queries based on different filter parameters. All queries will benefit from the index covering the following columns: * author_id target_type action id * project_id target_type action id * group_id target_type action id  More context: https://docs.gitlab.com/ee/development/database/efficient_in_operator_queries.html#the-inoperatoroptimization-module",
    "label": "",
    "id": "514"
  },
  {
    "raw_code": "def in_operator_array_params(scope:, array_data:, in_column: nil, in_values: nil)\n    array_scope_ids = array_data[:scope_ids]\n    array_scope_model = array_data[:scope_model]\n    array_mapping_column = array_data[:mapping_column]\n\n    # Adding non-existent record to generate valid SQL if array_scope_ids is empty\n    array_scope_ids << 0 if array_scope_ids.empty?\n\n    if in_column\n      # Builds Cartesian product of the in_values and the array_scope_ids (in this case: user_ids).\n      # The process is described here: https://docs.gitlab.com/ee/development/database/efficient_in_operator_queries.html#multiple-in-queries\n      # VALUES ((array_scope_ids[0], in_values[0]), (array_scope_ids[1], in_values[0]) ...)\n      cartesian = array_scope_ids.product(in_values)\n      column_list = Arel::Nodes::ValuesList.new(cartesian)\n\n      as = \"array_ids(id, #{Event.connection.quote_column_name(in_column)})\"\n      from = Arel::Nodes::Grouping.new(column_list).as(as)\n      {\n        array_scope: array_scope_model.select(:id, in_column).from(from),\n        array_mapping_scope: ->(primary_id_expression, in_column_expression) do\n          Event\n            .merge(scope)\n            .where(Event.arel_table[array_mapping_column].eq(primary_id_expression))\n            .where(Event.arel_table[in_column].eq(in_column_expression))\n        end",
    "comment": "This method builds the array_ parameters without in_column parameter: uses one IN filter: author_id with in_column: two IN filters: author_id, (target_type OR action) @param array_data [Hash] Must contain the scope_ids, scope_model, mapping_column keys",
    "label": "",
    "id": "515"
  },
  {
    "raw_code": "def enabled?\n        ::Labkit::FIPS.enabled?\n      end",
    "comment": "Returns whether we should be running in FIPS mode or not  @return [Boolean]",
    "label": "",
    "id": "516"
  },
  {
    "raw_code": "def self.allowed_job_keys\n      known_keys - WEB_ONLY_KEYS\n    end",
    "comment": "Sidekiq jobs may be deleted by matching keys in ApplicationContext. Filter out keys that aren't available in Sidekiq jobs.",
    "label": "",
    "id": "517"
  },
  {
    "raw_code": "def to_lazy_hash\n      {}.tap do |hash|\n        assign_hash_if_value(hash, :caller_id)\n        assign_hash_if_value(hash, :root_caller_id)\n        assign_hash_if_value(hash, :remote_ip)\n        assign_hash_if_value(hash, :related_class)\n        assign_hash_if_value(hash, :feature_category)\n        assign_hash_if_value(hash, :artifact_used_cdn)\n        assign_hash_if_value(hash, :artifacts_dependencies_size)\n        assign_hash_if_value(hash, :artifacts_dependencies_count)\n        assign_hash_if_value(hash, :merge_action_status)\n        assign_hash_if_value(hash, :bulk_import_entity_id)\n        assign_hash_if_value(hash, :sidekiq_destination_shard_redis)\n        assign_hash_if_value(hash, :auth_fail_reason)\n        assign_hash_if_value(hash, :auth_fail_token_id)\n        assign_hash_if_value(hash, :auth_fail_requested_scopes)\n        assign_hash_if_value(hash, :http_router_rule_action)\n        assign_hash_if_value(hash, :http_router_rule_type)\n        assign_hash_if_value(hash, :bulk_import_entity_id)\n\n        hash[:user] = -> { username } if include_user?\n        hash[:user_id] = -> { user_id } if include_user?\n        hash[:scoped_user] = -> { scoped_user&.username } if include_scoped_user?\n        hash[:scoped_user_id] = -> { scoped_user&.id } if include_scoped_user?\n        hash[:project] = -> { project_path } if include_project?\n        hash[:organization_id] = -> { organization&.id } if set_values.include?(:organization)\n        hash[:root_namespace] = -> { root_namespace_path } if include_namespace?\n        hash[:client_id] = -> { client } if include_client?\n        hash[:pipeline_id] = -> { job&.pipeline_id } if set_values.include?(:job)\n        hash[:job_id] = -> { job&.id } if set_values.include?(:job)\n        hash[:artifact_size] = -> { artifact&.size } if set_values.include?(:artifact)\n        hash[:kubernetes_agent_id] = -> { kubernetes_agent&.id } if set_values.include?(:kubernetes_agent)\n      end",
    "comment": "rubocop: disable Metrics/AbcSize rubocop: disable Metrics/CyclomaticComplexity -- inherently leads to higher cyclomatic due to all the conditional assignments, the added complexity from adding more abstractions like `assign_hash_if_value` is not worth the tradeoff. rubocop: disable Metrics/PerceivedComplexity -- same as above",
    "label": "",
    "id": "518"
  },
  {
    "raw_code": "def use\n      Labkit::Context.with_context(to_lazy_hash) { yield }\n    end",
    "comment": "rubocop: enable Metrics/CyclomaticComplexity rubocop: enable Metrics/AbcSize rubocop: enable Metrics/PerceivedComplexity",
    "label": "",
    "id": "519"
  },
  {
    "raw_code": "def to_exclusive_sentence(array)\n      array.to_sentence(two_words_connector: _(' or '), last_word_connector: _(', or '))\n    end",
    "comment": "Wraps ActiveSupport's Array#to_sentence to convert the given array to a comma-separated sentence joined with localized 'or' Strings instead of 'and'.",
    "label": "",
    "id": "520"
  },
  {
    "raw_code": "def build(object, **options)\n        # Objects are sometimes wrapped in a BatchLoader instance\n        case object.itself\n        when Board\n          board_url(object, **options)\n        when ::Ci::Build\n          instance.project_job_url(object.project, object, **options)\n        when ::Ci::Pipeline\n          instance.project_pipeline_url(object.project, object, **options)\n        when Commit\n          commit_url(object, **options)\n        when Compare\n          compare_url(object, **options)\n        when Group\n          instance.group_canonical_url(object, **options)\n        when WorkItem\n          instance.work_item_url(object, **options)\n        when Issue\n          instance.issue_url(object, **options)\n        when MergeRequest\n          instance.merge_request_url(object, **options)\n        when Milestone\n          instance.milestone_url(object, **options)\n        when Note\n          note_url(object, **options)\n        when AntiAbuse::Reports::Note\n          abuse_report_note_url(object, **options)\n        when Release\n          instance.release_url(object, **options)\n        when ::Organizations::Organization\n          instance.root_url(organization_path: object.path, **options)\n        when Project\n          instance.project_url(object, **options)\n        when Snippet\n          snippet_url(object, **options)\n        when User\n          instance.user_url(object, **options)\n        when Namespaces::UserNamespace\n          instance.user_url(object.owner, **options)\n        when Namespaces::ProjectNamespace\n          instance.project_url(object.project, **options)\n        when Wiki\n          wiki_url(object, **options)\n        when WikiPage\n          wiki_page_url(object.wiki, object, **options)\n        when WikiPage::Meta\n          wiki_page_url(object.container.wiki, object.canonical_slug, **options)\n        when ::DesignManagement::Design\n          design_url(object, **options)\n        when ::Packages::Package\n          package_url(object, **options)\n        when ::Key\n          instance.user_settings_ssh_key_url(object)\n        else\n          raise NotImplementedError, \"No URL builder defined for #{object.inspect}\"\n        end",
    "comment": "Using a case statement here is preferable for readability and maintainability. See discussion in https://gitlab.com/gitlab-org/gitlab/-/issues/217397  rubocop:disable Metrics/AbcSize rubocop:disable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "521"
  },
  {
    "raw_code": "def board_url(board, **options)\n        if board.project_board?\n          instance.project_board_url(board.resource_parent, board, **options)\n        else\n          instance.group_board_url(board.resource_parent, board, **options)\n        end",
    "comment": "rubocop:enable Metrics/AbcSize rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "522"
  },
  {
    "raw_code": "def localized_templates_table\n        [\n          ProjectTemplate.new('rails', 'Ruby on Rails', _('Includes an MVC structure, Gemfile, Rakefile, along with many others, to help you get started'), 'https://gitlab.com/gitlab-org/project-templates/rails', 'illustrations/logos/rails.svg'),\n          ProjectTemplate.new('spring', 'Spring', _('Includes an MVC structure, mvnw and pom.xml to help you get started'), 'https://gitlab.com/gitlab-org/project-templates/spring', 'illustrations/logos/spring.svg'),\n          ProjectTemplate.new('express', 'NodeJS Express', _('Includes an MVC structure to help you get started'), 'https://gitlab.com/gitlab-org/project-templates/express', 'illustrations/logos/express.svg'),\n          ProjectTemplate.new('iosswift', 'iOS (Swift)', _('A ready-to-go template for use with iOS Swift apps'), 'https://gitlab.com/gitlab-org/project-templates/iosswift', 'illustrations/logos/swift.svg'),\n          ProjectTemplate.new('dotnetcore', '.NET Core', _('A .NET Core console application template, customizable for any .NET Core project'), 'https://gitlab.com/gitlab-org/project-templates/dotnetcore', 'illustrations/third-party-logos/dotnet.svg'),\n          ProjectTemplate.new('android', 'Android', _('A ready-to-go template for use with Android apps'), 'https://gitlab.com/gitlab-org/project-templates/android', 'illustrations/logos/android.svg'),\n          ProjectTemplate.new('gomicro', 'Go Micro', _('Go Micro is a framework for micro service development'), 'https://gitlab.com/gitlab-org/project-templates/go-micro', 'illustrations/logos/gomicro.svg'),\n          ProjectTemplate.new('astro', 'Pages/Astro', _('Template for GitLab Pages and Astro. Astro is a static site generator written in JavaScript.'), 'https://gitlab.com/pages/astro', 'illustrations/third-party-logos/astro.svg'),\n          ProjectTemplate.new('docusaurus', 'Pages/Docusaurus', _('Template for GitLab Pages and Docusaurus. Docusaurus is a static site generator written in React.'), 'https://gitlab.com/pages/docusaurus'),\n          ProjectTemplate.new('hugo', 'Pages/Hugo', _('Template for GitLab Pages and Hugo. Hugo is a static site generator written in Go.'), 'https://gitlab.com/pages/hugo', 'illustrations/logos/hugo.svg'),\n          ProjectTemplate.new('jekyll', 'Pages/Jekyll', _('Template for GitLab Pages and Jekyll. Jekyll is a static site generator written in Ruby.'), 'https://gitlab.com/pages/jekyll', 'illustrations/logos/jekyll.svg'),\n          ProjectTemplate.new('nextjs', 'Pages/Next.js', _('Template for GitLab Pages and Next.js. Next.js is a React framework for building web applications.'), 'https://gitlab.com/pages/nextjs'),\n          ProjectTemplate.new('nuxt', 'Pages/Nuxt', _('Template for GitLab Pages and Nuxt. Nuxt is a Vue framework for building web applications.'), 'https://gitlab.com/pages/nuxt', 'illustrations/third-party-logos/nuxt.svg'),\n          ProjectTemplate.new('plainhtml', 'Pages/Plain HTML', _('Template for GitLab Pages using plain HTML, CSS, and JavaScript.'), 'https://gitlab.com/pages/plain-html'),\n          ProjectTemplate.new('gitpod_spring_petclinic', 'Gitpod/Spring Petclinic', _('A Gitpod configured Webapplication in Spring and Java'), 'https://gitlab.com/gitlab-org/project-templates/gitpod-spring-petclinic', 'illustrations/logos/gitpod.svg'),\n          ProjectTemplate.new('salesforcedx', 'SalesforceDX', _('A project boilerplate for Salesforce App development with Salesforce Developer tools'), 'https://gitlab.com/gitlab-org/project-templates/salesforcedx'),\n          ProjectTemplate.new('serverless_framework', 'Serverless Framework/JS', _('A basic page and serverless function that uses AWS Lambda, AWS API Gateway, and GitLab Pages'), 'https://gitlab.com/gitlab-org/project-templates/serverless-framework', 'illustrations/logos/serverless_framework.svg'),\n          ProjectTemplate.new('tencent_serverless_framework', 'Tencent Serverless Framework/NextjsSSR', _('A project boilerplate for Tencent Serverless Framework that uses Next.js SSR'), 'https://gitlab.com/gitlab-org/project-templates/nextjsssr_demo', 'illustrations/logos/tencent_serverless_framework.svg'),\n          ProjectTemplate.new('jsonnet', 'Jsonnet for Dynamic Child Pipelines', _('An example showing how to use Jsonnet with GitLab dynamic child pipelines'), 'https://gitlab.com/gitlab-org/project-templates/jsonnet'),\n          ProjectTemplate.new('cluster_management', 'GitLab Cluster Management', _('An example project for managing Kubernetes clusters integrated with GitLab'), 'https://gitlab.com/gitlab-org/project-templates/cluster-management'),\n          ProjectTemplate.new('kotlin_native_linux', 'Kotlin Native Linux', _('A basic template for developing Linux programs using Kotlin Native'), 'https://gitlab.com/gitlab-org/project-templates/kotlin-native-linux'),\n          ProjectTemplate.new('typo3_distribution', 'TYPO3 Distribution', _('A template for starting a new TYPO3 project'), 'https://gitlab.com/gitlab-org/project-templates/typo3-distribution', 'illustrations/logos/typo3.svg'),\n          ProjectTemplate.new('laravel', 'Laravel Framework', _('A basic folder structure of a Laravel application, to help you get started.'), 'https://gitlab.com/gitlab-org/project-templates/laravel', 'illustrations/logos/laravel.svg'),\n          ProjectTemplate.new('nist_80053r5', 'NIST 800-53r5', _('A project containing issues for security and privacy controls published by the U.S. National Institute of Standards and Technology'), 'https://gitlab.com/gitlab-org/project-templates/nist_80053r5'),\n          ProjectTemplate.new('gitlab_components', 'GitLab CI/CD components', _('A basic folder structure and sample files for a CI/CD components project.'), 'https://gitlab.com/gitlab-org/project-templates/gitlab-component-template')\n        ]\n      end",
    "comment": "TODO: Review child inheritance of this table (see: https://gitlab.com/gitlab-org/gitlab/-/merge_requests/41699#note_430928221)",
    "label": "",
    "id": "523"
  },
  {
    "raw_code": "def build_omniauth_customized_providers\n      %i[bitbucket jwt]\n    end",
    "comment": "We override this in EE",
    "label": "",
    "id": "524"
  },
  {
    "raw_code": "def self.leak_mem(memory_mb, duration_s)\n      start_time = Time.now\n\n      retainer = []\n      # Add `n` 1mb chunks of memory to the retainer array\n      memory_mb.times { retainer << (\"x\" * 1.megabyte) }\n\n      duration_left = [start_time + duration_s - Time.now, 0].max\n      Kernel.sleep(duration_left)\n    end",
    "comment": "leak_mem will retain the specified amount of memory and sleep. On return, the memory will be released.",
    "label": "",
    "id": "525"
  },
  {
    "raw_code": "def self.cpu_spin(duration_s)\n      return unless Gitlab::Metrics::System.thread_cpu_time\n\n      expected_end_time = Gitlab::Metrics::System.thread_cpu_time + duration_s\n\n      rand while Gitlab::Metrics::System.thread_cpu_time < expected_end_time\n    end",
    "comment": "cpu_spin will consume all CPU on a single core for the specified duration",
    "label": "",
    "id": "526"
  },
  {
    "raw_code": "def self.db_spin(duration_s, interval_s)\n      expected_end_time = Time.now + duration_s\n\n      while Time.now < expected_end_time\n        ApplicationRecord.connection.execute(\"SELECT 1\")\n\n        end_interval_time = Time.now + [duration_s, interval_s].min\n        rand while Time.now < end_interval_time\n      end",
    "comment": "db_spin will query the database in a tight loop for the specified duration",
    "label": "",
    "id": "527"
  },
  {
    "raw_code": "def self.sleep(duration_s)\n      Kernel.sleep(duration_s)\n    end",
    "comment": "sleep will sleep for the specified duration",
    "label": "",
    "id": "528"
  },
  {
    "raw_code": "def self.kill(signal)\n      Process.kill(signal, Process.pid)\n    end",
    "comment": "Kill will send the given signal to the current process.",
    "label": "",
    "id": "529"
  },
  {
    "raw_code": "def supervise(pid_or_pids, &on_process_death)\n      @pids = Array(pid_or_pids).to_set\n      @on_process_death = on_process_death\n\n      trap_signals!\n\n      start\n    end",
    "comment": "Starts a supervision loop for the given process ID(s).  If any or all processes go away, the IDs of any dead processes will be yielded to the given block, so callers can act on them.  If the block returns a non-empty list of IDs, the supervisor will start observing those processes instead. Otherwise it will shut down.",
    "label": "",
    "id": "530"
  },
  {
    "raw_code": "def shutdown(signal = :TERM)\n      return unless @alive\n\n      stop_processes(signal)\n    end",
    "comment": "Shuts down the supervisor and all supervised processes with the given signal.",
    "label": "",
    "id": "531"
  },
  {
    "raw_code": "def available_themes\n      [\n        Theme.new(3, s_('NavigationTheme|Default'), 'ui-neutral', '#ececef'),\n        Theme.new(1, s_('NavigationTheme|Indigo'), 'ui-indigo', '#222261'),\n        Theme.new(4, s_('NavigationTheme|Blue'), 'ui-blue', '#0b2640'),\n        Theme.new(5, s_('NavigationTheme|Green'), 'ui-green', '#0e4328'),\n        Theme.new(9, s_('NavigationTheme|Red'), 'ui-red', '#580d02'),\n        Theme.new(2, s_('NavigationTheme|Gray'), 'ui-gray', '#28272d')\n      ]\n    end",
    "comment": "All available Themes",
    "label": "",
    "id": "532"
  },
  {
    "raw_code": "def body_classes\n      available_themes.collect(&:css_class).uniq.join(' ')\n    end",
    "comment": "Convenience method to get a space-separated String of all the theme classes that might be applied to the `body` element  Returns a String",
    "label": "",
    "id": "533"
  },
  {
    "raw_code": "def map_deprecated_themes\n      {\n        # Light indigo to indigo\n        6 => 1,\n        # Light blue to blue\n        7 => 4,\n        # Light green to green\n        8 => 5,\n        # Light red to red\n        10 => 9\n      }\n    end",
    "comment": "Maps deprecated light themes to their default counterpart",
    "label": "",
    "id": "534"
  },
  {
    "raw_code": "def by_id(id)\n      # Map deprecated IDs to new values\n      mapped_id = map_deprecated_themes[id] || id\n\n      available_themes.detect { |t| t.id == mapped_id } || default\n    end",
    "comment": "Get a Theme by its ID  If the ID is invalid, returns the default Theme.  id - Integer ID  Returns a Theme",
    "label": "",
    "id": "535"
  },
  {
    "raw_code": "def count\n      available_themes.size\n    end",
    "comment": "Returns the number of defined Themes",
    "label": "",
    "id": "536"
  },
  {
    "raw_code": "def default\n      by_id(default_id)\n    end",
    "comment": "Get the default Theme  Returns a Theme",
    "label": "",
    "id": "537"
  },
  {
    "raw_code": "def each(&block)\n      available_themes.each(&block)\n    end",
    "comment": "Iterate through each Theme  Yields the Theme object",
    "label": "",
    "id": "538"
  },
  {
    "raw_code": "def for_user(user)\n      if user\n        by_id(user.theme_id)\n      else\n        default\n      end",
    "comment": "Get the Theme for the specified user, or the default  user - User record  Returns a Theme",
    "label": "",
    "id": "539"
  },
  {
    "raw_code": "def markup?(filename)\n      EXTENSIONS.include?(extension(filename))\n    end",
    "comment": "Public: Determines if a given filename is compatible with GitHub::Markup.  filename - Filename string to check  Returns boolean",
    "label": "",
    "id": "540"
  },
  {
    "raw_code": "def gitlab_markdown?(filename)\n      MARKDOWN_EXTENSIONS.include?(extension(filename))\n    end",
    "comment": "Public: Determines if a given filename is compatible with GitLab-flavored Markdown.  filename - Filename string to check  Returns boolean",
    "label": "",
    "id": "541"
  },
  {
    "raw_code": "def asciidoc?(filename)\n      ASCIIDOC_EXTENSIONS.include?(extension(filename))\n    end",
    "comment": "Public: Determines if the given filename has AsciiDoc extension.  filename - Filename string to check  Returns boolean",
    "label": "",
    "id": "542"
  },
  {
    "raw_code": "def plain?(filename)\n      extension(filename) == 'txt' || plain_filename?(filename)\n    end",
    "comment": "Public: Determines if the given filename is plain text.  filename - Filename string to check  Returns boolean",
    "label": "",
    "id": "543"
  },
  {
    "raw_code": "def estimate_batch_distinct_count(relation, column = nil, *args, **kwargs)\n        Gitlab::Usage::Metrics::Query.for(:estimate_batch_distinct_count, relation, column)\n      end",
    "comment": "For estimated distinct count use exact query instead of hll buckets query, because it can't be used to obtain estimations without supplementary ruby code present in Gitlab::Database::PostgresHll::BatchDistinctCounter",
    "label": "",
    "id": "544"
  },
  {
    "raw_code": "def initialize(path_type)\n        @path_type = path_type\n      end",
    "comment": "@params path_type [symbol] type of path to do \"-\" redirection https://gitlab.com/gitlab-org/gitlab/-/issues/16854",
    "label": "",
    "id": "545"
  },
  {
    "raw_code": "def self.url_helpers\n      @url_helpers ||= Gitlab::Application.routes.url_helpers\n    end",
    "comment": "Returns the URL helpers Module.  This method caches the output as Rails' \"url_helpers\" method creates an anonymous module every time it's called.  Returns a Module.",
    "label": "",
    "id": "546"
  },
  {
    "raw_code": "def summarize\n      return [] if offset < 0\n\n      commits_hsh = fetch_last_cached_commits_list\n      prerender_commit_full_titles!(commits_hsh.values)\n\n      commits_hsh.map do |path_key, commit|\n        commit = cache_commit(commit)\n\n        {\n          file_name: File.basename(path_key).force_encoding(Encoding::UTF_8),\n          commit: commit,\n          commit_path: commit_path(commit),\n          commit_title_html: markdown_field(commit, :full_title)\n        }\n      end",
    "comment": "Creates a summary of the tree entries for a commit, within the window of entries defined by the offset and limit parameters. This consists of two return values:  - An Array of Hashes containing the following keys: - file_name:   The full path of the tree entry - commit:      The last ::Commit to touch this entry in the tree - commit_path: URI of the commit in the web interface - commit_title_html: Rendered commit title",
    "label": "",
    "id": "547"
  },
  {
    "raw_code": "def ensured_path\n      File.join(*[path, \"\"]) if path\n    end",
    "comment": "Ensure the path is in \"path/\" format",
    "label": "",
    "id": "548"
  },
  {
    "raw_code": "def self.create_channel(storage)\n      @channels ||= {}\n      @channels[storage] ||= GRPC::ClientStub.setup_channel(\n        nil, stub_address(storage), stub_creds(storage), channel_args\n      )\n    end",
    "comment": "Cache gRPC servers by storage. All the client stubs in the same process can share the underlying connection to the same host thanks to HTTP2 framing protocol that gRPC is built on top. This method is not thread-safe. It is intended to be a part of `stub`, method behind a mutex protection.",
    "label": "",
    "id": "549"
  },
  {
    "raw_code": "def self.call(storage, service, rpc, request, remote_storage: nil, timeout: default_timeout, gitaly_context: {}, &block)\n      Gitlab::GitalyClient::Call.new(storage, service, rpc, request, remote_storage, timeout, gitaly_context: gitaly_context).call(&block)\n    end",
    "comment": "All Gitaly RPC call sites should use GitalyClient.call. This method makes sure that per-request authentication headers are set.  This method optionally takes a block which receives the keyword arguments hash 'kwargs' that will be passed to gRPC. This allows the caller to modify or augment the keyword arguments. The block must return a hash.  For example:  GitalyClient.call(storage, service, rpc, request) do |kwargs| kwargs.merge(deadline: Time.now + 10) end  The optional remote_storage keyword argument is used to enable inter-gitaly calls. Say you have an RPC that needs to pull data from one repository to another. For example, to fetch a branch from a (non-deduplicated) fork into the fork parent. In that case you would send an RPC call to the Gitaly server hosting the fork parent, and in the request, you would tell that Gitaly server to pull Git data from the fork. How does that Gitaly server connect to the Gitaly server the forked repo lives on? This is the problem `remote_storage:` solves: it adds address and authentication information to the call, as gRPC metadata (under the `gitaly-servers` header). The request would say \"pull from repo X on gitaly-2\". In the Ruby code you pass `remote_storage: 'gitaly-2'`. And then the metadata would say \"gitaly-2 is at network address tcp://10.0.1.2:8075\". ",
    "label": "",
    "id": "550"
  },
  {
    "raw_code": "def self.real_time\n      Time.at(Process.clock_gettime(Process::CLOCK_REALTIME))\n    end",
    "comment": "For some time related tasks we can't rely on `Time.now` since it will be affected by Timecop in some tests, and the clock of some gitaly-related components (grpc's c-core and gitaly server) use system time instead of timecop's time, so tests will fail. `Time.at(Process.clock_gettime(Process::CLOCK_REALTIME))` will circumvent timecop.",
    "label": "",
    "id": "551"
  },
  {
    "raw_code": "def self.fetch_relative_path\n      return unless Gitlab::SafeRequestStore.active?\n      return if Gitlab::SafeRequestStore[:gitlab_git_relative_path].blank?\n\n      Gitlab::SafeRequestStore.fetch(:gitlab_git_relative_path)\n    end",
    "comment": "The GitLab `internal/allowed/` API sets the :gitlab_git_relative_path variable. This provides the repository relative path which can be used to locate snapshot repositories in Gitaly which act as a quarantine repository until a transaction is committed.",
    "label": "",
    "id": "552"
  },
  {
    "raw_code": "def self.route_to_primary\n      return {} unless Gitlab::SafeRequestStore.active?\n\n      return {} if Gitlab::SafeRequestStore[:gitlab_git_env].blank?\n\n      { 'gitaly-route-repository-accessor-policy' => 'primary-only' }\n    end",
    "comment": "Gitlab::Git::HookEnv will set the :gitlab_git_env variable in case we're running in the context of a Gitaly hook call, which may make use of quarantined object directories. We thus need to pass along the path of the quarantined object directory to Gitaly, otherwise it won't be able to find these quarantined objects. Given that the quarantine directory is generated with a random name, they'll have different names when multiple Gitaly nodes take part in a single transaction. As a result, we are forced to route all requests to the primary node which has injected the quarantine object directory to us.",
    "label": "",
    "id": "553"
  },
  {
    "raw_code": "def self.enforce_gitaly_request_limits(call_site)\n      # Only count limits in request-response environments\n      return unless Gitlab::SafeRequestStore.active?\n\n      # This is this actual number of times this call was made. Used for information purposes only\n      actual_call_count = increment_call_count(\"gitaly_#{call_site}_actual\")\n\n      return unless enforce_gitaly_request_limits?\n\n      # Check if this call is nested within a allow_n_plus_1_calls\n      # block and skip check if it is\n      return if get_call_count(:gitaly_call_count_exception_block_depth) > 0\n\n      # This is the count of calls outside of a `allow_n_plus_1_calls` block\n      # It is used for enforcement but not statistics\n      permitted_call_count = increment_call_count(\"gitaly_#{call_site}_permitted\")\n\n      count_stack\n\n      return if permitted_call_count <= MAXIMUM_GITALY_CALLS\n\n      raise TooManyInvocationsError.new(call_site, actual_call_count, max_call_count, max_stacks)\n    end",
    "comment": "Ensures that Gitaly is not being abuse through n+1 misuse etc",
    "label": "",
    "id": "554"
  },
  {
    "raw_code": "def self.allow_ref_name_caching\n      return yield unless Gitlab::SafeRequestStore.active?\n      return yield if ref_name_caching_allowed?\n\n      begin\n        Gitlab::SafeRequestStore[:allow_ref_name_caching] = true\n        yield\n      ensure\n        Gitlab::SafeRequestStore[:allow_ref_name_caching] = false\n      end",
    "comment": "Normally a FindCommit RPC will cache the commit with its SHA instead of a ref name, since it's possible the branch is mutated afterwards. However, for read-only requests that never mutate the branch, this method allows caching of the ref name directly.",
    "label": "",
    "id": "555"
  },
  {
    "raw_code": "def self.get_request_count\n      get_call_count(\"gitaly_call_actual\")\n    end",
    "comment": "Returns the of the number of Gitaly calls made for this request",
    "label": "",
    "id": "556"
  },
  {
    "raw_code": "def self.default_timeout\n      timeout(:gitaly_timeout_default)\n    end",
    "comment": "The default timeout on all Gitaly calls",
    "label": "",
    "id": "557"
  },
  {
    "raw_code": "def self.count_stack\n      return unless Gitlab::SafeRequestStore.active?\n\n      stack_string = Gitlab::BacktraceCleaner.clean_backtrace(caller).drop(1).join(\"\\n\")\n\n      Gitlab::SafeRequestStore[:stack_counter] ||= {}\n\n      count = Gitlab::SafeRequestStore[:stack_counter][stack_string] || 0\n      Gitlab::SafeRequestStore[:stack_counter][stack_string] = count + 1\n    end",
    "comment": "Count a stack. Used for n+1 detection",
    "label": "",
    "id": "558"
  },
  {
    "raw_code": "def self.max_call_count\n      return 0 unless Gitlab::SafeRequestStore.active?\n\n      stack_counter = Gitlab::SafeRequestStore[:stack_counter]\n      return 0 unless stack_counter\n\n      stack_counter.values.max\n    end",
    "comment": "Returns a count for the stack which called Gitaly the most times. Used for n+1 detection",
    "label": "",
    "id": "559"
  },
  {
    "raw_code": "def self.max_stacks\n      return unless Gitlab::SafeRequestStore.active?\n\n      stack_counter = Gitlab::SafeRequestStore[:stack_counter]\n      return unless stack_counter\n\n      max = max_call_count\n      return if max == 0\n\n      stack_counter.select { |_, v| v == max }.keys\n    end",
    "comment": "Returns the stacks that calls Gitaly the most times. Used for n+1 detection",
    "label": "",
    "id": "560"
  },
  {
    "raw_code": "def self.unwrap_detailed_error(err)\n      e = decode_detailed_error(err)\n\n      return e if e.nil? || !e.respond_to?(:error) || e.error.nil? || !e.error.respond_to?(:to_s)\n\n      unwrapped_error = e[e.error.to_s]\n\n      unwrapped_error || e\n    end",
    "comment": "This method attempts to unwrap a detailed error from a Gitaly RPC error. It first decodes the detailed error using decode_detailed_error. If successful, it tries to extract the unwrapped error by calling the method named by the error attribute on the decoded error object.",
    "label": "",
    "id": "561"
  },
  {
    "raw_code": "def set_merge_commit(child_commit:)\n        @merge_commit ||= direct_ancestor? ? self : child_commit.merge_commit\n      end",
    "comment": "@param child_commit [CommitDecorator] @param first_parent [Boolean] whether `self` is the first parent of `child_commit`",
    "label": "",
    "id": "562"
  },
  {
    "raw_code": "def initialize(commits, relevant_commit_ids: nil)\n      @commits = commits\n      @id_to_commit = {}\n      @commits.each do |commit|\n        @id_to_commit[commit.id] = CommitDecorator.new(commit)\n\n        if relevant_commit_ids\n          relevant_commit_ids.delete(commit.id)\n          break if relevant_commit_ids.empty? # Only limit the analyze up to relevant_commit_ids\n        end",
    "comment": "@param commits [Array] list of commits, must be ordered from the child (tip) of the graph back to the ancestors",
    "label": "",
    "id": "563"
  },
  {
    "raw_code": "def mark_all_direct_ancestors(commit)\n      loop do\n        commit = get_commit(commit.parent_ids.first)\n\n        break unless commit\n\n        commit.direct_ancestor = true\n      end",
    "comment": "Mark all direct ancestors. If child commit is a direct ancestor, its first parent is also a direct ancestor. We assume direct ancestors matches the trail of the target branch over time, This assumption is correct most of the time, especially for gitlab managed merges, but there are exception cases which can't be solved.",
    "label": "",
    "id": "564"
  },
  {
    "raw_code": "def configure!(store)\n        ###\n        # Add subscriptions here:\n\n        store.subscribe ::MergeRequests::UpdateHeadPipelineWorker, to: ::Ci::PipelineCreatedEvent\n        # Currently it is used only for DuoWorkflows::Workflow. DuoWorkflows::Workflow, pipeline can never be in manual.\n        # That's why a constraint on manual is added. In future if this needs to be used at other places where manual\n        # needs to be considered, then remove the if block and just verify that DuoWorkflows::Workflow is working fine.\n        # Mostly it will be fine.\n        store.subscribe ::Ci::Workloads::UpdateWorkloadStatusEventWorker, to: ::Ci::PipelineFinishedEvent,\n          if: ->(event) { event.data[:status] != 'manual' }\n        store.subscribe ::Namespaces::UpdateRootStatisticsWorker, to: ::Projects::ProjectDeletedEvent\n\n        store.subscribe ::MergeRequests::ProcessAutoMergeFromEventWorker,\n          to: ::MergeRequests::AutoMerge::TitleDescriptionUpdateEvent\n        store.subscribe ::MergeRequests::ProcessAutoMergeFromEventWorker, to: ::MergeRequests::DraftStateChangeEvent\n        store.subscribe ::MergeRequests::ProcessAutoMergeFromEventWorker, to: ::MergeRequests::DiscussionsResolvedEvent\n        store.subscribe ::MergeRequests::ProcessAutoMergeFromEventWorker, to: ::MergeRequests::MergeableEvent\n        store.subscribe ::MergeRequests::CreateApprovalEventWorker, to: ::MergeRequests::ApprovedEvent\n        store.subscribe ::MergeRequests::CreateApprovalNoteWorker, to: ::MergeRequests::ApprovedEvent\n        store.subscribe ::MergeRequests::ResolveTodosAfterApprovalWorker, to: ::MergeRequests::ApprovedEvent\n        store.subscribe ::MergeRequests::ExecuteApprovalHooksWorker, to: ::MergeRequests::ApprovedEvent\n        store.subscribe ::Ml::ExperimentTracking::AssociateMlCandidateToPackageWorker,\n          to: ::Packages::PackageCreatedEvent,\n          if: ->(event) { ::Ml::ExperimentTracking::AssociateMlCandidateToPackageWorker.handles_event?(event) }\n        store.subscribe ::Ci::InitializePipelinesIidSequenceWorker, to: ::Projects::ProjectCreatedEvent\n        store.subscribe ::Pages::DeletePagesDeploymentWorker, to: ::Projects::ProjectArchivedEvent\n        store.subscribe ::Pages::ResetPagesDefaultDomainRedirectWorker, to: ::Pages::Domains::PagesDomainDeletedEvent\n        store.subscribe ::MergeRequests::ProcessDraftNotePublishedWorker, to: ::MergeRequests::DraftNotePublishedEvent\n\n        subscribe_to_member_destroyed_events(store)\n        subscribe_to_namespace_transfered_events(store)\n      end",
    "comment": "Define all event subscriptions using:  store.subscribe(DomainA::SomeWorker, to: DomainB::SomeEvent)  It is possible to subscribe to a subset of events matching a condition:  store.subscribe(DomainA::SomeWorker, to: DomainB::SomeEvent), if: ->(event) { event.data == :some_value } ",
    "label": "",
    "id": "565"
  },
  {
    "raw_code": "def detect_libgit2_binary?(data)\n      detect = detect_encoding(data, limit: 8000)\n      detect && detect[:type] == :binary\n    end",
    "comment": "EncodingDetector checks the first 1024 * 1024 bytes for NUL byte, libgit2 checks only the first 8000 (https://github.com/libgit2/libgit2/blob/2ed855a9e8f9af211e7274021c2264e600c0f86b/src/filter.h#L15), which is what we use below to keep a consistent behavior.",
    "label": "",
    "id": "566"
  },
  {
    "raw_code": "def encode_utf8_no_detect(message)\n      message = force_encode_utf8(message)\n      return message if message.valid_encoding?\n\n      message.encode(Encoding::UTF_8, invalid: :replace, undef: :replace)\n    end",
    "comment": "This is like encode_utf8 except we skip autodetection of the encoding. We assume the data must be interpreted as UTF-8.",
    "label": "",
    "id": "567"
  },
  {
    "raw_code": "def encode_utf8_with_escaping!(message)\n      message = force_encode_utf8(message)\n      return message if message.valid_encoding?\n\n      unless message.valid_encoding?\n        message = message.chars.map { |char| char.valid_encoding? ? char : escape_chars(char) }.join\n      end",
    "comment": "This method escapes unsupported UTF-8 characters instead of deleting them",
    "label": "",
    "id": "568"
  },
  {
    "raw_code": "def unquote_path(filename)\n      return filename unless filename[0] == '\"'\n\n      filename = filename[1..-2].gsub(/\\\\(?:([#{ESCAPED_CHARS.keys.join}\\\\])|(\\d{3}))/) do\n        if c = Regexp.last_match(1)\n          c == \"\\\\\" ? \"\\\\\" : ESCAPED_CHARS[c]\n        elsif c = Regexp.last_match(2)\n          c.to_i(8).chr\n        end",
    "comment": "rubocop:disable Style/AsciiComments `unquote_path` decode filepaths that are returned by some git commands. The path may be returned in double-quotes if it contains special characters, that are encoded in octal. Also, some characters (see `ESCAPED_CHARS`) are escaped. eg. \"\\311\\240\\304\\253\\305\\247\\305\\200\\310\\247\\306\\200\" (quotes included) is decoded as   Based on `unquote_c_style` from git source https://github.com/git/git/blob/v2.35.1/quote.c#L399 rubocop:enable Style/AsciiComments",
    "label": "",
    "id": "569"
  },
  {
    "raw_code": "def escape_chars(char)\n      bytes = char.bytes\n\n      return char unless bytes.one?\n\n      \"%#{bytes.first.to_s(16).upcase}\"\n    end",
    "comment": "Escapes \\x80 - \\xFF characters not supported by UTF-8",
    "label": "",
    "id": "570"
  },
  {
    "raw_code": "def escape_unicode(char)\n      \"\\\\u#{char.bytes.map { |i| i.to_s(16).rjust(4, '0') }.join}\"\n    end",
    "comment": "Escapes characters to unicode",
    "label": "",
    "id": "571"
  },
  {
    "raw_code": "def self.enabled_for_env?\n      Rails.env.development? || Rails.env.test?\n    end",
    "comment": "Returns true if we should enable tracking of query counts.  This is only enabled in development and test to ensure we don't produce any errors that users of other environments can't do anything about themselves.",
    "label": "",
    "id": "572"
  },
  {
    "raw_code": "def self.with_suppressed\n      prev_value = @suppressed\n      @suppressed = true\n\n      yield\n    ensure\n      @suppressed = prev_value\n    end",
    "comment": "Skips the query limiting middleware for all requests within the block",
    "label": "",
    "id": "573"
  },
  {
    "raw_code": "def self.disable!(issue_url, new_threshold: 200)\n      raise ArgumentError, 'new_threshold cannot exceed 2_000' unless new_threshold < 2_000\n\n      unless issue_url.start_with?('https://')\n        raise(\n          ArgumentError,\n          'You must provide a valid issue URL in order to allow a block of code'\n        )\n      end",
    "comment": "Allows the current request to execute a higher number of SQL queries.  This method should _only_ be used when there's a corresponding issue to reduce the number of queries.  The issue URL is only meant to push developers into creating an issue instead of blindly disabling for offending blocks of code.  The new_threshold is so that we don't allow unlimited number of SQL queries while the issue is being fixed.",
    "label": "",
    "id": "574"
  },
  {
    "raw_code": "def self.enable!\n      Gitlab::SafeRequestStore[:query_limiting_override_threshold] = nil\n    end",
    "comment": "Enables query limiting for the request.",
    "label": "",
    "id": "575"
  },
  {
    "raw_code": "def oci_repository_path_regex\n      @oci_repository_path_regex ||= %r{\\A[a-zA-Z0-9]+([._-][a-zA-Z0-9]+)*\\z}\n    end",
    "comment": "Project path must conform to this regex. See https://gitlab.com/gitlab-org/gitlab/-/issues/27483",
    "label": "",
    "id": "576"
  },
  {
    "raw_code": "def container_repository_name_regex(other_accepted_chars = nil)\n      strong_memoize_with(:container_repository_name_regex, other_accepted_chars) do\n        %r{\\A[a-z0-9]+(([._/]|__|-*)[a-z0-9#{other_accepted_chars}])*\\z}\n      end",
    "comment": " Docker Distribution Registry repository / tag name rules  See https://github.com/docker/distribution/blob/master/reference/regexp.go. ",
    "label": "",
    "id": "577"
  },
  {
    "raw_code": "def container_registry_tag_regex\n      @container_registry_tag_regex ||= /\\w[\\w.-]{0,127}/\n    end",
    "comment": " We do not use regexp anchors here because these are not allowed when used as a routing constraint. ",
    "label": "",
    "id": "578"
  },
  {
    "raw_code": "def cluster_agent_name_regex\n      /\\A[a-z0-9]([-a-z0-9]*[a-z0-9])?\\z/\n    end",
    "comment": "https://gitlab.com/gitlab-org/cluster-integration/gitlab-agent/-/blob/master/doc/identity_and_auth.md#agent-identity-and-name",
    "label": "",
    "id": "579"
  },
  {
    "raw_code": "def kubernetes_dns_subdomain_regex\n      /\\A[a-z0-9]([a-z0-9\\-\\.]*[a-z0-9])?\\z/\n    end",
    "comment": "Pod name adheres to DNS Subdomain Names(RFC 1123) naming convention https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names",
    "label": "",
    "id": "580"
  },
  {
    "raw_code": "def logs_section_prefix_regex\n      /section_((?:start)|(?:end)):(\\d+):([a-zA-Z0-9_.-]+)/\n    end",
    "comment": "The section start, e.g. section_start:12345678:NAME",
    "label": "",
    "id": "581"
  },
  {
    "raw_code": "def logs_section_options_regex\n      /(\\[(?:\\w+=\\w+)(?:, ?(?:\\w+=\\w+))*\\])?/\n    end",
    "comment": "The optional section options, e.g. [collapsed=true]",
    "label": "",
    "id": "582"
  },
  {
    "raw_code": "def logs_section_suffix_regex\n      /\\r\\033\\[0K/\n    end",
    "comment": "The region end, always: \\r\\e\\[0K",
    "label": "",
    "id": "583"
  },
  {
    "raw_code": "def jira_issue_key_regex(expression_escape: '\\b')\n      /#{expression_escape}([A-Z][A-Z_0-9]+-\\d+)/\n    end",
    "comment": "Based on Jira's project key format https://confluence.atlassian.com/adminjiraserver073/changing-the-project-key-format-861253229.html",
    "label": "",
    "id": "584"
  },
  {
    "raw_code": "def aws_account_id_regex\n      /\\A\\d{12}\\z/\n    end",
    "comment": "https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html",
    "label": "",
    "id": "585"
  },
  {
    "raw_code": "def aws_arn_regex\n      /\\Aarn:\\S+\\z/\n    end",
    "comment": "https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html",
    "label": "",
    "id": "586"
  },
  {
    "raw_code": "def sep_by_1(separator, part)\n      %r{#{part} (#{separator} #{part})*}x\n    end",
    "comment": "One or more `part`s, separated by separator",
    "label": "",
    "id": "587"
  },
  {
    "raw_code": "def track_and_raise_exception(exception, extra = {}, tags = {})\n        process_exception(exception, extra: extra, tags: tags)\n\n        raise exception\n      end",
    "comment": "This should be used when you want to passthrough exception handling: rescue and raise to be caught in upper layers of the application.  If the exception implements the method `sentry_extra_data` and that method returns a Hash, then the return value of that method will be merged into `extra`. Exceptions can use this mechanism to provide structured data to sentry in addition to their message and back-trace.",
    "label": "",
    "id": "588"
  },
  {
    "raw_code": "def track_and_raise_for_dev_exception(exception, extra = {}, tags = {})\n        process_exception(exception, extra: extra, tags: tags)\n\n        raise exception if should_raise_for_dev?\n      end",
    "comment": "This can be used for investigating exceptions that can be recovered from in code. The exception will still be raised in development and test environments.  That way we can track down these exceptions with as much information as we need to resolve them.  If the exception implements the method `sentry_extra_data` and that method returns a Hash, then the return value of that method will be merged into `extra`. Exceptions can use this mechanism to provide structured data to sentry in addition to their message and back-trace.  Provide an issue URL for follow up. as `issue_url: 'http://gitlab.com/gitlab-org/gitlab/issues/111'`",
    "label": "",
    "id": "589"
  },
  {
    "raw_code": "def track_exception(exception, extra = {}, tags = {})\n        process_exception(exception, extra: extra, tags: tags)\n      end",
    "comment": "This should be used when you want to track the exception and not raise with the default trackers (Sentry and Logger).  If the exception implements the method `sentry_extra_data` and that method returns a Hash, then the return value of that method will be merged into `extra`. Exceptions can use this mechanism to provide structured data to sentry in addition to their message and back-trace.",
    "label": "",
    "id": "590"
  },
  {
    "raw_code": "def log_exception(exception, extra = {})\n        process_exception(exception, extra: extra, trackers: [Logger])\n      end",
    "comment": "This should be used when you only want to log the exception, but not send it to Sentry or raise.  If the exception implements the method `sentry_extra_data` and that method returns a Hash, then the return value of that method will be merged into `extra`. Exceptions can use this mechanism to provide structured data to sentry in addition to their message and back-trace.",
    "label": "",
    "id": "591"
  },
  {
    "raw_code": "def log_and_raise_exception(exception, extra = {})\n        process_exception(exception, extra: extra, trackers: [Logger])\n\n        raise exception\n      end",
    "comment": "This should be used when you want to log the exception and passthrough exception handling: rescue and raise to be catched in upper layers of the application.  If the exception implements the method `sentry_extra_data` and that method returns a Hash, then the return value of that method will be merged into `extra`. Exceptions can use this mechanism to provide structured data to sentry in addition to their message and back-trace.",
    "label": "",
    "id": "592"
  },
  {
    "raw_code": "def sentry_dsn\n        env_sentry_dsn || database_sentry_dsn\n      end",
    "comment": "Some configuration attributes like `dsn`, and `environment` can be configured both via `ENV` and `Application Settings`. The reason being is while GitLab.com uses application_settings in Geo installations, we can't override values in the primary database. Setting this value in application_settings would propagate the value to all Geo nodes, which doesn't solve that particular problem.",
    "label": "",
    "id": "593"
  },
  {
    "raw_code": "def custom_fingerprinting(event, ex)\n        return event unless CUSTOM_FINGERPRINTING.include?(ex.class.name)\n\n        event.fingerprint = [ex.class.name, ex.message]\n      end",
    "comment": "Group common, mostly non-actionable exceptions by type and message, rather than cause",
    "label": "",
    "id": "594"
  },
  {
    "raw_code": "def self.render(file_name, input, context)\n      html = render_markup(file_name, input, context).force_encoding(input.encoding)\n\n      context[:pipeline] ||= :markup\n\n      html = Banzai.render(html, context)\n      html.html_safe\n    end",
    "comment": "Public: Converts the provided markup into HTML.  input         - the source text in a markup format ",
    "label": "",
    "id": "595"
  },
  {
    "raw_code": "def insertions(programming_languages)\n      lang_to_id = programming_languages.to_h { |p| [p.name, p.id] }\n\n      (languages - previous_language_names).map do |new_lang|\n        {\n          project_id: @repository.project.id,\n          share: detection[new_lang][:value],\n          programming_language_id: lang_to_id[new_lang]\n        }\n      end",
    "comment": "Newly detected languages, returned in a structure accepted by ApplicationRecord.legacy_bulk_insert",
    "label": "",
    "id": "596"
  },
  {
    "raw_code": "def updates\n      to_update = @repository_languages.select do |lang|\n        detection.key?(lang.name) && detection[lang.name][:value] != lang.share\n      end",
    "comment": "updates analyses which records only require updating of their share",
    "label": "",
    "id": "597"
  },
  {
    "raw_code": "def deletions\n      @repository_languages.filter_map do |repo_lang|\n        next if detection.key?(repo_lang.name)\n\n        repo_lang.programming_language_id\n      end",
    "comment": "Returns the ids of the programming languages that do not occur in the detection as current repository languages",
    "label": "",
    "id": "598"
  },
  {
    "raw_code": "def all_queues_yml_outdated?\n        foss_workers, ee_workers, jh_workers = workers_for_all_queues_yml\n\n        return true if foss_workers != YAML.load_file(FOSS_QUEUE_CONFIG_PATH)\n\n        return true if Gitlab.ee? && ee_workers != YAML.load_file(EE_QUEUE_CONFIG_PATH)\n\n        Gitlab.jh? && File.exist?(JH_QUEUE_CONFIG_PATH) && jh_workers != YAML.load_file(JH_QUEUE_CONFIG_PATH)\n      end",
    "comment": "YAML.load_file is OK here as we control the file contents",
    "label": "",
    "id": "599"
  },
  {
    "raw_code": "def jh_queues_for_sidekiq_queues_yml\n        []\n      end",
    "comment": "Override in JH repo",
    "label": "",
    "id": "600"
  },
  {
    "raw_code": "def sidekiq_queues_yml_outdated?\n        config_queues = YAML.load_file(SIDEKIQ_QUEUES_PATH)[:queues]\n\n        queues_for_sidekiq_queues_yml != config_queues\n      end",
    "comment": "YAML.load_file is OK here as we control the file contents",
    "label": "",
    "id": "601"
  },
  {
    "raw_code": "def worker_queue_mappings\n        workers\n          .reject { |worker| worker.klass.is_a?(Gitlab::SidekiqConfig::DummyWorker) }\n          .to_h { |worker| [worker.klass.to_s, ::Gitlab::SidekiqConfig::WorkerRouter.global.route(worker.klass)] }\n      end",
    "comment": "Returns a hash of worker class name => mapped queue name",
    "label": "",
    "id": "602"
  },
  {
    "raw_code": "def current_worker_queue_mappings\n        worker_queue_mappings\n          .select { |worker, queue| Sidekiq.default_configuration.queues.include?(queue) }\n          .to_h\n      end",
    "comment": "Like worker_queue_mappings, but only for the queues running in the current Sidekiq process",
    "label": "",
    "id": "603"
  },
  {
    "raw_code": "def routing_queues\n        @routing_queues ||= workers.map do |worker|\n          if worker.klass.is_a?(Gitlab::SidekiqConfig::DummyWorker)\n            worker.queue\n          else\n            ::Gitlab::SidekiqConfig::WorkerRouter.global.route(worker.klass)\n          end",
    "comment": "Get the list of queues from all available workers following queue routing rules. Sidekiq::Queue.all fetches the list of queues from Redis. It may contain some redundant, obsolete queues from previous iterations of GitLab.",
    "label": "",
    "id": "604"
  },
  {
    "raw_code": "def initialize(ancestors_base, descendants_base = ancestors_base, options: {})\n      raise ArgumentError, \"Model of ancestors_base does not match model of descendants_base\" if ancestors_base.model != descendants_base.model\n\n      @ancestors_base = ancestors_base\n      @descendants_base = descendants_base\n      @model = ancestors_base.model\n      @unscoped_model = @model.unscoped\n      @options = options\n    end",
    "comment": "ancestors_base - An instance of ActiveRecord::Relation for which to get parent objects. descendants_base - An instance of ActiveRecord::Relation for which to get child objects. If omitted, ancestors_base is used.",
    "label": "",
    "id": "605"
  },
  {
    "raw_code": "def descendants\n      base_and_descendants.id_not_in(descendants_base.select(:id))\n    end",
    "comment": "Returns the set of descendants of a given relation, but excluding the given relation",
    "label": "",
    "id": "606"
  },
  {
    "raw_code": "def max_descendants_depth\n      base_and_descendants(with_depth: true).maximum(DEPTH_COLUMN)\n    end",
    "comment": "Returns the maximum depth starting from the base A base object with no children has a maximum depth of `1`",
    "label": "",
    "id": "607"
  },
  {
    "raw_code": "def ancestors(upto: nil, hierarchy_order: nil)\n      base_and_ancestors(upto: upto, hierarchy_order: hierarchy_order).id_not_in(ancestors_base.select(:id))\n    end",
    "comment": "Returns the set of ancestors of a given relation, but excluding the given relation  Passing an `upto` will stop the recursion once the specified parent_id is reached. So all ancestors *lower* than the specified ancestor will be included.",
    "label": "",
    "id": "608"
  },
  {
    "raw_code": "def base_and_ancestors(upto: nil, hierarchy_order: nil)\n      upto_id = upto.try(:id) || upto\n      cte = base_and_ancestors_cte(upto_id, hierarchy_order)\n\n      recursive_query = if hierarchy_order\n                          # othewise depth won't be available for outer query\n                          cte.apply_to(unscoped_model.all.select(objects_table[Arel.star])).order(depth: hierarchy_order)\n                        else\n                          cte.apply_to(unscoped_model.all)\n                        end",
    "comment": "Returns a relation that includes the ancestors_base set of objects and all their ancestors (recursively).  Passing an `upto` will stop the recursion once the specified parent_id is reached. So all ancestors *lower* than the specified ancestor will be included.  Passing a `hierarchy_order` with either `:asc` or `:desc` will cause the recursive query order from most nested object to root or from the root ancestor to most nested object respectively. This uses a `depth` column where `1` is defined as the depth for the base and increment as we go up each parent.  Note: By default the order is breadth-first rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "609"
  },
  {
    "raw_code": "def base_and_descendants(with_depth: false)\n      outer_select_relation = unscoped_model.all\n      outer_select_relation = outer_select_relation.select(objects_table[Arel.star]) if with_depth # Otherwise Active Record will not select `depth` as it's not a table column\n\n      read_only(base_and_descendants_cte(with_depth: with_depth).apply_to(outer_select_relation))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Returns a relation that includes the descendants_base set of objects and all their descendants (recursively).  When `with_depth` is `true`, a `depth` column is included where it starts with `1` for the base objects and incremented as we go down the descendant tree",
    "label": "",
    "id": "610"
  },
  {
    "raw_code": "def base_and_descendant_ids\n      read_only(base_and_descendant_ids_cte.apply_to(unscoped_model.select(objects_table[:id])))\n    end",
    "comment": "Returns a relation that includes ID of the descendants_base set of objects and all their descendants IDs (recursively).",
    "label": "",
    "id": "611"
  },
  {
    "raw_code": "def all_objects\n      ancestors = base_and_ancestors_cte\n      descendants = base_and_descendants_cte\n\n      ancestors_table = ancestors.alias_to(objects_table)\n      descendants_table = descendants.alias_to(objects_table)\n\n      ancestors_scope = unscoped_model.from(ancestors_table)\n      descendants_scope = unscoped_model.from(descendants_table)\n\n      relation = unscoped_model\n        .with\n        .recursive(ancestors.to_arel, descendants.to_arel)\n        .from_union([\n          ancestors_scope,\n          descendants_scope\n        ])\n\n      read_only(relation)\n    end",
    "comment": "Returns a relation that includes the base objects, their ancestors, and the descendants of the base objects.  The resulting query will roughly look like the following:  WITH RECURSIVE ancestors AS ( ... ), descendants AS ( ... ) SELECT * FROM ( SELECT * FROM ancestors namespaces  UNION  SELECT * FROM descendants namespaces ) groups;  Using this approach allows us to further add criteria to the relation with Rails thinking it's selecting data the usual way.  If nested objects are not supported, ancestors_base is returned. rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "612"
  },
  {
    "raw_code": "def remove_depth_and_maintain_order(relation, hierarchy_order: :asc)\n      joined_relation = model.joins(\"INNER JOIN (#{relation.select(:id, :depth).to_sql}) namespaces_join_table on namespaces_join_table.id = #{model.table_name}.id\").order(\"namespaces_join_table.depth\" => hierarchy_order)\n\n      model.from(Arel::Nodes::As.new(joined_relation.arel, objects_table))\n    end",
    "comment": "Remove the extra `depth` field using an INNER JOIN to avoid breaking UNION queries and ordering the rows based on the `depth` column to maintain the row order.  rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "613"
  },
  {
    "raw_code": "def base_and_ancestors_cte(stop_id = nil, hierarchy_order = nil)\n      cte = SQL::RecursiveCTE.new(:base_and_ancestors)\n\n      base_query = ancestors_base.except(:order)\n      base_query = base_query.select(\"1 as #{DEPTH_COLUMN}\", \"ARRAY[#{objects_table.name}.id] AS tree_path\", \"false AS tree_cycle\", base_query.default_select_columns) if hierarchy_order\n\n      cte << base_query\n\n      # Recursively get all the ancestors of the base set.\n      parent_query = unscoped_model\n        .from(from_tables(cte))\n        .where(ancestor_conditions(cte))\n        .except(:order)\n\n      if hierarchy_order\n        quoted_objects_table_name = model.connection.quote_table_name(objects_table.name)\n\n        parent_query = parent_query.select(\n          cte.table[DEPTH_COLUMN] + 1,\n          \"tree_path || #{quoted_objects_table_name}.id\",\n          \"#{quoted_objects_table_name}.id = ANY(tree_path)\",\n          parent_query.default_select_columns\n        ).where(cte.table[:tree_cycle].eq(false))\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "614"
  },
  {
    "raw_code": "def base_and_descendants_cte(with_depth: false)\n      cte = SQL::RecursiveCTE.new(:base_and_descendants)\n\n      base_query = descendants_base.except(:order)\n      base_query = base_query.select(\"1 AS #{DEPTH_COLUMN}\", \"ARRAY[#{objects_table.name}.id] AS tree_path\", \"false AS tree_cycle\", base_query.default_select_columns) if with_depth\n\n      cte << base_query\n\n      # Recursively get all the descendants of the base set.\n      descendants_query = unscoped_model\n        .from(from_tables(cte))\n        .where(descendant_conditions(cte))\n        .except(:order)\n\n      if with_depth\n        quoted_objects_table_name = model.connection.quote_table_name(objects_table.name)\n\n        descendants_query = descendants_query.select(\n          cte.table[DEPTH_COLUMN] + 1,\n          \"tree_path || #{quoted_objects_table_name}.id\",\n          \"#{quoted_objects_table_name}.id = ANY(tree_path)\",\n          descendants_query.default_select_columns\n        ).where(cte.table[:tree_cycle].eq(false))\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "615"
  },
  {
    "raw_code": "def base_and_descendant_ids_cte\n      cte = SQL::RecursiveCTE.new(:base_and_descendants)\n\n      base_query = descendants_base.except(:order).select(objects_table[:id])\n\n      cte << base_query\n\n      # Recursively get all the descendants of the base set.\n      descendants_query = unscoped_model\n        .select(objects_table[:id])\n        .from(from_tables(cte))\n        .where(descendant_conditions(cte))\n        .except(:order)\n\n      cte << descendants_query\n      cte\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "616"
  },
  {
    "raw_code": "def objects_table\n      model.arel_table\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "617"
  },
  {
    "raw_code": "def self.color_for(value)\n      Color.new(\"##{Digest::SHA256.hexdigest(value.to_s)[0..5]}\")\n    end",
    "comment": "Generate a hex color based on hex-encoded value",
    "label": "",
    "id": "618"
  },
  {
    "raw_code": "def to_srgb(value)\n      normalized = value / 255.0\n      normalized <= 0.03928 ? normalized / 12.92 : ((normalized + 0.055) / 1.055)**2.4\n    end",
    "comment": "Implementation should match https://gitlab.com/gitlab-org/gitlab-ui/-/blob/6245128c7256e3d8db164b92e9580c79d47e9183/src/utils/utils.js#L52-55",
    "label": "",
    "id": "619"
  },
  {
    "raw_code": "def relative_luminance(rgb)\n      # WCAG 2.1 formula: https://www.w3.org/TR/WCAG21/#dfn-relative-luminance\n      # -\n      # WCAG 3.0 will use APAC\n      # Using APAC would be the ultimate goal, but was dismissed by engineering as of now\n      # See https://gitlab.com/gitlab-org/gitlab-ui/-/merge_requests/3418#note_1370107090\n      (0.2126 * to_srgb(rgb[0])) + (0.7152 * to_srgb(rgb[1])) + (0.0722 * to_srgb(rgb[2]))\n    end",
    "comment": "Implementation should match https://gitlab.com/gitlab-org/gitlab-ui/-/blob/6245128c7256e3d8db164b92e9580c79d47e9183/src/utils/utils.js#L57-64",
    "label": "",
    "id": "620"
  },
  {
    "raw_code": "def light?\n      return false unless valid?\n\n      luminance = relative_luminance(rgb)\n      light_luminance = relative_luminance([255, 255, 255])\n      dark_luminance = relative_luminance([31, 30, 36])\n\n      contrast_light = (light_luminance + 0.05) / (luminance + 0.05)\n      contrast_dark = (luminance + 0.05) / (dark_luminance + 0.05)\n\n      # Using a threshold contrast of 2.4 instead of 3\n      # as this will solve weird color combinations in the mid tones\n      #\n      # Note that this is the negated condition from GitLab UI,\n      # because the GitLab UI implementation returns the text color,\n      # while this defines whether a background color is light\n      !(contrast_light >= 2.4 || contrast_light > contrast_dark)\n    end",
    "comment": "Implementation should match https://gitlab.com/gitlab-org/gitlab-ui/-/blob/6245128c7256e3d8db164b92e9580c79d47e9183/src/utils/utils.js#L66-91",
    "label": "",
    "id": "621"
  },
  {
    "raw_code": "def circuit_breaker_options\n      {\n        sleep_window: 24.hours,\n        time_window: 10.minutes,\n        volume_threshold: 5\n      }\n    end",
    "comment": "Disable CodeNavigation feature for 24 hours after several timeouts caused by a slow SQL query",
    "label": "",
    "id": "622"
  },
  {
    "raw_code": "def self.notified_projects\n      Gitlab::Redis::SharedState.with do |redis|\n        redis.hgetall(DELETION_TRACKING_REDIS_KEY)\n      end",
    "comment": "Redis key 'inactive_projects_deletion_warning_email_notified' is a hash. It stores the date when the deletion warning notification email was sent for an inactive project. The fields and values look like: {\"project:1\"=>\"2022-04-22\", \"project:5\"=>\"2022-04-22\", \"project:7\"=>\"2022-04-25\"} @return [Hash]",
    "label": "",
    "id": "623"
  },
  {
    "raw_code": "def granular_update(key, value, operation)\n      full_key = cache_key(key)\n\n      log_records = [\"granular_update\", \"Key: #{key}\", \"Value: #{value}\", \"Operation: #{operation}\"]\n\n      with do |redis|\n        # Watch Redis key to track changes during the update operation.\n        # If watch fails, it means that cache being modified by another process.\n        # Granular update should be cancelled because it might create inconsistencies\n        result = redis.watch(full_key) do |wredis|\n          if redis.exists?(full_key) # rubocop:disable CodeReuse/ActiveRecord -- it's a valid method\n            log_records << \"Key exists!\"\n            wredis.multi do |multi|\n              case operation\n              when :add\n                multi.sadd(full_key, value)\n              when :remove\n                multi.srem(full_key, value)\n              end",
    "comment": "Atomically updates a Redis set cache by adding or removing a single value. This method uses Redis WATCH/MULTI to ensure thread-safe operations and only operates on existing cache keys to avoid creating stale cache entries.  @param key [String] The cache key to update @param value [String] The value to add or remove from the set @param operation [Symbol] Either :add to add the value or :remove to remove it  Example: cache.granular_update('branch_names', 'feature-branch', :add) cache.granular_update('tag_names', 'v1.0.0', :remove)",
    "label": "",
    "id": "624"
  },
  {
    "raw_code": "def search(key, pattern, &block)\n      full_key = cache_key(key)\n\n      with do |redis|\n        exists = redis.exists?(full_key) # rubocop:disable CodeReuse/ActiveRecord\n        write(key, yield) unless exists\n\n        redis.sscan_each(full_key, match: pattern)\n      end",
    "comment": "Searches the cache set using SSCAN with the MATCH option. The MATCH parameter is the pattern argument. See https://redis.io/commands/scan#the-match-option for more information. Returns an Enumerator that enumerates all SSCAN hits.",
    "label": "",
    "id": "625"
  },
  {
    "raw_code": "def self.latest_cached_markdown_version(local_version:)\n      local_version ||= Gitlab::CurrentSettings.current_application_settings.local_markdown_version\n\n      CACHE_COMMONMARK_VERSION_SHIFTED | local_version\n    end",
    "comment": "We could be called by a method that is inside the Gitlab::CurrentSettings object. In this case we need to pass in the local_markdown_version in order to avoid an infinite loop. See usaage in `app/models/concerns/cache_markdown_field.rb` Otherwise pass in `nil`",
    "label": "",
    "id": "626"
  },
  {
    "raw_code": "def self.database_base_models_with_gitlab_shared\n      @database_base_models_with_gitlab_shared ||=\n        all_database_connections\n          .select { |_, db| db.has_gitlab_shared? }\n          .transform_values(&:connection_class)\n          .compact.with_indifferent_access.freeze\n    end",
    "comment": "This returns a list of databases that contains all the gitlab_shared schema tables.",
    "label": "",
    "id": "627"
  },
  {
    "raw_code": "def self.database_base_models_using_load_balancing\n      @database_base_models_using_load_balancing ||=\n        all_database_connections\n          .select { |_, db| db.uses_load_balancing? }\n          .transform_values(&:connection_class)\n          .compact.with_indifferent_access.freeze\n    end",
    "comment": "This returns a list of databases whose connection supports database load balancing. We can't reuse the database_base_models since not all connections do support load balancing.",
    "label": "",
    "id": "628"
  },
  {
    "raw_code": "def self.application_record_for_connection(connection)\n      @gitlab_base_models ||=\n        database_base_models\n          .transform_values { |v| v == ActiveRecord::Base ? ApplicationRecord : v }\n\n      @gitlab_base_models[db_config_name(connection)]\n    end",
    "comment": "Returns the application record that created the given connection. In single database mode, this always returns ApplicationRecord.",
    "label": "",
    "id": "629"
  },
  {
    "raw_code": "def self.schemas_to_base_models\n      @schemas_to_base_models ||=\n        all_gitlab_schemas.transform_values do |schema|\n          all_database_connections\n            .values\n            .select { |db| db.gitlab_schemas.include?(schema.name) }\n            .filter_map { |db| db.connection_class_or_fallback(all_database_connections) }\n            .uniq\n        end.compact.with_indifferent_access.freeze\n    end",
    "comment": "This returns a list of base models with connection associated for a given gitlab_schema",
    "label": "",
    "id": "630"
  },
  {
    "raw_code": "def self.default_pool_size\n      headroom =\n        (ENV[\"DB_POOL_HEADROOM\"].presence || DEFAULT_POOL_HEADROOM).to_i\n\n      Gitlab::Runtime.max_threads + headroom\n    end",
    "comment": "We configure the database connection pool size automatically based on the configured concurrency. We also add some headroom, to make sure we don't run out of connections when more threads besides the 'user-facing' ones are running.  Read more about this in doc/development/database/client_side_connection_pool.md",
    "label": "",
    "id": "631"
  },
  {
    "raw_code": "def self.upgrade_path\n      path_data = YAML.safe_load_file(Rails.root.join('config/upgrade_path.yml'))\n      Gitlab::Utils::UpgradePath.new(path_data, Gitlab.version_info)\n    end",
    "comment": "Expose path information so that we can use it to make sure migrations are healthy",
    "label": "",
    "id": "632"
  },
  {
    "raw_code": "def self.min_schema_gitlab_version\n      upgrade_path.last_required_stop\n    end",
    "comment": "Migrations before this version may have been removed.",
    "label": "",
    "id": "633"
  },
  {
    "raw_code": "def self.has_config?(database_name)\n      ActiveRecord::Base.configurations\n        .configs_for(env_name: Rails.env, name: database_name.to_s, include_hidden: true)\n        .present?\n    end",
    "comment": "Database configured. Returns true even if the database is shared",
    "label": "",
    "id": "634"
  },
  {
    "raw_code": "def self.has_database?(database_name)\n      db_config = ::Gitlab::Database.database_base_models[database_name]&.connection_db_config\n      db_config.present? && db_config_share_with(db_config).nil?\n    end",
    "comment": "Database configured. Returns false if the database is shared",
    "label": "",
    "id": "635"
  },
  {
    "raw_code": "def self.check_for_non_superuser\n      user = PgUser.find_by('usename = CURRENT_USER')\n      am_i_superuser = user.usesuper\n\n      Gitlab::AppLogger.info(\n        \"Account details: User: \\\"#{user.usename}\\\", UseSuper: (#{am_i_superuser})\"\n      )\n\n      raise 'Error: detected superuser' if am_i_superuser\n    rescue ActiveRecord::StatementInvalid\n      raise 'User CURRENT_USER not found'\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "636"
  },
  {
    "raw_code": "def self.random\n      \"RANDOM()\"\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "637"
  },
  {
    "raw_code": "def self.quote_table_name(table_name)\n      if ::Gitlab.next_rails?\n        ApplicationRecord.adapter_class.quote_table_name(table_name)\n      else\n        ApplicationRecord.connection.quote_table_name(table_name)\n      end",
    "comment": "We use ApplicationRecord here but these quoting helpers can be used on any query fragment since all our databases use the same PostgreSQL adapter",
    "label": "",
    "id": "638"
  },
  {
    "raw_code": "def self.gitlab_schemas_for_connection(connection)\n      db_config = self.db_config_for_connection(connection)\n\n      # connection might not be yet adopted (returning NullPool, and no connection_klass)\n      # in such cases it is fine to ignore such connections\n      return unless db_config\n\n      db_config_name = db_config.name.delete_suffix(LoadBalancing::LoadBalancer::REPLICA_SUFFIX)\n\n      primary_model = self.database_base_models[db_config_name.to_sym]\n\n      return unless primary_model\n\n      self.schemas_to_base_models.select do |_, child_models|\n        child_models.any? do |child_model|\n          child_model == primary_model || \\\n            # The model might indicate a child connection, ensure that this is enclosed in a `db_config`\n            self.database_base_models[self.db_config_share_with(child_model.connection_db_config)] == primary_model\n        end",
    "comment": "This returns all matching schemas that a given connection can use Since the `ActiveRecord::Base` might change the connection (from main to ci) This does not look at literal connection names, but rather compares models that are holders for a given db_config_name",
    "label": "",
    "id": "639"
  },
  {
    "raw_code": "def self.db_config_name(connection)\n      db_config = db_config_for_connection(connection)\n      db_config&.name || 'unknown'\n    end",
    "comment": "At the moment, the connection can only be retrieved by Gitlab::Database::LoadBalancer#read or #read_write or from the ActiveRecord directly. Therefore, if the load balancer doesn't recognize the connection, this method returns the primary role directly. In future, we may need to check for other sources. Expected returned names: main, main_replica, ci, ci_replica, unknown",
    "label": "",
    "id": "640"
  },
  {
    "raw_code": "def self.db_config_share_with(db_config)\n      # no sharing\n      return if db_config.database_tasks?\n\n      database_connection_info = all_database_connections[db_config.name]\n\n      if database_connection_info\n        database_connection_info.fallback_database&.to_s\n      else\n        # legacy behaviour\n        'main'\n      end",
    "comment": "If the `database_tasks: false` is being used, return the expected fallback database for this database configuration",
    "label": "",
    "id": "641"
  },
  {
    "raw_code": "def self.read_minimum_migration_version\n      Dir.open(\n        Rails.root.join('db/migrate')\n      ).filter_map { |f| /\\A\\d{14}/.match(f)&.to_s }.map(&:to_i).min\n    end",
    "comment": "Determines minimum viable migration version, determined by the timestamp of the earliest migration file.",
    "label": "",
    "id": "642"
  },
  {
    "raw_code": "def self.install_transaction_metrics_patches!\n      ActiveRecord::Base.prepend(ActiveRecordBaseTransactionMetrics)\n    end",
    "comment": "Monkeypatch rails with upgraded database observability",
    "label": "",
    "id": "643"
  },
  {
    "raw_code": "def transaction(**options, &block)\n          transaction_type = get_transaction_type(connection.transaction_open?, options[:requires_new])\n\n          ::Gitlab::Database::Metrics.subtransactions_increment(self.name) if transaction_type == :sub_transaction\n\n          super(**options, &block)\n        end",
    "comment": "A patch over ApplicationRecord.transaction that provides observability into transactional methods.",
    "label": "",
    "id": "644"
  },
  {
    "raw_code": "def prepare_wiki_data(data)\n        project_id = data.dig(:project, :id)\n        slug = data.dig(:object_attributes, :slug)\n        version_id = data.dig(:object_attributes, :version_id)\n        return data unless [project_id, slug, version_id].all?(&:present?)\n\n        wiki = ProjectWiki.find_by_id(project_id)\n        return data unless wiki\n\n        page = wiki.find_page(slug, version_id)\n        return data unless page\n\n        data.deep_merge(object_attributes: { content: Gitlab::HookData::WikiPageBuilder.new(page).page_content })\n      end",
    "comment": "Wiki webhook data does not have \"content\" attribute yet. As Wiki content is versioned in git, we can lazily retrieve the content from source control and it will be identical to when webhook event was triggered. This is an optimization to serializing wiki content data which can sometimes be over the Sidekiq payload limit.",
    "label": "",
    "id": "645"
  },
  {
    "raw_code": "def get_class_attribute(name)\n        class_attributes[name] || superclass_attributes(name)\n      end",
    "comment": "Returns an attribute declared on this class or its parent class. This approach allows declared attributes to be inherited by child classes.",
    "label": "",
    "id": "646"
  },
  {
    "raw_code": "def initialize(key_base64)\n      @key = key_base64\n    end",
    "comment": "Gets the base64 encoded string representing a rsa or dsa key ",
    "label": "",
    "id": "647"
  },
  {
    "raw_code": "def popen(cmd, path = nil, vars = {}, &block)\n      result = popen_with_detail(cmd, path, vars, &block)\n\n      # Process#waitpid returns Process::Status, which holds a 16-bit value.\n      # The higher-order 8 bits hold the exit() code (`exitstatus`).\n      # The lower-order bits holds whether the process was terminated.\n      # If the process didn't exit normally, `exitstatus` will be `nil`,\n      # but we still want a non-zero code, even if the value is\n      # platform-dependent.\n      status = result.status&.exitstatus || result.status.to_i\n\n      [\"#{result.stdout}#{result.stderr}\", status]\n    end",
    "comment": "Returns [stdout + stderr, status] status is either the exit code or the signal that killed the process",
    "label": "",
    "id": "648"
  },
  {
    "raw_code": "def initialize(mappings, logger: nil)\n      @mappings = mappings\n      @logger = logger\n    end",
    "comment": "mappings is a hash of WorkerClassName => target_queue_name",
    "label": "",
    "id": "649"
  },
  {
    "raw_code": "def migrate_set(sidekiq_set)\n      source_queues_regex = Regexp.union(mappings.keys)\n      scanned = 0\n      migrated = 0\n\n      estimated_size = Sidekiq.redis { |c| c.zcard(sidekiq_set) }\n      logger&.info(\"Processing #{sidekiq_set} set. Estimated size: #{estimated_size}.\")\n\n      Sidekiq.redis do |c|\n        c.zscan(sidekiq_set) do |job, score|\n          if scanned > 0 && scanned % LOG_FREQUENCY == 0\n            logger&.info(\"In progress. Scanned records: #{scanned}. Migrated records: #{migrated}.\")\n          end",
    "comment": "Migrate jobs in SortedSets, i.e. scheduled and retry sets.",
    "label": "",
    "id": "650"
  },
  {
    "raw_code": "def migrate_queues\n      routing_rules_queues = mappings.values.uniq\n      logger&.info(\"List of queues based on routing rules: #{routing_rules_queues}\")\n      Sidekiq.redis do |conn|\n        # Redis 6 supports conn.scan_each(match: \"queue:*\", type: 'list')\n        conn.scan(\"MATCH\", \"queue:*\") do |key|\n          # Redis 5 compatibility\n          next unless conn.type(key) == 'list'\n\n          queue_from = key.split(':', 2).last\n          next if routing_rules_queues.include?(queue_from)\n\n          logger&.info(\"Migrating #{queue_from} queue\")\n\n          migrated = 0\n          while queue_length(queue_from) > 0\n            begin\n              if migrated >= 0 && migrated % LOG_FREQUENCY_QUEUES == 0\n                logger&.info(\"Migrating from #{queue_from}. Total: #{queue_length(queue_from)}. Migrated: #{migrated}.\")\n              end",
    "comment": "Migrates jobs from queues that are outside the mappings",
    "label": "",
    "id": "651"
  },
  {
    "raw_code": "def self.set(jid, expire = DEFAULT_EXPIRATION)\n      return unless expire\n\n      with_redis do |redis|\n        redis.set(key_for(jid), 1, ex: expire.to_i)\n      end",
    "comment": "Starts tracking of the given job.  jid - The Sidekiq job ID expire - The expiration time of the Redis key.",
    "label": "",
    "id": "652"
  },
  {
    "raw_code": "def self.unset(jid)\n      with_redis do |redis|\n        redis.del(key_for(jid))\n      end",
    "comment": "Stops the tracking of the given job.  jid - The Sidekiq job ID to remove.",
    "label": "",
    "id": "653"
  },
  {
    "raw_code": "def self.expire(jid, expire = DEFAULT_EXPIRATION)\n      with_redis do |redis|\n        redis.expire(key_for(jid), expire.to_i)\n      end",
    "comment": "Refreshes the timeout on the key if it exists  jid = The Sidekiq job ID expire - The expiration time of the Redis key.",
    "label": "",
    "id": "654"
  },
  {
    "raw_code": "def self.all_completed?(job_ids)\n      self.num_running(job_ids) == 0\n    end",
    "comment": "Returns true if all the given job have been completed.  job_ids - The Sidekiq job IDs to check.  Returns true or false.",
    "label": "",
    "id": "655"
  },
  {
    "raw_code": "def self.running?(job_id)\n      num_running([job_id]) > 0\n    end",
    "comment": "Returns true if the given job is running or enqueued.  job_id - The Sidekiq job ID to check.",
    "label": "",
    "id": "656"
  },
  {
    "raw_code": "def self.num_running(job_ids)\n      responses = self.job_status(job_ids)\n\n      responses.count(&:present?)\n    end",
    "comment": "Returns the number of jobs that are running or enqueued.  job_ids - The Sidekiq job IDs to check.",
    "label": "",
    "id": "657"
  },
  {
    "raw_code": "def self.num_completed(job_ids)\n      job_ids.size - self.num_running(job_ids)\n    end",
    "comment": "Returns the number of jobs that have completed.  job_ids - The Sidekiq job IDs to check.",
    "label": "",
    "id": "658"
  },
  {
    "raw_code": "def self.job_status(job_ids)\n      return [] if job_ids.empty?\n\n      keys = job_ids.map { |jid| key_for(jid) }\n\n      status = with_redis do |redis|\n        Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n          if Gitlab::Redis::ClusterUtil.cluster?(redis)\n            Gitlab::Redis::ClusterUtil.batch_get(keys, redis)\n          else\n            redis.mget(*keys)\n          end",
    "comment": "Returns the job status for each of the given job IDs.  job_ids - The Sidekiq job IDs to check.  Returns an array of true or false indicating job completion. true = job is still running or enqueued false = job completed",
    "label": "",
    "id": "659"
  },
  {
    "raw_code": "def self.completed_jids(job_ids)\n      statuses = job_status(job_ids)\n\n      completed = []\n      job_ids.zip(statuses).each do |job_id, status|\n        completed << job_id unless status\n      end",
    "comment": "Returns the JIDs that are completed  job_ids - The Sidekiq job IDs to check.  Returns an array of completed JIDs",
    "label": "",
    "id": "660"
  },
  {
    "raw_code": "def from_request(request)\n      category = request.headers[\"HTTP_X_GITLAB_FEATURE_CATEGORY\"].presence\n\n      return unless category && valid?(category)\n\n      return unless ::Gitlab::RequestForgeryProtection.verified?(request.env)\n\n      category\n    end",
    "comment": "If valid, returns a feature category from the given request.",
    "label": "",
    "id": "661"
  },
  {
    "raw_code": "def initialize(filename)\n      @filename = filename\n    end",
    "comment": "Create a new rotator. +filename+ is used to store values by +calculate!+, and to update the database with new and old values in +apply!+ and +rollback!+, respectively.",
    "label": "",
    "id": "662"
  },
  {
    "raw_code": "def rotate!(old_key:, new_key:)\n      old_key ||= Gitlab::Application.credentials.otp_key_base\n\n      raise ArgumentError, \"Old key is the same as the new key\" if old_key == new_key\n      raise ArgumentError, \"New key is too short! Must be 256 bits\" if new_key.size < 64\n\n      write_csv do |csv|\n        User.transaction do\n          User.with_two_factor.in_batches do |relation| # rubocop: disable Cop/InBatches\n            rows = relation.pluck(:id, :encrypted_otp_secret, :encrypted_otp_secret_iv, :encrypted_otp_secret_salt)\n            rows.each do |row|\n              user = %i[id ciphertext iv salt].zip(row).to_h\n              new_value = reencrypt(user, old_key, new_key)\n\n              User.where(id: user[:id]).update_all(encrypted_otp_secret: new_value)\n              csv << [user[:id], user[:ciphertext], new_value]\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "663"
  },
  {
    "raw_code": "def rollback!\n      User.transaction do\n        CSV.foreach(filename, headers: HEADERS, return_headers: false) do |row|\n          User.where(id: row['user_id']).update_all(encrypted_otp_secret: row['old_value'])\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "664"
  },
  {
    "raw_code": "def initialize(user)\n      @user = user\n    end",
    "comment": "user - The User object for which to calculate the authorizations.",
    "label": "",
    "id": "665"
  },
  {
    "raw_code": "def recursive_cte\n      cte = Gitlab::SQL::RecursiveCTE.new(:namespaces_cte)\n      members = Member.arel_table\n      namespaces = Namespace.arel_table\n      group_group_links = GroupGroupLink.arel_table\n\n      # Namespaces the user is a member of.\n      cte << user.groups_with_active_memberships\n        .select([namespaces[:id], members[:access_level]])\n        .except(:order)\n\n      # Namespaces shared with any of the group\n      cte << Group.select([namespaces[:id],\n        least(\n          members[:access_level],\n          group_group_links[:group_access],\n          'access_level'\n        )])\n                  .joins(join_group_group_links)\n                  .joins(join_members_on_group_group_links)\n\n      # Sub groups of any groups the user is a member of.\n      cte << Group.select([\n        namespaces[:id],\n        greatest(members[:access_level], cte.table[:access_level], 'access_level')\n      ])\n        .joins(join_cte(cte))\n        .joins(join_members_on_namespaces)\n        .except(:order)\n\n      cte\n    end",
    "comment": "Builds a recursive CTE that gets all the groups the current user has access to, including any nested groups and any shared groups.",
    "label": "",
    "id": "666"
  },
  {
    "raw_code": "def join_members_on_namespaces\n      members = Member.arel_table\n      namespaces = Namespace.arel_table\n\n      cond = members[:source_id]\n        .eq(namespaces[:id])\n        .and(members[:source_type].eq('Namespace'))\n        .and(members[:requested_at].eq(nil))\n        .and(members[:user_id].eq(user.id))\n        .and(members[:state].eq(::Member::STATE_ACTIVE))\n        .and(members[:access_level].gt(Gitlab::Access::MINIMAL_ACCESS))\n\n      Arel::Nodes::OuterJoin.new(members, Arel::Nodes::On.new(cond))\n    end",
    "comment": "Builds a LEFT JOIN to join optional memberships onto the CTE.",
    "label": "",
    "id": "667"
  },
  {
    "raw_code": "def join_cte(cte)\n      namespaces = Namespace.arel_table\n      cond = cte.table[:id].eq(namespaces[:parent_id])\n\n      Arel::Nodes::InnerJoin.new(cte.table, Arel::Nodes::On.new(cond))\n    end",
    "comment": "Builds an INNER JOIN to join namespaces onto the CTE.",
    "label": "",
    "id": "668"
  },
  {
    "raw_code": "def read(length = nil, outbuf = nil)\n      out = length&.positive? ? nil : []\n\n      length ||= size - tell\n\n      until length <= 0 || eof?\n        data = get_chunk\n        break if data.empty?\n\n        chunk_bytes = [BUFFER_SIZE - chunk_offset, length].min\n        data_slice = data.byteslice(0, chunk_bytes)\n\n        out ||= []\n        out << data_slice\n        @tell += data_slice.bytesize\n        length -= data_slice.bytesize\n      end",
    "comment": "https://www.rubydoc.info/stdlib/core/IO:read says: When this method is called at end of file, it returns nil or \"\", depending on length: read, read(nil), and read(0) return \"\", read(positive_integer) returns nil.",
    "label": "",
    "id": "669"
  },
  {
    "raw_code": "def in_range?\n      @chunk_range&.include?(tell)\n    end",
    "comment": " The below methods are not implemented in IO class ",
    "label": "",
    "id": "670"
  },
  {
    "raw_code": "def initialize(finder, project = nil, fast_fail: false, store_in_redis_cache: false)\n      @finder = finder\n      @project = project\n      @fast_fail = fast_fail\n      @cache = Gitlab::SafeRequestStore[CACHE_KEY] ||= initialize_cache\n      @store_in_redis_cache = store_in_redis_cache\n    end",
    "comment": "finder - The finder class to use for retrieving the issuables. fast_fail - restrict counting to a shorter period, degrading gracefully on failure",
    "label": "",
    "id": "671"
  },
  {
    "raw_code": "def [](state)\n      state = state.to_sym if cast_state_to_symbol?(state)\n\n      cache_for_finder[state] || 0\n    end",
    "comment": "Returns the count for the given state.  state - The name of the state as either a String or a Symbol.  Returns an Integer.",
    "label": "",
    "id": "672"
  },
  {
    "raw_code": "def fetch_once(key, **kwargs)\n        Gitlab::SafeRequestStore.fetch(key) do\n          Rails.cache.fetch(key, **kwargs) do\n            yield\n          end",
    "comment": "Utility method for performing a fetch but only once per request, storing the returned value in the request store, if active.",
    "label": "",
    "id": "673"
  },
  {
    "raw_code": "def delete(key)\n        Rails.cache.delete(key)\n      end",
    "comment": "Hook for EE",
    "label": "",
    "id": "674"
  },
  {
    "raw_code": "def find_with_user_password(login, password, increment_failed_attempts: false, request: nil)\n        # Avoid resource intensive checks if login credentials are not provided\n        return unless login.present? && password.present?\n\n        # Nothing to do here if internal auth is disabled and LDAP is\n        # not configured\n        return unless authenticate_using_internal_or_ldap_password?\n\n        Gitlab::Auth::UniqueIpsLimiter.limit_user! do\n          user = User.find_by_login(login)\n\n          break if user && !user.can_log_in_with_non_expired_password?\n\n          authenticators = password_authenticators_for_user(user)\n\n          # return found user that was authenticated first for given login credentials\n          authenticated_user, authenticator = authenticators.find do |authenticator|\n            authenticated_user = authenticator.login(login, password)\n            break authenticated_user, authenticator if authenticated_user\n          end",
    "comment": "Find and return a user if the provided password is valid for various authenticators (OAuth, LDAP, Local Database).  Specify `increment_failed_attempts: true` to increment Devise `failed_attempts`. CAUTION: Avoid incrementing failed attempts when authentication falls through different mechanisms, as in `.find_for_git_client`. This may lead to unwanted access locks when the value provided for `password` was actually a PAT, deploy token, etc.",
    "label": "",
    "id": "675"
  },
  {
    "raw_code": "def optional_scopes\n        all_available_scopes + OPENID_SCOPES + PROFILE_SCOPES + AI_WORKFLOW_SCOPES + DYNAMIC_SCOPES - DEFAULT_SCOPES\n      end",
    "comment": "Other available scopes",
    "label": "",
    "id": "676"
  },
  {
    "raw_code": "def self.blindly_signal_pgroup!(signal)\n      old_trap = trap(signal, 'IGNORE')\n      begin\n        Process.kill(signal, 0)\n      ensure\n        trap(signal, old_trap)\n      end",
    "comment": "The process group leader can forward INT and TERM signals to the whole group. However, the forwarded signal is *also* received by the leader, which could lead to an infinite loop. We can avoid this by temporarily ignoring the forwarded signal. This may cause us to miss some repeated signals from outside the process group, but that isn't fatal.",
    "label": "",
    "id": "677"
  },
  {
    "raw_code": "def token_valid?(token_to_check)\n      HMACToken.new(actor, container).token_valid?(token_to_check) && valid_user?\n    end",
    "comment": "When the token is an lfs one and the actor is blocked or the password has been changed, the token is no longer valid",
    "label": "",
    "id": "678"
  },
  {
    "raw_code": "def self.sanitize_masked_url(url)\n      url.gsub(%r{//#{MASKED_USERINFO_REGEX}@}o, '//*****:*****@')\n    end",
    "comment": "The url associated with records like `WebHookLog` may contain masked portions represented by paired curly brackets in the URL. As this prohibits straightforward parsing of the URL, we can use a variation of the existing USERINFO regex for these cases.",
    "label": "",
    "id": "679"
  },
  {
    "raw_code": "def self.add_event(*args)\n      current_transaction&.add_event(*args)\n    end",
    "comment": "Tracks an event.  See `Gitlab::Metrics::Transaction#add_event` for more details.",
    "label": "",
    "id": "680"
  },
  {
    "raw_code": "def self.current_transaction\n      WebTransaction.current || BackgroundTransaction.current\n    end",
    "comment": "Allow access from other metrics related middlewares",
    "label": "",
    "id": "681"
  },
  {
    "raw_code": "def self.series_prefix\n      @series_prefix ||= Gitlab::Runtime.sidekiq? ? 'sidekiq_' : 'rails_'\n    end",
    "comment": "Returns the prefix to use for the name of a series.",
    "label": "",
    "id": "682"
  },
  {
    "raw_code": "def self.measure(name)\n      trans = current_transaction\n\n      return yield unless trans\n\n      real_start = System.monotonic_time\n      cpu_start = System.cpu_time\n\n      retval = yield\n\n      cpu_stop = System.cpu_time\n      real_stop = System.monotonic_time\n\n      real_time = (real_stop - real_start)\n      cpu_time = cpu_stop - cpu_start\n\n      trans.observe(\"gitlab_#{name}_real_duration_seconds\".to_sym, real_time) do\n        docstring \"Measure #{name}\"\n        buckets EXECUTION_MEASUREMENT_BUCKETS\n      end",
    "comment": "Measures the execution time of a block.  Example:  Gitlab::Metrics.measure(:find_by_username_duration) do UserFinder.new(some_username).find_by_username end  name - The name of the field to store the execution time in.  Returns the value yielded by the supplied block.",
    "label": "",
    "id": "683"
  },
  {
    "raw_code": "def initialize(checker)\n      @checker = checker\n    end",
    "comment": "@param checker [RepositorySizeChecker]",
    "label": "",
    "id": "684"
  },
  {
    "raw_code": "def self.profile(url, logger: nil, post_data: nil, user: nil, private_token: nil, headers: {}, profiler_options: {})\n      app = ActionDispatch::Integration::Session.new(Rails.application)\n      verb = :get\n\n      if post_data\n        verb = :post\n        headers['Content-Type'] = 'application/json'\n      end",
    "comment": "Takes a URL to profile (can be a fully-qualified URL, or an absolute path) and returns the profiler result. Formatting that result is the caller's responsibility. Requests are GET requests unless post_data is passed.  Optional arguments: - logger: will be used for SQL logging, including a summary at the end of the log file of the total time spent per model class.  - post_data: a string of raw POST data to use. Changes the HTTP verb to POST.  - user: a user to authenticate as.  - private_token: instead of providing a user instance, the token can be given as a string. Takes precedence over the user option.  - profiler_options: A keyword Hash of arguments passed to the profiler. Defaults: { mode: :wall, out: <some temporary file>, interval: 1000, raw: true }",
    "label": "",
    "id": "685"
  },
  {
    "raw_code": "def http_auth?\n      request_format && super\n    end",
    "comment": "If the request format is not known, send a redirect instead of a 401 response, since this is the outcome we're most likely to want",
    "label": "",
    "id": "686"
  },
  {
    "raw_code": "def self.queue_duration_for_job(job, with_buffering_duration: true)\n      # Old gitlab-shell messages don't provide enqueued_at/created_at attributes\n      enqueued_at = job['enqueued_at'] || job['created_at']\n      return unless enqueued_at\n\n      enqueued_at_time = convert_to_time(enqueued_at)\n      return unless enqueued_at_time\n\n      buffering_duration = buffering_duration_for_job(job) # concurrency_limit_buffered_at -> enqueued_at\n      queueing_duration = round_elapsed_time(enqueued_at_time) # enqueued_at -> now\n\n      if with_buffering_duration && !queueing_duration.nil? && !buffering_duration.nil?\n        queueing_duration += buffering_duration\n      end",
    "comment": "Returns the total queuing duration for a Sidekiq job in seconds, as a float, if the `enqueued_at` field or `created_at` field is available. Includes buffering duration if a job was buffered by ConcurrencyLimit middleware.  * If the job doesn't contain sufficient information, returns nil * If the job has a start time in the future, returns 0 * If the job contains an invalid start time value, returns nil @param [Hash] job a Sidekiq job, represented as a hash @param [Boolean] with_buffering_duration whether to include buffering duration by ConcurrencyLimit",
    "label": "",
    "id": "687"
  },
  {
    "raw_code": "def self.buffering_duration_for_job(job)\n      buffered_at = job['concurrency_limit_buffered_at']\n      return unless buffered_at\n\n      # Old gitlab-shell messages don't provide enqueued_at/created_at attributes\n      enqueued_at = job['enqueued_at'] || job['created_at']\n      return unless enqueued_at\n\n      buffered_at_time = convert_to_time(buffered_at)\n      return unless buffered_at_time\n\n      enqueued_at_time = convert_to_time(enqueued_at)\n      return unless enqueued_at\n\n      round_elapsed_time(buffered_at_time, enqueued_at_time)\n    end",
    "comment": "Returns the time a Sidekiq job waiting from being buffered until enqueued again in seconds, as a float, if the `concurrency_limit_buffered_at` and `enqueued_at`/`created_at` field is available.  * If the job doesn't contain sufficient information, returns nil * If the job has a start time in the future, returns 0 * If the job contains an invalid start time value, returns nil @param [Hash] job a Sidekiq job, represented as a hash",
    "label": "",
    "id": "688"
  },
  {
    "raw_code": "def self.enqueue_latency_for_scheduled_job(job)\n      scheduled_at = job['scheduled_at']\n      enqueued_at = job['enqueued_at']\n\n      return unless scheduled_at && enqueued_at\n\n      scheduled_at_time = convert_to_time(scheduled_at)\n      enqueued_at_time = convert_to_time(enqueued_at)\n\n      return unless scheduled_at_time && enqueued_at_time\n\n      round_elapsed_time(scheduled_at_time, enqueued_at_time)\n    end",
    "comment": "Returns the time it took for a scheduled job to be enqueued in seconds, as a float, if the `scheduled_at` and `enqueued_at` fields are available.  * If the job doesn't contain sufficient information, returns nil * If the job has a start time in the future, returns 0 * If the job contains an invalid start time value, returns nil @param [Hash] job a Sidekiq job, represented as a hash",
    "label": "",
    "id": "689"
  },
  {
    "raw_code": "def self.elapsed_by_absolute_time(start, end_time)\n      (end_time - start).to_f.round(DURATION_PRECISION)\n    end",
    "comment": "Calculates the time in seconds, as a float, from the provided start time until now  @param [Time] start",
    "label": "",
    "id": "690"
  },
  {
    "raw_code": "def self.convert_to_time(time_value)\n      return time_value if time_value.is_a?(Time)\n      return Time.iso8601(time_value) if time_value.is_a?(String)\n      return Time.at(time_value) if time_value.is_a?(Numeric) && time_value > 0\n    rescue ArgumentError\n      # Swallow invalid dates. Better to loose some observability\n      # than bring all background processing down because of a date\n      # formatting bug in a client\n    end",
    "comment": "Convert a representation of a time into a `Time` value  @param time_value String, Float time representation, or nil",
    "label": "",
    "id": "691"
  },
  {
    "raw_code": "def search_paths(query)\n      return [] if query.blank? || ref.blank?\n\n      escaped_query = RE2::Regexp.escape(query)\n      query_regexp = Gitlab::EncodingHelper.encode_utf8_no_detect(\"(?i)#{escaped_query}\")\n      repository.search_files_by_regexp(query_regexp, ref)\n    end",
    "comment": "Overridden in Gitlab::WikiFileFinder",
    "label": "",
    "id": "692"
  },
  {
    "raw_code": "def self.initialize_instance(*args, recreate: false, **options)\n      if @instance\n        if recreate\n          @instance.stop\n        else\n          raise \"#{name} singleton instance already initialized\"\n        end",
    "comment": "Options: - recreate: We usually only allow a single instance per process to exist; this can be overridden with this switch, so that existing instances are stopped and recreated.",
    "label": "",
    "id": "693"
  },
  {
    "raw_code": "def initialize(**options)\n      @synchronous = options[:synchronous]\n      @mutex = Mutex.new\n    end",
    "comment": "Possible options: - synchronous [Boolean] if true, turns `start` into a blocking call",
    "label": "",
    "id": "694"
  },
  {
    "raw_code": "def start_working\n      true\n    end",
    "comment": "Executed in lock context before starting thread Needs to return success",
    "label": "",
    "id": "695"
  },
  {
    "raw_code": "def run_thread\n      raise NotImplementedError\n    end",
    "comment": "Executed in separate thread",
    "label": "",
    "id": "696"
  },
  {
    "raw_code": "def stop_working\n      # no-ops\n    end",
    "comment": "Executed in lock context",
    "label": "",
    "id": "697"
  },
  {
    "raw_code": "def filter_by_label(items, labels = {})\n      items.select do |item|\n        metadata = item.fetch(\"metadata\", {})\n        item_labels = metadata.fetch(\"labels\", nil)\n        next unless item_labels\n\n        labels.all? { |k, v| item_labels[k.to_s] == v }\n      end",
    "comment": "Filters an array of pods (as returned by the kubernetes API) by their labels",
    "label": "",
    "id": "698"
  },
  {
    "raw_code": "def filter_by_annotation(items, annotations = {})\n      items.select do |item|\n        metadata = item.fetch(\"metadata\", {})\n        item_annotations = metadata.fetch(\"annotations\", nil)\n        next unless item_annotations\n\n        annotations.all? { |k, v| item_annotations[k.to_s] == v }\n      end",
    "comment": "Filters an array of pods (as returned by the kubernetes API) by their annotations",
    "label": "",
    "id": "699"
  },
  {
    "raw_code": "def filter_by_project_environment(items, app, env)\n      filter_by_annotation(items, {\n        'app.gitlab.com/app' => app,\n        'app.gitlab.com/env' => env\n      })\n    end",
    "comment": "Filters an array of pods (as returned by the kubernetes API) by their project and environment",
    "label": "",
    "id": "700"
  },
  {
    "raw_code": "def terminals_for_pod(api_url, namespace, pod)\n      metadata = pod.fetch(\"metadata\", {})\n      status   = pod.fetch(\"status\", {})\n      spec     = pod.fetch(\"spec\", {})\n\n      containers = spec[\"containers\"]\n      pod_name   = metadata[\"name\"]\n      phase      = status[\"phase\"]\n\n      return unless containers.present? && pod_name.present? && phase == \"Running\"\n\n      created_at = begin\n        DateTime.parse(metadata[\"creationTimestamp\"])\n      rescue StandardError\n        nil\n      end",
    "comment": "Converts a pod (as returned by the kubernetes API) into a terminal",
    "label": "",
    "id": "701"
  },
  {
    "raw_code": "def self.allowed_user_ids\n      l1_cache_backend.fetch(ALLOWED_USER_IDS_KEY, expires_in: EXPIRY_TIME_L1_CACHE) do\n        l2_cache_backend.fetch(ALLOWED_USER_IDS_KEY, expires_in: EXPIRY_TIME_L2_CACHE) do\n          group = Group.find_by_id(allowed_group_id)\n\n          if group\n            GroupMembersFinder.new(group).execute.pluck(:user_id)\n          else\n            []\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "702"
  },
  {
    "raw_code": "def self.expire_allowed_user_ids_cache\n      l1_cache_backend.delete(ALLOWED_USER_IDS_KEY)\n      l2_cache_backend.delete(ALLOWED_USER_IDS_KEY)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "703"
  },
  {
    "raw_code": "def uncached_data\n        build_payload(:with_value)\n      end",
    "comment": "Build the Usage Ping JSON payload from metrics YAML definitions which have instrumentation class set",
    "label": "",
    "id": "704"
  },
  {
    "raw_code": "def self.parse(path)\n      repo_path = path.delete_prefix('/').delete_suffix('.git')\n\n      # Detect the repo type based on the path, the first one tried is the project\n      # type, which does not have a suffix.\n      Gitlab::GlRepository.types.each do |_name, type|\n        # If the project path does not end with the defined suffix, try the next\n        # type.\n        # We'll always try to find a project with an empty suffix (for the\n        # `Gitlab::GlRepository::PROJECT` type.\n        next unless type.valid?(repo_path)\n\n        # Removing the suffix (.wiki, .design, ...) from the project path\n        full_path = repo_path.chomp(type.path_suffix)\n        container, project = find_container(type, full_path)\n        next unless container\n\n        redirected_path = repo_path if redirected?(container, repo_path)\n        return [container, project, type, redirected_path]\n      end",
    "comment": "Returns an array containing: - The repository container - The related project (if available) - The repository type - The original container path (if redirected)  @returns [HasRepository, Project, String, String]",
    "label": "",
    "id": "705"
  },
  {
    "raw_code": "def self.find_container(type, full_path)\n      return [nil, nil] if full_path.blank?\n\n      if type.snippet?\n        snippet = find_snippet(full_path)\n\n        [snippet, snippet&.project]\n      elsif type.wiki?\n        wiki = find_wiki(full_path)\n\n        [wiki, wiki.try(:project)]\n      elsif type.design?\n        design_management_repository = find_design_management_repository(full_path)\n\n        [design_management_repository, design_management_repository.project]\n      else\n        project = find_project(full_path)\n\n        [project, project]\n      end",
    "comment": "Returns an array containing: - The repository container - The related project (if available)  @returns [HasRepository, Project, String]",
    "label": "",
    "id": "706"
  },
  {
    "raw_code": "def self.find_snippet(snippet_path)\n      snippet_id, project_path = extract_snippet_info(snippet_path)\n      return unless snippet_id\n\n      project = find_project(project_path) if project_path\n\n      Snippet.find_by_id_and_project(id: snippet_id, project: project)\n    end",
    "comment": "Snippet_path can be either: - snippets/1 - h5bp/html5-boilerplate/snippets/53",
    "label": "",
    "id": "707"
  },
  {
    "raw_code": "def self.find_wiki(container_path)\n      container = Routable.find_by_full_path(container_path, follow_redirects: true)\n\n      # In CE, Group#wiki is not available so this will return nil for a group path.\n      container&.try(:wiki)\n    end",
    "comment": "Wiki path can be either: - namespace/project - group/subgroup/project  And also in EE: - group - group/subgroup",
    "label": "",
    "id": "708"
  },
  {
    "raw_code": "def run_with_circuit(service_name, options = {}, &block)\n        circuit(service_name, options).run(exception: false, &block)\n      end",
    "comment": "@param [String] unique name for the circuit @param options [Hash] an options hash setting optional values per circuit",
    "label": "",
    "id": "709"
  },
  {
    "raw_code": "def bidi_warning\n        _(\"Potentially unwanted character detected: Unicode BiDi Control\")\n      end",
    "comment": "Warning message used to highlight bidi characters in the GUI",
    "label": "",
    "id": "710"
  },
  {
    "raw_code": "def validate(ref_name, skip_head_ref_check: false)\n      return false if ref_name.to_s.empty? # #blank? raises an ArgumentError for invalid encodings\n      return false if ref_name.start_with?(*(EXPANDED_PREFIXES + DISALLOWED_PREFIXES))\n      return false if ref_name == 'HEAD' && !skip_head_ref_check\n\n      begin\n        Rugged::Reference.valid_name?(\"refs/heads/#{ref_name}\")\n      rescue ArgumentError\n        false\n      end",
    "comment": "Validates a given name against the git reference specification  Returns true for a valid reference name, false otherwise",
    "label": "",
    "id": "711"
  },
  {
    "raw_code": "def move_namespace(path, parent_path_was, parent_path)\n      parent_path_was ||= ''\n      parent_path ||= ''\n      new_parent_folder = File.join(root_dir, parent_path)\n      FileUtils.mkdir_p(new_parent_folder)\n      from = File.join(root_dir, parent_path_was, path)\n      to = File.join(root_dir, parent_path, path)\n      move(from, to, \"\")\n    end",
    "comment": "nil parent_path (or parent_path_was) represents a root namespace",
    "label": "",
    "id": "712"
  },
  {
    "raw_code": "def in_lock(key, ttl: 1.minute, retries: 10, sleep_sec: 0.01.seconds)\n      raise ArgumentError, 'Key needs to be specified' unless key\n\n      Gitlab::Instrumentation::ExclusiveLock.increment_requested_count\n\n      lease = SleepingLock.new(key, timeout: ttl, delay: sleep_sec)\n\n      with_instrumentation(:wait) do\n        lease.obtain(1 + retries)\n      end",
    "comment": " This helper method blocks a process/thread until the lease can be acquired, either due to the lease TTL expiring, or due to the current holder explicitly releasing their hold.  If the lease cannot be obtained, raises `FailedToObtainLockError`.  @param [String] key The lock the thread will try to acquire. Only one thread in one process across all Rails instances can hold this named lock at any one time. @param [Float] ttl: The length of time the lock will be valid for. The lock will be automatically be released after this time, so any work should be completed within this time. @param [Integer] retries: The maximum number of times we will re-attempt to acquire the lock. The maximum number of attempts will be `retries + 1`: one for the initial attempt, and then one for every re-try. @param [Float|Proc] sleep_sec: Either a number of seconds to sleep, or a proc that computes the sleep time given the number of preceding attempts (from 1 to retries - 1)  Note: It's basically discouraged to use this method in a webserver thread, because this ties up all thread related resources until all `retries` are consumed. This could potentially eat up all connection pools.",
    "label": "",
    "id": "713"
  },
  {
    "raw_code": "def initialize(repository, extra_namespace: nil, expires_in: 1.day)\n      @repository = repository\n      @namespace = repository.full_path.to_s\n      @namespace += \":#{repository.project.id}\" if repository.project\n      @namespace = \"#{@namespace}:#{extra_namespace}\" if extra_namespace\n      @expires_in = expires_in\n    end",
    "comment": "@param repository [Repository] @param extra_namespace [String] @param expires_in [Integer] expiry time for hash store keys",
    "label": "",
    "id": "714"
  },
  {
    "raw_code": "def cache_key(type)\n      \"#{type}:#{namespace}:hash\"\n    end",
    "comment": "@param type [String] @return [String]",
    "label": "",
    "id": "715"
  },
  {
    "raw_code": "def delete(*keys)\n      return 0 if keys.empty?\n\n      with do |redis|\n        keys = keys.map { |key| cache_key(key) }\n\n        Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n          if Gitlab::Redis::ClusterUtil.cluster?(redis)\n            Gitlab::Redis::ClusterUtil.batch_unlink(keys, redis)\n          else\n            redis.unlink(*keys)\n          end",
    "comment": "@param keys [String] one or multiple keys to delete @return [Integer] the number of keys successfully deleted",
    "label": "",
    "id": "716"
  },
  {
    "raw_code": "def key?(key, h_key)\n      with { |redis| redis.hexists(cache_key(key), h_key) }\n    end",
    "comment": "Check if the provided hash key exists in the hash.  @param key [String] @param h_key [String] the key to check presence in Redis @return [True, False]",
    "label": "",
    "id": "717"
  },
  {
    "raw_code": "def read_members(key, hash_keys = [])\n      raise InvalidKeysProvidedError unless hash_keys.is_a?(Array) && hash_keys.any?\n\n      with do |redis|\n        # Fetch an array of values for the supplied keys\n        values = redis.hmget(cache_key(key), hash_keys)\n\n        # Turn it back into a hash\n        hash_keys.zip(values).to_h\n      end",
    "comment": "Read the values of a set of keys from the hash store, and return them as a hash of those keys and their values.  @param key [String] @param hash_keys [Array<String>] an array of keys to retrieve from the store @return [Hash] a Ruby hash of the provided keys and their values from the store",
    "label": "",
    "id": "718"
  },
  {
    "raw_code": "def write(key, hash)\n      raise InvalidHashProvidedError unless hash.is_a?(Hash) && hash.any?\n\n      full_key = cache_key(key)\n\n      hash = standardize_hash(hash)\n\n      with do |redis|\n        results = redis.pipelined do |pipeline|\n          # Set each hash key to the provided value\n          hash.each do |h_key, h_value|\n            pipeline.hset(full_key, h_key, h_value)\n          end",
    "comment": "Write a hash to the store. All keys and values will be strings when stored.  @param key [String] @param hash [Hash] the hash to be written to Redis @return [Boolean] whether all operations were successful or not",
    "label": "",
    "id": "719"
  },
  {
    "raw_code": "def fetch_and_add_missing(key, h_keys, &block)\n      # Check the cache for all supplied keys\n      cache_values = read_members(key, h_keys)\n\n      # Find the results which returned nil (meaning they're not in the cache)\n      missing = cache_values.select { |_, v| v.nil? }.keys\n\n      if missing.any?\n        new_values = {}\n\n        # Run the block, which updates the new_values hash\n        yield(missing, new_values)\n\n        # Ensure all values are converted to strings, to ensure merging hashes\n        # below returns standardised data.\n        new_values = standardize_hash(new_values)\n\n        # Write the new values to the hset\n        write(key, new_values)\n\n        # Merge the two sets of values to return a complete hash\n        cache_values.merge!(new_values)\n      end",
    "comment": "A variation on the `fetch` pattern of other cache stores. This method allows you to attempt to fetch a group of keys from the hash store, and for any keys that are missing values a block is then called to provide those values, which are then written back into Redis. Both sets of data are then combined and returned as one hash.  @param key [String] @param h_keys [Array<String>] the keys to fetch or add to the cache @yieldparam missing_keys [Array<String>] the keys missing from the cache @yieldparam new_values [Hash] the hash to be populated by the block @return [Hash] the amalgamated hash of cached and uncached values",
    "label": "",
    "id": "720"
  },
  {
    "raw_code": "def standardize_hash(hash)\n      hash.to_h { |k, v| [k.to_s, v.to_s] }\n    end",
    "comment": "Take a hash and convert both keys and values to strings, for insertion into Redis.  @param hash [Hash] @return [Hash] the stringified hash",
    "label": "",
    "id": "721"
  },
  {
    "raw_code": "def record_metrics(key, cache_values, missing_keys)\n      cache_hits = cache_values.delete_if { |_, v| v.nil? }\n\n      # Increment the counter if we have hits\n      metrics_hit_counter.increment(full_hit: missing_keys.empty?, store_type: key) if cache_hits.any?\n\n      # Track the number of hits we got\n      metrics_hit_histogram.observe({ type: \"hits\", store_type: key }, cache_hits.size)\n      metrics_hit_histogram.observe({ type: \"misses\", store_type: key }, missing_keys.size)\n    end",
    "comment": "Record metrics in Prometheus.  @param key [String] the basic key, e.g. :merged_branch_names. Not record-specific. @param cache_values [Hash] the hash response from the cache read @param missing_keys [Array<String>] the array of missing keys from the cache read",
    "label": "",
    "id": "722"
  },
  {
    "raw_code": "def self.types_in_paths(paths)\n      types = Set.new\n\n      paths.each do |path|\n        type = type_of(path)\n\n        types << type if type\n      end",
    "comment": "Returns an Array of file types based on the given paths.  This method can be used to check if a list of file paths (e.g. of changed files) involve any special files such as a README or a LICENSE file.  Example:  types_in_paths(%w{README.md foo/bar.txt}) # => [:readme]",
    "label": "",
    "id": "723"
  },
  {
    "raw_code": "def self.type_of(path)\n      PATTERNS.each do |type, search|\n        did_match = if search.is_a?(Regexp)\n                      path =~ search\n                    else\n                      path.casecmp(search) == 0\n                    end",
    "comment": "Returns the type of a file path, or nil if none could be detected.  Returned types are Symbols such as `:readme`, `:version`, etc.  Example:  type_of('README.md') # => :readme type_of('VERSION') # => :version",
    "label": "",
    "id": "724"
  },
  {
    "raw_code": "def initialize(jobs_remaining = 0, key = \"#{KEY_PREFIX}:#{SecureRandom.uuid}\")\n      @key = key\n      @jobs_remaining = jobs_remaining\n      @finished = []\n    end",
    "comment": "jobs_remaining - the number of jobs left to wait for key - The key of this waiter.",
    "label": "",
    "id": "725"
  },
  {
    "raw_code": "def wait(timeout = 10)\n      deadline = Time.now.utc + timeout\n\n      Gitlab::Redis::SharedState.with do |redis|\n        while jobs_remaining > 0\n          # Redis will not take fractional seconds. Prefer waiting too long over\n          # not waiting long enough\n          seconds_left = (deadline - Time.now.utc).ceil\n\n          # Redis interprets 0 as \"wait forever\", so skip the final `blpop` call\n          break if seconds_left <= 0\n\n          list, jid = redis.blpop(key, timeout: seconds_left)\n\n          # timed out\n          break unless list && jid\n\n          @finished << jid\n          @jobs_remaining -= 1\n        end",
    "comment": "Waits for all the jobs to be completed.  timeout - The maximum amount of seconds to block the caller for. This ensures we don't indefinitely block a caller in case a job takes long to process, or is never processed.",
    "label": "",
    "id": "726"
  },
  {
    "raw_code": "def expire(*keys)\n      return 0 if keys.empty?\n\n      with do |redis|\n        keys_to_expire = keys.map { |key| cache_key(key) }\n\n        Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n          if Gitlab::Redis::ClusterUtil.cluster?(redis)\n            Gitlab::Redis::ClusterUtil.batch_unlink(keys_to_expire, redis)\n          else\n            redis.unlink(*keys_to_expire)\n          end",
    "comment": "Returns the number of keys deleted by Redis",
    "label": "",
    "id": "727"
  },
  {
    "raw_code": "def try_include?(key, value)\n      full_key = cache_key(key)\n\n      with do |redis|\n        redis.multi do |multi|\n          multi.sismember(full_key, value.to_s)\n          multi.exists?(full_key) # rubocop:disable CodeReuse/ActiveRecord\n        end",
    "comment": "Like include?, but also tells us if the cache was populated when it ran by returning two booleans: [member_exists, set_exists]",
    "label": "",
    "id": "728"
  },
  {
    "raw_code": "def position_mapping\n      @position_mapping ||= begin\n        mapping = []\n        rich_pos = 0\n        (0..raw_line.length).each do |raw_pos|\n          rich_char = rich_line[rich_pos]\n\n          # The raw and rich lines are the same except for HTML tags,\n          # so skip over any `<...>` segment\n          while rich_char == '<'\n            until rich_char == '>'\n              rich_pos += 1\n              rich_char = rich_line[rich_pos]\n            end",
    "comment": "Mapping of character positions in the raw line, to the rich (highlighted) line",
    "label": "",
    "id": "729"
  },
  {
    "raw_code": "def collapse_ranges(positions, mode)\n      return [] if positions.empty?\n\n      ranges = []\n\n      start = prev = positions[0]\n      range = MarkerRange.new(start, prev, mode: mode)\n      positions[1..].each do |pos|\n        if pos == prev + 1\n          range = MarkerRange.new(start, pos, mode: mode)\n          prev = pos\n        else\n          ranges << range\n          start = prev = pos\n          range = MarkerRange.new(start, prev, mode: mode)\n        end",
    "comment": "Takes an array of integers, and returns an array of ranges covering the same integers",
    "label": "",
    "id": "730"
  },
  {
    "raw_code": "def snippets\n      SnippetsFinder.new(current_user, finder_params)\n        .execute\n        .includes(:author)\n        .reorder(updated_at: :desc)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "731"
  },
  {
    "raw_code": "def snippet_titles\n      snippets.search(query)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "732"
  },
  {
    "raw_code": "def emoji_public_absolute_path\n      Rails.root.join(\"public/-/emojis/#{EMOJI_VERSION}\")\n    end",
    "comment": "Return a Pathname to emoji's current versioned folder  @return [Pathname] Absolute Path to versioned emojis folder in `public`",
    "label": "",
    "id": "733"
  },
  {
    "raw_code": "def gl_emoji_tag(emoji, options = {})\n      return unless emoji\n\n      data = {\n        name: emoji.name,\n        unicode_version: emoji.unicode_version\n      }\n      options = { title: emoji.description, data: data }.merge(options)\n\n      ActionController::Base.helpers.content_tag('gl-emoji', emoji.codepoints, options)\n    end",
    "comment": "CSS sprite fallback takes precedence over image fallback @param [TanukiEmoji::Character] emoji @param [Hash] options",
    "label": "",
    "id": "734"
  },
  {
    "raw_code": "def self.replace_string_placeholders(string, placeholder_regex = nil, limit: 0, &block)\n      return string if string.blank? || placeholder_regex.blank? || !block\n\n      replace_placeholders(string, placeholder_regex, limit: limit, &block)\n    end",
    "comment": "This method accepts the following paras - string: the string to be analyzed - placeholder_regex: i.e. /(project_path|project_id|default_branch|commit_sha)/ - limit: limits the number of replacements in the string. Set to 0 for unlimited - block: this block will be called with each placeholder found in the string using the placeholder regex. If the result of the block is nil, the original placeholder will be returned.",
    "label": "",
    "id": "735"
  },
  {
    "raw_code": "def replace_placeholders(string, placeholder_regex, limit: 0, &block)\n        Gitlab::Utils::Gsub\n          .gsub_with_limit(string, placeholder_full_regex(placeholder_regex), limit: limit) do |match_data|\n          yield(match_data[2]) || match_data[0]\n        end",
    "comment": "If the result of the block is nil, then the placeholder is returned",
    "label": "",
    "id": "736"
  },
  {
    "raw_code": "def self.ghost_user_id\n      key = 'github-import/ghost-user-id'\n\n      Gitlab::Cache::Import::Caching.read_integer(key) || Gitlab::Cache::Import::Caching.write(key, Users::Internal.ghost.id)\n    end",
    "comment": "Returns the ID of the ghost user.",
    "label": "",
    "id": "737"
  },
  {
    "raw_code": "def self.formatted_import_url(project)\n      url = URI.parse(project.unsafe_import_url)\n\n      unless url.host == 'github.com'\n        url.user = nil\n        url.password = nil\n        url.path = \"/api/v3\"\n        url.to_s\n      end",
    "comment": "Get formatted GitHub import URL. If github.com is in the import URL, this will return nil and octokit will use the default github.com API URL",
    "label": "",
    "id": "738"
  },
  {
    "raw_code": "def replace_gsub(text, limit: 0)\n      new_text = +''\n      remainder = text\n      count = 0\n\n      matched = match(remainder)\n\n      until matched.nil? || matched.to_a.compact.empty?\n        partitioned = remainder.partition(matched.to_s)\n        new_text << partitioned.first\n        remainder = partitioned.last\n\n        new_text << yield(matched)\n\n        if limit > 0\n          count += 1\n          break if count >= limit\n        end",
    "comment": "There is no built-in replace with block support (like `gsub`).  We can accomplish the same thing by parsing and rebuilding the string with the substitutions.",
    "label": "",
    "id": "739"
  },
  {
    "raw_code": "def extract_named_group(name, match)\n      return unless match\n\n      match_position = regexp.named_capturing_groups[name.to_s]\n      raise RegexpError, \"Invalid named capture group: #{name}\" unless match_position\n\n      match[match_position - 1]\n    end",
    "comment": "#scan returns an array of the groups captured, rather than MatchData. Use this to give the capture group name and grab the proper value",
    "label": "",
    "id": "740"
  },
  {
    "raw_code": "def self.with_fallback(pattern, multiline: false)\n      UntrustedRegexp.new(pattern, multiline: multiline)\n    rescue RegexpError\n      raise if Feature.enabled?(:disable_unsafe_regexp)\n\n      if Feature.enabled?(:ci_unsafe_regexp_logger, type: :ops)\n        Gitlab::AppJsonLogger.info(\n          class: self.name,\n          regexp: pattern.to_s,\n          fabricated: 'unsafe ruby regexp'\n        )\n      end",
    "comment": "Handles regular expressions with the preferred RE2 library where possible via UntustedRegex. Falls back to Ruby's built-in regular expression library when the syntax would be invalid in RE2.  One difference between these is `(?m)` multi-line mode. Ruby regex enables this by default, but also handles `^` and `$` differently. See: https://www.regular-expressions.info/modifiers.html",
    "label": "",
    "id": "741"
  },
  {
    "raw_code": "def initialize_scan_regexp\n      if regexp.number_of_capturing_groups == 0\n        RE2::Regexp.new('(' + regexp.source + ')')\n      else\n        regexp\n      end",
    "comment": "RE2 scan operates differently to Ruby scan when there are no capture groups, so work around it",
    "label": "",
    "id": "742"
  },
  {
    "raw_code": "def allowed_level?(level)\n        valid_level?(level) && non_restricted_level?(level)\n      end",
    "comment": "Level should be a numeric value, e.g. `20` Return true if the specified level is allowed for the current user.",
    "label": "",
    "id": "743"
  },
  {
    "raw_code": "def application?\n        puma? || sidekiq?\n      end",
    "comment": "Whether we are executing in an actual application context i.e. Puma or Sidekiq.",
    "label": "",
    "id": "744"
  },
  {
    "raw_code": "def multi_threaded?\n        application?\n      end",
    "comment": "Whether we are executing in a multi-threaded environment. For now this is equivalent to meaning Puma or Sidekiq, but this could change in the future.",
    "label": "",
    "id": "745"
  },
  {
    "raw_code": "def all_visible?\n      not_visible_nodes.empty?\n    end",
    "comment": "this method is stateful, it tracks if all nodes from `references` calls are visible or not",
    "label": "",
    "id": "746"
  },
  {
    "raw_code": "def _mime_type\n      if defined? @_mime_type\n        @_mime_type\n      else\n        guesses = ::MIME::Types.type_for(extname.to_s)\n\n        # Prefer text mime types over binary\n        @_mime_type = guesses.detect { |type| type.ascii? } || guesses.first\n      end",
    "comment": "Internal: Lookup mime type for extension.  Returns a MIME::Type rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "747"
  },
  {
    "raw_code": "def mime_type\n      _mime_type ? _mime_type.to_s : 'text/plain'\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables Public: Get the actual blob mime type  Examples  # => 'text/plain' # => 'text/html'  Returns a mime type String.",
    "label": "",
    "id": "748"
  },
  {
    "raw_code": "def secret_token\n        @secret_token ||= File.read(Gitlab.config.gitlab_shell.secret_file).chomp\n      end",
    "comment": "Retrieve GitLab Shell secret token  @return [String] secret token",
    "label": "",
    "id": "749"
  },
  {
    "raw_code": "def ensure_secret_token!\n        return if File.exist?(File.join(Gitlab.config.gitlab_shell.path, '.gitlab_shell_secret'))\n\n        generate_and_link_secret_token\n      end",
    "comment": "Ensure gitlab shell has a secret token stored in the secret_file if that was never generated, generate a new one",
    "label": "",
    "id": "750"
  },
  {
    "raw_code": "def version_required\n        @version_required ||= File.read(Rails.root\n                                        .join('GITLAB_SHELL_VERSION')).strip\n      end",
    "comment": "Returns required GitLab shell version  @return [String] version from the manifest file",
    "label": "",
    "id": "751"
  },
  {
    "raw_code": "def version\n        @version ||= File.read(gitlab_shell_version_file).chomp if File.readable?(gitlab_shell_version_file)\n      end",
    "comment": "Return GitLab shell version  @return [String] version",
    "label": "",
    "id": "752"
  },
  {
    "raw_code": "def generate_and_link_secret_token\n        secret_file = Gitlab.config.gitlab_shell.secret_file\n        shell_path = Gitlab.config.gitlab_shell.path\n\n        unless File.size?(secret_file)\n          # Generate a new token of 16 random hexadecimal characters and store it in secret_file.\n          @secret_token = SecureRandom.hex(16)\n          File.write(secret_file, @secret_token)\n        end",
    "comment": "Create (if necessary) and link the secret token file",
    "label": "",
    "id": "753"
  },
  {
    "raw_code": "def repository_exists?(storage, dir_name)\n      Gitlab::Git::Repository.new(storage, dir_name, nil, nil).exists?\n    rescue GRPC::Internal\n      false\n    end",
    "comment": "Check if repository exists on disk  @example Check if repository exists repository_exists?('default', 'gitlab-org/gitlab.git')  @return [Boolean] whether repository exists or not @param [String] storage project's storage path @param [Object] dir_name repository dir name  @deprecated",
    "label": "",
    "id": "754"
  },
  {
    "raw_code": "def search_context\n        builder = Builder.new(view_context)\n\n        builder.with_snippet(@snippet) if @snippet.present?\n        @snippets.each(&builder.method(:with_snippet)) if @snippets.present?\n        builder.with_project(@project) if @project.present? && @project.persisted?\n        builder.with_group(@group) if @group.present? && @group.persisted?\n        builder.with_ref(@ref) if @ref.present?\n\n        builder.build!\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables  Introspect the current controller's assignments and and builds the proper SearchContext object for it.",
    "label": "",
    "id": "755"
  },
  {
    "raw_code": "def self.formats(current_settings)\n      return [] unless current_settings.kroki_enabled\n\n      # If PlantUML is enabled, PlantUML diagrams will be processed by the PlantUML server.\n      # In other words, the PlantUML server has precedence over Kroki since both can process PlantUML diagrams.\n      diagram_formats = if current_settings.plantuml_enabled\n                          DIAGRAMS_FORMATS_WO_PLANTUML\n                        else\n                          DIAGRAMS_FORMATS\n                        end",
    "comment": "Get the list of diagram formats that are currently enabled  Returns an Array of diagram formats. If Kroki is not enabled, returns an empty Array.",
    "label": "",
    "id": "756"
  },
  {
    "raw_code": "def add_gon_feature_flags\n      # Use `push_to_gon_attributes` directly since we have a computed feature flag with\n      # an opt-out in ui_for_organizations_enabled?\n      push_to_gon_attributes(:features, :ui_for_organizations, ui_for_organizations_enabled?)\n      push_frontend_feature_flag(:organization_switching, current_user)\n      push_frontend_feature_flag(:find_and_replace, current_user)\n      # To be removed with https://gitlab.com/gitlab-org/gitlab/-/issues/399248\n      push_frontend_feature_flag(:remove_monitor_metrics)\n      push_frontend_feature_flag(:work_item_view_for_issues)\n      push_frontend_feature_flag(:new_project_creation_form, current_user, type: :wip)\n      push_frontend_feature_flag(:work_items_client_side_boards, current_user)\n      push_frontend_feature_flag(:glql_work_items, current_user, type: :wip)\n      push_frontend_feature_flag(:glql_aggregation, current_user, type: :wip)\n      push_frontend_feature_flag(:glql_typescript, current_user, type: :wip)\n      push_frontend_feature_flag(:whats_new_featured_carousel)\n      push_frontend_feature_flag(:extensible_reference_filters, current_user)\n      push_frontend_feature_flag(:paneled_view, current_user)\n      push_frontend_feature_flag(:disallow_immediate_deletion, current_user)\n\n      # Expose the Project Studio user preference as if it were a feature flag\n      push_force_frontend_feature_flag(:project_studio_enabled, Users::ProjectStudio.new(current_user).enabled?)\n    end",
    "comment": "Initialize gon.features with any flags that should be made globally available to the frontend",
    "label": "",
    "id": "757"
  },
  {
    "raw_code": "def push_frontend_feature_flag(name, *args, **kwargs)\n      enabled = Feature.enabled?(name, *args, **kwargs)\n\n      push_to_gon_attributes(:features, name, enabled)\n    end",
    "comment": "Exposes the state of a feature flag to the frontend code.  name - The name of the feature flag, e.g. `my_feature`. args - Any additional arguments to pass to `Feature.enabled?`. This allows you to check if a flag is enabled for a particular user.",
    "label": "",
    "id": "758"
  },
  {
    "raw_code": "def push_force_frontend_feature_flag(name, enabled)\n      raise ArgumentError, 'enabled flag must be a Boolean' unless enabled.in?([true, false])\n\n      push_to_gon_attributes(:features, name, enabled)\n    end",
    "comment": "Exposes the state of a feature flag to the frontend code. Can be used for more complex feature flag checks.  name - The name of the feature flag, e.g. `my_feature`. enabled - Boolean to be pushed directly to the frontend. Should be fetched by checking a feature flag.",
    "label": "",
    "id": "759"
  },
  {
    "raw_code": "def current_organization\n      return unless ::Current.organization_assigned\n\n      Organizations::FallbackOrganizationTracker.without_tracking { ::Current.organization }\n    end",
    "comment": "`::Current.organization` is only valid within the context of a request, but it can be called from everywhere. So how do we avoid accidentally calling it outside of the context of a request? We banned it with Rubocop.  This method is acceptable because it is only included by controllers. This method intentionally looks like Devise's `current_user` method, which has similar properties. rubocop:disable Gitlab/AvoidCurrentOrganization -- This method follows the spirit of the rule",
    "label": "",
    "id": "760"
  },
  {
    "raw_code": "def check_path_traversal!(path)\n      return unless path\n\n      path = path.to_s if path.is_a?(Gitlab::HashedPath)\n      raise PathTraversalAttackError, 'Invalid path' unless path.is_a?(String)\n\n      path = ::Gitlab::Utils.decode_path(path)\n\n      if path_traversal?(path)\n        logger.warn(message: \"Potential path traversal attempt detected\", path: path.to_s)\n        raise PathTraversalAttackError, 'Invalid path'\n      end",
    "comment": "Ensure that the relative path will not traverse outside the base directory We url decode the path to avoid passing invalid paths forward in url encoded format. Also see https://gitlab.com/gitlab-org/gitlab/-/merge_requests/24223#note_284122580 It also checks for backslash '\\', which is sometimes a File::ALT_SEPARATOR.",
    "label": "",
    "id": "761"
  },
  {
    "raw_code": "def rate_limits # rubocop:disable Metrics/AbcSize\n        {\n          ai_action: { threshold: -> { application_settings.ai_action_api_rate_limit }, interval: 8.hours },\n          auto_rollback_deployment: { threshold: 1, interval: 3.minutes },\n          autocomplete_users: { threshold: -> { application_settings.autocomplete_users_limit }, interval: 1.minute },\n          autocomplete_users_unauthenticated: { threshold: -> { application_settings.autocomplete_users_unauthenticated_limit }, interval: 1.minute },\n          bulk_delete_todos: { threshold: 6, interval: 1.minute },\n          bulk_import: { threshold: 6, interval: 1.minute },\n          ci_job_processed_subscription: { threshold: 50, interval: 1.minute },\n          code_suggestions_api_endpoint: { threshold: -> { application_settings.code_suggestions_api_rate_limit }, interval: 1.minute },\n          create_organization_api: { threshold: -> { application_settings.create_organization_api_limit }, interval: 1.minute },\n          delete_all_todos: { threshold: 1, interval: 5.minutes },\n          downstream_pipeline_trigger: {\n            threshold: -> { application_settings.downstream_pipeline_trigger_limit_per_project_user_sha }, interval: 1.minute\n          },\n          email_verification: { threshold: 10, interval: 10.minutes },\n          email_verification_code_send: { threshold: 10, interval: 1.hour },\n          expanded_diff_files: { threshold: 6, interval: 1.minute },\n          fetch_google_ip_list: { threshold: 10, interval: 1.minute },\n          github_import: { threshold: 6, interval: 1.minute },\n          fogbugz_import: { threshold: 1, interval: 1.minute },\n          gitea_import: { threshold: 1, interval: 1.minute },\n          gitlab_shell_operation: { threshold: application_settings.gitlab_shell_operation_limit, interval: 1.minute },\n          glql: { threshold: 1, interval: 15.minutes },\n          group_api: { threshold: -> { application_settings.group_api_limit }, interval: 1.minute },\n          group_archive_unarchive_api: { threshold: -> { application_settings.group_archive_unarchive_api_limit }, interval: 1.minute },\n          group_download_export: { threshold: -> { application_settings.group_download_export_limit }, interval: 1.minute },\n          group_export: { threshold: -> { application_settings.group_export_limit }, interval: 1.minute },\n          group_import: { threshold: -> { application_settings.group_import_limit }, interval: 1.minute },\n          group_invited_groups_api: { threshold: -> { application_settings.group_invited_groups_api_limit }, interval: 1.minute },\n          group_projects_api: { threshold: -> { application_settings.group_projects_api_limit }, interval: 1.minute },\n          group_shared_groups_api: { threshold: -> { application_settings.group_shared_groups_api_limit }, interval: 1.minute },\n          groups_api: { threshold: -> { application_settings.groups_api_limit }, interval: 1.minute },\n          import_source_user_notification: { threshold: 1, interval: 8.hours },\n          issues_create: { threshold: -> { application_settings.issues_create_limit }, interval: 1.minute },\n          jobs_index: { threshold: -> { application_settings.project_jobs_api_rate_limit }, interval: 1.minute },\n          large_blob_download: { threshold: 5, interval: 1.minute },\n          members_delete: { threshold: -> { application_settings.members_delete_limit }, interval: 1.minute },\n          namespace_exists: { threshold: 20, interval: 1.minute },\n          notes_create: { threshold: -> { application_settings.notes_create_limit }, interval: 1.minute },\n          notification_emails: { threshold: 1000, interval: 1.day },\n          oauth_dynamic_registration: { threshold: 5, interval: 1.hour },\n          permanent_email_failure: { threshold: 5, interval: 1.day },\n          phone_verification_send_code: { threshold: 5, interval: 1.day },\n          phone_verification_verify_code: { threshold: 5, interval: 1.day },\n          pipelines_create: { threshold: -> { application_settings.pipeline_limit_per_project_user_sha }, interval: 1.minute },\n          play_pipeline_schedule: { threshold: 1, interval: 1.minute },\n          profile_add_new_email: { threshold: 5, interval: 1.minute },\n          profile_resend_email_confirmation: { threshold: 5, interval: 1.minute },\n          profile_update_username: { threshold: 10, interval: 1.minute },\n          project_api: { threshold: -> { application_settings.project_api_limit }, interval: 1.minute },\n          project_download_export: { threshold: -> { application_settings.project_download_export_limit }, interval: 1.minute },\n          project_export: { threshold: -> { application_settings.project_export_limit }, interval: 1.minute },\n          project_fork_sync: { threshold: 10, interval: 30.minutes },\n          project_generate_new_export: { threshold: -> { application_settings.project_export_limit }, interval: 1.minute },\n          project_import: { threshold: -> { application_settings.project_import_limit }, interval: 1.minute },\n          project_invited_groups_api: { threshold: -> { application_settings.project_invited_groups_api_limit }, interval: 1.minute },\n          project_repositories_archive: { threshold: 5, interval: 1.minute },\n          project_repositories_changelog: { threshold: 5, interval: 1.minute },\n          project_repositories_health: { threshold: 5, interval: 1.hour },\n          project_testing_integration: { threshold: 5, interval: 1.minute },\n          projects_api: { threshold: -> { application_settings.projects_api_limit }, interval: 10.minutes },\n          projects_api_rate_limit_unauthenticated: {\n            threshold: -> { application_settings.projects_api_rate_limit_unauthenticated }, interval: 10.minutes\n          },\n          raw_blob: { threshold: -> { application_settings.raw_blob_request_limit }, interval: 1.minute },\n          search_rate_limit: { threshold: -> { application_settings.search_rate_limit }, interval: 1.minute },\n          search_rate_limit_unauthenticated: { threshold: -> { application_settings.search_rate_limit_unauthenticated }, interval: 1.minute },\n          temporary_email_failure: { threshold: 300, interval: 1.day },\n          update_environment_canary_ingress: { threshold: 1, interval: 1.minute },\n          update_namespace_name: { threshold: -> { application_settings.update_namespace_name_rate_limit }, interval: 1.hour },\n          user_contributed_projects_api: { threshold: -> { application_settings.user_contributed_projects_api_limit }, interval: 1.minute },\n          user_followers: { threshold: -> { application_settings.users_api_limit_followers }, interval: 1.minute },\n          user_following: { threshold: -> { application_settings.users_api_limit_following }, interval: 1.minute },\n          user_gpg_key: { threshold: -> { application_settings.users_api_limit_gpg_key }, interval: 1.minute },\n          user_gpg_keys: { threshold: -> { application_settings.users_api_limit_gpg_keys }, interval: 1.minute },\n          user_projects_api: { threshold: -> { application_settings.user_projects_api_limit }, interval: 1.minute },\n          user_sign_in: { threshold: 5, interval: 10.minutes },\n          user_sign_up: { threshold: 20, interval: 1.minute },\n          user_ssh_key: { threshold: -> { application_settings.users_api_limit_ssh_key }, interval: 1.minute },\n          user_ssh_keys: { threshold: -> { application_settings.users_api_limit_ssh_keys }, interval: 1.minute },\n          user_starred_projects_api: { threshold: -> { application_settings.user_starred_projects_api_limit }, interval: 1.minute },\n          user_status: { threshold: -> { application_settings.users_api_limit_status }, interval: 1.minute },\n          username_exists: { threshold: 20, interval: 1.minute },\n          users_get_by_id: { threshold: -> { application_settings.users_get_by_id_limit }, interval: 10.minutes },\n          vertex_embeddings_api: { threshold: 450, interval: 1.minute },\n          web_hook_calls: { interval: 1.minute },\n          web_hook_calls_low: { interval: 1.minute },\n          web_hook_calls_mid: { interval: 1.minute },\n          web_hook_event_resend: { threshold: 5, interval: 1.minute },\n          web_hook_test: { threshold: 5, interval: 1.minute }\n        }.freeze\n      end",
    "comment": "Application rate limits  Threshold value can be either an Integer or a Proc in order to not evaluate it's value every time this method is called and only do that when it's needed.",
    "label": "",
    "id": "762"
  },
  {
    "raw_code": "def throttled?(key, scope:, resource: nil, threshold: nil, interval: nil, users_allowlist: nil, peek: false)\n        raise InvalidKeyError unless rate_limits[key]\n\n        strategy = resource.present? ? IncrementPerActionedResource.new(resource.id) : IncrementPerAction.new\n\n        _throttled?(key, scope: scope, strategy: strategy, threshold: threshold, interval: interval, users_allowlist: users_allowlist, peek: peek)\n      end",
    "comment": "Increments the given key and returns true if the action should be throttled.  @param key [Symbol] Key attribute registered in `.rate_limits` @param scope [Array<ActiveRecord>] Array of ActiveRecord models, Strings or Symbols to scope throttling to a specific request (e.g. per user per project) @param resource [ActiveRecord] An ActiveRecord model to count an action for (e.g. limit unique project (resource) downloads (action) to five per user (scope)) @param threshold [Integer] Optional threshold value to override default one registered in `.rate_limits` @param interval [Integer] Optional interval value to override default one registered in `.rate_limits` @param users_allowlist [Array<String>] Optional list of usernames to exclude from the limit. This param will only be functional if Scope includes a current user. @param peek [Boolean] Optional. When true the key will not be incremented but the current throttled state will be returned.  @return [Boolean] Whether or not a request should be throttled",
    "label": "",
    "id": "763"
  },
  {
    "raw_code": "def resource_usage_throttled?(key, scope:, resource_key:, threshold:, interval:, peek: false)\n        strategy = IncrementResourceUsagePerAction.new(resource_key)\n\n        _throttled?(key, scope: scope, strategy: strategy, threshold: threshold, interval: interval, peek: peek)\n      end",
    "comment": "Increments the resource usage for a given key and returns true if the action should be throttled.  @param key [Symbol] Key attribute registered in `.rate_limits` @param scope [<ActiveRecord>] Array of ActiveRecord models, Strings or Symbols to scope throttling to a specific request (e.g. per user per project) @param resource_key [Symbol] Key attribute in SafeRequestStore @param threshold [Integer] Threshold value to override default one registered in `.rate_limits` @param interval [Integer] Interval value to override default one registered in `.rate_limits`  @return [Boolean] Whether or not a request should be throttled",
    "label": "",
    "id": "764"
  },
  {
    "raw_code": "def throttled_request?(request, current_user, key, scope:, **options)\n        if ::Gitlab::Throttle.bypass_header.present? && request.get_header(Gitlab::Throttle.bypass_header) == '1'\n          return false\n        end",
    "comment": "Similar to #throttled? above but checks for the bypass header in the request and logs the request when it is over the rate limit  @param request [Http::Request] - Web request used to check the header and log @param current_user [User] Current user of the request, it can be nil @param key [Symbol] Key attribute registered in `.rate_limits` @param scope [Array<ActiveRecord>] Array of ActiveRecord models, Strings or Symbols to scope throttling to a specific request (e.g. per user per project) @param resource [ActiveRecord] An ActiveRecord model to count an action for (e.g. limit unique project (resource) downloads (action) to five per user (scope)) @param threshold [Integer] Optional threshold value to override default one registered in `.rate_limits` @param interval [Integer] Optional interval value to override default one registered in `.rate_limits` @param users_allowlist [Array<String>] Optional list of usernames to exclude from the limit. This param will only be functional if Scope includes a current user. @param peek [Boolean] Optional. When true the key will not be incremented but the current throttled state will be returned.  @return [Boolean] Whether or not a request should be throttled",
    "label": "",
    "id": "765"
  },
  {
    "raw_code": "def peek(key, scope:, threshold: nil, interval: nil, users_allowlist: nil)\n        throttled?(key, peek: true, scope: scope, threshold: threshold, interval: interval, users_allowlist: users_allowlist)\n      end",
    "comment": "Returns the current rate limited state without incrementing the count.  @param key [Symbol] Key attribute registered in `.rate_limits` @param scope [Array<ActiveRecord>] Array of ActiveRecord models to scope throttling to a specific request (e.g. per user per project) @param threshold [Integer] Optional threshold value to override default one registered in `.rate_limits` @param interval [Integer] Optional interval value to override default one registered in `.rate_limits` @param users_allowlist [Array<String>] Optional list of usernames to exclude from the limit. This param will only be functional if Scope includes a current user.  @return [Boolean] Whether or not a request is currently throttled",
    "label": "",
    "id": "766"
  },
  {
    "raw_code": "def log_request(request, type, current_user, logger = Gitlab::AuthLogger)\n        request_information = {\n          message: 'Application_Rate_Limiter_Request',\n          env: type,\n          remote_ip: request.ip,\n          method: request.request_method,\n          path: request_path(request)\n        }\n\n        if current_user\n          request_information.merge!({\n            user_id: current_user.id,\n            username: current_user.username\n          })\n        end",
    "comment": "Logs request using provided logger  @param request [Http::Request] - Web request to be logged @param type [Symbol] A symbol key that represents the request @param current_user [User] Current user of the request, it can be nil @param logger [Logger] Logger to log request to a specific log file. Defaults to Gitlab::AuthLogger",
    "label": "",
    "id": "767"
  },
  {
    "raw_code": "def observability_url\n      return ENV['OVERRIDE_OBSERVABILITY_QUERY_URL'] if ENV['OVERRIDE_OBSERVABILITY_QUERY_URL']\n      # TODO Make observability URL configurable https://gitlab.com/gitlab-org/opstrace/opstrace-ui/-/issues/80\n      # Dev, test and staging instances can all point to `observe.staging.gitlab.com` by default\n      return 'https://observe.staging.gitlab.com' if Gitlab.staging? || Gitlab.dev_or_test_env?\n\n      'https://observe.gitlab.com'\n    end",
    "comment": "Returns the GitLab Observability URL ",
    "label": "",
    "id": "768"
  },
  {
    "raw_code": "def by_email(email, *additional_keys, expires_in: DEFAULT_EXPIRY)\n        key = email_key(email)\n        subkey = additional_keys.join(\":\")\n\n        Gitlab::SafeRequestStore.fetch([key, subkey]) do\n          with do |redis|\n            # Look for existing cache value\n            cached = redis.hget(key, subkey)\n\n            # Return the cached entry if set\n            break cached unless cached.nil?\n\n            # Otherwise, call the block to get the value\n            to_cache = yield(email, *additional_keys).to_s\n\n            # Set it in the cache\n            redis.hset(key, subkey, to_cache)\n\n            # Update the expiry time\n            redis.expire(key, expires_in)\n\n            # Return this new value\n            break to_cache\n          end",
    "comment": "Look up cached avatar data by email address. This accepts a block to provide the value to be cached in the event nothing is found.  Multiple calls in the same request will be served from the request store.  @param email [String] @param additional_keys [*Object] all must respond to `#to_s` @param expires_in [ActiveSupport::Duration, Integer] @yield [email, *additional_keys] yields the supplied params back to the block @return [String]",
    "label": "",
    "id": "769"
  },
  {
    "raw_code": "def delete_by_email(*emails)\n        return 0 if emails.empty?\n\n        with do |redis|\n          keys = emails.map { |email| email_key(email) }\n\n          Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n            if Gitlab::Redis::ClusterUtil.cluster?(redis)\n              Gitlab::Redis::ClusterUtil.batch_unlink(keys, redis)\n            else\n              redis.unlink(*keys)\n            end",
    "comment": "Remove one or more emails from the cache  @param emails [String] one or more emails to delete @return [Integer] the number of keys deleted",
    "label": "",
    "id": "770"
  },
  {
    "raw_code": "def email_key(email)\n        \"#{BASE_KEY}:v#{VERSION}:#{email}\"\n      end",
    "comment": "@param email [String] @return [String]",
    "label": "",
    "id": "771"
  },
  {
    "raw_code": "def self.trap_signals(signals)\n      signals.each do |signal|\n        trap(signal) do\n          yield signal\n        end",
    "comment": "Traps the given signals and yields the block whenever these signals are received.  The block is passed the name of the signal.  Example:  trap_signals(%i(HUP TERM)) do |signal| ... end",
    "label": "",
    "id": "772"
  },
  {
    "raw_code": "def self.modify_signals(signals, command)\n      signals.each { |signal| trap(signal, command) }\n    end",
    "comment": "Traps the given signals with the given command.  Example:  modify_signals(%i(HUP TERM), 'DEFAULT')",
    "label": "",
    "id": "773"
  },
  {
    "raw_code": "def self.all_alive?(pids)\n      pids.each do |pid|\n        return false unless process_alive?(pid)\n      end",
    "comment": "Returns true if all the processes are alive.",
    "label": "",
    "id": "774"
  },
  {
    "raw_code": "def highlight_map(*)\n      {}\n    end",
    "comment": "highlighting is only performed by Elasticsearch backed results",
    "label": "",
    "id": "775"
  },
  {
    "raw_code": "def aggregations(*)\n      []\n    end",
    "comment": "aggregations are only performed by Elasticsearch backed results",
    "label": "",
    "id": "776"
  },
  {
    "raw_code": "def counts(*)\n      {}\n    end",
    "comment": "direct counts are only performed by Elasticsearch backed results",
    "label": "",
    "id": "777"
  },
  {
    "raw_code": "def apply_sort(results, scope: nil)\n      # Due to different uses of sort param we prefer order_by when\n      # present\n      sort_by = ::Gitlab::Search::SortOptions.sort_and_direction(order_by, sort)\n\n      # Reset sort to default if the chosen one is not supported by scope\n      if Gitlab::Search::SortOptions::SCOPE_ONLY_SORT[sort_by] &&\n          Gitlab::Search::SortOptions::SCOPE_ONLY_SORT[sort_by].exclude?(scope)\n        sort_by = nil\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "778"
  },
  {
    "raw_code": "def projects\n      scope = limit_projects\n      scope = scope.non_archived unless filters[:include_archived]\n\n      scope.search(query, include_namespace: true, use_minimum_char_limit: false)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "779"
  },
  {
    "raw_code": "def filter_milestones_by_project(milestones)\n      candidate_project_ids = project_ids_relation\n\n      candidate_project_ids = candidate_project_ids.non_archived unless filters[:include_archived]\n\n      project_ids = milestones.of_projects(candidate_project_ids).select(:project_id).distinct.pluck(:project_id) # rubocop: disable CodeReuse/ActiveRecord\n\n      return Milestone.none if project_ids.nil?\n\n      authorized_project_ids_relation = Project.id_in(project_ids).ids_with_issuables_available_for(current_user)\n\n      milestones.of_projects(authorized_project_ids_relation)\n    end",
    "comment": "Filter milestones by authorized projects. For performance reasons project_id is being plucked to be used on a smaller query.",
    "label": "",
    "id": "780"
  },
  {
    "raw_code": "def project_events_created_between(start_time, end_time, features:)\n      Array(features).reduce(Event.none) do |events, feature|\n        events.or(contribution_events(start_time, end_time).where(project_id: authed_projects(feature)))\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- no need to move this to ActiveRecord model",
    "label": "",
    "id": "781"
  },
  {
    "raw_code": "def authed_projects(feature)\n      strong_memoize(\"#{feature}_projects\") do\n        # no need to check features access of current user, if the contributor opted-in\n        # to show all private events anyway - otherwise they would get filtered out again\n        next contributed_project_ids if contributor.include_private_contributions?\n\n        # rubocop: disable CodeReuse/ActiveRecord -- no need to move this to ActiveRecord model\n        ProjectFeature\n          .with_feature_available_for_user(feature, current_user)\n          .where(project_id: contributed_project_ids)\n          .pluck(:project_id)\n        # rubocop: enable CodeReuse/ActiveRecord\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "782"
  },
  {
    "raw_code": "def contributed_project_ids\n      # re-running the contributed projects query in each union is expensive, so\n      # use IN(project_ids...) instead. It's the intersection of two users so\n      # the list will be (relatively) short\n      @contributed_project_ids ||= projects.distinct.pluck(:id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- no need to move this to ActiveRecord model",
    "label": "",
    "id": "783"
  },
  {
    "raw_code": "def contribution_events(start_time, end_time)\n      contributor.events.created_between(start_time, end_time)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "784"
  },
  {
    "raw_code": "def host\n        asset_host = ActionController::Base.asset_host\n        if asset_host.nil? || asset_host == Gitlab.config.gitlab.base_url\n          nil\n        else\n          Gitlab.config.gitlab.base_url\n        end",
    "comment": "we only want to create full urls when there's a different asset_host configured.",
    "label": "",
    "id": "785"
  },
  {
    "raw_code": "def can_access_git?\n      true\n    end",
    "comment": "This bypasses the `can?(:access_git)`-check we normally do in `UserAccess` for CI. That way if a user was able to trigger a pipeline, then the build is allowed to clone the project.",
    "label": "",
    "id": "786"
  },
  {
    "raw_code": "def self.configurator\n        ->(chain) do\n          middlewares.each { |middleware| chain.add(middleware) }\n        end",
    "comment": "The result of this method should be passed to Sidekiq's `config.client_middleware` method eg: `config.client_middleware(&Gitlab::SidekiqMiddleware::Client.configurator)`",
    "label": "",
    "id": "787"
  },
  {
    "raw_code": "def self.configurator(metrics: true, arguments_logger: true, skip_jobs: true)\n        ->(chain) do\n          middlewares(metrics: metrics, arguments_logger: arguments_logger, skip_jobs: skip_jobs).each do |middleware|\n            chain.add(middleware)\n          end",
    "comment": "The result of this method should be passed to Sidekiq's `config.server_middleware` method eg: `config.server_middleware(&Gitlab::SidekiqMiddleware::Server.configurator)`",
    "label": "",
    "id": "788"
  },
  {
    "raw_code": "def aggregate(aggregate_query, time: Time.now, transform_value: :to_f)\n      response = query(aggregate_query, time: time)\n      response.to_h do |result|\n        key = block_given? ? yield(result['metric']) : result['metric']\n        _timestamp, value = result['value']\n        [key, value.public_send(transform_value)] # rubocop:disable GitlabSecurity/PublicSend\n      end",
    "comment": "Queries Prometheus with the given aggregate query and groups the results by mapping metric labels to their respective values.  @return [Hash] mapping labels to their aggregate numeric values, or the empty hash if no results were found",
    "label": "",
    "id": "789"
  },
  {
    "raw_code": "def method_missing(name, *args, **kwargs, &block)\n        application_settings = current_application_settings\n\n        return application_settings.send(name, *args, **kwargs, &block) if application_settings.respond_to?(name)\n\n        if respond_to_organization_setting?(name, false)\n          return ::Organizations::OrganizationSetting.for(::Current.organization.id).send(name, *args, **kwargs, &block)\n        end",
    "comment": "rubocop:disable GitlabSecurity/PublicSend -- Method calls are forwarded to one of the setting classes",
    "label": "",
    "id": "790"
  },
  {
    "raw_code": "def respond_to_missing?(name, include_private = false)\n        current_application_settings.respond_to?(name, include_private) || respond_to_organization_setting?(name, include_private) || super\n      end",
    "comment": "rubocop:enable GitlabSecurity/PublicSend",
    "label": "",
    "id": "791"
  },
  {
    "raw_code": "def self.by_id(id)\n      available_modes.detect { |s| s.id == id } || default\n    end",
    "comment": "Get a Mode by its ID  If the ID is invalid, returns the default Mode.  id - Integer ID  Returns a Mode",
    "label": "",
    "id": "792"
  },
  {
    "raw_code": "def self.default\n      by_id(default_id)\n    end",
    "comment": "Get the default Mode  Returns a Mode",
    "label": "",
    "id": "793"
  },
  {
    "raw_code": "def self.each(&block)\n      available_modes.each(&block)\n    end",
    "comment": "Iterate through each Mode  Yields the Mode object",
    "label": "",
    "id": "794"
  },
  {
    "raw_code": "def self.for_user(user)\n      if user\n        by_id(user.color_mode_id)\n      else\n        default\n      end",
    "comment": "Get the Mode for the specified user, or the default  user - User record  Returns a Mode",
    "label": "",
    "id": "795"
  },
  {
    "raw_code": "def identify_using_user(identifier)\n      user_id = identifier.gsub(\"user-\", \"\")\n\n      identify_with_cache(:user, user_id) do\n        User.find_by(id: user_id)\n      end",
    "comment": "Tries to identify a user based on a user identifier (e.g. \"user-123\"). rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "796"
  },
  {
    "raw_code": "def identify_using_ssh_key(identifier)\n      key_id = identifier.gsub(\"key-\", \"\")\n\n      identify_with_cache(:ssh_key, key_id) do\n        User.find_by_ssh_key_id(key_id)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Tries to identify a user based on an SSH key identifier (e.g. \"key-123\"). Deploy keys are excluded.",
    "label": "",
    "id": "797"
  },
  {
    "raw_code": "def identify_using_deploy_key(identifier)\n      key_id = identifier.gsub(\"key-\", \"\")\n\n      DeployKey.find_by_id(key_id)\n    end",
    "comment": "Tries to identify a deploy key using a SSH key identifier (e.g. \"key-123\").",
    "label": "",
    "id": "798"
  },
  {
    "raw_code": "def send_url(\n        url,\n        allow_localhost: true,\n        allow_redirects: false,\n        method: 'GET',\n        body: nil,\n        ssrf_filter: false,\n        headers: {},\n        timeouts: {},\n        response_statuses: {},\n        response_headers: {},\n        allowed_endpoints: [],\n        restrict_forwarded_response_headers: {}\n      )\n        params = {\n          'URL' => url,\n          'AllowRedirects' => allow_redirects,\n          'AllowLocalhost' => allow_localhost,\n          'AllowedEndpoints' => allowed_endpoints,\n          'SSRFFilter' => ssrf_filter,\n          'Body' => body.to_s,\n          'Header' => headers.transform_values { |v| Array.wrap(v) },\n          'ResponseHeaders' => response_headers.transform_values { |v| Array.wrap(v) },\n          'Method' => method\n        }.merge(restrict_forwarded_response_headers_params(restrict_forwarded_response_headers)).compact\n\n        if timeouts.present?\n          params['DialTimeout'] = \"#{timeouts[:open]}s\" if timeouts[:open]\n          params['ResponseHeaderTimeout'] = \"#{timeouts[:read]}s\" if timeouts[:read]\n        end",
    "comment": "response_statuses can be set for 'error' and 'timeout'. They are optional. Their values must be a symbol accepted by Rack::Utils::SYMBOL_TO_STATUS_CODE. Example: response_statuses : { error: :internal_server_error, timeout: :bad_request } timeouts can be given for the opening the connection and reading the response headers. Their values must be given in seconds. Example: timeouts: { open: 5, read: 5 } rubocop:disable Metrics/ParameterLists -- all arguments needed",
    "label": "",
    "id": "799"
  },
  {
    "raw_code": "def send_scaled_image(location, width, content_type)\n        params = {\n          'Location' => location,\n          'Width' => width,\n          'ContentType' => content_type\n        }\n\n        [\n          SEND_DATA_HEADER,\n          \"send-scaled-img:#{encode(params)}\"\n        ]\n      end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "800"
  },
  {
    "raw_code": "def encode(hash)\n        Base64.urlsafe_encode64(Gitlab::Json.dump(hash))\n      end",
    "comment": "This is the outermost encoding of a senddata: header. It is safe for inclusion in HTTP response headers",
    "label": "",
    "id": "801"
  },
  {
    "raw_code": "def encode_binary(binary)\n        Base64.encode64(binary)\n      end",
    "comment": "This is for encoding individual fields inside the senddata JSON that contain binary data. In workhorse, the corresponding struct field should be type []byte",
    "label": "",
    "id": "802"
  },
  {
    "raw_code": "def as_json(*_args)\n      options.as_json\n    end",
    "comment": "Allow #to_json serialization",
    "label": "",
    "id": "803"
  },
  {
    "raw_code": "def display_version_info\n        server_version = ServerInfo.new.version_info\n        return server_version if server_version&.valid?\n        return version_info_from_file if version_info_from_file.valid?\n\n        Gitlab.version_info\n      end",
    "comment": "Return GitLab KAS version info for display This is the version that is displayed on the `frontend`. This is also used to check if the version of an existing agent does not match the latest agent version. If the getServerInfo RPC call fails, we fallback to GITLAB_KAS_VERSION file; If the GITLAB_KAS_VERSION file contains a SHA, we defer instead to the Gitlab version.  For further details, see: https://gitlab.com/gitlab-org/gitlab/-/merge_requests/149794  @return [Gitlab::VersionInfo] version_info",
    "label": "",
    "id": "804"
  },
  {
    "raw_code": "def install_version_info\n        server_version = ServerInfo.new.version_info\n        return server_version.without_patch if server_version&.valid?\n        return version_info_from_file if version_info_from_file.valid?\n\n        Gitlab.version_info.without_patch\n      end",
    "comment": "Return GitLab KAS version info for installation This is the version used as the image tag when generating the command to install a Gitlab agent. If the getServerInfo RPC call fails, we fallback to GITLAB_KAS_VERSION file; If the GITLAB_KAS_VERSION file contains a SHA, we defer instead to the Gitlab version without the patch. This could mean that it might point to a Gitlab agent version that is several patches behind the latest one.  Further details: https://gitlab.com/gitlab-org/gitlab/-/merge_requests/149794  @return [Gitlab::VersionInfo] version_info",
    "label": "",
    "id": "805"
  },
  {
    "raw_code": "def external_url\n        Gitlab.config.gitlab_kas.external_url\n      end",
    "comment": "Return GitLab KAS external_url  @return [String] external_url",
    "label": "",
    "id": "806"
  },
  {
    "raw_code": "def internal_url\n        Gitlab.config.gitlab_kas.internal_url\n      end",
    "comment": "Return GitLab KAS internal_url  @return [String] internal_url",
    "label": "",
    "id": "807"
  },
  {
    "raw_code": "def enabled?\n        !!Gitlab.config['gitlab_kas']&.fetch('enabled', false)\n      end",
    "comment": "Return whether GitLab KAS is enabled  @return [Boolean] external_url",
    "label": "",
    "id": "808"
  },
  {
    "raw_code": "def loop_until(timeout: nil, limit: 1_000_000)\n      raise ArgumentError unless limit\n\n      start = Time.now\n\n      limit.times do\n        return true unless yield\n\n        return false if timeout && (Time.now - start) > timeout\n      end",
    "comment": " This helper method repeats the same task until it's expired.  Note: ExpiredLoopError does not happen until the given block finished. Please do not use this method for heavy or asynchronous operations.",
    "label": "",
    "id": "809"
  },
  {
    "raw_code": "def initialize(task, **options)\n      @task = task\n      @synchronous = options[:synchronous]\n      @name = options[:name] || self.class.name.demodulize.underscore\n      # We use a monitor, not a Mutex, because monitors allow for re-entrant locking.\n      @mutex = ::Monitor.new\n      @state = :idle\n    end",
    "comment": "Possible options: - name [String] used to identify the task in thread listings and logs (defaults to 'background_task') - synchronous [Boolean] if true, turns `start` into a blocking call",
    "label": "",
    "id": "810"
  },
  {
    "raw_code": "def self.clear\n      with_redis { |redis| redis.del(HEALTHY_SHARDS_KEY) }\n    end",
    "comment": "Clears the Redis set storing the list of healthy shards",
    "label": "",
    "id": "811"
  },
  {
    "raw_code": "def self.update(shards)\n      with_redis do |redis|\n        redis.multi do |m|\n          m.del(HEALTHY_SHARDS_KEY)\n          m.sadd(HEALTHY_SHARDS_KEY, shards) unless shards.blank?\n          m.expire(HEALTHY_SHARDS_KEY, HEALTHY_SHARDS_TIMEOUT)\n        end",
    "comment": "Updates the list of healthy shards using a Redis set  shards - An array of shard names to store",
    "label": "",
    "id": "812"
  },
  {
    "raw_code": "def self.cached_healthy_shards\n      with_redis { |redis| redis.smembers(HEALTHY_SHARDS_KEY) }\n    end",
    "comment": "Returns an array of strings of healthy shards",
    "label": "",
    "id": "813"
  },
  {
    "raw_code": "def self.healthy_shard?(shard_name)\n      with_redis { |redis| redis.sismember(HEALTHY_SHARDS_KEY, shard_name) }\n    end",
    "comment": "Checks whether the given shard name is in the list of healthy shards.  shard_name - The string to check",
    "label": "",
    "id": "814"
  },
  {
    "raw_code": "def self.healthy_shard_count\n      with_redis { |redis| redis.scard(HEALTHY_SHARDS_KEY) }\n    end",
    "comment": "Returns the number of healthy shards in the Redis set",
    "label": "",
    "id": "815"
  },
  {
    "raw_code": "def self.feature_available?(_feature)\n      false\n    end",
    "comment": "overridden in EE",
    "label": "",
    "id": "816"
  },
  {
    "raw_code": "def self.enabled?\n      false\n    end",
    "comment": "overridden in EE",
    "label": "",
    "id": "817"
  },
  {
    "raw_code": "def self.feature_file_path(_feature)\n      nil\n    end",
    "comment": "overridden in EE",
    "label": "",
    "id": "818"
  },
  {
    "raw_code": "def perform_request(http_method, path, options, &block)\n        method_name = http_method::METHOD.downcase.to_sym\n\n        unless ::Gitlab::HTTP_V2::SUPPORTED_HTTP_METHODS.include?(method_name)\n          raise ArgumentError, \"Unsupported HTTP method: '#{method_name}'.\"\n        end",
    "comment": "TODO: This method is subject to be removed We have this for now because we explicitly use the `perform_request` method in some places.",
    "label": "",
    "id": "819"
  },
  {
    "raw_code": "def without_decompression_limit\n        return yield unless Gitlab::SafeRequestStore.active?\n\n        begin\n          prev = Gitlab::SafeRequestStore[:disable_net_http_decompression]\n          Gitlab::SafeRequestStore[:disable_net_http_decompression] = true\n          yield\n        ensure\n          Gitlab::SafeRequestStore[:disable_net_http_decompression] = prev\n        end",
    "comment": "Disables the decompression limit validation for the duration of the given block.  SECURITY WARNING: Only use this method for requests to trusted web servers that are not user-controlled. For requests to user-controlled servers, set `accept-encoding: identity` in the request headers to request the source server not return a compressed response.",
    "label": "",
    "id": "820"
  },
  {
    "raw_code": "def self.protected_paths_enabled?\n      self.settings.throttle_protected_paths_enabled?\n    end",
    "comment": "Returns true if we should use the Admin area protected paths throttle",
    "label": "",
    "id": "821"
  },
  {
    "raw_code": "def image_safe_for_scaling?\n      extension_match?(SAFE_IMAGE_FOR_SCALING_EXT)\n    end",
    "comment": "For the time being, we restrict image scaling requests to the most popular and safest formats only, which are JPGs and PNGs. See https://gitlab.com/gitlab-org/gitlab/-/issues/237848 for more info.",
    "label": "",
    "id": "822"
  },
  {
    "raw_code": "def parse(string, opts = {})\n        # Parse nil as nil\n        return if string.nil?\n\n        # First we should ensure this really is a string, not some other\n        # type which purports to be a string. This handles some legacy\n        # usage of the JSON class.\n        string = string.to_s unless string.is_a?(String)\n\n        legacy_mode = legacy_mode_enabled?(opts.delete(:legacy_mode))\n\n        log_oversize_object(string)\n\n        data = adapter_load(string, **opts)\n\n        handle_legacy_mode!(data) if legacy_mode\n\n        data\n      end",
    "comment": "Parse a string and convert it to a Ruby object  @param string [String] the JSON string to convert to Ruby objects @param opts [Hash] an options hash in the standard JSON gem format @return [Boolean, String, Array, Hash] @raise [JSON::ParserError] raised if parsing fails",
    "label": "",
    "id": "823"
  },
  {
    "raw_code": "def dump(object)\n        adapter_dump(object)\n      end",
    "comment": "Restricted method for converting a Ruby object to JSON. If you need to pass options to this, you should use `.generate` instead, as the underlying implementation of this varies wildly based on the adapter in use.  This method does, in some situations, differ in the data it returns compared to .generate. Counter-intuitively, this is closest in terms of response to JSON.generate and to the default ActiveSupport .to_json method.  @param object [Object] the object to convert to JSON @return [String]",
    "label": "",
    "id": "824"
  },
  {
    "raw_code": "def generate(object, opts = {})\n        adapter_generate(object, opts)\n      end",
    "comment": "Generates JSON for an object. In Oj this takes fewer options than .dump, in the JSON gem this is the only method which takes an options argument.  @param object [Hash, Array, Object] must be hash, array, or an object that responds to .to_h or .to_json @param opts [Hash] an options hash with fewer supported settings than .dump @return [String]",
    "label": "",
    "id": "825"
  },
  {
    "raw_code": "def pretty_generate(object, opts = {})\n        ::JSON.pretty_generate(object, opts)\n      end",
    "comment": "Generates JSON for an object and makes it look purdy  The Oj variant in this looks seriously weird but these are the settings needed to emulate the style generated by the JSON gem.  NOTE: This currently ignores Oj, because Oj doesn't generate identical formatting, issue: https://github.com/ohler55/oj/issues/608  @param object [Hash, Array, Object] must be hash, array, or an object that responds to .to_h or .to_json @param opts [Hash] an options hash with fewer supported settings than .dump @return [String]",
    "label": "",
    "id": "826"
  },
  {
    "raw_code": "def parser_error\n        ::JSON::ParserError\n      end",
    "comment": "The standard parser error we should be returning. Defined in a method so we can potentially override it later.  @return [JSON::ParserError]",
    "label": "",
    "id": "827"
  },
  {
    "raw_code": "def adapter_load(string, *args, **opts)\n        opts = standardize_opts(opts)\n\n        Oj.load(string, opts)\n      rescue Oj::ParseError, EncodingError, Encoding::UndefinedConversionError, JSON::GeneratorError => ex\n        raise parser_error, ex\n      end",
    "comment": "Convert JSON string into Ruby through toggleable adapters.  Must rescue adapter-specific errors and return `parser_error`, and must also standardize the options hash to support each adapter as they all take different options.  @param string [String] the JSON string to convert to Ruby objects @param opts [Hash] an options hash in the standard JSON gem format @return [Boolean, String, Array, Hash] @raise [JSON::ParserError]",
    "label": "",
    "id": "828"
  },
  {
    "raw_code": "def adapter_dump(object, *args, **opts)\n        Oj.dump(object, opts)\n      end",
    "comment": "Take a Ruby object and convert it to a string. This method varies based on the underlying JSON interpreter. Oj treats this like JSON treats `.generate`. JSON.dump takes no options.  This supports these options to ensure this difference is recorded here, as it's very surprising. The public interface is more restrictive to prevent adapter-specific options being passed.  @overload adapter_dump(object, opts) @param object [Object] the object to convert to JSON @param opts [Hash] options as named arguments, only supported by Oj  @overload adapter_dump(object, anIO, limit) @param object [Object] the object, will have JSON.generate called on it @param anIO [Object] an IO-like object that responds to .write, default nil @param limit [Fixnum] the nested array/object limit, default nil @raise [ArgumentError] when depth limit exceeded  @return [String]",
    "label": "",
    "id": "829"
  },
  {
    "raw_code": "def adapter_generate(object, opts = {})\n        opts = standardize_opts(opts)\n\n        Oj.generate(object, opts)\n      end",
    "comment": "Generates JSON for an object but with fewer options, using toggleable adapters.  @param object [Hash, Array, Object] must be hash, array, or an object that responds to .to_h or .to_json @param opts [Hash] an options hash with fewer supported settings than .dump @return [String]",
    "label": "",
    "id": "830"
  },
  {
    "raw_code": "def standardize_opts(opts)\n        opts ||= {}\n        opts[:mode] = :rails\n        opts[:symbol_keys] = opts[:symbolize_keys] || opts[:symbolize_names]\n\n        opts\n      end",
    "comment": "Take a JSON standard options hash and standardize it to work across adapters An example of this is Oj taking :symbol_keys instead of :symbolize_names  @param opts [Hash, Nil] @return [Hash]",
    "label": "",
    "id": "831"
  },
  {
    "raw_code": "def legacy_mode_enabled?(arg_value)\n        arg_value.nil? ? false : arg_value\n      end",
    "comment": "@param [Nil, Boolean] an extracted :legacy_mode key from the opts hash @return [Boolean]",
    "label": "",
    "id": "832"
  },
  {
    "raw_code": "def handle_legacy_mode!(data)\n        raise parser_error if INVALID_LEGACY_TYPES.any? { |type| data.is_a?(type) }\n      end",
    "comment": "If legacy mode is enabled, we need to raise an error depending on the values provided in the string. This will be deprecated.  @param data [Boolean, String, Array, Hash, Object] @return [Boolean, String, Array, Hash, Object] @raise [JSON::ParserError]",
    "label": "",
    "id": "833"
  },
  {
    "raw_code": "def self.call(object, env = nil)\n        return object.to_s if object.is_a?(PrecompiledJson)\n\n        ::Gitlab::Json.dump(object)\n      end",
    "comment": "Convert an object to JSON.  The `env` param is ignored because it's not needed in either our formatter or Grape's, but it is passed through for consistency.  If explicitly supplied with a `PrecompiledJson` instance it will skip conversion and return it directly. This is mostly used in caching.  @param object [Object] @return [String]",
    "label": "",
    "id": "834"
  },
  {
    "raw_code": "def initialize(value)\n        @value = value\n      end",
    "comment": "@overload PrecompiledJson.new(\"foo\") @param value [String]  @overload PrecompiledJson.new([\"foo\", \"bar\"]) @param value [Array<String>]",
    "label": "",
    "id": "835"
  },
  {
    "raw_code": "def to_s\n        return @value if @value.is_a?(String)\n        return \"[#{@value.join(',')}]\" if @value.is_a?(Array)\n\n        raise UnsupportedFormatError\n      end",
    "comment": "Convert the value to a String. This will invoke `#to_s` on the members of the value if it's an array.  @return [String] @raise [NoMethodError] if the objects in an array doesn't support to_s @raise [PrecompiledJson::UnsupportedFormatError] if the value is neither a String or Array",
    "label": "",
    "id": "836"
  },
  {
    "raw_code": "def self.encode(object, limit: 25.megabytes)\n        buffer = StringIO.new\n        buffer_size = 0\n\n        ::Yajl::Encoder.encode(object) do |data_chunk|\n          chunk_size = data_chunk.bytesize\n\n          raise LimitExceeded if buffer_size + chunk_size > limit\n\n          buffer << data_chunk\n          buffer_size += chunk_size\n        end",
    "comment": "Generates JSON for an object or raise an error if the resulting json string is too big  @param object [Hash, Array, Object] must be hash, array, or an object that responds to .to_h or .to_json @param limit [Integer] max size of the resulting json string @return [String] @raise [LimitExceeded] if the resulting json string is bigger than the specified limit",
    "label": "",
    "id": "837"
  },
  {
    "raw_code": "def stringify(jsonified)\n        ::Gitlab::Json.dump(jsonified)\n      rescue EncodingError => ex\n        # Raise the same error as the default implementation if we encounter\n        # an error. These are usually related to invalid UTF-8 errors.\n        raise JSON::GeneratorError, ex\n      end",
    "comment": "Rails doesn't provide a way of changing the JSON adapter for render calls in controllers, so here we're overriding the parent class method to use our generator, and it's monkey-patched in config/initializers/active_support_json.rb",
    "label": "",
    "id": "838"
  },
  {
    "raw_code": "def assert_project!\n      raise \"No project! #{project.inspect} is not a Project\" unless project.is_a?(::Project)\n    end",
    "comment": "Any method that assumes that it is operating on a project should make this explicit by calling `#assert_project!`. TODO: remove when we make this class polymorphic enough not to care about projects See: https://gitlab.com/gitlab-org/gitlab/-/issues/227635",
    "label": "",
    "id": "839"
  },
  {
    "raw_code": "def users\n      groups = group.self_and_hierarchy_intersecting_with_user_groups(current_user)\n      members = GroupMember.where(group: groups).non_invite\n\n      users = super\n\n      users.where(id: members.select(:user_id))\n    end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "840"
  },
  {
    "raw_code": "def issuable_params\n      super.merge(group_id: group.id, include_subgroups: true)\n    end",
    "comment": "rubocop:enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "841"
  },
  {
    "raw_code": "def download_ability\n      raise NotImplementedError\n    end",
    "comment": "@return [Symbol] the name of a Declarative Policy ability to check",
    "label": "",
    "id": "842"
  },
  {
    "raw_code": "def push_ability\n      raise NotImplementedError\n    end",
    "comment": "@return [Symbol] the name of a Declarative Policy ability to check",
    "label": "",
    "id": "843"
  },
  {
    "raw_code": "def build_can_download_code?\n      authentication_abilities.include?(:build_download_code) && user_access.can_do_action?(:build_download_code)\n    end",
    "comment": "when accessing via the CI_JOB_TOKEN",
    "label": "",
    "id": "844"
  },
  {
    "raw_code": "def can_download?\n      deploy_key_can_download_code? ||\n        deploy_token_can_download? ||\n        build_can_download? ||\n        user_can_download? ||\n        guest_can_download?\n    end",
    "comment": "When overriding this method, be careful using super as deploy_token_can_download? and build_can_download? do not consider the download_ability in the inheriting class for deploy tokens and builds",
    "label": "",
    "id": "845"
  },
  {
    "raw_code": "def check_additional_conditions!; end\n\n    def repository_access_level\n      project&.repository_access_level\n    end\n  end",
    "comment": "overriden in EE",
    "label": "",
    "id": "846"
  },
  {
    "raw_code": "def self.cache_users_mapping(project_id, mapping)\n      cache_class.write_multiple(mapping, key_prefix: jira_user_key_prefix(project_id))\n    end",
    "comment": "Caches the mapping of jira_account_id -> gitlab user id project_id - id of a project mapping - hash in format of jira_account_id -> gitlab user id",
    "label": "",
    "id": "847"
  },
  {
    "raw_code": "def method_missing(*)\n      nil\n    end",
    "comment": "Mimic behavior of OpenStruct, which absorbs any calls into undefined properties to return `nil`.",
    "label": "",
    "id": "848"
  },
  {
    "raw_code": "def self.static_verification?\n      static_verification = Gitlab::Utils.to_boolean(ENV['STATIC_VERIFICATION'], default: false)\n\n      if static_verification && Rails.env.production?\n        warn '[WARNING] Static Verification bypass is enabled in Production.'\n      end",
    "comment": "Check whether codebase is going through static verification in order to skip executing parts of the codebase  @return [Boolean] Is the code going through static verification?",
    "label": "",
    "id": "849"
  },
  {
    "raw_code": "def ask_to_continue\n      return if Gitlab::Utils.to_boolean(ENV['GITLAB_ASSUME_YES'])\n\n      answer = prompt(Rainbow(\"Do you want to continue (yes/no)? \").blue, %w[yes no])\n      raise Gitlab::TaskAbortedByUserError unless answer == \"yes\"\n    end",
    "comment": "Ask if the user wants to continue  Returns \"yes\" the user chose to continue Raises Gitlab::TaskAbortedByUserError if the user chose *not* to continue",
    "label": "",
    "id": "850"
  },
  {
    "raw_code": "def os_name\n      os_name = run_command(%w[lsb_release -irs])\n      os_name ||=\n        if File.readable?('/etc/system-release')\n          File.read('/etc/system-release')\n        elsif File.readable?('/etc/debian_version')\n          \"Debian #{File.read('/etc/debian_version')}\"\n        elsif File.readable?('/etc/SuSE-release')\n          File.read('/etc/SuSE-release')\n        elsif os_x_version = run_command(%w[sw_vers -productVersion])\n          \"Mac OS X #{os_x_version}\"\n        elsif File.readable?('/etc/os-release')\n          File.read('/etc/os-release').match(/PRETTY_NAME=\\\"(.+)\\\"/)[1]\n        end",
    "comment": "Check which OS is running  It will primarily use lsb_relase to determine the OS. It has fallbacks to Debian, SuSE, OS X and systems running systemd.",
    "label": "",
    "id": "851"
  },
  {
    "raw_code": "def prompt(message, choices = nil)\n      begin\n        print(message)\n        answer = $stdin.gets.chomp\n      end while choices.present? && choices.exclude?(answer)\n      answer\n    end",
    "comment": "Prompt the user to input something  message - the message to display before input choices - array of strings of acceptable answers or nil for any answer  Returns the user's answer",
    "label": "",
    "id": "852"
  },
  {
    "raw_code": "def prompt_for_password(message = 'Enter password: ')\n      unless $stdin.tty?\n        print(message)\n        return $stdin.gets.chomp\n      end",
    "comment": "Prompt the user to input a password  message - custom message to display before input",
    "label": "",
    "id": "853"
  },
  {
    "raw_code": "def run_and_match(command, regexp)\n      run_command(command).try(:match, regexp)\n    end",
    "comment": "Runs the given command and matches the output against the given pattern  Returns nil if nothing matched Returns the MatchData if the pattern matched  see also #run_command see also String#match",
    "label": "",
    "id": "854"
  },
  {
    "raw_code": "def run_command(command)\n      output, _ = Gitlab::Popen.popen(command)\n      output\n    rescue Errno::ENOENT\n      '' # if the command does not exist, return an empty string\n    end",
    "comment": "Runs the given command  Returns '' if the command was not found Returns the output of the command otherwise  see also #run_and_match",
    "label": "",
    "id": "855"
  },
  {
    "raw_code": "def run_command!(command)\n      output, status = Gitlab::Popen.popen(command)\n\n      raise Gitlab::TaskFailedError, output unless status == 0\n\n      output\n    end",
    "comment": "Runs the given command and raises a Gitlab::TaskFailedError exception if the command does not exit with 0  Returns the output of the command otherwise",
    "label": "",
    "id": "856"
  },
  {
    "raw_code": "def get_version(component_version)\n      # If not a valid version string following SemVer it is probably a branch name or a SHA\n      # commit of one of our own component so it doesn't need `v` prepended\n      return component_version unless /^\\d+\\.\\d+\\.\\d+(-rc\\d+)?$/.match?(component_version)\n\n      \"v#{component_version}\"\n    end",
    "comment": "this function implements the same logic we have in omnibus for dealing with components version",
    "label": "",
    "id": "857"
  },
  {
    "raw_code": "def self.body_classes\n      available_schemes.collect(&:css_class).uniq.join(' ')\n    end",
    "comment": "Convenience method to get a space-separated String of all the color scheme classes that might be applied to a code block.  Returns a String",
    "label": "",
    "id": "858"
  },
  {
    "raw_code": "def self.by_id(id)\n      available_schemes.detect { |s| s.id == id } || default\n    end",
    "comment": "Get a Scheme by its ID  If the ID is invalid, returns the default Scheme.  id - Integer ID  Returns a Scheme",
    "label": "",
    "id": "859"
  },
  {
    "raw_code": "def self.count\n      available_schemes.size\n    end",
    "comment": "Returns the number of defined Schemes",
    "label": "",
    "id": "860"
  },
  {
    "raw_code": "def self.default\n      by_id(Gitlab::CurrentSettings.default_syntax_highlighting_theme)\n    end",
    "comment": "Get the default Scheme  Returns a Scheme",
    "label": "",
    "id": "861"
  },
  {
    "raw_code": "def self.default_dark\n      by_id(Gitlab::CurrentSettings.default_dark_syntax_highlighting_theme)\n    end",
    "comment": "Get the default dark Scheme  Returns a Scheme",
    "label": "",
    "id": "862"
  },
  {
    "raw_code": "def self.each(&block)\n      available_schemes.each(&block)\n    end",
    "comment": "Iterate through each Scheme  Yields the Scheme object",
    "label": "",
    "id": "863"
  },
  {
    "raw_code": "def self.for_user(user)\n      return default unless user\n\n      if user_prefers_dark_mode?(user)\n        dark_for_user(user)\n      else\n        light_for_user(user)\n      end",
    "comment": "Get the Scheme for the specified user, or the default  user - User record  Returns a Scheme",
    "label": "",
    "id": "864"
  },
  {
    "raw_code": "def self.light_for_user(user)\n      if user\n        by_id(user.color_scheme_id)\n      else\n        default\n      end",
    "comment": "Get the light Scheme for the specified user, or the default  user - User record  Returns a Scheme",
    "label": "",
    "id": "865"
  },
  {
    "raw_code": "def self.dark_for_user(user)\n      if user && !user.dark_color_scheme_id.nil?\n        by_id(user.dark_color_scheme_id)\n      else\n        default_dark\n      end",
    "comment": "Get the dark Scheme for the specified user, or the default  user - User record  Returns a Scheme",
    "label": "",
    "id": "866"
  },
  {
    "raw_code": "def self.user_id(user)\n      user_id = user&.id\n\n      return UNKNOWN_ID unless user_id\n      raise ArgumentError, 'must pass a user instance' unless user.is_a?(User)\n\n      Gitlab::CryptoHelper.sha256(\"#{instance_id}#{user_id}\")\n    end",
    "comment": "Generates a globally unique user_id. This allows us to anonymously identify even self-managed users and instances that make requests into GitLab infrastructure.",
    "label": "",
    "id": "867"
  },
  {
    "raw_code": "def initialize(logger = Gitlab::AppLogger)\n      @logger = logger\n    end",
    "comment": "Initializes the class  @param [Gitlab::Logger] logger",
    "label": "",
    "id": "868"
  },
  {
    "raw_code": "def accessible?\n      open_authorized_keys_file('r') { true }\n    rescue Errno::ENOENT, Errno::EACCES\n      false\n    end",
    "comment": "Checks if the file is accessible or not  @return [Boolean]",
    "label": "",
    "id": "869"
  },
  {
    "raw_code": "def create\n      open_authorized_keys_file(File::CREAT) { true }\n    rescue Errno::EACCES\n      false\n    end",
    "comment": "Creates the authorized_keys file if it doesn't exist  @return [Boolean]",
    "label": "",
    "id": "870"
  },
  {
    "raw_code": "def add_key(id, key)\n      lock do\n        public_key = strip(key)\n        logger.info(\"Adding key (#{id}): #{public_key}\")\n        open_authorized_keys_file('a') { |file| file.puts(key_line(id, public_key)) }\n      end",
    "comment": "Add id and its key to the authorized_keys file  @param [String] id identifier of key prefixed by `key-` @param [String] key public key to be added @return [Boolean]",
    "label": "",
    "id": "871"
  },
  {
    "raw_code": "def batch_add_keys(keys)\n      lock(300) do # Allow 300 seconds (5 minutes) for batch_add_keys\n        open_authorized_keys_file('a') do |file|\n          keys.each do |key|\n            public_key = strip(key.key)\n            logger.info(\"Adding key (#{key.shell_id}): #{public_key}\")\n            file.puts(key_line(key.shell_id, public_key))\n          end",
    "comment": "Atomically add all the keys to the authorized_keys file  @param [Array<::Key>] keys list of Key objects to be added @return [Boolean]",
    "label": "",
    "id": "872"
  },
  {
    "raw_code": "def remove_key(id)\n      lock do\n        logger.info(\"Removing key (#{id})\")\n        open_authorized_keys_file('r+') do |f|\n          while line = f.gets\n            next unless line.start_with?(\"command=\\\"#{command(id)}\\\"\")\n\n            f.seek(-line.length, IO::SEEK_CUR)\n            # Overwrite the line with #'s. Because the 'line' variable contains\n            # a terminating '\\n', we write line.length - 1 '#' characters.\n            f.write('#' * (line.length - 1))\n          end",
    "comment": "Remove key by ID from the authorized_keys file  @param [String] id identifier of the key to be removed prefixed by `key-` @return [Boolean]",
    "label": "",
    "id": "873"
  },
  {
    "raw_code": "def clear\n      open_authorized_keys_file('w') { |file| file.puts '# Managed by gitlab-rails' }\n\n      true\n    end",
    "comment": "Clear the authorized_keys file  @return [Boolean]",
    "label": "",
    "id": "874"
  },
  {
    "raw_code": "def list_key_ids\n      logger.info('Listing all key IDs')\n\n      [].tap do |a|\n        open_authorized_keys_file('r') do |f|\n          f.each_line do |line|\n            key_id = line.match(/key-(\\d+)/)\n\n            next unless key_id\n\n            a << key_id[1].chomp.to_i\n          end",
    "comment": "Read the authorized_keys file and return IDs of each key  @return [Array<Integer>]",
    "label": "",
    "id": "875"
  },
  {
    "raw_code": "def initialize(gl_repository)\n      @gl_repository = gl_repository\n      @key = \"git-receive-pack-reference-counter:#{gl_repository}\"\n    end",
    "comment": "Reference Counter instance  @example Gitlab::ReferenceCounter.new('project-1')  @see Gitlab::GlRepository::RepoType.identifier_for_repositorable @param [String] gl_repository repository identifier",
    "label": "",
    "id": "876"
  },
  {
    "raw_code": "def value\n      Gitlab::Redis::SharedState.with do |redis|\n        (redis.get(key) || 0).to_i\n      end",
    "comment": "Return the actual counter value  @return [Integer] value",
    "label": "",
    "id": "877"
  },
  {
    "raw_code": "def increase\n      redis_cmd do |redis|\n        redis.incr(key)\n        redis.expire(key, REFERENCE_EXPIRE_TIME)\n      end",
    "comment": "Increase the counter  @return [Boolean] whether operation was a success",
    "label": "",
    "id": "878"
  },
  {
    "raw_code": "def decrease\n      redis_cmd do |redis|\n        current_value = redis.decr(key)\n        if current_value < 0\n          Gitlab::AppLogger.warn(\"Reference counter for #{gl_repository} decreased \" \\\n            \"when its value was less than 1. Resetting the counter.\")\n          redis.del(key)\n        end",
    "comment": "Decrease the counter  @return [Boolean] whether operation was a success",
    "label": "",
    "id": "879"
  },
  {
    "raw_code": "def reset!\n      redis_cmd do |redis|\n        redis.del(key)\n      end",
    "comment": "Reset the reference counter  @private Used internally by SRE and debugging purpose @return [Boolean] whether reset was a success",
    "label": "",
    "id": "880"
  },
  {
    "raw_code": "def expires_in\n      Gitlab::Redis::SharedState.with do |redis|\n        redis.ttl(key)\n      end",
    "comment": "When the reference counter would expire  @api private Used internally by SRE and debugging purpose @return [Integer] Number in seconds until expiration or false if never",
    "label": "",
    "id": "881"
  },
  {
    "raw_code": "def notes_finder(type)\n      note_finder = NotesFinder.new(@current_user, search: query, target_type: type, project: project)\n      note_finder.execute.user.order(updated_at: :desc)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "882"
  },
  {
    "raw_code": "def commits(limit:)\n      @commits ||= find_commits(query, limit: limit)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "883"
  },
  {
    "raw_code": "def cache_method(name, fallback: nil)\n        uncached_name = alias_uncached_method(name)\n\n        define_method(name) do\n          cache_method_output(name, fallback: fallback) do\n            __send__(uncached_name) # rubocop:disable GitlabSecurity/PublicSend\n          end",
    "comment": "Caches and strongly memoizes the method.  This only works for methods that do not take any arguments.  name     - The name of the method to be cached. fallback - A value to fall back to if the repository does not exist, or in case of a Git error. Defaults to nil.",
    "label": "",
    "id": "884"
  },
  {
    "raw_code": "def cache_method_as_redis_set(name, fallback: nil)\n        uncached_name = alias_uncached_method(name)\n\n        define_method(name) do\n          cache_method_output_as_redis_set(name, fallback: fallback) do\n            __send__(uncached_name) # rubocop:disable GitlabSecurity/PublicSend\n          end",
    "comment": "Caches and strongly memoizes the method as a Redis Set.  This only works for methods that do not take any arguments. The method should return an Array of Strings to be cached.  In addition to overriding the named method, a \"name_include?\" method is defined. This uses the \"SISMEMBER\" query to efficiently check membership without needing to load the entire set into memory.  name     - The name of the method to be cached. fallback - A value to fall back to if the repository does not exist, or in case of a Git error. Defaults to nil.  It is not safe to use this method prior to the release of 12.3, since 12.2 does not correctly invalidate the redis set cache value. A mixed code environment containing both 12.2 and 12.3 nodes breaks, while a mixed code environment containing both 12.3 and 12.4 nodes will work.",
    "label": "",
    "id": "885"
  },
  {
    "raw_code": "def cache_method_asymmetrically(name)\n        uncached_name = alias_uncached_method(name)\n\n        define_method(name) do\n          cache_method_output_asymmetrically(name) do\n            __send__(uncached_name) # rubocop:disable GitlabSecurity/PublicSend\n          end",
    "comment": "Caches truthy values from the method. All values are strongly memoized, and cached in RequestStore.  Currently only used to cache `exists?` since stale false values are particularly troublesome. This can occur, for example, when an NFS mount is temporarily down.  This only works for methods that do not take any arguments.  name - The name of the method to be cached.",
    "label": "",
    "id": "886"
  },
  {
    "raw_code": "def memoize_method(name, fallback: nil)\n        uncached_name = alias_uncached_method(name)\n\n        define_method(name) do\n          memoize_method_output(name, fallback: fallback) do\n            __send__(uncached_name) # rubocop:disable GitlabSecurity/PublicSend\n          end",
    "comment": "Strongly memoizes the method.  This only works for methods that do not take any arguments.  name     - The name of the method to be memoized. fallback - A value to fall back to if the repository does not exist, or in case of a Git error. Defaults to nil. The fallback value is not memoized.",
    "label": "",
    "id": "887"
  },
  {
    "raw_code": "def alias_uncached_method(name)\n        uncached_name = :\"_uncached_#{name}\"\n\n        alias_method(uncached_name, name)\n\n        uncached_name\n      end",
    "comment": "Prepends \"_uncached_\" to the target method name  Returns the uncached method name",
    "label": "",
    "id": "888"
  },
  {
    "raw_code": "def request_store_cache\n      raise NotImplementedError\n    end",
    "comment": "RequestStore-backed RepositoryCache to be used. Should be overridden by the including class",
    "label": "",
    "id": "889"
  },
  {
    "raw_code": "def cache\n      raise NotImplementedError\n    end",
    "comment": "RepositoryCache to be used. Should be overridden by the including class",
    "label": "",
    "id": "890"
  },
  {
    "raw_code": "def redis_set_cache\n      raise NotImplementedError\n    end",
    "comment": "RepositorySetCache to be used. Should be overridden by the including class",
    "label": "",
    "id": "891"
  },
  {
    "raw_code": "def redis_hash_cache\n      raise NotImplementedError\n    end",
    "comment": "RepositoryHashCache to be used. Should be overridden by the including class",
    "label": "",
    "id": "892"
  },
  {
    "raw_code": "def cached_methods\n      raise NotImplementedError\n    end",
    "comment": "List of cached methods. Should be overridden by the including class",
    "label": "",
    "id": "893"
  },
  {
    "raw_code": "def cache_method_output(name, fallback: nil, &block)\n      memoize_method_output(name, fallback: fallback) do\n        cache.fetch(name, &block)\n      end",
    "comment": "Caches and strongly memoizes the supplied block.  name     - The name of the method to be cached. fallback - A value to fall back to if the repository does not exist, or in case of a Git error. Defaults to nil.",
    "label": "",
    "id": "894"
  },
  {
    "raw_code": "def cache_method_output_as_redis_set(name, fallback: nil, &block)\n      memoize_method_output(name, fallback: fallback) do\n        redis_set_cache.fetch(name, &block).sort\n      end",
    "comment": "Caches and strongly memoizes the supplied block as a Redis Set. The result will be provided as a sorted array.  name     - The name of the method to be cached. fallback - A value to fall back to if the repository does not exist, or in case of a Git error. Defaults to nil.",
    "label": "",
    "id": "895"
  },
  {
    "raw_code": "def cache_method_output_asymmetrically(name, &block)\n      memoize_method_output(name) do\n        request_store_cache.fetch(name) do\n          cache.fetch_without_caching_false(name, &block)\n        end",
    "comment": "Caches truthy values from the supplied block. All values are strongly memoized, and cached in RequestStore.  Currently only used to cache `exists?` since stale false values are particularly troublesome. This can occur, for example, when an NFS mount is temporarily down.  name - The name of the method to be cached.",
    "label": "",
    "id": "896"
  },
  {
    "raw_code": "def memoize_method_output(name, fallback: nil, &block)\n      no_repository_fallback(name, fallback: fallback) do\n        strong_memoize(memoizable_name(name), &block)\n      end",
    "comment": "Strongly memoizes the supplied block.  name     - The name of the method to be memoized. fallback - A value to fall back to if the repository does not exist, or in case of a Git error. Defaults to nil. The fallback value is not memoized.",
    "label": "",
    "id": "897"
  },
  {
    "raw_code": "def no_repository_fallback(name, fallback: nil, &block)\n      # Avoid unnecessary gRPC invocations\n      return fallback if fallback && fallback_early?(name)\n\n      yield\n    rescue Gitlab::Git::Repository::NoRepository\n      # Even if the `#exists?` check in `fallback_early?` passes, these errors\n      # might still occur (for example because of a non-existing HEAD). We\n      # want to gracefully handle this and not memoize anything.\n      fallback\n    end",
    "comment": "Returns the fallback value if the repository does not exist",
    "label": "",
    "id": "898"
  },
  {
    "raw_code": "def expire_method_caches(methods)\n      methods.each do |name|\n        unless cached_methods.include?(name.to_sym)\n          Gitlab::AppLogger.error \"Requested to expire non-existent method '#{name}' for Repository\"\n          next\n        end",
    "comment": "Expires the caches of a specific set of methods",
    "label": "",
    "id": "899"
  },
  {
    "raw_code": "def fallback_early?(method_name)\n      # Avoid infinite loop\n      return false if method_name == :exists?\n\n      !exists?\n    end",
    "comment": "All cached repository methods depend on the existence of a Git repository, so if the repository doesn't exist, we already know not to call it.",
    "label": "",
    "id": "900"
  },
  {
    "raw_code": "def using_tmp_keychain(&block)\n      if MUTEX.locked? && MUTEX.owned?\n        optimistic_using_tmp_keychain(&block)\n      else\n        ActiveSupport::Dependencies.interlock.permit_concurrent_loads do\n          MUTEX.synchronize do\n            optimistic_using_tmp_keychain(&block)\n          end",
    "comment": "Allows thread safe switching of temporary keychain files  1. The current thread may use nesting of temporary keychain 2. Another thread needs to wait for the lock to be released",
    "label": "",
    "id": "901"
  },
  {
    "raw_code": "def current_home_dir\n      GPGME::Engine.info.first.home_dir || GPGME::Engine.dirinfo('homedir')\n    end",
    "comment": "1. Returns the custom home directory if one has been set by calling `GPGME::Engine.home_dir=` 2. Returns the default home directory otherwise",
    "label": "",
    "id": "902"
  },
  {
    "raw_code": "def generate_configuration(toml_data, config_path, force: false)\n      FileUtils.rm_f(config_path) if force\n\n      File.open(config_path, File::WRONLY | File::CREAT | File::EXCL) do |f|\n        f.puts toml_data\n      end",
    "comment": "rubocop:disable Rails/Output",
    "label": "",
    "id": "903"
  },
  {
    "raw_code": "def configuration_toml(gitaly_dir, storage_paths, options)\n          socket_path = ensure_single_socket!\n          config = { socket_path: socket_path.delete_prefix('unix:') }\n\n          if Rails.env.test?\n            # Override the set gitaly_address since Praefect is in the loop\n            socket_path = File.join(gitaly_dir, options[:gitaly_socket] || \"gitaly.socket\")\n            prometheus_listen_addr = options[:prometheus_listen_addr]\n\n            config = {\n              socket_path: socket_path.delete_prefix('unix:'),\n              auth: { token: 'secret' },\n              # Compared to production, tests run in constrained environments. This\n              # number is meant to grow with the number of concurrent rails requests /\n              # sidekiq jobs, and concurrency will be low anyway in test.\n              git: {\n                catfile_cache_size: 5,\n                use_bundled_binaries: true\n              },\n              prometheus_listen_addr: prometheus_listen_addr\n            }.compact\n          end",
    "comment": "We cannot create config.toml files for all possible Gitaly configuations. For instance, if Gitaly is running on another machine then it makes no sense to write a config.toml file on the current machine. This method will only generate a configuration for the most common and simplest case: when we have exactly one Gitaly process and we are sure it is running locally because it uses a Unix socket. For development and testing purposes, an extra storage is added to gitaly, which is not known to Rails, but must be explicitly stubbed.",
    "label": "",
    "id": "904"
  },
  {
    "raw_code": "def ensure_single_socket!\n          addresses = Gitlab.config.repositories.storages.map { |_, storage| storage[:gitaly_address] }.uniq\n\n          raise ArgumentError, \"Your gitlab.yml contains more than one gitaly_address.\" if addresses.length > 1\n\n          address = addresses.first\n\n          if URI(address).scheme != 'unix'\n            raise ArgumentError, \"Automatic config.toml generation only supports 'unix:' addresses.\"\n          end",
    "comment": "We cannot create config.toml files for all possible Gitaly configurations. For instance, if Gitaly is running on another machine then it makes no sense to write a config.toml file on the current machine. This method validates that we have the most common and simplest case: when we have exactly one Gitaly process and we are sure it is running locally because it uses a Unix socket.",
    "label": "",
    "id": "905"
  },
  {
    "raw_code": "def enabled?\n      return false unless feature_flag_defined?\n      return false unless available?\n      return false unless ::Feature.enabled?(:gitlab_experiment, type: :ops)\n\n      feature_flag_instance.state != :off\n    end",
    "comment": "For this rollout strategy to consider an experiment as enabled, we must:  - have a feature flag yaml file that declares it. - be in an environment that permits it. - not have rolled out the feature flag at all (no percent of actors, no inclusions, etc.)",
    "label": "",
    "id": "906"
  },
  {
    "raw_code": "def execute_assignment\n      super if ::Feature.enabled?(feature_flag_name, self, type: :experiment)\n    end",
    "comment": "For assignment we first check to see if our feature flag is enabled for \"self\". This is done by calling `#flipper_id` (used behind the scenes by `Feature`). By default this is our `experiment.id` (or more specifically, the context key, which is an anonymous SHA generated using the details of an experiment.  If the `Feature.enabled?` check is false, we return nil implicitly, which will assign the control. Otherwise we call super, which will assign a variant based on our provided distribution rules. Otherwise we will assign a variant evenly across the behaviours without control.",
    "label": "",
    "id": "907"
  },
  {
    "raw_code": "def flipper_id\n      return experiment.flipper_id if experiment.respond_to?(:flipper_id)\n\n      \"Experiment;#{id}\"\n    end",
    "comment": "This is what's provided to the `Feature.enabled?` call that will be used to determine experiment inclusion. An experiment may provide an override for this method to make the experiment work on user, group, or projects.  For example, when running an experiment on a project, you could make the experiment assignable by project (using chatops) by implementing a `flipper_id` method in the experiment:  def flipper_id context.project.flipper_id end  Or even cleaner, simply delegate it:  delegate :flipper_id, to: -> { context.project }",
    "label": "",
    "id": "908"
  },
  {
    "raw_code": "def self.install\n      require 'stackprof'\n      require 'tmpdir'\n\n      if Gitlab::Runtime.sidekiq?\n        Sidekiq.configure_server do |config|\n          config.on :startup do\n            on_worker_start\n          end",
    "comment": "this is a workaround for sidekiq, which defines its own SIGUSR2 handler. by defering to the sidekiq startup event, we get to set up our own handler late enough. see also: https://github.com/mperham/sidekiq/pull/4653",
    "label": "",
    "id": "909"
  },
  {
    "raw_code": "def self.throttled_response_headers(matched, match_data)\n      # Match data example:\n      # {:discriminator=>\"127.0.0.1\", :count=>12, :period=>60 seconds, :limit=>1, :epoch_time=>1609833930}\n      # Source: https://github.com/rack/rack-attack/blob/v6.3.0/lib/rack/attack/throttle.rb#L33\n      period = match_data[:period]\n      limit = match_data[:limit]\n      rounded_limit = (limit.to_f * 1.minute / match_data[:period]).ceil\n      observed = match_data[:count]\n      now = match_data[:epoch_time]\n      retry_after = period - (now % period)\n      reset_time = Time.at(now + retry_after) # rubocop:disable Rails/TimeZone\n      {\n        'RateLimit-Name' => matched.to_s,\n        'RateLimit-Limit' => rounded_limit.to_s,\n        'RateLimit-Observed' => observed.to_s,\n        'RateLimit-Remaining' => (limit > observed ? limit - observed : 0).to_s,\n        'RateLimit-Reset' => reset_time.to_i.to_s,\n        'RateLimit-ResetTime' => reset_time.httpdate,\n        'Retry-After' => retry_after.to_s\n      }\n    end",
    "comment": "Rate Limit HTTP headers are not standardized anywhere. This is the latest draft submitted to IETF: https://github.com/ietf-wg-httpapi/ratelimit-headers/blob/main/draft-ietf-httpapi-ratelimit-headers.md  This method implement the most viable parts of the headers. Those headers will be sent back to the client when it gets throttled.  - RateLimit-Limit: indicates the request quota associated to the client in 60 seconds. The time window for the quota here is supposed to be mirrored to throttle_*_period_in_seconds application settings. However, our HAProxy as well as some ecosystem libraries are using a fixed 60-second window. Therefore, the returned limit is approximately rounded up to fit into that window.  - RateLimit-Observed: indicates the current request amount associated to the client within the time window.  - RateLimit-Remaining: indicates the remaining quota within the time window. It is the result of RateLimit-Limit - RateLimit-Remaining  - Retry-After: the remaining duration in seconds until the quota is reset. This is a standardized HTTP header: https://www.rfc-editor.org/rfc/rfc7231#page-69  - RateLimit-Reset: the point of time that the request quota is reset, in Unix time  - RateLimit-ResetTime: the point of time that the request quota is reset, in HTTP date format ",
    "label": "",
    "id": "910"
  },
  {
    "raw_code": "def self.normalized_base_url(url)\n      parsed = Utils.parse_url(url)\n      return unless parsed\n\n      if parsed.port\n        format(\"%{scheme}://%{host}:%{port}\", scheme: parsed.scheme, host: parsed.host, port: parsed.port)\n      else\n        format(\"%{scheme}://%{host}\", scheme: parsed.scheme, host: parsed.host)\n      end",
    "comment": "Returns hostname of a URL.  @param url [String] URL to parse @return [String|Nilclass] Normalized base URL, or nil if url was unparsable.",
    "label": "",
    "id": "911"
  },
  {
    "raw_code": "def system_usage_data\n        {\n          counts: {\n            assignee_lists: count(List.assignee),\n            ci_external_pipelines: count(::Ci::Pipeline.external),\n            ci_pipeline_config_auto_devops: count(::Ci::Pipeline.auto_devops_source),\n            ci_pipeline_config_repository: count(::Ci::Pipeline.repository_source),\n            ci_triggers: count(::Ci::Trigger),\n            ci_pipeline_schedules: count(::Ci::PipelineSchedule),\n            auto_devops_enabled: count(::ProjectAutoDevops.enabled),\n            auto_devops_disabled: count(::ProjectAutoDevops.disabled),\n            deploy_keys: count(DeployKey),\n            feature_flags: count(Operations::FeatureFlag),\n            environments: count(::Environment),\n            clusters: count(::Clusters::Cluster),\n            clusters_enabled: count(::Clusters::Cluster.enabled),\n            project_clusters_enabled: count(::Clusters::Cluster.enabled.project_type),\n            group_clusters_enabled: count(::Clusters::Cluster.enabled.group_type),\n            instance_clusters_enabled: count(::Clusters::Cluster.enabled.instance_type),\n            clusters_disabled: count(::Clusters::Cluster.disabled),\n            project_clusters_disabled: count(::Clusters::Cluster.disabled.project_type),\n            group_clusters_disabled: count(::Clusters::Cluster.disabled.group_type),\n            instance_clusters_disabled: count(::Clusters::Cluster.disabled.instance_type),\n            clusters_platforms_eks: count(::Clusters::Cluster.aws_installed.enabled),\n            clusters_platforms_gke: count(::Clusters::Cluster.gcp_installed.enabled),\n            clusters_platforms_user: count(::Clusters::Cluster.user_provided.enabled),\n            clusters_management_project: count(::Clusters::Cluster.with_management_project),\n            kubernetes_agents: count(::Clusters::Agent),\n            kubernetes_agents_with_token: distinct_count(::Clusters::AgentToken, :agent_id),\n            in_review_folder: count(::Environment.in_review_folder),\n            groups: count(Group),\n            issues: add_metric('CountIssuesMetric', time_frame: 'all'),\n            issues_created_from_gitlab_error_tracking_ui: count(SentryIssue),\n            issues_with_associated_zoom_link: count(ZoomMeeting.added_to_issue),\n            issues_using_zoom_quick_actions: distinct_count(ZoomMeeting, :issue_id),\n            incident_issues: count(::Issue.with_issue_type(:incident), start: minimum_id(Issue), finish: maximum_id(Issue)),\n            alert_bot_incident_issues: count(::Issue.authored(::Users::Internal.alert_bot), start: minimum_id(Issue), finish: maximum_id(Issue)),\n            keys: count(Key),\n            label_lists: count(List.label),\n            lfs_objects: count(LfsObject),\n            milestone_lists: count(List.milestone),\n            milestones: count(Milestone),\n            projects_with_packages: distinct_count(::Packages::Package, :project_id),\n            packages: count(::Packages::Package),\n            pages_domains: count(PagesDomain),\n            pool_repositories: count(PoolRepository),\n            projects: count(Project),\n            projects_creating_incidents: distinct_count(Issue.with_issue_type(:incident), :project_id),\n            projects_imported_from_github: count(Project.where(import_type: 'github')),\n            projects_with_repositories_enabled: count(ProjectFeature.where('repository_access_level > ?', ProjectFeature::DISABLED)),\n            projects_with_error_tracking_enabled: count(::ErrorTracking::ProjectErrorTrackingSetting.where(enabled: true)),\n            projects_with_enabled_alert_integrations: distinct_count(::AlertManagement::HttpIntegration.active, :project_id),\n            projects_with_terraform_reports: distinct_count(::Ci::JobArtifact.of_report_type(:terraform), :project_id),\n            projects_with_terraform_states: distinct_count(::Terraform::State, :project_id),\n            protected_branches: count(ProtectedBranch),\n            protected_branches_except_default: count(ProtectedBranch.where.not(name: ['main', 'master', Gitlab::CurrentSettings.default_branch_name])),\n            releases: count(Release),\n            remote_mirrors: count(RemoteMirror),\n            suggestions: count(Suggestion),\n            terraform_reports: count(::Ci::JobArtifact.of_report_type(:terraform)),\n            terraform_states: count(::Terraform::State),\n            todos: count(Todo),\n            uploads: count(Upload),\n            web_hooks: count(WebHook),\n            labels: count(Label),\n            merge_requests: count(MergeRequest),\n            notes: count(Note)\n          }.merge(\n            user_preferences_usage,\n            service_desk_counts\n          )\n        }\n      end",
    "comment": "rubocop: disable Metrics/AbcSize rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "912"
  },
  {
    "raw_code": "def system_usage_data_license\n        {\n          license: {}\n        }\n      end",
    "comment": "rubocop: enable Metrics/AbcSize rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "913"
  },
  {
    "raw_code": "def user_preferences_usage\n        {\n          user_preferences_user_gitpod_enabled: count(UserPreference.with_user.gitpod_enabled.merge(User.active))\n        }\n      end",
    "comment": "augmented in EE",
    "label": "",
    "id": "914"
  },
  {
    "raw_code": "def usage_activity_by_stage(key = :usage_activity_by_stage, time_period = {})\n        {\n          key => {\n            configure: usage_activity_by_stage_configure(time_period),\n            create: usage_activity_by_stage_create(time_period),\n            enablement: usage_activity_by_stage_enablement(time_period),\n            manage: usage_activity_by_stage_manage(time_period),\n            monitor: usage_activity_by_stage_monitor(time_period),\n            package: usage_activity_by_stage_package(time_period),\n            plan: usage_activity_by_stage_plan(time_period),\n            release: usage_activity_by_stage_release(time_period),\n            verify: usage_activity_by_stage_verify(time_period)\n          }\n        }\n      end",
    "comment": "Source: https://gitlab.com/gitlab-data/analytics/blob/master/transform/snowflake-dbt/data/ping_metrics_to_stage_mapping_data.csv",
    "label": "",
    "id": "915"
  },
  {
    "raw_code": "def usage_activity_by_stage_configure(time_period)\n        {\n          clusters_management_project: clusters_user_distinct_count(::Clusters::Cluster.with_management_project, time_period),\n          clusters_disabled: clusters_user_distinct_count(::Clusters::Cluster.disabled, time_period),\n          clusters_enabled: clusters_user_distinct_count(::Clusters::Cluster.enabled, time_period),\n          clusters_platforms_gke: clusters_user_distinct_count(::Clusters::Cluster.gcp_installed.enabled, time_period),\n          clusters_platforms_eks: clusters_user_distinct_count(::Clusters::Cluster.aws_installed.enabled, time_period),\n          clusters_platforms_user: clusters_user_distinct_count(::Clusters::Cluster.user_provided.enabled, time_period),\n          instance_clusters_disabled: clusters_user_distinct_count(::Clusters::Cluster.disabled.instance_type, time_period),\n          instance_clusters_enabled: clusters_user_distinct_count(::Clusters::Cluster.enabled.instance_type, time_period),\n          group_clusters_disabled: clusters_user_distinct_count(::Clusters::Cluster.disabled.group_type, time_period),\n          group_clusters_enabled: clusters_user_distinct_count(::Clusters::Cluster.enabled.group_type, time_period),\n          project_clusters_disabled: clusters_user_distinct_count(::Clusters::Cluster.disabled.project_type, time_period),\n          project_clusters_enabled: clusters_user_distinct_count(::Clusters::Cluster.enabled.project_type, time_period),\n          # These two `projects_slack_x` metrics are owned by the Manage stage, but are in this method as their key paths can't change.\n          # See https://gitlab.com/gitlab-org/gitlab/-/merge_requests/123442#note_1427961339.\n          projects_slack_notifications_active: distinct_count(::Project.with_slack_integration.where(time_period), :creator_id),\n          projects_slack_slash_active: distinct_count(::Project.with_slack_slash_commands_integration.where(time_period), :creator_id)\n        }\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord rubocop: disable UsageData/LargeTable",
    "label": "",
    "id": "916"
  },
  {
    "raw_code": "def usage_activity_by_stage_create(time_period)\n        {\n          deploy_keys: distinct_count(::DeployKey.where(time_period), :user_id),\n          keys: distinct_count(::Key.regular_keys.where(time_period), :user_id),\n          projects_with_disable_overriding_approvers_per_merge_request: count(::Project.where(time_period.merge(disable_overriding_approvers_per_merge_request: true))),\n          projects_without_disable_overriding_approvers_per_merge_request: count(::Project.where(time_period.merge(disable_overriding_approvers_per_merge_request: [false, nil]))),\n          remote_mirrors: distinct_count(::Project.with_remote_mirrors.where(time_period), :creator_id),\n          snippets: distinct_count(::Snippet.where(time_period), :author_id)\n        }\n      end",
    "comment": "rubocop: enable UsageData/LargeTable rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "917"
  },
  {
    "raw_code": "def usage_activity_by_stage_enablement(time_period)\n        {}\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Empty placeholder allows this to match the pattern used by other sections",
    "label": "",
    "id": "918"
  },
  {
    "raw_code": "def usage_activity_by_stage_manage(time_period)\n        {\n          # rubocop: disable UsageData/LargeTable\n          events: stage_manage_events(time_period),\n          groups: distinct_count(::GroupMember.where(time_period), :user_id),\n          users_created: count(::User.where(time_period), start: minimum_id(User), finish: maximum_id(User)),\n          omniauth_providers: filtered_omniauth_provider_names.reject { |name| name == 'group_saml' },\n          user_auth_by_provider: distinct_count_user_auth_by_provider(time_period),\n          bulk_imports: {\n            gitlab_v1: count(::BulkImport.where(**time_period, source_type: :gitlab))\n          },\n          group_imports: group_imports(time_period)\n        }\n      end",
    "comment": "Omitted because no user, creator or author associated: `campaigns_imported_from_github`, `ldap_group_links` rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "919"
  },
  {
    "raw_code": "def usage_activity_by_stage_monitor(time_period)\n        # Calculate histogram only for overall as other time periods aren't available/useful here.\n        integrations_histogram = time_period.empty? ? histogram(::AlertManagement::HttpIntegration.active, :project_id, buckets: 1..100) : nil\n\n        {\n          clusters: distinct_count(::Clusters::Cluster.where(time_period), :user_id),\n          operations_dashboard_default_dashboard: count(::User.active.with_dashboard('operations').where(time_period),\n            start: minimum_id(User),\n            finish: maximum_id(User)),\n          projects_with_error_tracking_enabled: distinct_count(::Project.with_enabled_error_tracking.where(time_period), :creator_id),\n          projects_with_incidents: distinct_count(::Issue.with_issue_type(:incident).where(time_period), :project_id),\n          # We are making an assumption here that all alert_management_alerts are associated with an issue of type\n          # incident. In reality this is very close to the truth and allows more efficient queries.\n          # More info in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/121297#note_1416999956\n          projects_with_alert_incidents: distinct_count(::AlertManagement::Alert.where(time_period).where.not(issue_id: nil), :project_id),\n          projects_with_enabled_alert_integrations_histogram: integrations_histogram\n        }.compact\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "920"
  },
  {
    "raw_code": "def usage_activity_by_stage_package(time_period)\n        {\n          projects_with_packages: distinct_count(::Project.with_packages.where(time_period), :creator_id)\n        }\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "921"
  },
  {
    "raw_code": "def usage_activity_by_stage_plan(time_period)\n        time_frame = metric_time_period(time_period)\n        {\n          issues: add_metric('CountUsersCreatingIssuesMetric', time_frame: time_frame),\n          notes: distinct_count(::Note.where(time_period), :author_id),\n          projects: distinct_count(::Project.where(time_period), :creator_id),\n          todos: distinct_count(::Todo.where(time_period), :author_id),\n          service_desk_enabled_projects: distinct_count_service_desk_enabled_projects(time_period),\n          service_desk_issues: count(::Issue.service_desk.where(time_period)),\n          projects_jira_active: distinct_count(::Project.with_active_integration(::Integrations::Jira).where(time_period), :creator_id),\n          projects_jira_dvcs_server_active: distinct_count(::Project.with_active_integration(::Integrations::Jira).with_jira_dvcs_server.where(time_period), :creator_id)\n        }\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Omitted because no user, creator or author associated: `boards`, `labels`, `milestones`, `uploads` Omitted because too expensive: `epics_deepest_relationship_level` rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "922"
  },
  {
    "raw_code": "def usage_activity_by_stage_release(time_period)\n        time_frame = metric_time_period(time_period)\n        {\n          deployments: distinct_count(::Deployment.where(time_period), :user_id),\n          failed_deployments: distinct_count(::Deployment.failed.where(time_period), :user_id),\n          releases: distinct_count(::Release.where(time_period), :author_id),\n          successful_deployments: distinct_count(::Deployment.success.where(time_period), :user_id),\n          releases_with_milestones: add_metric('CountUsersAssociatingMilestonesToReleasesMetric', time_frame: time_frame)\n        }\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Omitted because no user, creator or author associated: `environments`, `feature_flags`, `in_review_folder`, `pages_domains` rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "923"
  },
  {
    "raw_code": "def usage_activity_by_stage_verify(time_period)\n        {\n          ci_builds: distinct_count(::Ci::Build.where(time_period), :user_id),\n          ci_external_pipelines: distinct_count(::Ci::Pipeline.external.where(time_period), :user_id, start: minimum_id(User), finish: maximum_id(User)),\n          ci_internal_pipelines: distinct_count(::Ci::Pipeline.internal.where(time_period), :user_id, start: minimum_id(User), finish: maximum_id(User)),\n          ci_pipeline_config_auto_devops: distinct_count(::Ci::Pipeline.auto_devops_source.where(time_period), :user_id, start: minimum_id(User), finish: maximum_id(User)),\n          ci_pipeline_config_repository: distinct_count(::Ci::Pipeline.repository_source.where(time_period), :user_id, start: minimum_id(User), finish: maximum_id(User)),\n          ci_pipeline_schedules: distinct_count(::Ci::PipelineSchedule.where(time_period), :owner_id),\n          ci_pipelines: distinct_count(::Ci::Pipeline.where(time_period), :user_id, start: minimum_id(User), finish: maximum_id(User)),\n          ci_triggers: distinct_count(::Ci::Trigger.where(time_period), :owner_id)\n        }\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Omitted because no user, creator or author associated: `ci_runners` rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "924"
  },
  {
    "raw_code": "def with_metadata\n        result = nil\n        error = nil\n\n        duration = Benchmark.realtime do\n          result = yield\n        rescue StandardError => e\n          error = e\n          Gitlab::ErrorTracking.track_and_raise_for_dev_exception(error)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "925"
  },
  {
    "raw_code": "def service_desk_counts\n        # rubocop: disable UsageData/LargeTable:\n        projects_with_service_desk = ::Project.where(service_desk_enabled: true)\n        # rubocop: enable UsageData/LargeTable:\n        {\n          service_desk_enabled_projects: count(projects_with_service_desk),\n          service_desk_issues: count(\n            ::Issue.where(\n              project: projects_with_service_desk,\n              author: ::Users::Internal.support_bot,\n              confidential: true\n            )\n          )\n        }\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "926"
  },
  {
    "raw_code": "def clear_memoized\n        CE_MEMOIZED_VALUES.each { |v| clear_memoization(v) }\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "927"
  },
  {
    "raw_code": "def cluster_integrations_user_distinct_count(integrations, time_period)\n        distinct_count(integrations.where(time_period).enabled.joins(:cluster), 'clusters.user_id')\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "928"
  },
  {
    "raw_code": "def omniauth_provider_names\n        ::Gitlab.config.omniauth.providers.map(&:name)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "929"
  },
  {
    "raw_code": "def filtered_omniauth_provider_names\n        omniauth_provider_names.reject { |name| name.starts_with?('ldap') }\n      end",
    "comment": "LDAP provider names are set by customers and could include sensitive info (server names, etc). LDAP providers normally don't appear in omniauth providers but filter to ensure no internal details leak via usage ping.",
    "label": "",
    "id": "930"
  },
  {
    "raw_code": "def distinct_count_user_auth_by_provider(time_period)\n        counts = auth_providers_except_ldap.index_with do |provider|\n          distinct_count(\n            ::AuthenticationEvent.success.for_provider(provider).where(time_period), :user_id)\n        end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "931"
  },
  {
    "raw_code": "def auth_providers\n        strong_memoize(:auth_providers) do\n          ::AuthenticationEvent.providers\n        end",
    "comment": "rubocop:enable CodeReuse/ActiveRecord rubocop:disable UsageData/LargeTable",
    "label": "",
    "id": "932"
  },
  {
    "raw_code": "def auth_providers_except_ldap\n        auth_providers.reject { |provider| provider.starts_with?('ldap') }\n      end",
    "comment": "rubocop:enable UsageData/LargeTable",
    "label": "",
    "id": "933"
  },
  {
    "raw_code": "def self.from_range(range)\n      return range if range.is_a?(self)\n\n      new(range.begin, range.end, exclude_end: range.exclude_end?)\n    end",
    "comment": "Converts Range object to MarkerRange class",
    "label": "",
    "id": "934"
  },
  {
    "raw_code": "def initialize(xml, disallowed_types = nil)\n      return unless xml.present?\n\n      if xml.size > MAX_XML_SIZE\n        raise ArgumentError, format(_(\"The XML file must be less than %{max_size} MB.\"),\n          max_size: MAX_XML_SIZE / 1.megabyte)\n      end",
    "comment": "Override the default Nokogiri parser in to allow parsing huge XML files",
    "label": "",
    "id": "935"
  },
  {
    "raw_code": "def initialize(current_size_proc:, limit:, namespace:, enabled: true)\n      @current_size_proc = current_size_proc\n      @limit = limit\n      @namespace = namespace\n      @enabled = enabled && limit != 0\n    end",
    "comment": "@param current_size_proc [Proc] returns repository size in bytes",
    "label": "",
    "id": "936"
  },
  {
    "raw_code": "def current_size\n      @current_size ||= @current_size_proc.call\n    end",
    "comment": "@return [Integer] bytes",
    "label": "",
    "id": "937"
  },
  {
    "raw_code": "def changes_will_exceed_size_limit?(change_size, _project)\n      return false unless enabled?\n\n      above_size_limit? || exceeded_size(change_size) > 0\n    end",
    "comment": "@param change_size [int] in bytes",
    "label": "",
    "id": "938"
  },
  {
    "raw_code": "def exceeded_size(change_size = 0)\n      size = current_size + change_size - limit\n\n      [size, 0].max\n    end",
    "comment": "@param change_size [int] in bytes",
    "label": "",
    "id": "939"
  },
  {
    "raw_code": "def dump_json(data)\n      Gitlab::Json.dump(data)\n    end",
    "comment": "Override Labkit's default impl, which uses the default Ruby platform json module.",
    "label": "",
    "id": "940"
  },
  {
    "raw_code": "def total_value_count_estimate\n      @total_value_count_estimate ||= body.count('{[,:')\n    end",
    "comment": "Estimates the total number of values in the JSON response by counting: : => Number of key-value pairs , => Number of elements in arrays (off by one since [1, 2, 3] has just 2 commas) [ => Number of arrays { => Number of objects",
    "label": "",
    "id": "941"
  },
  {
    "raw_code": "def self.throttle(key, group: nil, period: 1.hour, count: 1, &block)\n      group ||= block.source_location.join(':')\n\n      return if new(\"el:throttle:#{group}:#{key}\", timeout: period.to_i / count).waiting?\n\n      yield\n    end",
    "comment": "yield to the {block} at most {count} times per {period}  Defaults to once per hour.  For example:  # toot the train horn at most every 20min: throttle(locomotive.id, count: 3, period: 1.hour) { toot_train_horn } # Brake suddenly at most once every minute: throttle(locomotive.id, period: 1.minute) { brake_suddenly } # Specify a uniqueness group: throttle(locomotive.id, group: :locomotive_brake) { brake_suddenly }  If a group is not specified, each block will get a separate group to itself.",
    "label": "",
    "id": "942"
  },
  {
    "raw_code": "def self.reset_all!(scope = '*')\n      Gitlab::Redis::SharedState.with do |redis|\n        redis.scan_each(match: redis_shared_state_key(scope)).each do |key|\n          redis.del(key)\n        end",
    "comment": "Removes any existing exclusive_lease from redis Don't run this in a live system without making sure no one is using the leases",
    "label": "",
    "id": "943"
  },
  {
    "raw_code": "def try_obtain\n      report_lock_attempt_inside_transaction unless self.class.skip_transaction_check?\n\n      # Performing a single SET is atomic\n      Gitlab::Redis::SharedState.with do |redis|\n        redis.set(@redis_shared_state_key, @uuid, nx: true, ex: @timeout) && @uuid\n      end",
    "comment": "Try to obtain the lease. Return lease UUID on success, false if the lease is already taken.",
    "label": "",
    "id": "944"
  },
  {
    "raw_code": "def waiting?\n      !try_obtain\n    end",
    "comment": "This lease is waiting to obtain",
    "label": "",
    "id": "945"
  },
  {
    "raw_code": "def renew\n      Gitlab::Redis::SharedState.with do |redis|\n        result = redis.eval(LUA_RENEW_SCRIPT, keys: [@redis_shared_state_key], argv: [@uuid, @timeout.to_i])\n        result == @uuid\n      end",
    "comment": "Try to renew an existing lease. Return lease UUID on success, false if the lease is taken by a different UUID or inexistent.",
    "label": "",
    "id": "946"
  },
  {
    "raw_code": "def exists?\n      Gitlab::Redis::SharedState.with do |redis|\n        redis.exists?(@redis_shared_state_key) # rubocop:disable CodeReuse/ActiveRecord\n      end",
    "comment": "Returns true if the key for this lease is set.",
    "label": "",
    "id": "947"
  },
  {
    "raw_code": "def ttl\n      Gitlab::Redis::SharedState.with do |redis|\n        ttl = redis.ttl(@redis_shared_state_key)\n\n        ttl if ttl > 0\n      end",
    "comment": "Returns the TTL of the Redis key.  This method will return `nil` if no TTL could be obtained.",
    "label": "",
    "id": "948"
  },
  {
    "raw_code": "def cancel\n      self.class.cancel(@redis_shared_state_key, @uuid)\n    end",
    "comment": "Gives up this lease, allowing it to be obtained by others.",
    "label": "",
    "id": "949"
  },
  {
    "raw_code": "def same_uuid?\n      ::Gitlab::Redis::SharedState.with do |redis|\n        redis.get(@redis_shared_state_key) == @uuid\n      end",
    "comment": "Returns true if the UUID for the key hasn't changed.",
    "label": "",
    "id": "950"
  },
  {
    "raw_code": "def attributes\n          attrs = super\n          html_fields = cached_markdown_fields.html_fields\n          whitelisted = cached_markdown_fields.html_fields_whitelisted\n          exclude_fields = html_fields - whitelisted\n\n          attrs.except!(*exclude_fields)\n          attrs.delete('cached_markdown_version') if whitelisted.empty?\n\n          attrs\n        end",
    "comment": "Always exclude _html fields from attributes (including serialization). They contain unredacted HTML, which would be a security issue",
    "label": "",
    "id": "951"
  },
  {
    "raw_code": "def label_ids\n        return if jira_issue.fields['labels'].blank?\n\n        Gitlab::JiraImport::HandleLabelsService.new(project, jira_issue.fields['labels']).execute\n      end",
    "comment": "We already create labels in Gitlab::JiraImport::LabelsImporter stage but there is a possibility it may fail or new labels were created on the Jira in the meantime",
    "label": "",
    "id": "952"
  },
  {
    "raw_code": "def self.build(\n        id:, title:, active: false, icon: '', href: '', view: '',\n        css_class: nil, data: nil, partial: nil, component: nil\n      )\n        {\n          id: id,\n          type: :item,\n          title: title,\n          active: active,\n          icon: icon,\n          href: href,\n          view: view.to_s,\n          css_class: css_class,\n          data: data || { testid: 'menu-item-link', qa_title: title },\n          partial: partial,\n          component: component\n        }\n      end",
    "comment": "We want to have all keyword arguments for type safety. Ordinarily we could introduce a params object, but that's kind of what this is already :/. We could also take a hash and manually check every entry, but it's much more maintainable to do rely on native Ruby. rubocop: disable Metrics/ParameterLists",
    "label": "",
    "id": "953"
  },
  {
    "raw_code": "def import_issues\n        fetch_resources(:issues, repo, state: :all, sort: :created, direction: :asc, per_page: 100) do |issues|\n          issues.each do |raw|\n            raw = raw.to_h\n            gh_issue = IssueFormatter.new(project, raw, client, source_user_mapper)\n\n            begin\n              issuable =\n                if gh_issue.pull_request?\n                  MergeRequest.find_by(target_project_id: project.id, iid: gh_issue.number)\n                else\n                  gh_issue.create!\n                end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "954"
  },
  {
    "raw_code": "def import_pull_requests\n        fetch_resources(:pull_requests, repo, state: :all, sort: :created, direction: :asc, per_page: 100) do |prs|\n          prs.each do |raw|\n            raw = raw.to_h\n            gh_pull_request = PullRequestFormatter.new(project, raw, client, source_user_mapper)\n\n            next unless gh_pull_request.valid?\n\n            begin\n              restore_source_branch(gh_pull_request) unless gh_pull_request.source_branch_exists?\n              restore_target_branch(gh_pull_request) unless gh_pull_request.target_branch_exists?\n\n              merge_request = gh_pull_request.create!\n\n              # Gitea doesn't return PR in the Issue API endpoint, so labels must be assigned at this stage\n              apply_labels(merge_request, raw) if project.gitea_import?\n            rescue StandardError => e\n              errors << {\n                type: :pull_request,\n                url: Gitlab::UrlSanitizer.sanitize(gh_pull_request.url),\n                errors: e.message\n              }\n            ensure\n              clean_up_restored_branches(gh_pull_request)\n            end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "955"
  },
  {
    "raw_code": "def import_comments(issuable_type)\n        resource_type = \"#{issuable_type}_comments\".to_sym\n\n        # Two notes here:\n        # 1. We don't have a distinctive attribute for comments (unlike issues\n        # iid), so we fetch the last inserted note, compare it against every\n        # comment in the current imported page until we find match, and that's\n        # where start importing\n        # 2. GH returns comments for _both_ issues and PRs through\n        # issues_comments API, while pull_requests_comments returns only\n        # comments on diffs, so select last note not based on noteable_type but\n        # on line_code\n        line_code_is = issuable_type == :pull_requests ? 'NOT NULL' : 'NULL'\n        last_note    = project.notes.where(\"line_code IS #{line_code_is}\").last\n\n        fetch_resources(resource_type, repo, per_page: 100) do |comments|\n          if last_note\n            discard_inserted_comments(comments, last_note)\n            last_note = nil\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "956"
  },
  {
    "raw_code": "def create_comments(comments)\n        ActiveRecord::Base.no_touching do\n          comments.each do |raw|\n            raw = raw.to_h\n\n            comment = CommentFormatter.new(project, raw, client, source_user_mapper)\n\n            # GH does not return info about comment's parent, so we guess it by checking its URL!\n            *_, parent, iid = URI(raw[:html_url]).path.split('/')\n\n            issuable = if parent == 'issues'\n                         Issue.find_by(project_id: project.id, iid: iid)\n                       else\n                         MergeRequest.find_by(target_project_id: project.id, iid: iid)\n                       end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "957"
  },
  {
    "raw_code": "def discard_inserted_comments(comments, last_note)\n        last_note_attrs = nil\n\n        cut_off_index = comments.find_index do |raw|\n          comment           = CommentFormatter.new(project, raw.to_h)\n          comment_attrs     = comment.attributes\n          last_note_attrs ||= last_note.slice(*comment_attrs.keys)\n\n          comment_attrs.with_indifferent_access == last_note_attrs\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "958"
  },
  {
    "raw_code": "def create_record\n        association = project.public_send(project_association) # rubocop:disable GitlabSecurity/PublicSend\n\n        association.find_or_create_by!(find_condition) do |record|\n          record.attributes = attributes\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- Existing legacy code",
    "label": "",
    "id": "959"
  },
  {
    "raw_code": "def push_placeholder_references(record, contributing_users: nil)\n        contributing_users ||= contributing_user_formatters\n\n        contributing_users.each do |user_reference_column, user_formatter|\n          push_placeholder_reference(record, user_reference_column, user_formatter.source_user)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "960"
  },
  {
    "raw_code": "def contributing_user_formatters\n        raise NotImplementedError\n      end",
    "comment": "A hash of user_reference_columns and its corresponding UserFormatter objects must be defined on each formatter in order to save it using #create!",
    "label": "",
    "id": "961"
  },
  {
    "raw_code": "def milestone\n        if raw_data[:milestone].present?\n          milestone = MilestoneFormatter.new(project, raw_data[:milestone])\n          project.milestones.find_by(milestone.find_condition)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "962"
  },
  {
    "raw_code": "def skip_wiki\n        repo[:has_wiki]\n      end",
    "comment": " If the GitHub project repository has wiki, we should not create the default wiki. Otherwise the GitHub importer will fail because the wiki repository already exist. ",
    "label": "",
    "id": "963"
  },
  {
    "raw_code": "def write_session(_env, sid, session, options)\n        key = cache_key(sid.private_id)\n        if session\n          @cache.write(key, session, expires_in: options[:redis_expiry])\n        else\n          @cache.delete(key)\n        end",
    "comment": "Overrides https://github.com/rails/rails/blob/v7.2.2.1/actionpack/lib/action_dispatch/middleware/session/cache_store.rb#L37-L46 The only difference is the `expires_in` value is now based on the new option that we set in the intializer above",
    "label": "",
    "id": "964"
  },
  {
    "raw_code": "def cache_key(id)\n        id\n      end",
    "comment": "ActionDispatch::Session::CacheStore (superclass) prepends hardcoded \"_session_id:\" to the cache key which doesn't match the previous implementation of Gitlab::Sessions::RedisStore.",
    "label": "",
    "id": "965"
  },
  {
    "raw_code": "def build(deployment, status, status_changed_at)\n        # Deployments will not have a deployable when created using the API.\n        deployable_url =\n          if deployment.deployable.instance_of?(::Ci::Build)\n            Gitlab::UrlBuilder.build(deployment.deployable)\n          end",
    "comment": "NOTE: Time-sensitive attributes should be explicitly passed as argument instead of reading from database.",
    "label": "",
    "id": "966"
  },
  {
    "raw_code": "def build(\n        project:, user:, ref:, oldrev: nil, newrev: nil,\n        commits: [], commits_count: nil, message: nil, push_options: {},\n        with_changed_files: true\n      )\n        commits = Array(commits)\n\n        # Total commits count\n        commits_count ||= commits.size\n\n        # Get latest 20 commits ASC\n        commits_limited = commits.last(20)\n\n        # For performance purposes maximum 20 latest commits\n        # will be passed as post receive hook data.\n        # n+1: https://gitlab.com/gitlab-org/gitlab-foss/issues/38259\n        commit_attrs = Gitlab::GitalyClient.allow_n_plus_1_calls do\n          commits_limited.map do |commit|\n            commit.hook_attrs(with_changed_files: with_changed_files)\n          end",
    "comment": "Produce a hash of post-receive data  data = { before: String, after: String, ref: String, ref_protected: Boolean, user_id: String, user_name: String, user_username: String, user_email: String project_id: Fixnum, project: { id: Fixnum, name: String, description: String, web_url: String, avatar_url: String, git_ssh_url: String, git_http_url: String, namespace: String, visibility_level: Fixnum, path_with_namespace: String, default_branch: String } repository: { name: String, url: String, description: String, homepage: String, }, commits: Array, total_commits_count: Fixnum, push_options: Hash }  rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "967"
  },
  {
    "raw_code": "def build_sample(project, user, is_tag = false)\n        # Use sample data if repo has no commit\n        # (expect the case of test service configuration settings)\n        return sample_data(is_tag) if project.empty_repo?\n\n        ref = if is_tag\n                \"#{Gitlab::Git::TAG_REF_PREFIX}#{sample_tag_name(project) || DEFAULT_TAG_NAME}\"\n              else\n                \"#{Gitlab::Git::BRANCH_REF_PREFIX}#{project.default_branch}\"\n              end",
    "comment": "This method provides a sample data generated with existing project and commits to test webhooks",
    "label": "",
    "id": "968"
  },
  {
    "raw_code": "def build(build)\n        project = build.project\n        commit = build.pipeline\n        user = build.user\n\n        author_url = build_author_url(build.commit, commit)\n\n        attrs = {\n          object_kind: 'build',\n\n          ref: build.ref,\n          tag: build.tag,\n          before_sha: build.before_sha,\n          sha: build.sha,\n          retries_count: build.retries_count,\n\n          # TODO: should this be not prefixed with build_?\n          # Leaving this way to have backward compatibility\n          build_id: build.id,\n          build_name: build.name,\n          build_stage: build.stage_name,\n          build_status: build.status,\n          build_created_at: build.created_at,\n          build_started_at: build.started_at,\n          build_finished_at: build.finished_at,\n          build_created_at_iso: build.created_at&.iso8601,\n          build_started_at_iso: build.started_at&.iso8601,\n          build_finished_at_iso: build.finished_at&.iso8601,\n          build_duration: build.duration,\n          build_queued_duration: build.queued_duration,\n          build_allow_failure: build.allow_failure,\n          build_failure_reason: build.failure_reason,\n          pipeline_id: commit.id,\n          runner: build_runner(build.runner),\n\n          # TODO: do we still need it?\n          project_id: project.id,\n          project_name: project.full_name,\n\n          user: user.try(:hook_attrs),\n\n          commit: {\n            # note: commit.id is actually the pipeline id\n            id: commit.id,\n            name: commit.name,\n            sha: commit.sha,\n            message: commit.git_commit_message,\n            author_name: commit.git_author_name,\n            author_email: commit.git_author_email,\n            author_url: author_url,\n            status: commit.status,\n            duration: commit.duration,\n            started_at: commit.started_at,\n            finished_at: commit.finished_at,\n            started_at_iso: commit.started_at&.iso8601,\n            finished_at_iso: commit.finished_at&.iso8601\n          },\n\n          repository: {\n            name: project.name,\n            url: project.url_to_repo,\n            description: project.description,\n            homepage: project.web_url,\n            git_http_url: project.http_url_to_repo,\n            git_ssh_url: project.ssh_url_to_repo,\n            visibility_level: project.visibility_level\n          },\n\n          project: project.hook_attrs(backward: false),\n\n          environment: build_environment(build)\n        }\n\n        attrs[:source_pipeline] = source_pipeline_attrs(commit.source_pipeline) if commit.source_pipeline.present?\n        attrs\n      end",
    "comment": "rubocop:disable Metrics/AbcSize -- build webhook payload",
    "label": "",
    "id": "969"
  },
  {
    "raw_code": "def build(note, user, action)\n        raise ArgumentError, 'invalid action' unless action.in?(VALID_ACTIONS)\n\n        project = note.project\n        data = build_base_data(project, user, note, action)\n\n        if note.for_commit?\n          data[:commit] = build_data_for_commit(project, user, note)\n        elsif note.for_issue?\n          data[:issue] = Gitlab::HookData::IssueBuilder.new(note.noteable).build\n        elsif note.for_merge_request?\n          data[:merge_request] = Gitlab::HookData::MergeRequestBuilder.new(note.noteable).build\n        elsif note.for_snippet?\n          data[:snippet] = note.noteable.hook_attrs\n        end",
    "comment": "Produce a hash of post-receive data  For all notes:  data = { object_kind: \"note\", event_type: \"confidential_note\", user: { name: String, username: String, avatar_url: String } project_id: Integer, repository: { name: String, url: String, description: String, homepage: String, } object_attributes: { <hook data for note> } <note-specific data>: { } note-specific data is a hash with one of the following keys and contains the hook data for that type. - commit - issue - merge_request - snippet ",
    "label": "",
    "id": "970"
  },
  {
    "raw_code": "def update(project, user, changes, refs)\n        {\n          event_name: 'repository_update',\n\n          user_id: user.id,\n          user_name: user.name,\n          user_email: user.email,\n          user_avatar: user.avatar_url,\n\n          project_id: project.id,\n          project: project.hook_attrs,\n\n          changes: changes,\n\n          refs: refs\n        }\n      end",
    "comment": "Produce a hash of post-receive data",
    "label": "",
    "id": "971"
  },
  {
    "raw_code": "def single_change(oldrev, newrev, ref)\n        {\n          before: oldrev,\n          after: newrev,\n          ref: ref\n        }\n      end",
    "comment": "Produce a hash of partial data for a single change",
    "label": "",
    "id": "972"
  },
  {
    "raw_code": "def gitlab_team_member?(_user_id)\n        nil\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "973"
  },
  {
    "raw_code": "def track_weak_password_error(user, controller, method_name)\n          return unless user.errors[:password].grep(/must not contain commonly used combinations.*/).any?\n\n          Gitlab::Tracking.event(\n            'Gitlab::Tracking::Helpers::WeakPasswordErrorEvent',\n            'track_weak_password_error',\n            controller: controller,\n            method: method_name\n          )\n        end",
    "comment": "Tracks information if a user record has a weak password. No-op unless the error is present.  Captures a minimal set of information, so that GitLab remains unaware of which users / demographics are attempting to choose weak passwords.",
    "label": "",
    "id": "974"
  },
  {
    "raw_code": "def optimize_repository(eager: false)\n        strategy = if eager\n                     Gitaly::OptimizeRepositoryRequest::Strategy::STRATEGY_EAGER\n                   else\n                     Gitaly::OptimizeRepositoryRequest::Strategy::STRATEGY_HEURISTICAL\n                   end",
    "comment": "Optimize the repository. By default, this will perform heuristical housekeeping in the repository, which is the recommended approach and will only optimize what needs to be optimized. If `eager = true`, then Gitaly will instead be asked to perform eager housekeeping. As a consequence the housekeeping run will take a _lot_ longer. It is not recommended to use eager housekeeping in general, but only in situations where it is explicitly required.",
    "label": "",
    "id": "975"
  },
  {
    "raw_code": "def fetch_remote(url, refmap:, ssh_auth:, forced:, no_tags:, timeout:, prune: true, http_authorization_header: \"\", resolved_address: \"\")\n        request = Gitaly::FetchRemoteRequest.new(\n          repository: @gitaly_repo,\n          force: forced,\n          no_tags: no_tags,\n          timeout: timeout,\n          no_prune: !prune,\n          remote_params: Gitaly::Remote.new(\n            url: url,\n            mirror_refmaps: Array.wrap(refmap).map(&:to_s),\n            http_authorization_header: http_authorization_header,\n            resolved_address: resolved_address\n          )\n        )\n\n        if ssh_auth&.ssh_mirror_url?\n          if ssh_auth.ssh_key_auth? && ssh_auth.ssh_private_key.present?\n            request.ssh_key = ssh_auth.ssh_private_key\n          end",
    "comment": "rubocop: disable Metrics/ParameterLists The `remote` parameter is going away soonish anyway, at which point the Rubocop warning can be enabled again.",
    "label": "",
    "id": "976"
  },
  {
    "raw_code": "def create_repository(default_branch = nil, object_format: nil)\n        request = Gitaly::CreateRepositoryRequest.new(repository: @gitaly_repo, default_branch: encode_binary(default_branch), object_format: gitaly_object_format(object_format))\n        gitaly_client_call(@storage, :repository_service, :create_repository, request, timeout: GitalyClient.fast_timeout)\n      end",
    "comment": "rubocop: enable Metrics/ParameterLists",
    "label": "",
    "id": "977"
  },
  {
    "raw_code": "def gitaly_client_call(...)\n        unless repository_actor\n          Gitlab::ErrorTracking.track_and_raise_for_dev_exception(\n            Feature::InvalidFeatureFlagError.new(\"gitaly_client_call called without setting repository_actor\")\n          )\n        end",
    "comment": "gitaly_client_call performs Gitaly calls including collected feature flag actors. The actors are retrieved from repository actor and memoized. The service must set `self.repository_actor = a_repository` beforehand.",
    "label": "",
    "id": "978"
  },
  {
    "raw_code": "def gitaly_feature_flag_actors(repository)\n        container = find_repository_container(repository)\n        {\n          repository: repository,\n          user: Feature::Gitaly.user_actor,\n          project: Feature::Gitaly.project_actor(container),\n          group: Feature::Gitaly.group_actor(container)\n        }\n      end",
    "comment": "gitaly_feature_flag_actors returns a hash of actors implied from input repository.",
    "label": "",
    "id": "979"
  },
  {
    "raw_code": "def user_actor\n        strong_memoize(:user_actor) do\n          Feature::Gitaly.user_actor\n        end",
    "comment": "Use actor here means the user who originally perform the action. It is collected from ApplicationContext. As this information is widely propagated in all entry points, User actor should be available everywhere, even in background jobs.",
    "label": "",
    "id": "980"
  },
  {
    "raw_code": "def project_actor\n        strong_memoize(:project_actor) do\n          Feature::Gitaly.project_actor(repository_container)\n        end",
    "comment": "TODO: replace this project actor by Repo actor",
    "label": "",
    "id": "981"
  },
  {
    "raw_code": "def find_changed_paths(objects, merge_commit_diff_mode: nil, find_renames: false, diff_filters: nil)\n        request = find_changed_paths_request(objects, merge_commit_diff_mode, find_renames, diff_filters)\n\n        return [] if request.nil?\n\n        response = gitaly_client_call(@repository.storage, :diff_service, :find_changed_paths, request, timeout: GitalyClient.medium_timeout)\n        response.flat_map do |msg|\n          msg.paths.map do |path|\n            Gitlab::Git::ChangedPath.new(\n              status: path.status,\n              path: EncodingHelper.encode!(path.path),\n              old_path: EncodingHelper.encode!(path.old_path),\n              old_mode: path.old_mode.to_s(8),\n              new_mode: path.new_mode.to_s(8),\n              old_blob_id: path.old_blob_id,\n              new_blob_id: path.new_blob_id\n            )\n          end",
    "comment": "When finding changed paths and passing a sha for a merge commit we can specify how to diff the commit.  When diffing a merge commit and merge_commit_diff_mode is :all_parents file paths are only returned if changed in both parents (or all parents if diffing an octopus merge)  This means if we create a merge request that includes a merge commit of changes already existing in the target branch, we can omit those changes when looking up the changed paths.  e.g. 1. User branches from master to new branch named feature/foo_bar 2. User changes ./foo_bar.rb and commits change to feature/foo_bar 3. Another user merges a change to ./bar_baz.rb to master 4. User merges master into feature/foo_bar 5. User pushes to GitLab 6. GitLab checks which files have changed  case merge_commit_diff_mode when :all_parents ['foo_bar.rb'] when :include_merges ['foo_bar.rb', 'bar_baz.rb'], else # defaults to :include_merges behavior ['foo_bar.rb', 'bar_baz.rb'], ",
    "label": "",
    "id": "982"
  },
  {
    "raw_code": "def list_new_commits(revisions)\n        git_env = Gitlab::Git::HookEnv.all(@gitaly_repo.gl_repository)\n        if git_env['GIT_OBJECT_DIRECTORY_RELATIVE'].present?\n          # If we have a quarantine environment, then we can optimize the check\n          # by doing a ListAllCommitsRequest. Instead of walking through\n          # references, we just walk through all quarantined objects, which is\n          # a lot more efficient. To do so, we throw away any alternate object\n          # directories, which point to the main object directory of the\n          # repository, and only keep the object directory which points into\n          # the quarantine object directory.\n          quarantined_repo = @gitaly_repo.dup\n          quarantined_repo.git_alternate_object_directories = Google::Protobuf::RepeatedField.new(:string)\n\n          request = Gitaly::ListAllCommitsRequest.new(\n            repository: quarantined_repo\n          )\n\n          response = gitaly_client_call(@repository.storage, :commit_service, :list_all_commits, request, timeout: GitalyClient.medium_timeout)\n\n          quarantined_commits = consume_commits_response(response)\n          quarantined_commit_ids = quarantined_commits.map(&:id)\n\n          # While in general the quarantine directory would only contain objects\n          # which are actually new, this is not guaranteed by Git. In fact,\n          # git-push(1) may sometimes push objects which already exist in the\n          # target repository. We do not want to return those from this method\n          # though given that they're not actually new.\n          #\n          # To fix this edge-case we thus have to filter commits down to those\n          # which don't yet exist. To do so, we must check for object existence\n          # in the main repository, but the object directory of our repository\n          # points into the object quarantine. This can be fixed by unsetting\n          # it, which will cause us to use the normal repository as indicated by\n          # its relative path again.\n          main_repo = @gitaly_repo.dup\n          main_repo.git_object_directory = \"\"\n\n          # Check object existence of all quarantined commits' IDs.\n          quarantined_commit_existence = object_existence_map(quarantined_commit_ids, gitaly_repo: main_repo)\n\n          # And then we reject all quarantined commits which exist in the main\n          # repository already.\n          quarantined_commits.reject! { |c| quarantined_commit_existence[c.id] }\n\n          quarantined_commits\n        else\n          list_commits(Array.wrap(revisions) + %w[--not --all])\n        end",
    "comment": "List all commits which are new in the repository. If commits have been pushed into the repo",
    "label": "",
    "id": "983"
  },
  {
    "raw_code": "def object_existence_map(revisions, gitaly_repo: @gitaly_repo)\n        return {} unless revisions.present?\n\n        enum = Enumerator.new do |y|\n          revisions.each_slice(100).with_index do |revisions_subset, i|\n            params = { revisions: revisions_subset }\n            params[:repository] = gitaly_repo if i == 0\n\n            y.yield Gitaly::CheckObjectsExistRequest.new(**params)\n          end",
    "comment": "Check whether the given revisions exist. Returns a hash mapping the revision name to either `true` if the revision exists, or `false` otherwise. This function accepts all revisions as specified by gitrevisions(1).",
    "label": "",
    "id": "984"
  },
  {
    "raw_code": "def check\n        request = Grpc::Health::V1::HealthCheckRequest.new\n        response = GitalyClient.call(@storage, :health_check, :check, request, timeout: GitalyClient.fast_timeout)\n\n        { success: response&.status == :SERVING }\n      rescue GRPC::BadStatus => e\n        { success: false, message: e.to_s }\n      end",
    "comment": "Sends a gRPC health ping to the Gitaly server for the storage shard.",
    "label": "",
    "id": "985"
  },
  {
    "raw_code": "def self.allow_disk_access\n        temporarily_allow(ALLOW_KEY) { yield }\n      end",
    "comment": "If your code needs this method then your code needs to be fixed.",
    "label": "",
    "id": "986"
  },
  {
    "raw_code": "def initialize(repository)\n        @repository = repository\n        @gitaly_repo = repository.gitaly_repository\n        @storage = repository.storage\n\n        self.repository_actor = repository\n      end",
    "comment": "'repository' is a Gitlab::Git::Repository",
    "label": "",
    "id": "987"
  },
  {
    "raw_code": "def user_cherry_pick(\n        user:, commit:, branch_name:, message:,\n        start_branch_name:, start_repository:, author_name: nil, author_email: nil, dry_run: false, target_sha: nil\n      )\n        request = Gitaly::UserCherryPickRequest.new(\n          repository: @gitaly_repo,\n          user: gitaly_user(user),\n          commit: commit.to_gitaly_commit,\n          branch_name: encode_binary(branch_name),\n          message: encode_binary(message),\n          start_branch_name: encode_binary(start_branch_name.to_s),\n          start_repository: start_repository.gitaly_repository,\n          commit_author_name: encode_binary(author_name),\n          commit_author_email: encode_binary(author_email),\n          dry_run: dry_run,\n          timestamp: Google::Protobuf::Timestamp.new(seconds: Time.now.utc.to_i),\n          expected_old_oid: target_sha\n        )\n\n        response = gitaly_client_call(\n          @repository.storage,\n          :operation_service,\n          :user_cherry_pick,\n          request,\n          remote_storage: start_repository.storage,\n          timeout: GitalyClient.long_timeout,\n          gitaly_context: { 'enable_secrets_check' => true }\n        )\n\n        Gitlab::Git::OperationService::BranchUpdate.from_gitaly(response.branch_update)\n      rescue GRPC::InvalidArgument => ex\n        raise Gitlab::Git::CommandError, ex\n      rescue GRPC::BadStatus => e\n        detailed_error = GitalyClient.decode_detailed_error(e)\n\n        case detailed_error.try(:error)\n        when :access_check\n          access_check_error = detailed_error.access_check\n          # These messages were returned from internal/allowed API calls\n          raise Gitlab::Git::PreReceiveError.new(fallback_message: access_check_error.error_message)\n        when :cherry_pick_conflict\n          raise Gitlab::Git::Repository::CreateTreeError, 'CONFLICT'\n        when :changes_already_applied\n          raise Gitlab::Git::Repository::CreateTreeError, 'EMPTY'\n        when :target_branch_diverged\n          raise Gitlab::Git::CommitError, 'branch diverged'\n        else\n          raise e\n        end",
    "comment": "rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "988"
  },
  {
    "raw_code": "def user_revert(user:, commit:, branch_name:, message:, start_branch_name:, start_repository:, dry_run: false)\n        request = Gitaly::UserRevertRequest.new(\n          repository: @gitaly_repo,\n          user: gitaly_user(user),\n          commit: commit.to_gitaly_commit,\n          branch_name: encode_binary(branch_name),\n          message: encode_binary(message),\n          start_branch_name: encode_binary(start_branch_name.to_s),\n          start_repository: start_repository.gitaly_repository,\n          dry_run: dry_run,\n          timestamp: Google::Protobuf::Timestamp.new(seconds: Time.now.utc.to_i)\n        )\n\n        response = gitaly_client_call(\n          @repository.storage,\n          :operation_service,\n          :user_revert,\n          request,\n          remote_storage: start_repository.storage,\n          timeout: GitalyClient.long_timeout\n        )\n\n        if response.pre_receive_error.presence\n          raise Gitlab::Git::PreReceiveError, response.pre_receive_error\n        elsif response.commit_error.presence\n          raise Gitlab::Git::CommitError, response.commit_error\n        elsif response.create_tree_error.presence\n          raise Gitlab::Git::Repository::CreateTreeError, response.create_tree_error_code\n        end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "989"
  },
  {
    "raw_code": "def user_commit_files(\n        user, branch_name, commit_message, actions, author_email, author_name, start_branch_name,\n        start_repository, force = false, start_sha = nil, sign = true, target_sha = nil)\n        req_enum = Enumerator.new do |y|\n          header = user_commit_files_request_header(user, branch_name,\n            commit_message, actions, author_email, author_name, start_branch_name,\n            start_repository, force, start_sha, sign, target_sha)\n\n          y.yield Gitaly::UserCommitFilesRequest.new(header: header)\n\n          actions.each do |action|\n            action_header = user_commit_files_action_header(action)\n            y.yield Gitaly::UserCommitFilesRequest.new(\n              action: Gitaly::UserCommitFilesAction.new(header: action_header)\n            )\n\n            reader = binary_io(action[:content])\n\n            until reader.eof?\n              chunk = reader.read(MAX_MSG_SIZE)\n\n              y.yield Gitaly::UserCommitFilesRequest.new(\n                action: Gitaly::UserCommitFilesAction.new(content: chunk)\n              )\n            end",
    "comment": "rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "990"
  },
  {
    "raw_code": "def user_commit_patches(user, branch_name:, patches:, target_sha: nil)\n        header = Gitaly::UserApplyPatchRequest::Header.new(\n          repository: @gitaly_repo,\n          user: gitaly_user(user),\n          target_branch: encode_binary(branch_name),\n          timestamp: Google::Protobuf::Timestamp.new(seconds: Time.now.utc.to_i),\n          expected_old_oid: target_sha\n        )\n        reader = binary_io(patches)\n\n        chunks = Enumerator.new do |chunk|\n          chunk.yield Gitaly::UserApplyPatchRequest.new(header: header)\n\n          until reader.eof?\n            patch_chunk = reader.read(MAX_MSG_SIZE)\n\n            chunk.yield(Gitaly::UserApplyPatchRequest.new(patches: patch_chunk))\n          end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "991"
  },
  {
    "raw_code": "def consume_final_message(response_enum)\n        response_enum.next\n      rescue StopIteration\n      else\n        raise 'expected response stream to finish'\n      end",
    "comment": "consume_final_message consumes the final message that contains the status from the response stream and raises an exception if it wasn't the last one.",
    "label": "",
    "id": "992"
  },
  {
    "raw_code": "def user_commit_files_request_header(\n        user, branch_name, commit_message, actions, author_email, author_name,\n        start_branch_name, start_repository, force, start_sha, sign, target_sha\n      )\n        Gitaly::UserCommitFilesRequestHeader.new(\n          repository: @gitaly_repo,\n          user: gitaly_user(user),\n          branch_name: encode_binary(branch_name),\n          commit_message: encode_binary(commit_message),\n          commit_author_name: encode_binary(author_name),\n          commit_author_email: encode_binary(author_email),\n          start_branch_name: encode_binary(start_branch_name),\n          start_repository: start_repository&.gitaly_repository,\n          force: force,\n          start_sha: encode_binary(start_sha),\n          sign: sign,\n          expected_old_oid: target_sha,\n          timestamp: Google::Protobuf::Timestamp.new(seconds: Time.now.utc.to_i)\n        )\n      end",
    "comment": "rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "993"
  },
  {
    "raw_code": "def user_commit_files_action_header(action)\n        Gitaly::UserCommitFilesActionHeader.new(\n          action: action[:action].upcase.to_sym,\n          file_path: encode_binary(action[:file_path]),\n          previous_path: encode_binary(action[:previous_path]),\n          base64_content: action[:encoding] == 'base64',\n          execute_filemode: !!action[:execute_filemode],\n          infer_content: !!action[:infer_content]\n        )\n      rescue RangeError\n        raise ArgumentError, \"Unknown action '#{action[:action]}'\"\n      end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "994"
  },
  {
    "raw_code": "def diff_blobs(\n        blob_pairs, diff_mode: DIFF_MODES[:unspecified], whitespace_changes: WHITESPACE_CHANGES[:unspecified],\n        patch_bytes_limit: 0\n      )\n        request = Gitaly::DiffBlobsRequest.new(\n          repository: @gitaly_repo,\n          blob_pairs: blob_pairs,\n          diff_mode: diff_mode,\n          whitespace_changes: whitespace_changes,\n          patch_bytes_limit: patch_bytes_limit\n        )\n\n        response = gitaly_client_call(@storage, :diff_service, :diff_blobs, request,\n          timeout: GitalyClient.medium_timeout)\n\n        GitalyClient::DiffBlobsStitcher.new(response)\n      end",
    "comment": "Requests diffs between blob pairs via Gitaly's DiffBlobs RPC using blob_pairs field.  @param blob_pairs [Array<Gitaly::DiffBlobsRequest::BlobPair>] Array of blob ID pairs to diff @param diff_mode [Symbol] Diff output mode (:unspecified, :word) @param whitespace_changes [Symbol] Whitespace handling (:unspecified, :ignore_spaces, :ignore_all_spaces) @param patch_bytes_limit [Integer] Maximum patch size in bytes (0 = unlimited) @return [GitalyClient::DiffBlobsStitcher] Streamed diff responses from Gitaly",
    "label": "",
    "id": "995"
  },
  {
    "raw_code": "def diff_blobs_with_raw_info(\n        raw_info,\n        diff_mode: DIFF_MODES[:unspecified],\n        whitespace_changes: WHITESPACE_CHANGES[:unspecified],\n        patch_bytes_limit: 0\n      )\n        request = Gitaly::DiffBlobsRequest.new(\n          repository: @gitaly_repo,\n          raw_info: raw_info,\n          diff_mode: diff_mode,\n          whitespace_changes: whitespace_changes,\n          patch_bytes_limit: patch_bytes_limit\n        )\n\n        response = gitaly_client_call(@storage, :diff_service, :diff_blobs, request,\n          timeout: GitalyClient.medium_timeout)\n\n        GitalyClient::DiffBlobsStitcher.new(response)\n      end",
    "comment": "Requests diffs between blob pairs via Gitaly's DiffBlobs RPC using raw_info field. More efficient for large batches of files compared to blob_pairs approach.  @param raw_info [Array<Gitaly::ChangedPaths>] Array of changed path information @param diff_mode [Symbol] Diff output mode (:unspecified, :word) @param whitespace_changes [Symbol] Whitespace handling (:unspecified, :ignore_spaces, :ignore_all_spaces) @param patch_bytes_limit [Integer] Maximum patch size in bytes (0 = unlimited) @return [GitalyClient::DiffBlobsStitcher] Streamed diff responses from Gitaly",
    "label": "",
    "id": "996"
  },
  {
    "raw_code": "def initialize(repository)\n        @repository = repository\n        @gitaly_repo = repository.gitaly_repository\n        @storage = repository.storage\n\n        self.repository_actor = repository\n      end",
    "comment": "'repository' is a Gitlab::Git::Repository",
    "label": "",
    "id": "997"
  },
  {
    "raw_code": "def tag_names_contains_sha(sha, limit: 0)\n        request = Gitaly::ListTagNamesContainingCommitRequest.new(\n          repository: @gitaly_repo,\n          commit_id: sha,\n          limit: limit\n        )\n\n        response = gitaly_client_call(@storage, :ref_service, :list_tag_names_containing_commit, request, timeout: GitalyClient.medium_timeout)\n        consume_ref_contains_sha_response(response, :tag_names)\n      end",
    "comment": "Limit: 0 implies no limit, thus all tag names will be returned",
    "label": "",
    "id": "998"
  },
  {
    "raw_code": "def branch_names_contains_sha(sha, limit: 0)\n        request = Gitaly::ListBranchNamesContainingCommitRequest.new(\n          repository: @gitaly_repo,\n          commit_id: sha,\n          limit: limit\n        )\n\n        response = gitaly_client_call(@storage, :ref_service, :list_branch_names_containing_commit, request, timeout: GitalyClient.medium_timeout)\n        consume_ref_contains_sha_response(response, :branch_names)\n      end",
    "comment": "Limit: 0 implies no limit, thus all tag names will be returned",
    "label": "",
    "id": "999"
  },
  {
    "raw_code": "def list_refs(patterns = [Gitlab::Git::BRANCH_REF_PREFIX], pointing_at_oids: [], peel_tags: false, dynamic_timeout: nil, sort_by: nil, pagination_params: nil)\n        request = Gitaly::ListRefsRequest.new(\n          repository: @gitaly_repo,\n          patterns: patterns.map { |p| encode_binary(p) },\n          pointing_at_oids: pointing_at_oids,\n          peel_tags: peel_tags,\n          pagination_params: pagination_params\n        )\n        request.sort_by = Gitlab::GitalyClient::ListRefsSort.new(sort_by).gitaly_sort_by if sort_by\n\n        timeout = dynamic_timeout || GitalyClient.fast_timeout\n\n        response = gitaly_client_call(@storage, :ref_service, :list_refs, request, timeout: timeout)\n        consume_list_refs_response(response)\n      end",
    "comment": "peel_tags slows down the request by a factor of 3-4",
    "label": "",
    "id": "1000"
  },
  {
    "raw_code": "def initialize(project)\n        @project = project\n\n        @page_counter = Gitlab::Import::PageCounter.new(project, collection_method, 'bitbucket-server-importer')\n        @already_processed_cache_key =\n          format(ALREADY_PROCESSED_CACHE_KEY, project: project.id, collection: collection_method)\n        @job_waiter_cache_key =\n          format(JOB_WAITER_CACHE_KEY, project: project.id, collection: collection_method)\n        @job_waiter_remaining_cache_key = format(JOB_WAITER_REMAINING_CACHE_KEY, project: project.id,\n          collection: collection_method)\n\n        # The enqueued job counter is used to calculate job delays and distribute\n        # them over time. When the stage worker restarts, the counter resets to\n        # prevent jobs from being queued too far into the future. Such logic may\n        # result in more jobs being executed when the stage worker resumes. An\n        # alternative solution would complicate the delay logic, so for simplicity\n        # we accept more jobs being executed.\n        @enqueued_job_counter = 0\n      end",
    "comment": "project - An instance of `Project`.",
    "label": "",
    "id": "1001"
  },
  {
    "raw_code": "def id_for_already_processed_cache(object)\n        raise NotImplementedError\n      end",
    "comment": "Returns the ID to use for the cache used for checking if an object has already been processed or not.  object - The object we may want to import.",
    "label": "",
    "id": "1002"
  },
  {
    "raw_code": "def sidekiq_worker_class\n        raise NotImplementedError\n      end",
    "comment": "The Sidekiq worker class used for scheduling the importing of objects in parallel.",
    "label": "",
    "id": "1003"
  },
  {
    "raw_code": "def collection_method\n        raise NotImplementedError\n      end",
    "comment": "The name of the method to call to retrieve the data to import.",
    "label": "",
    "id": "1004"
  },
  {
    "raw_code": "def mark_as_processed(object)\n        id = id_for_already_processed_cache(object)\n\n        Gitlab::Cache::Import::Caching.set_add(already_processed_cache_key, id)\n      end",
    "comment": "Marks the given object as \"already processed\".",
    "label": "",
    "id": "1005"
  },
  {
    "raw_code": "def initialize(project)\n        @project = project\n      end",
    "comment": "project - An instance of `Project`",
    "label": "",
    "id": "1006"
  },
  {
    "raw_code": "def uid(object)\n        # We want this to only match either placeholder or email\n        # depending on the flag state. There should be no fall-through.\n        if user_mapping_enabled?(project)\n          return unless object[:username]\n\n          if project.root_ancestor.user_namespace? && user_mapping_to_personal_namespace_owner_enabled?(project)\n            return project.root_ancestor.owner_id\n          end",
    "comment": "Object should behave as a object so we can remove object.is_a?(Hash) check This will be fixed in https://gitlab.com/gitlab-org/gitlab/-/issues/412328",
    "label": "",
    "id": "1007"
  },
  {
    "raw_code": "def import_merge_event(merge_request, merge_event)\n          log_info(import_stage: 'import_merge_event', message: 'starting', iid: merge_request.iid)\n\n          user_id = if user_mapping_enabled?(project)\n                      user_finder.uid(\n                        username: merge_event.committer_username,\n                        display_name: merge_event.committer_name\n                      )\n                    else\n                      user_finder.find_user_id(by: :email, value: merge_event.committer_email)\n                    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1008"
  },
  {
    "raw_code": "def import_approved_event(merge_request, approved_event)\n          log_info(\n            import_stage: 'import_approved_event',\n            message: 'starting',\n            iid: merge_request.iid,\n            event_id: approved_event.id\n          )\n\n          user_id = if user_mapping_enabled?(project)\n                      user_finder.uid(\n                        username: approved_event.approver_username,\n                        display_name: approved_event.approver_name\n                      )\n                    else\n                      user_finder.find_user_id(by: :email, value: approved_event.approver_email)\n                    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1009"
  },
  {
    "raw_code": "def concurrent_import_jobs_limit\n          # Reduce fetch limit (from 100) to avoid Gitlab::Git::ResourceExhaustedError\n          50\n        end",
    "comment": "To avoid overloading Gitaly, we use a smaller limit for pull requests than the one defined in the application settings.",
    "label": "",
    "id": "1010"
  },
  {
    "raw_code": "def initialize(project, merge_request)\n            @project = project\n            @user_finder = UserFinder.new(project)\n            @formatter = Gitlab::ImportFormatter.new\n            @merge_request = merge_request\n          end",
    "comment": "@param project [Project] @param merge_request [MergeRequest]",
    "label": "",
    "id": "1011"
  },
  {
    "raw_code": "def amount_to_be_flushed\n        redis_state do |redis|\n          redis.eval(LUA_FLUSH_INCREMENT_SCRIPT, keys: [key, flushed_key])\n        end",
    "comment": "amount_to_be_flushed returns the total value to be flushed. The total value is the sum of the following: - current value in the increment_key - any existing value in the flushed_key that has not been flushed",
    "label": "",
    "id": "1012"
  },
  {
    "raw_code": "def get_server_info\n        request = Gitlab::Agent::ServerInfo::Rpc::GetServerInfoRequest.new\n\n        stub_for(:server_info)\n          .get_server_info(request, metadata: metadata)\n          .current_server_info\n      end",
    "comment": "Return GitLab KAS server info This method only returns information about a single KAS server instance without taking into account that there are potentially multiple KAS replicas running, which may not have the same server info. This is particularly the case during a rollout.",
    "label": "",
    "id": "1013"
  },
  {
    "raw_code": "def write_multiple(mapping)\n          with_redis do |redis|\n            Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n              redis.pipelined do |pipelined|\n                mapping.each do |raw_key, value|\n                  key = cache_key_for(raw_key)\n\n                  pipelined.set(key, gzip_compress(value.to_json), ex: EXPIRATION)\n                end",
    "comment": "Sets multiple keys to a given value. The value is serialized as JSON.  mapping - Write multiple cache values at once",
    "label": "",
    "id": "1014"
  },
  {
    "raw_code": "def read_multiple(raw_keys)\n          return [] if raw_keys.empty?\n\n          keys = raw_keys.map { |id| cache_key_for(id) }\n\n          content =\n            with_redis do |redis|\n              Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n                if Gitlab::Redis::ClusterUtil.cluster?(redis)\n                  redis.pipelined do |pipeline|\n                    keys.each { |key| pipeline.get(key) }\n                  end",
    "comment": "Reads multiple cache keys at once.  raw_keys - An Array of unique cache keys, without namespaces.  It returns a list of deserialized diff lines. Ex.: [[Gitlab::Diff::Line, ...], [Gitlab::Diff::Line]]",
    "label": "",
    "id": "1015"
  },
  {
    "raw_code": "def clear_multiple(raw_keys)\n          return [] if raw_keys.empty?\n\n          keys = raw_keys.map { |id| cache_key_for(id) }\n\n          with_redis do |redis|\n            Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n              if Gitlab::Redis::ClusterUtil.cluster?(redis)\n                Gitlab::Redis::ClusterUtil.batch_unlink(keys, redis)\n              else\n                redis.del(keys)\n              end",
    "comment": "Clears multiple cache keys at once.  raw_keys - An Array of unique cache keys, without namespaces.  It returns the number of cache keys cleared. Ex.: 42",
    "label": "",
    "id": "1016"
  },
  {
    "raw_code": "def find_by_id(id)\n        diff_files_indexed_by_id[id]\n      end",
    "comment": "Returns a Gitlab::Diff::File with the given ID (`unique_identifier` in Gitlab::Diff::File).",
    "label": "",
    "id": "1017"
  },
  {
    "raw_code": "def load_highlight(diff_note_ids: nil)\n        ids = highlightable_collection_ids(diff_note_ids)\n        return if ids.empty?\n\n        cached_content = read_cache(ids)\n\n        uncached_ids = ids.select.each_with_index { |_, i| cached_content[i].nil? }\n        mapping = highlighted_lines_by_ids(uncached_ids)\n\n        HighlightCache.write_multiple(mapping) if mapping.any?\n\n        diffs = diff_files_indexed_by_id.values_at(*ids)\n\n        diffs.zip(cached_content).each do |diff, cached_lines|\n          next unless diff && cached_lines\n\n          diff.highlighted_diff_lines = cached_lines\n        end",
    "comment": "Writes cache and preloads highlighted diff lines for highlightable object IDs, in @collection.  - Highlight cache is written just for uncached diff files - The cache content is not updated (there's no need to do so) - Load only the related diff note ids",
    "label": "",
    "id": "1018"
  },
  {
    "raw_code": "def highlighted_lines_by_ids(ids)\n        diff_files_indexed_by_id.slice(*ids).transform_values do |file|\n          file.highlighted_diff_lines.map(&:to_hash)\n        end",
    "comment": "Processes the diff lines highlighting for diff files matching the given IDs.  Returns a Hash with { id => [Array of Gitlab::Diff::line], ...]",
    "label": "",
    "id": "1019"
  },
  {
    "raw_code": "def storage_key\n          strong_memoize(:storage_key) do\n            re = /(?<storage>.+)_shard_.+/\n            md = re.match(self.name.demodulize.underscore)\n            (md && md[:storage]) || self.name.demodulize.underscore\n          end",
    "comment": "TODO: To be used by https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/395 as a 'label' alias. The 2 acceptable formats for a demodulized name are: <storage>_shard_<shard> or <storage>.",
    "label": "",
    "id": "1020"
  },
  {
    "raw_code": "def allow_cross_slot_commands\n          Thread.current[:allow_cross_slot_commands] ||= 0\n          Thread.current[:allow_cross_slot_commands] += 1\n\n          yield\n        ensure\n          Thread.current[:allow_cross_slot_commands] -= 1\n        end",
    "comment": "Keep track of the call stack to allow nested calls to work.",
    "label": "",
    "id": "1021"
  },
  {
    "raw_code": "def extract_hash_tag(key)\n          s = key.index('{')\n\n          return key unless s\n\n          e = key.index('}', s + 1)\n\n          return key unless e\n\n          key[s + 1..e - 1]\n        end",
    "comment": "This is almost identical to Redis::Cluster::Command#extract_hash_tag, except that it returns the original string if no hash tag is found. ",
    "label": "",
    "id": "1022"
  },
  {
    "raw_code": "def send_command(method, command, *args, &block)\n        super\n      rescue ::RedisClient::Cluster::NodeMightBeDown => e\n        # rubocop:disable Gitlab/ModuleWithInstanceVariables -- this class is used to monkeypatch RedisClient::Cluster::Router\n        slots_map = Gitlab::Instrumentation::RedisClusterRouter.format_slotmap(@node.instance_variable_get(:@slots))\n        Gitlab::ErrorTracking.log_exception(\n          e,\n          node_keys: @node.node_keys,\n          slots_map: slots_map\n        )\n\n        inst = instrumentation_class(@config)\n        inst.instance_count_exception(e) if inst\n        # rubocop:enable Gitlab/ModuleWithInstanceVariables\n\n        raise e\n      end",
    "comment": "Patch the `send_command` method in RedisClient::Cluster::Router See https://github.com/redis-rb/redis-cluster-client/blob/v0.8.2/lib/redis_client/cluster/router.rb#L34  When a Redis Cluster is in a fail state, we might not have metrics on the server-side. This allows the application to dump its local topology state to get the client-side perspective of any cluster failure.",
    "label": "",
    "id": "1023"
  },
  {
    "raw_code": "def compact_array(arr)\n          return \"\" if arr.empty?\n\n          range = \"\"\n          prev = nil\n          arr.each do |i|\n            if prev.nil?\n              range += i.to_s\n            elsif prev + 1 < i\n              range += \"-#{prev},#{i}\"\n            end",
    "comment": "compact_array converts an array of integers into a range string e.g. [0, 1, 2, 4, 5, 6] to \"0-2,4-6\"",
    "label": "",
    "id": "1024"
  },
  {
    "raw_code": "def __subject__\n          @subject\n        end",
    "comment": "Presenters should always access the subject through an explicit getter defined with `presents ..., as:`, the `__subject__` method is only intended for internal use.",
    "label": "",
    "id": "1025"
  },
  {
    "raw_code": "def declarative_policy_delegate\n          __subject__\n        end",
    "comment": "delegate all #can? queries to the subject",
    "label": "",
    "id": "1026"
  },
  {
    "raw_code": "def check\n          catch_timeout 10.seconds do\n            redis_instance_class_name.with(&:ping)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1027"
  },
  {
    "raw_code": "def initialize(*checks)\n          @checks = checks\n        end",
    "comment": "This accepts an array of objects implementing `:readiness` that returns `::Gitlab::HealthChecks::Result`",
    "label": "",
    "id": "1028"
  },
  {
    "raw_code": "def success?\n          http_status / 100 == 2\n        end",
    "comment": "We accept 2xx",
    "label": "",
    "id": "1029"
  },
  {
    "raw_code": "def initialize(hash, prefix: nil, connector: '.')\n        @hash = hash\n      end",
    "comment": "Validates the hash size using `Gitlab::Utils::DeepSize` before merging keys using `Gitlab::Utils::InlineHash`",
    "label": "",
    "id": "1030"
  },
  {
    "raw_code": "def merge_keys(hash, prefix: nil, connector: '.')\n        result = {}\n        pairs =\n          if prefix\n            base_prefix = \"#{prefix}#{connector}\"\n            hash.map { |key, value| [\"#{base_prefix}#{key}\", value] }\n          else\n            hash.to_a\n          end",
    "comment": "Transforms a Hash into an inline Hash by merging its nested keys.  Input  { 'root_param' => 'Root', 12 => 'number', symbol: 'symbol', nested_param: { key: 'Value' }, 'very' => { 'deep' => { 'nested' => { 12 => 'Deep nested value' } } } }   Result  { 'root_param' => 'Root', 12 => 'number', symbol: symbol, 'nested_param.key' => 'Value', 'very.deep.nested.12' => 'Deep nested value' } ",
    "label": "",
    "id": "1031"
  },
  {
    "raw_code": "def self.execute_every(period, cache_key, skip_in_development: true)\n        return yield if skip_in_development && Rails.env.development?\n        return yield unless period\n\n        Gitlab::Redis::SharedState.with do |redis|\n          key_set = redis.set(cache_key, 1, ex: period, nx: true)\n          break false unless key_set\n\n          yield\n        end",
    "comment": "Executes a block of code at most once within a given time period using Redis for throttling. This is useful for scheduled tasks that should not execute too frequently.  @param [ActiveSupport::Duration, Integer] period: The minimum time period between executions @param [String] cache_key: The key to use for Redis caching (must be unique for each distinct operation) @param [Boolean] skip_in_development: If true, always executes in development environment (default: true) @param [Proc] block: The code to execute if throttling conditions are met  @return [Object, false] The result of the block or false if execution was throttled  @example Execute a task at most once every hour Gitlab::Utils::RedisThrottle.execute_every(1.hour, 'my_task:hourly_job') do puts \"This will run once per hour\" end ",
    "label": "",
    "id": "1032"
  },
  {
    "raw_code": "def sanitize_unsafe_links(env)\n        # sanitize calls this with every node, so no need to check child nodes\n        remove_unsafe_links(env, sanitize_children: false)\n      end",
    "comment": "sanitize 6.0 requires only a context argument. Do not add any default arguments to this method.",
    "label": "",
    "id": "1033"
  },
  {
    "raw_code": "def safe_protocol?(scheme)\n        return false unless scheme\n\n        scheme = scheme\n          .strip\n          .downcase\n          .gsub(/[^A-Za-z\\+\\.\\-]+/, '')\n\n        UNSAFE_PROTOCOLS.none?(scheme)\n      end",
    "comment": "Remove all invalid scheme characters before checking against the list of unsafe protocols.  See https://www.rfc-editor.org/rfc/rfc3986#section-3.1 ",
    "label": "",
    "id": "1034"
  },
  {
    "raw_code": "def obfuscated_email(email, deform: false)\n        return email if email.empty?\n\n        masker_class = deform ? Deform : Symmetrical\n        masker_class.new(email).masked\n      end",
    "comment": "Replaces most visible characters with * to obfuscate an email address deform adds a fix number of * to ensure the address cannot be guessed. Also obfuscates TLD with **",
    "label": "",
    "id": "1035"
  },
  {
    "raw_code": "def obfuscate_emails_in_text(text)\n        return text unless text.present?\n\n        text.gsub(EMAIL_REGEXP_WITH_CAPTURING_GROUP) do |email|\n          obfuscated_email(email, deform: true)\n        end",
    "comment": "Runs email address obfuscation on the given text.",
    "label": "",
    "id": "1036"
  },
  {
    "raw_code": "def css_to_xpath(css)\n          xpath = ::Nokogiri::CSS.xpath_for(css)\n\n          # Due to https://github.com/sparklemotion/nokogiri/issues/572,\n          # we remove the leading `//` and add `descendant-or-self::`\n          # in order to ensure we're searching from this node and all\n          # descendants.\n          xpath.map { |t| \"descendant-or-self::#{t[2..]}\" }.join('|')\n        end",
    "comment": "Use Nokogiri to convert a css selector into an xpath selector. Nokogiri can use css selectors with `doc.search()`.  However for large node trees, it is _much_ slower than using xpath, by several orders of magnitude. https://gitlab.com/gitlab-org/gitlab/-/issues/329186",
    "label": "",
    "id": "1037"
  },
  {
    "raw_code": "def merge(elements)\n        merged, *other_elements = elements\n\n        other_elements.each do |element|\n          merged = merge_hash_tree(merged, element)\n        end",
    "comment": "Deep merges an array of elements which can be hashes, arrays, or other objects.  [{ hello: [\"world\"] }, { hello: \"Everyone\" }, { hello: { greetings: ['Bonjour', 'Hello', 'Hallo', 'Dzien dobry'] } }, \"Goodbye\", \"Hallo\"] =>  [ { hello: [ \"world\", \"Everyone\", { greetings: ['Bonjour', 'Hello', 'Hallo', 'Dzien dobry'] } ] }, \"Goodbye\" ]",
    "label": "",
    "id": "1038"
  },
  {
    "raw_code": "def crush(array_or_hash)\n        if array_or_hash.is_a?(Array)\n          crush_array(array_or_hash)\n        else\n          crush_hash(array_or_hash)\n        end",
    "comment": "This extracts all keys and values from a hash into an array  { hello: \"world\", this: { crushes: [\"an entire\", \"hash\"] } } => [:hello, \"world\", :this, :crushes, \"an entire\", \"hash\"]",
    "label": "",
    "id": "1039"
  },
  {
    "raw_code": "def gsub_with_limit(text, pattern, limit:)\n        new_text = +''\n        remainder = text\n        count = 0\n\n        matched = remainder.match(pattern)\n\n        until matched.nil? || matched.to_a.compact.empty?\n          new_text << matched.pre_match\n          remainder = matched.post_match\n\n          new_text << (yield(matched) || '').to_s\n\n          if limit > 0\n            count += 1\n            break if count >= limit\n          end",
    "comment": "This performs the same basic function as a `gsub`. However this version allows us to break out of the replacement loop when the limit is reached. This is the same algorithm used for Gitlab::UntrustedRegexp.replace_gsub",
    "label": "",
    "id": "1040"
  },
  {
    "raw_code": "def linked?(file)\n          stat = to_file_stat(file)\n\n          stat.symlink? || shares_hard_link?(stat)\n        end",
    "comment": "Returns true if: - File or directory is a symlink. - File shares a hard link.",
    "label": "",
    "id": "1041"
  },
  {
    "raw_code": "def shares_hard_link?(file)\n          stat = to_file_stat(file)\n\n          stat.file? && stat.nlink > 1\n        end",
    "comment": "Returns: - true if file shares a hard link with another file. - false if file is a directory, as directories cannot be hard linked.",
    "label": "",
    "id": "1042"
  },
  {
    "raw_code": "def estimate(object)\n          case object\n          when Hash\n            estimate_hash(object)\n          when Array\n            estimate_array(object)\n          when String\n            estimate_string(object)\n          when Integer\n            estimate_integer(object)\n          when Float\n            estimate_float(object)\n          when DateTime, Time\n            estimate_time(object)\n          when NilClass\n            NULL_SIZE\n          else\n            # might be incorrect, but #to_s is safe, #to_json might be disabled for some objects: User\n            estimate_string(object.to_s)\n          end",
    "comment": "Returns: integer (number of bytes)",
    "label": "",
    "id": "1043"
  },
  {
    "raw_code": "def checksum(skip_memoization: false)\n          return @checksum if @checksum.present? && !skip_memoization\n\n          @checksum = Digest::SHA256.hexdigest(entries.to_s)\n        end",
    "comment": "Used by BackgroundMigration/DictionaryFile cop to invalidate its cache if the contents of `db/docs/batched_background_migrations` changes.",
    "label": "",
    "id": "1044"
  },
  {
    "raw_code": "def self.log_limited_array(array, sentinel: '...')\n        return [] unless array.is_a?(Array) || array.is_a?(Enumerator::Lazy)\n\n        total_length = 0\n        limited_array = array.take_while do |arg|\n          total_length += JsonSizeEstimator.estimate(arg)\n\n          total_length <= MAXIMUM_ARRAY_LENGTH\n        end.to_a\n\n        limited_array.push(sentinel) if total_length > MAXIMUM_ARRAY_LENGTH\n\n        limited_array\n      end",
    "comment": "Prepare an array for logging by limiting its JSON representation to around 10 kilobytes. Once we hit the limit, add the sentinel value as the last item in the returned array.",
    "label": "",
    "id": "1045"
  },
  {
    "raw_code": "def self.clear_key(batch_key)\n        return if ::BatchLoader::Executor.current.nil?\n\n        items_to_clear = ::BatchLoader::Executor.current.items_by_block.select do |k, v|\n          # The Hash key here is [source_location, batch_key], so we just check k[1]\n          k[1] == batch_key\n        end",
    "comment": "Clears batched items under the specified batch key https://github.com/exAspArk/batch-loader#batch-key",
    "label": "",
    "id": "1046"
  },
  {
    "raw_code": "def override(method_name)\n        return unless Gitlab::Environment.static_verification?\n\n        Override.extensions[self] ||= Extension.new(self)\n        Override.extensions[self].add_method_name(method_name)\n      end",
    "comment": "Instead of writing patterns like this:  def f raise NotImplementedError unless defined?(super)  true end  We could write it like:  extend ::Gitlab::Utils::Override  override :f def f true end  This would make sure we're overriding something. See: https://gitlab.com/gitlab-org/gitlab/issues/1819",
    "label": "",
    "id": "1047"
  },
  {
    "raw_code": "def histogram(relation, column, buckets:, bucket_size: buckets.size)\n        with_metadata do\n          # Using lambda to avoid exposing histogram specific methods\n          parameters_valid = -> do\n            error_message =\n              if buckets.first == buckets.last\n                'Lower bucket bound cannot equal to upper bucket bound'\n              elsif bucket_size == 0\n                'Bucket size cannot be zero'\n              elsif bucket_size > MAX_BUCKET_SIZE\n                \"Bucket size #{bucket_size} exceeds the limit of #{MAX_BUCKET_SIZE}\"\n              end",
    "comment": "We don't support batching with histograms. Please avoid using this method on large tables. See https://gitlab.com/gitlab-org/gitlab/-/issues/323949.  rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1048"
  },
  {
    "raw_code": "def add(*args)\n        with_metadata do\n          break -1 if args.any?(&:negative?)\n\n          args.sum\n        rescue StandardError => error\n          Gitlab::ErrorTracking.track_and_raise_for_dev_exception(error)\n          FALLBACK\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1049"
  },
  {
    "raw_code": "def track_usage_event(event_name, values)\n        Gitlab::UsageDataCounters::HLLRedisCounter.track_event(event_name.to_s, values: values)\n      end",
    "comment": "@param event_name [String, Symbol] the event name @param values [Array|String] the values counted",
    "label": "",
    "id": "1050"
  },
  {
    "raw_code": "def compact(traversal_ids, limit)\n          traversal_ids = traversal_ids.sort_by { |arr| [-arr.length, arr] }\n          traversal_ids = compact_once(traversal_ids) while traversal_ids.size > limit\n\n          traversal_ids\n        end",
    "comment": "This class compacts an array of traversal_ids by finding the most common namespace and consolidating all children into an entry for that namespace. It continues this process until the size of the final array is less than the limit. If it cannot achieve the limit it raises a CompactionLimitCannotBeAchievedError.  The traversal_ids input will look like the array below where each element in the sub-arrays is a namespace id.  [ [1, 21], [1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 2, 12, 13], [1, 6, 7], [1, 6, 8], [9, 10, 11] ]  The limit input is the maximum number of elements in the final array. The compact method calls the compact_once method until the size of the final array is less than the limit. It then returns the compacted list of traversal_ids If it cannot achieve the limit it raises a CompactionLimitCannotBeAchievedError.",
    "label": "",
    "id": "1051"
  },
  {
    "raw_code": "def compact_once(traversal_ids)\n          most_common_namespace_path = find_most_common_namespace_path(traversal_ids)\n\n          compacted_traversal_ids = traversal_ids.map do |traversal_id|\n            if starts_with?(traversal_id, most_common_namespace_path)\n              most_common_namespace_path\n            else\n              traversal_id\n            end",
    "comment": "The compact_once method finds the most common namespace and compacts all children into an entry for that namespace. It then returns the compacted list of traversal_ids.",
    "label": "",
    "id": "1052"
  },
  {
    "raw_code": "def validate!(origin_project_traversal_ids, compacted_traversal_ids)\n          compacted_traversal_ids.each do |compacted_path|\n            # Fail if there are unexpected entries\n            raise UnexpectedCompactionEntry unless origin_project_traversal_ids.find do |original_path|\n              starts_with?(original_path, compacted_path)\n            end",
    "comment": "The validate method performs two checks on the compacted_traversal_ids 1. If there are redundant traversal_ids, for example [1,2,3,4] and [1,2,3] 2. If there are unexpected entries, meaning a traversal_id not present in the origin_project_traversal_ids If either case is found, it will raise an error Otherwise, it will return true",
    "label": "",
    "id": "1053"
  },
  {
    "raw_code": "def find_most_common_namespace_path(traversal_ids)\n          # namespace_counts is a tally of the number of times each namespace path occurs in the traversal_ids array\n          # after removing any namespace paths that occur only once\n          # The namespace path is the traversal_id without the last element\n          namespace_counts = traversal_ids.each_with_object([]) do |traversal_id, result|\n            result << traversal_id[0...-1] if traversal_id.size > 1\n          end.tally\n\n          # namespace is the namespace path that occurs the most times in the traversal_ids array after removing\n          # any namespace paths that occur only once since compaction isn't necessary for those\n          namespace = namespace_counts.reject { |_k, v| v == 1 }.sort_by { |k, v| [k.size, v] }.reverse.to_h.first\n\n          # if namespace is nil it means there are no more namespaces to compact so\n          # we raise a CompactionLimitCannotBeAchievedError\n          raise CompactionLimitCannotBeAchievedError if namespace.nil?\n\n          # return the most common namespace path\n          namespace.first\n        end",
    "comment": "find_most_common_namespace_path method takes an array of traversal_ids and returns the most common namespace For example, given the following traversal_ids it would return [1, 2]  [ [1, 21], [1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 2, 12, 13], [1, 6, 7], [1, 6, 8], [9, 10, 11] ]",
    "label": "",
    "id": "1054"
  },
  {
    "raw_code": "def starts_with?(traversal_id, namespace_path)\n          traversal_id.first(namespace_path.length) == namespace_path\n        end",
    "comment": "The starts_with? method returns true if the first n elements of the traversal_id match the namespace_path For example:  starts_with?([1, 2, 3], [1, 2]) #=> true starts_with?([1, 2], [1, 2, 3]) #=> false starts_with?([1, 2, 3], [1, 2, 3]) #=> true starts_with?([1, 2, 3], [1, 2, 3, 4]) #=> false",
    "label": "",
    "id": "1055"
  },
  {
    "raw_code": "def expand_on_ancestors(validators)\n          delegator_class.ancestors.each do |ancestor|\n            next if delegator_class == ancestor # ancestor includes itself\n\n            validator_ancestor = validators[ancestor]\n\n            next unless validator_ancestor\n\n            add_allowlist(validator_ancestor.allowed_method_names)\n          end",
    "comment": "This will make sure allowlist we put into ancestors are all included",
    "label": "",
    "id": "1056"
  },
  {
    "raw_code": "def increment(cache_key, expiry)\n        raise NotImplementedError\n      end",
    "comment": "Increment the rate limit count and return the new count value",
    "label": "",
    "id": "1057"
  },
  {
    "raw_code": "def read(cache_key)\n        raise NotImplementedError\n      end",
    "comment": "Return the rate limit count. Should be 0 if there is no data in the cache.",
    "label": "",
    "id": "1058"
  },
  {
    "raw_code": "def topology_instance_service_memory(instance, instance_data_by_type)\n        result = {}\n        instance_data_by_type.each do |memory_type, instance_data|\n          topology_data_for_instance(instance, instance_data).each do |metric, memory_bytes|\n            job = metric['job']\n            key = \"process_memory_#{memory_type}\".to_sym\n\n            result[job] ||= {}\n            result[job][key] ||= memory_bytes\n          end",
    "comment": "Given a hash mapping memory set types to Prometheus response data, returns a hash mapping instance/node names to services and their respective memory use in bytes",
    "label": "",
    "id": "1059"
  },
  {
    "raw_code": "def aggregate_by_labels(client, query, transform_value: :to_i)\n        client.aggregate(query, transform_value: transform_value) do |metric|\n          metric['instance'] = normalize_and_track_instance(metric['instance'])\n          metric\n        end",
    "comment": "Will retain a composite key that values are mapped to",
    "label": "",
    "id": "1060"
  },
  {
    "raw_code": "def map_instance_labels(query_result_vector, target_labels)\n        query_result_vector.to_h do |result|\n          key = normalize_and_track_instance(result['metric']['instance'])\n          value = result['metric'].slice(*target_labels).symbolize_keys\n          [key, value]\n        end",
    "comment": "Given query result vector, map instance to a hash of target labels key/value. @return [Hash] mapping instance to a hash of target labels key/value, or the empty hash if input empty vector",
    "label": "",
    "id": "1061"
  },
  {
    "raw_code": "def self.measure_thread_memory_allocations(previous)\n        return unless available?\n        return unless previous\n\n        current = Thread.current.memory_allocations\n        return unless current\n\n        # calculate difference in a memory allocations\n        result = previous.to_h do |key, value|\n          [KEY_MAPPING.fetch(key), current[key].to_i - value]\n        end",
    "comment": "This method returns a hash with the following keys: - mem_objects:     number of allocated heap slots (as reflected by GC) - mem_mallocs:     number of malloc calls - mem_bytes:       number of bytes allocated by malloc for objects that did not fit into a heap slot - mem_total_bytes: number of bytes allocated for both objects consuming an object slot and objects that required a malloc (mem_malloc_bytes)",
    "label": "",
    "id": "1062"
  },
  {
    "raw_code": "def interval_with_jitter\n        sleep_s + rand(sleep_max_delta_s)\n      end",
    "comment": "Returns the sleep interval with a random adjustment. The random adjustment is put in place to ensure continued availability.",
    "label": "",
    "id": "1063"
  },
  {
    "raw_code": "def stats(format: STATS_DEFAULT_FORMAT)\n        dump_stats(StringIO.new, format: format).string\n      end",
    "comment": "Return jemalloc stats as a string.",
    "label": "",
    "id": "1064"
  },
  {
    "raw_code": "def dump_stats(io, format: STATS_DEFAULT_FORMAT)\n        verify_format!(format)\n\n        format_settings = STATS_FORMATS[format]\n\n        with_malloc_stats_print do |stats_print|\n          write_stats(stats_print, io, format_settings)\n        end",
    "comment": "Streams jemalloc stats to the given IO object.",
    "label": "",
    "id": "1065"
  },
  {
    "raw_code": "def configure\n        yield configuration\n      end",
    "comment": " Configuration for Watchdog, see Gitlab::Memory::Watchdog::Configurator for examples.",
    "label": "",
    "id": "1066"
  },
  {
    "raw_code": "def sleep_time_seconds\n          @sleep_time_seconds ||= DEFAULT_SLEEP_TIME_SECONDS\n        end",
    "comment": "Used to control the frequency with which the watchdog will wake up and poll the GC.",
    "label": "",
    "id": "1067"
  },
  {
    "raw_code": "def initialize(max_heap_fragmentation:)\n            @max_heap_fragmentation = max_heap_fragmentation\n            init_frag_limit_metrics\n          end",
    "comment": "max_heap_fragmentation: The degree to which the Ruby heap is allowed to be fragmented. Range [0,1].",
    "label": "",
    "id": "1068"
  },
  {
    "raw_code": "def reference_mem\n            @reference_mem ||= Gitlab::Metrics::System.memory_usage_uss_pss(pid: Gitlab::Cluster::PRIMARY_PID)\n          end",
    "comment": "On pre-fork systems this would be the primary process memory from which workers fork. Otherwise it is the current process' memory.  We initialize this lazily because in the initializer the application may not have finished booting yet, which would yield an incorrect baseline.",
    "label": "",
    "id": "1069"
  },
  {
    "raw_code": "def config_fallback\n          SharedState\n        end",
    "comment": "The data we store on DbLoadBalancing used to be stored on SharedState.",
    "label": "",
    "id": "1070"
  },
  {
    "raw_code": "def config_fallback\n          SharedState\n        end",
    "comment": "The data we store on TraceChunks used to be stored on SharedState.",
    "label": "",
    "id": "1071"
  },
  {
    "raw_code": "def self.config_fallback\n        SharedState\n      end",
    "comment": "The data we store on Sessions used to be stored on SharedState.",
    "label": "",
    "id": "1072"
  },
  {
    "raw_code": "def active_support_config\n          {\n            redis: pool,\n            pool: false,\n            compress: Gitlab::Utils.to_boolean(ENV.fetch('ENABLE_REDIS_CACHE_COMPRESSION', '1')),\n            namespace: CACHE_NAMESPACE,\n            expires_in: default_ttl_seconds\n          }\n        end",
    "comment": "Configuration options for Rails.cache, not Gitlab::Redis::Cache  Full list of options: https://api.rubyonrails.org/classes/ActiveSupport/Cache/RedisCacheStore.html#method-c-new pool argument event not documented in the link above is handled by RedisCacheStore see: https://github.com/rails/rails/blob/593893c901f87b4ed205751f72df41519b4d2da3/activesupport/lib/active_support/cache/redis_cache_store.rb#L165 and https://github.com/rails/rails/blob/ad790cb2f6bc724a89e4266b505b3c57d5089dae/activesupport/lib/active_support/cache.rb#L206",
    "label": "",
    "id": "1073"
  },
  {
    "raw_code": "def instances\n          @instances ||= begin\n            shard_configs = load_shard_config\n\n            if shard_configs.empty?\n              { SIDEKIQ_MAIN_SHARD_INSTANCE_NAME => self }\n            else\n              extra_instances = shard_configs.map do |key, value|\n                # Dynamically creates child classes of Wrapper and set them using the store name.\n                # The corresponding instrumentation classes is defined in in lib/gitlab/instrumentation/redis.rb\n                #\n                # The extra instance classes will be part of Gitlab::Redis::ALL_CLASSES which\n                # contains *Gitlab::Redis::Queues.instances.values. This allows the dynamically created classes to\n                # behave like other defined Wrapper classes (e.e. Cache, SharedState).\n                #\n                # The dynamically created classes will have Gitlab::Redis::QueuesShardXXX where XXX is user-defined.\n                new_klass = create_shard_class(key, value)\n                Gitlab::Redis.const_set(new_klass.store_name, new_klass)\n                new_klass\n              end",
    "comment": "extra instances need to have the queues_shard_ prefix",
    "label": "",
    "id": "1074"
  },
  {
    "raw_code": "def config_fallback\n          Cache\n        end",
    "comment": "The data we store on FeatureFlag is currently stored on Cache.",
    "label": "",
    "id": "1075"
  },
  {
    "raw_code": "def add(key:, value:, expiry:)\n        validate_key!(key)\n\n        Gitlab::Redis::SharedState.with do |redis|\n          redis.multi do |multi|\n            Array.wrap(value).each_slice(BATCH_SIZE) { |batch| multi.pfadd(key, batch) }\n\n            multi.expire(key, expiry)\n          end",
    "comment": "Check a basic format for the Redis key in order to ensure the keys are in the same hash slot  Examples of keys project:{1}:set_a project:{1}:set_b project:{2}:set_c 2020-216-{project_action} i_{analytics}_dev_ops_score-2020-32",
    "label": "",
    "id": "1076"
  },
  {
    "raw_code": "def generate(args, kwargs = nil)\n        command = args.flat_map do |element|\n          case element\n          when Hash\n            element.flatten\n          else\n            element\n          end",
    "comment": "Ref: https://github.com/redis-rb/redis-client/blob/v0.19.1/lib/redis_client/command_builder.rb we modify the command builder to convert nil to strings as this behaviour was present in https://github.com/redis/redis-rb/blob/v4.8.0/lib/redis/connection/command_helper.rb#L20  Note that we only adopt the Ruby3.x-compatible logic in .generate. Symbol.method_defined?(:name) is true in Ruby 3",
    "label": "",
    "id": "1077"
  },
  {
    "raw_code": "def set(key, value, options = nil)\n        ttl = get_ttl(options)\n        if ttl\n          setex(key, ttl.to_i, value, raw: true)\n        else\n          super(key, value)\n        end",
    "comment": "copies ::Redis::Store::Ttl implementation in a redis-v5 compatible manner",
    "label": "",
    "id": "1078"
  },
  {
    "raw_code": "def setnx(key, value, options = nil)\n        ttl = get_ttl(options)\n        if ttl\n          multi do |m|\n            m.setnx(key, value)\n            m.expire(key, ttl)\n          end",
    "comment": "copies ::Redis::Store::Ttl implementation in a redis-v5 compatible manner",
    "label": "",
    "id": "1079"
  },
  {
    "raw_code": "def create_using_pool(primary_pool, secondary_pool, instance_name)\n          new(\n            primary_pool: primary_pool,\n            secondary_pool: secondary_pool,\n            instance_name: instance_name,\n            primary_store: nil,\n            secondary_store: nil\n          )\n        end",
    "comment": "This initialises a MultiStore instance with references to 2 pools. All method calls must be wrapped in a `.with_borrowed_connection` scope.  This is the preferred way to use MultiStore as re-using connections from connection pools is more efficient than spinning up new ones.",
    "label": "",
    "id": "1080"
  },
  {
    "raw_code": "def create_using_client(primary_store, secondary_store, instance_name)\n          new(\n            primary_store: primary_store,\n            secondary_store: secondary_store,\n            instance_name: instance_name,\n            primary_pool: nil,\n            secondary_pool: nil\n          )\n        end",
    "comment": "Initialises a MultiStore with 2 client connections. This is used in situations where the caller does not use connection pools and holds long-running connections,e.g. ActionCable pubsub. This would create additional connections to the primary and secondary store on per MultiStore instance.",
    "label": "",
    "id": "1081"
  },
  {
    "raw_code": "def initialize(primary_pool:, secondary_pool:, primary_store:, secondary_store:, instance_name:)\n        @instance_name = instance_name\n        @primary_pool = primary_pool\n        @secondary_pool = secondary_pool\n        @primary_store = primary_store\n        @secondary_store = secondary_store\n\n        validate_attributes!\n      end",
    "comment": "To transition between two Redis store, `primary_pool` should be the connection pool for the target store, and `secondary_pool` should be the connection pool for the current store.  Transition is controlled with feature flags: - At the default state, all read and write operations are executed in the secondary instance. - Turning use_primary_and_secondary_stores_for_<instance_name> on: The store writes to both instances. The read commands are executed in the default store with no fallbacks. Other commands are executed in the the default instance (Secondary). - Turning use_primary_store_as_default_for_<instance_name> on: The behavior is the same as above, but other commands are executed in the primary now. - Turning use_primary_and_secondary_stores_for_<instance_name> off: commands are executed in the primary store.",
    "label": "",
    "id": "1082"
  },
  {
    "raw_code": "def with_readonly_pipeline\n        raise NestedReadonlyPipelineError if readonly_pipeline?\n\n        Thread.current[:readonly_pipeline] = true\n\n        yield\n      ensure\n        Thread.current[:readonly_pipeline] = false\n      end",
    "comment": "Pipelines are sent to both instances by default since they could execute both read and write commands.  But for pipelines that only consists of read commands, this method can be used to scope the pipeline and send it only to the default store.",
    "label": "",
    "id": "1083"
  },
  {
    "raw_code": "def respond_to_missing?(command_name, include_private = false)\n        true\n      end",
    "comment": "rubocop:enable GitlabSecurity/PublicSend",
    "label": "",
    "id": "1084"
  },
  {
    "raw_code": "def is_a?(klass)\n        return true if klass == default_store.class\n\n        super(klass)\n      end",
    "comment": "This is needed because of Redis::Rack::Connection is requiring Redis::Store https://github.com/redis-store/redis-rack/blob/a833086ba494083b6a384a1a4e58b36573a9165d/lib/redis/rack/connection.rb#L15 Done similarly in https://github.com/lsegal/yard/blob/main/lib/yard/templates/template.rb#L122",
    "label": "",
    "id": "1085"
  },
  {
    "raw_code": "def close\n        return if primary_store.nil? || secondary_store.nil?\n\n        if same_redis_store?\n          # if same_redis_store?, `use_primary_store_as_default?` returns false\n          # but we should avoid a feature-flag check in `.close` to avoid checking out\n          # an ActiveRecord connection during clean up.\n          secondary_store.close\n        else\n          [primary_store, secondary_store].map(&:close).first\n        end",
    "comment": "connection_pool gem calls `#close` method:  https://github.com/mperham/connection_pool/blob/v2.4.1/lib/connection_pool.rb#L63  Let's define it explicitly instead of propagating it to method_missing",
    "label": "",
    "id": "1086"
  },
  {
    "raw_code": "def blpop(*args)\n        result = default_store.blpop(*args)\n        if !!result && use_primary_and_secondary_stores?\n          # special case to accommodate Gitlab::JobWaiter as blpop is only used in JobWaiter\n          # 1s should be sufficient wait time to account for delays between 1st and 2nd lpush\n          # https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2520#note_1630893702\n          non_default_store.blpop(args.first, timeout: 1)\n        end",
    "comment": "blpop blocks until an element to be popped exist in the list or after a timeout.",
    "label": "",
    "id": "1087"
  },
  {
    "raw_code": "def feature_table_exists?\n        # Use table_exists? (which uses ActiveRecord's schema cache) instead of Feature.feature_flags_available?\n        # as the latter runs a ';' SQL query which causes a connection to be checked out.\n        Feature::FlipperFeature.table_exists?\n      rescue StandardError\n        false\n      end",
    "comment": "@return [Boolean]",
    "label": "",
    "id": "1088"
  },
  {
    "raw_code": "def pipelined_both(command_name, *args, **kwargs, &block)\n        result_default = send_command(default_store, command_name, *args, **kwargs, &block)\n\n        begin\n          result_non_default = send_command(non_default_store, command_name, *args, **kwargs, &block)\n        rescue StandardError => e\n          log_error(e, command_name, multi_store_error_message: FAILED_TO_RUN_PIPELINE)\n        end",
    "comment": "Run the entire pipeline on both stores. We assume that `&block` is idempotent.",
    "label": "",
    "id": "1089"
  },
  {
    "raw_code": "def send_command(redis_instance, command_name, *args, **kwargs, &block)\n        if block\n          # Make sure that block is wrapped and executed only on the redis instance that is executing the block\n          redis_instance.send(command_name, *args, **kwargs) do |*params|\n            with_instance(redis_instance, *params, &block)\n          end",
    "comment": "rubocop:disable GitlabSecurity/PublicSend",
    "label": "",
    "id": "1090"
  },
  {
    "raw_code": "def with_instance(instance, *params)\n        @instance = instance\n\n        yield(*params)\n      ensure\n        @instance = nil\n      end",
    "comment": "rubocop:enable GitlabSecurity/PublicSend",
    "label": "",
    "id": "1091"
  },
  {
    "raw_code": "def config_fallback\n          Cache\n        end",
    "comment": "The data we store on RepositoryCache used to be stored on Cache.",
    "label": "",
    "id": "1092"
  },
  {
    "raw_code": "def with\n          multistore_pool.with do |multistore|\n            yield multistore\n          end",
    "comment": "overrides Wrapper's pool to use the multistore connection pool",
    "label": "",
    "id": "1093"
  },
  {
    "raw_code": "def with\n        super do |conn|\n          next yield conn unless conn.respond_to?(:with_borrowed_connection)\n\n          conn.with_borrowed_connection do\n            yield conn\n          end",
    "comment": "We intercept the returned connection and borrow the connections before yielding the block.",
    "label": "",
    "id": "1094"
  },
  {
    "raw_code": "def to_s\n        self.class.encode(@value)\n      end",
    "comment": "@return [String] the encoded boolean",
    "label": "",
    "id": "1095"
  },
  {
    "raw_code": "def encode(value)\n          raise NotABooleanError, value unless bool?(value)\n\n          [LABEL, to_string(value)].join(DELIMITER)\n        end",
    "comment": "Turn a boolean into a string for storage in Redis  @param value [Boolean] true or false @return [String] the encoded boolean @raise [NotABooleanError] if the value isn't true or false",
    "label": "",
    "id": "1096"
  },
  {
    "raw_code": "def decode(value)\n          raise NotAnEncodedBooleanStringError, value.class unless value.is_a?(String)\n\n          label, bool_str = *value.split(DELIMITER, 2)\n\n          raise NotAnEncodedBooleanStringError, label unless label == LABEL\n\n          from_string(bool_str)\n        end",
    "comment": "Decode a boolean string  @param value [String] the stored boolean string @return [Boolean] true or false @raise [NotAnEncodedBooleanStringError] if the provided value isn't an encoded boolean",
    "label": "",
    "id": "1097"
  },
  {
    "raw_code": "def true?(encoded_value)\n          decode(encoded_value)\n        end",
    "comment": "Decode a boolean string, then test if it's true  @param value [String] the stored boolean string @return [Boolean] is the value true? @raise [NotAnEncodedBooleanStringError] if the provided value isn't an encoded boolean",
    "label": "",
    "id": "1098"
  },
  {
    "raw_code": "def false?(encoded_value)\n          !true?(encoded_value)\n        end",
    "comment": "Decode a boolean string, then test if it's false  @param value [String] the stored boolean string @return [Boolean] is the value false? @raise [NotAnEncodedBooleanStringError] if the provided value isn't an encoded boolean",
    "label": "",
    "id": "1099"
  },
  {
    "raw_code": "def cluster?(obj)\n          if obj.is_a?(MultiStore)\n            cluster?(obj.primary_store) || cluster?(obj.secondary_store)\n          else\n            obj.is_a?(::Redis::Cluster)\n          end",
    "comment": "clusters? is used to select Redis command types, on `true`, the subsequent commands should be compatible with Redis Cluster.  When working with MultiStore, if even 1 of 2 stores is a Redis::Cluster, we should err on the side of caution and return `true `,",
    "label": "",
    "id": "1100"
  },
  {
    "raw_code": "def batch_get(keys, redis)\n          batch(keys, redis) do |pipeline, subset|\n            subset.map { |key| pipeline.get(key) }\n          end.flatten\n        end",
    "comment": "Redis cluster alternative to mget",
    "label": "",
    "id": "1101"
  },
  {
    "raw_code": "def config_fallback\n          Cache\n        end",
    "comment": "The data we store on RateLimiting used to be stored on Cache.",
    "label": "",
    "id": "1102"
  },
  {
    "raw_code": "def with_suppressed_errors(&block)\n          with(&block)\n        rescue ::Redis::BaseError, ::RedisClient::Error => e\n          Gitlab::ErrorTracking.track_and_raise_for_dev_exception(e)\n        end",
    "comment": "Rescue Redis errors so we do not take the site down when the rate limiting instance is down",
    "label": "",
    "id": "1103"
  },
  {
    "raw_code": "def rails_root\n          File.expand_path('../../..', __dir__)\n        end",
    "comment": "We need this local implementation of Rails.root because MailRoom doesn't load Rails.",
    "label": "",
    "id": "1104"
  },
  {
    "raw_code": "def inherit!(deps)\n          return unless deps\n\n          self.class.nodes.each do |key, factory|\n            next unless factory.inheritable?\n\n            new_entry = overwrite_entry(deps, key, self[key])\n\n            entries[key] = new_entry if new_entry&.specified?\n          end",
    "comment": "We inherit config entries from `default:` if the entry has the `inherit: true` flag set",
    "label": "",
    "id": "1105"
  },
  {
    "raw_code": "def entry_create!(key, value)\n          factory = self.class\n            .nodes[key]\n            .value(value)\n            .with(key: key, parent: self)\n\n          entries[key] = factory.create!\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1106"
  },
  {
    "raw_code": "def skip_config_hash_validation?\n          false\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1107"
  },
  {
    "raw_code": "def entry(key, entry, description: nil, default: nil, inherit: nil, reserved: nil, deprecation: nil, metadata: {})\n            entry_name = key.to_sym\n            raise ArgumentError, \"Entry '#{key}' already defined in '#{name}'\" if @nodes.to_h[entry_name]\n\n            factory = ::Gitlab::Config::Entry::Factory.new(entry)\n              .with(description: description)\n              .with(default: default)\n              .with(inherit: inherit)\n              .with(reserved: reserved)\n              .with(deprecation: deprecation)\n              .metadata(metadata)\n\n            @nodes ||= {}\n            @nodes[entry_name] = factory\n\n            helpers(entry_name)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1108"
  },
  {
    "raw_code": "def dynamic_helpers(*nodes)\n            helpers(*nodes, dynamic: true)\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1109"
  },
  {
    "raw_code": "def validate_nested_array_recursively(value, nesting_level, &validator_proc)\n            return true if validator_proc.call(value)\n            return false if nesting_level <= 0\n            return false unless value.is_a?(Array)\n\n            value.all? do |element|\n              validate_nested_array_recursively(element, nesting_level - 1, &validator_proc)\n            end",
    "comment": "rubocop: disable Performance/RedundantBlockCall Disables Rubocop rule for easier readability reasons.",
    "label": "",
    "id": "1110"
  },
  {
    "raw_code": "def documents\n          docs = config\n                  .split(MULTI_DOC_DIVIDER, max_documents_including_leading_delimiter)\n                  .map { |d| Yaml.new(d, additional_permitted_classes: additional_permitted_classes) }\n\n          docs.shift if docs.first.blank?\n          docs.reject!(&:blank?) if reject_empty\n          docs\n        end",
    "comment": "Valid YAML files can start with either a leading delimiter or no delimiter. To avoid counting a leading delimiter towards the document limit, this method splits the file by one more than the maximum number of permitted documents. It then discards the first document if it is blank.",
    "label": "",
    "id": "1111"
  },
  {
    "raw_code": "def increment_cache_hit(labels = {})\n        counter.increment(base_labels.merge(labels, cache_hit: true))\n      end",
    "comment": "Increase cache hit counter ",
    "label": "",
    "id": "1112"
  },
  {
    "raw_code": "def increment_cache_miss(labels = {})\n        counter.increment(base_labels.merge(labels, cache_hit: false))\n      end",
    "comment": "Increase cache miss counter ",
    "label": "",
    "id": "1113"
  },
  {
    "raw_code": "def observe_cache_generation(labels = {}, &block)\n        real_start = Gitlab::Metrics::System.monotonic_time\n\n        value = yield\n\n        histogram.observe(base_labels.merge(labels), Gitlab::Metrics::System.monotonic_time - real_start)\n\n        value\n      end",
    "comment": "Measure the duration of cacheable action  @example observe_cache_generation do cacheable_action end ",
    "label": "",
    "id": "1114"
  },
  {
    "raw_code": "def cache\n        Rails.cache\n      end",
    "comment": "@return [ActiveSupport::Cache::Store]",
    "label": "",
    "id": "1115"
  },
  {
    "raw_code": "def contextual_cache_key(presenter, object, context)\n        return object.cache_key if context.nil?\n\n        [presenter_class_name(presenter), object.cache_key, context.call(object)].flatten.join(\":\")\n      end",
    "comment": "Optionally uses a `Proc` to add context to a cache key  @param object [Object] must respond to #cache_key @param context [Proc] a proc that will be called with the object as an argument, and which should return a string or array of strings to be combined into the cache key @return [String]",
    "label": "",
    "id": "1116"
  },
  {
    "raw_code": "def cached_object(object, presenter:, presenter_args:, context:, expires_in:)\n        misses = 0\n\n        json = cache.fetch(contextual_cache_key(presenter, object, context), expires_in: expires_in) do\n          time_action(render_type: :object) do\n            misses += 1\n\n            Gitlab::Json.dump(presenter.represent(object, **presenter_args).as_json)\n          end",
    "comment": "Used for fetching or rendering a single object  @param object [Object] the object to render @param presenter [Grape::Entity] @param presenter_args [Hash] keyword arguments to be passed to the entity @param context [Proc] @param expires_in [ActiveSupport::Duration, Integer] an expiry time for the cache entry @return [String]",
    "label": "",
    "id": "1117"
  },
  {
    "raw_code": "def cached_collection(collection, presenter:, presenter_args:, context:, expires_in:)\n        misses = 0\n\n        json = fetch_multi(presenter, collection, context: context, expires_in: expires_in) do |obj|\n          time_action(render_type: :collection) do\n            misses += 1\n\n            Gitlab::Json.dump(presenter.represent(obj, **presenter_args).as_json)\n          end",
    "comment": "Used for fetching or rendering multiple objects  @param objects [Enumerable<Object>] the objects to render @param presenter [Grape::Entity] @param presenter_args [Hash] keyword arguments to be passed to the entity @param context [Proc] @param expires_in [ActiveSupport::Duration, Integer] an expiry time for the cache entry @return [Array<String>]",
    "label": "",
    "id": "1118"
  },
  {
    "raw_code": "def fetch_multi(presenter, *objs, context:, **kwargs)\n        objs.flatten!\n        map = multi_key_map(presenter, objs, context: context)\n\n        # TODO: `contextual_cache_key` should be constructed based on the guideline https://docs.gitlab.com/ee/development/redis.html#multi-key-commands.\n        Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n          cache.fetch_multi(*map.keys, **kwargs) do |key|\n            yield map[key]\n          end",
    "comment": "An adapted version of ActiveSupport::Cache::Store#fetch_multi.  The original method only provides the missing key to the block, not the missing object, so we have to create a map of cache keys to the objects to allow us to pass the object to the missing value block.  The result is that this is functionally identical to `#fetch`.",
    "label": "",
    "id": "1119"
  },
  {
    "raw_code": "def multi_key_map(presenter, objects, context:)\n        objects.index_by do |object|\n          contextual_cache_key(presenter, object, context)\n        end",
    "comment": "@param objects [Enumerable<Object>] objects which _must_ respond to `#cache_key` @param context [Proc] a proc that can be called to help generate each cache key @return [Hash]",
    "label": "",
    "id": "1120"
  },
  {
    "raw_code": "def initialize(\n        cache_identifier: nil,\n        feature_category: Client::DEFAULT_FEATURE_CATEGORY,\n        backing_resource: Client::DEFAULT_BACKING_RESOURCE\n      )\n        @cache_identifier = cache_identifier\n        @feature_category = fetch_feature_category!(feature_category)\n        @backing_resource = fetch_backing_resource!(backing_resource)\n      end",
    "comment": "@param cache_identifier [String] defines the location of the cache definition Example: \"ProtectedBranches::CacheService#fetch\" @param feature_category [Symbol] name of the feature category (from config/feature_categories.yml) @param backing_resource [Symbol] most affected resource by cache generation (full list: VALID_BACKING_RESOURCES) @return [Gitlab::Cache::Metadata]",
    "label": "",
    "id": "1121"
  },
  {
    "raw_code": "def store_in_cache_if_needed\n          return delete_from_cache unless commit\n          return unless sha\n          return unless ref\n\n          if commit.sha == sha && project.repository.root_ref == ref\n            store_in_cache\n          end",
    "comment": "We only cache the status for the HEAD commit of a project This status is rendered in project lists",
    "label": "",
    "id": "1122"
  },
  {
    "raw_code": "def self.read(raw_key, timeout: TIMEOUT)\n          key = cache_key_for(raw_key)\n          value = with_redis { |redis| redis.get(key) }\n\n          if value.present?\n            # We refresh the expiration time so frequently used keys stick\n            # around, removing the need for querying the database as much as\n            # possible.\n            #\n            # A key may be empty when we looked up a GitHub user (for example) but\n            # did not find a matching GitLab user. In that case we _don't_ want to\n            # refresh the TTL so we automatically pick up the right data when said\n            # user were to register themselves on the GitLab instance.\n            with_redis { |redis| redis.expire(key, timeout) }\n          end",
    "comment": "Reads a cache key.  If the key exists and has a non-empty value its TTL is refreshed automatically.  raw_key - The cache key to read. timeout - The new timeout of the key if the key is to be refreshed.",
    "label": "",
    "id": "1123"
  },
  {
    "raw_code": "def self.read_integer(raw_key, timeout: TIMEOUT)\n          value = read(raw_key, timeout: timeout)\n\n          value.to_i if value.present?\n        end",
    "comment": "Reads an integer from the cache, or returns nil if no value was found.  See Caching.read for more information.",
    "label": "",
    "id": "1124"
  },
  {
    "raw_code": "def self.write(raw_key, value, timeout: TIMEOUT)\n          validate_redis_value!(value)\n\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.set(key, value, ex: timeout)\n          end",
    "comment": "Sets a cache key to the given value.  raw_key - The cache key to write. value - The value to set. timeout - The time after which the cache key should expire.",
    "label": "",
    "id": "1125"
  },
  {
    "raw_code": "def self.increment(raw_key, timeout: TIMEOUT)\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            value = redis.incr(key)\n            redis.expire(key, timeout)\n\n            value\n          end",
    "comment": "Increment the integer value of a key by one. Sets the value to zero if missing before incrementing  raw_key - The cache key to increment. timeout - The time after which the cache key should expire. @return - the incremented value",
    "label": "",
    "id": "1126"
  },
  {
    "raw_code": "def self.increment_by(raw_key, value, timeout: TIMEOUT)\n          validate_redis_value!(value)\n\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.incrby(key, value)\n            redis.expire(key, timeout)\n          end",
    "comment": "Increment the integer value of a key by the given value. Sets the value to zero if missing before incrementing  raw_key - The cache key to increment. value - The value to increment the key timeout - The time after which the cache key should expire. @return - the incremented value",
    "label": "",
    "id": "1127"
  },
  {
    "raw_code": "def self.set_add(raw_key, value, timeout: TIMEOUT)\n          validate_redis_value!(value)\n\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.multi do |m|\n              m.sadd?(key, value)\n              m.expire(key, timeout)\n            end",
    "comment": "Adds a value to a set.  raw_key - The key of the set to add the value to. value - The value to add to the set. timeout - The new timeout of the key.",
    "label": "",
    "id": "1128"
  },
  {
    "raw_code": "def self.set_includes?(raw_key, value)\n          validate_redis_value!(value)\n\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.sismember(key, value || value.to_s)\n          end",
    "comment": "Returns true if the given value is present in the set.  raw_key - The key of the set to check. value - The value to check for.",
    "label": "",
    "id": "1129"
  },
  {
    "raw_code": "def self.set_count(raw_key)\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.scard(key)\n          end",
    "comment": "Returns the number of values in the set.  raw_key - The key of the set to check.",
    "label": "",
    "id": "1130"
  },
  {
    "raw_code": "def self.values_from_set(raw_key)\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.smembers(key)\n          end",
    "comment": "Returns the values of the given set.  raw_key - The key of the set to check.",
    "label": "",
    "id": "1131"
  },
  {
    "raw_code": "def self.limited_values_from_set(raw_key, limit: 1)\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.srandmember(key, limit)\n          end",
    "comment": "Returns a limited number of random values from the set.  raw_key - The key of the set to check. limit - Number of values to return (default: 1).",
    "label": "",
    "id": "1132"
  },
  {
    "raw_code": "def self.set_remove(raw_key, values = [])\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.srem(key, values)\n          end",
    "comment": "Removes the given values from the set.  raw_key - The key of the set to check. values - Array of values to remove from set.",
    "label": "",
    "id": "1133"
  },
  {
    "raw_code": "def self.write_multiple(mapping, key_prefix: nil, timeout: TIMEOUT)\n          with_redis do |redis|\n            Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n              redis.pipelined do |pipeline|\n                mapping.each do |raw_key, value|\n                  key = cache_key_for(\"#{key_prefix}#{raw_key}\")\n\n                  validate_redis_value!(value)\n\n                  pipeline.set(key, value, ex: timeout)\n                end",
    "comment": "Sets multiple keys to given values.  mapping - A Hash mapping the cache keys to their values. key_prefix - prefix inserted before each key timeout - The time after which the cache key should expire.",
    "label": "",
    "id": "1134"
  },
  {
    "raw_code": "def self.expire(raw_key, timeout)\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.expire(key, timeout)\n          end",
    "comment": "Sets the expiration time of a key.  raw_key - The key for which to change the timeout. timeout - The new timeout.",
    "label": "",
    "id": "1135"
  },
  {
    "raw_code": "def self.write_if_greater(raw_key, value, timeout: TIMEOUT)\n          validate_redis_value!(value)\n\n          key = cache_key_for(raw_key)\n          val = with_redis do |redis|\n            redis\n              .eval(WRITE_IF_GREATER_SCRIPT, keys: [key], argv: [value, timeout])\n          end",
    "comment": "Sets a key to the given integer but only if the existing value is smaller than the given value.  This method uses a Lua script to ensure the read and write are atomic.  raw_key - The key to set. value - The new value for the key. timeout - The key timeout in seconds.  Returns true when the key was overwritten, false otherwise.",
    "label": "",
    "id": "1136"
  },
  {
    "raw_code": "def self.hash_add(raw_key, field, value, timeout: TIMEOUT)\n          validate_redis_value!(value)\n\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.multi do |m|\n              m.hset(key, field, value)\n              m.expire(key, timeout)\n            end",
    "comment": "Adds a value to a hash.  raw_key - The key of the hash to add to. field - The field to add to the hash. value - The field value to add to the hash. timeout - The new timeout of the key.",
    "label": "",
    "id": "1137"
  },
  {
    "raw_code": "def self.values_from_hash(raw_key)\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.hgetall(key)\n          end",
    "comment": "Returns the values of the given hash.  raw_key - The key of the hash to check.",
    "label": "",
    "id": "1138"
  },
  {
    "raw_code": "def self.value_from_hash(raw_key, field, timeout: TIMEOUT)\n          key = cache_key_for(raw_key)\n\n          value = with_redis { |redis| redis.hget(key, field) }\n\n          with_redis { |redis| redis.expire(key, timeout) } if value.present?\n\n          value\n        end",
    "comment": "Returns a single value of the given hash.  raw_key - The key of the hash to check. field - The field to get from the hash.",
    "label": "",
    "id": "1139"
  },
  {
    "raw_code": "def self.hash_increment(raw_key, field, value, timeout: TIMEOUT)\n          return if value.to_i <= 0\n\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.multi do |m|\n              m.hincrby(key, field, value.to_i)\n              m.expire(key, timeout)\n            end",
    "comment": "Increments value of a field in a hash  raw_key - The key of the hash to add to. field - The field to increment. value - The field value to add to the hash. timeout - The new timeout of the key.",
    "label": "",
    "id": "1140"
  },
  {
    "raw_code": "def self.list_add(raw_key, value, timeout: TIMEOUT, limit: nil)\n          validate_redis_value!(value)\n\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.multi do |m|\n              m.rpush(key, value)\n              m.ltrim(key, -limit, -1) if limit\n              m.expire(key, timeout)\n            end",
    "comment": "Adds a value to a list.  raw_key - The key of the list to add to. value - The field value to add to the list. timeout - The new timeout of the key. limit - The maximum number of members in the list. Older members will be trimmed to this limit.",
    "label": "",
    "id": "1141"
  },
  {
    "raw_code": "def self.values_from_list(raw_key)\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.lrange(key, 0, -1)\n          end",
    "comment": "Returns the values of the given list.  raw_key - The key of the list.",
    "label": "",
    "id": "1142"
  },
  {
    "raw_code": "def self.del(raw_key)\n          key = cache_key_for(raw_key)\n\n          with_redis do |redis|\n            redis.del(key)\n          end",
    "comment": "Deletes a key  raw_key - Key name",
    "label": "",
    "id": "1143"
  },
  {
    "raw_code": "def hash_start\n        check_depth!\n        @depth += 1\n        @max_depth_reached = [@max_depth_reached, @depth].max\n\n        hash = {}\n        @hash_counts[hash.object_id] = 0 # rubocop:disable Lint/HashCompareByIdentity -- We want to track by object ID\n        @stack.push(hash)\n        hash\n      end",
    "comment": "Called when a hash starts",
    "label": "",
    "id": "1144"
  },
  {
    "raw_code": "def hash_end\n        @depth -= 1\n        hash = @stack.pop\n        @hash_counts.delete(hash.object_id)\n\n        @result = hash if @stack.empty?\n\n        hash\n      end",
    "comment": "Called when a hash ends",
    "label": "",
    "id": "1145"
  },
  {
    "raw_code": "def hash_key(key)\n        increment_element_count!\n\n        key\n      end",
    "comment": "Called for each key in a hash",
    "label": "",
    "id": "1146"
  },
  {
    "raw_code": "def hash_set(hash, key, value)\n        increment_element_count!\n\n        current_size = @hash_counts[hash.object_id] || 0 # rubocop:disable Lint/HashCompareByIdentity -- We want to track by object ID\n        check_hash_size!(current_size)\n\n        hash[key] = value\n        new_size = current_size + 1\n        @hash_counts[hash.object_id] = new_size # rubocop:disable Lint/HashCompareByIdentity -- We want to track by object ID\n        @max_hash_count = [@max_hash_count, new_size].max\n      end",
    "comment": "Called when a key/value pair is complete",
    "label": "",
    "id": "1147"
  },
  {
    "raw_code": "def array_start\n        check_depth!\n        @depth += 1\n        @max_depth_reached = [@max_depth_reached, @depth].max\n\n        array = []\n        @array_counts[array.object_id] = 0 # rubocop:disable Lint/HashCompareByIdentity -- We want to track by object ID\n        @stack.push(array)\n        array\n      end",
    "comment": "Called when an array starts",
    "label": "",
    "id": "1148"
  },
  {
    "raw_code": "def array_end\n        @depth -= 1\n        array = @stack.pop\n        @array_counts.delete(array.object_id)\n\n        @result = array if @stack.empty?\n\n        array\n      end",
    "comment": "Called when an array ends",
    "label": "",
    "id": "1149"
  },
  {
    "raw_code": "def add_value(value)\n        increment_element_count!\n        @result = value\n      end",
    "comment": "Called for root values (when not in a hash or array)",
    "label": "",
    "id": "1150"
  },
  {
    "raw_code": "def metadata\n        {\n          total_elements: @total_elements,\n          max_array_count: @max_array_count,\n          max_hash_count: @max_hash_count,\n          max_depth: @max_depth_reached\n        }\n      end",
    "comment": "Returns metadata about the parsed JSON structure",
    "label": "",
    "id": "1151"
  },
  {
    "raw_code": "def callback\n        method(:log_timeout_exception)\n      end",
    "comment": "returns the Proc to be used as the observer callback block",
    "label": "",
    "id": "1152"
  },
  {
    "raw_code": "def on_worker_start(&block)\n          if in_clustered_puma?\n            # Defer block execution\n            (@worker_start_hooks ||= []) << block\n          else\n            yield\n          end",
    "comment": " Hook registration methods (called from initializers) ",
    "label": "",
    "id": "1153"
  },
  {
    "raw_code": "def on_before_blackout_period(&block)\n          # Defer block execution\n          (@master_blackout_period ||= []) << block\n        end",
    "comment": "Read the config/initializers/cluster_events_before_phased_restart.rb",
    "label": "",
    "id": "1154"
  },
  {
    "raw_code": "def on_before_graceful_shutdown(&block)\n          # Defer block execution\n          (@master_graceful_shutdown ||= []) << block\n        end",
    "comment": "Read the config/initializers/cluster_events_before_phased_restart.rb",
    "label": "",
    "id": "1155"
  },
  {
    "raw_code": "def do_worker_start\n          call(:worker_start_hooks, @worker_start_hooks)\n        end",
    "comment": " Lifecycle integration methods (called from puma.rb, etc.) ",
    "label": "",
    "id": "1156"
  },
  {
    "raw_code": "def set_puma_options(options)\n          @puma_options = options\n        end",
    "comment": "Puma doesn't use singletons (which is good) but this means we need to pass through whether the puma server is running in single mode or cluster mode DEPRECATED for Puma 6 because options cannot be accessed directly until the config has been fully loaded due to https://github.com/puma/puma/pull/3297.",
    "label": "",
    "id": "1157"
  },
  {
    "raw_code": "def set_puma_worker_count(count)\n          @puma_worker_count = count\n        end",
    "comment": "Added to support Puma 7 to replace accessing Puma options directly.",
    "label": "",
    "id": "1158"
  },
  {
    "raw_code": "def stop_workers\n          if @status == :stop # rubocop:disable Gitlab/ModuleWithInstanceVariables\n            Gitlab::Cluster::LifecycleEvents.do_before_graceful_shutdown\n          end",
    "comment": "This looks at internal status of `Puma::Cluster` https://github.com/puma/puma/blob/v3.12.1/lib/puma/cluster.rb#L333",
    "label": "",
    "id": "1159"
  },
  {
    "raw_code": "def keys_for_aggregation(events:, start_date:, end_date:, used_in_aggregate_metric: false)\n        # we always keep 1 week of margin\n        # .end_of_week is necessary to make sure this works for 1 week long periods too\n        end_date = end_date.end_of_week - 1.week\n        (start_date.to_date..end_date.to_date).flat_map do |date|\n          events.map { |event| redis_key(event, date, used_in_aggregate_metric) }\n        end.uniq\n      end",
    "comment": "requires a #redis_key(event, date, used_in_aggregate_metric) method to be defined",
    "label": "",
    "id": "1160"
  },
  {
    "raw_code": "def self.key_overrides\n        strong_memoize(:key_overrides) do\n          YAML.safe_load(File.read(Gitlab::UsageDataCounters::HLLRedisCounter::KEY_OVERRIDES_PATH)).freeze\n        end",
    "comment": "We want to avoid reading the key_overrides every time a rule is used.",
    "label": "",
    "id": "1161"
  },
  {
    "raw_code": "def ==(other)\n        other.is_a?(self.class) &&\n          name == other.name &&\n          time_framed? == other.time_framed? &&\n          filter == other.filter &&\n          unique_identifier_name == other.unique_identifier_name &&\n          operator == other.operator\n      end",
    "comment": "Implementing `==` to make sure that `a == b` is true if and only if `a` and `b` have equal properties Checks equality by comparing each attribute. @param [Object] Object to be compared",
    "label": "",
    "id": "1162"
  },
  {
    "raw_code": "def hash\n        [name, time_framed?, filter, unique_identifier_name, operator].hash\n      end",
    "comment": "Calculates hash code according to all attributes. @return [Integer] Hash code Ensures 2 objects with the same attributes can't be keys in the same hash twice",
    "label": "",
    "id": "1163"
  },
  {
    "raw_code": "def monthly_time_range_db_params(column: nil)\n        { (column || DEFAULT_TIMESTAMP_COLUMN) => 30.days.ago..2.days.ago }\n      end",
    "comment": "This time range is skewed for batch counter performance. See https://gitlab.com/gitlab-org/gitlab/-/merge_requests/42972",
    "label": "",
    "id": "1164"
  },
  {
    "raw_code": "def raw_attributes\n        @attributes\n      end",
    "comment": "This method can be removed when the refactoring is complete. It is only here to limit access to @attributes in a gradual manner.",
    "label": "",
    "id": "1165"
  },
  {
    "raw_code": "def histogram(relation, column, buckets:, bucket_size: buckets.size)\n            count_grouped = relation.group(column).select(Arel.star.count.as('count_grouped'))\n            cte = Gitlab::SQL::CTE.new(:count_cte, count_grouped)\n\n            bucket_segments = bucket_size - 1\n            width_bucket = Arel::Nodes::NamedFunction\n              .new('WIDTH_BUCKET', [cte.table[:count_grouped], buckets.first, buckets.last, bucket_segments])\n              .as('buckets')\n\n            query = cte\n              .table\n              .project(width_bucket, cte.table[:count])\n              .group('buckets')\n              .order('buckets')\n              .with(cte.to_arel)\n\n            query.to_sql\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1166"
  },
  {
    "raw_code": "def raw_count_sql(relation, column, distinct = false)\n            column ||= relation.primary_key\n            node = node_to_operate(relation, column)\n\n            relation.unscope(:order).select(node.count(distinct)).to_sql\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1167"
  },
  {
    "raw_code": "def raw_sum_sql(relation, column)\n            node = node_to_operate(relation, column)\n\n            relation.unscope(:order).select(node.sum).to_sql\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1168"
  },
  {
    "raw_code": "def raw_average_sql(relation, column)\n            node = node_to_operate(relation, column)\n\n            relation.unscope(:order).select(node.average).to_sql\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1169"
  },
  {
    "raw_code": "def node_to_operate(relation, column)\n            if join_relation?(relation) && joined_column?(column)\n              table_name, column_name = column.split(\".\")\n              Arel::Table.new(table_name)[column_name]\n            else\n              relation.all.table[column]\n            end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1170"
  },
  {
    "raw_code": "def joined_column?(column)\n            column.is_a?(String) && column.include?(\".\")\n          end",
    "comment": "checks if the passed column is of format \"table.column\"",
    "label": "",
    "id": "1171"
  },
  {
    "raw_code": "def initialize(metric_definition)\n            super\n\n            raise ArgumentError, \"options events are required\" unless event_names.present?\n          end",
    "comment": "Usage example  In metric YAML defintion instrumentation_class: RedisHLLMetric events: - g_analytics_valuestream end",
    "label": "",
    "id": "1172"
  },
  {
    "raw_code": "def value\n            with_prometheus_client(verify: false, fallback: FALLBACK) do |client|\n              self.class.metric_value.call(client)\n            end",
    "comment": "Usage example  class GitalyApdexMetric < PrometheusMetric value do result = client.query('avg_over_time(gitlab_usage_ping:gitaly_apdex:ratio_avg_over_time_5m[1w])').first  break FALLBACK unless result  result['value'].last.to_f end end",
    "label": "",
    "id": "1173"
  },
  {
    "raw_code": "def self.integrations_name(options)\n            Integration.integration_name_to_type(options[:type])\n          end",
    "comment": "Usage Example  class ActiveGroupIntegrationsMetric < BaseIntegrationsMetric operation :count  relation do |database_time_constraints| Integration.active.where.not(group: nil).where(type: integrations_name(options)) end end",
    "label": "",
    "id": "1174"
  },
  {
    "raw_code": "def allowed_types\n            Integration.available_integration_names(include_dev: false, include_disabled: true)\n          end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "1175"
  },
  {
    "raw_code": "def instrumentation\n            metric_queries = IMPORTS_METRICS.map do |metric|\n              \"(#{metric.new(time_frame: time_frame).instrumentation})\"\n            end.join(' + ')\n\n            \"SELECT #{metric_queries}\"\n          end",
    "comment": "overwriting instrumentation to generate the appropriate sql query",
    "label": "",
    "id": "1176"
  },
  {
    "raw_code": "def instrumented_metrics_defintions\n          Gitlab::Usage::MetricDefinition.with_instrumentation_class\n        end",
    "comment": "Not all metrics definitions have instrumentation classes The value can be computed only for those that have it",
    "label": "",
    "id": "1177"
  },
  {
    "raw_code": "def paginate(finder)\n        return finder.execute(gitaly_pagination: false) if no_pagination?(finder)\n\n        return paginate_via_gitaly(finder) if keyset_pagination_enabled?(finder)\n        return paginate_first_page_via_gitaly(finder) if paginate_first_page?(finder)\n\n        records = ::Kaminari.paginate_array(finder.execute)\n        Gitlab::Pagination::OffsetPagination\n          .new(request_context)\n          .paginate(records)\n      end",
    "comment": "It is expected that the given finder will respond to `execute` method with `gitaly_pagination:` option and supports pagination via gitaly.",
    "label": "",
    "id": "1178"
  },
  {
    "raw_code": "def paginate_first_page_via_gitaly(finder)\n        finder.execute(gitaly_pagination: true).tap do |records|\n          total = finder.total\n          per_page = params[:per_page].presence || Kaminari.config.default_per_page\n          total_pages = (total / per_page.to_f).ceil\n          next_page = total_pages > 1 ? 2 : nil\n\n          Gitlab::Pagination::OffsetHeaderBuilder.new(\n            request_context: request_context, per_page: per_page, page: 1, next_page: next_page,\n            total: total, total_pages: total_pages\n          ).execute\n        end",
    "comment": "When first page is requested, we paginate the data via Gitaly Headers are added to immitate offset pagination, while it is the default option",
    "label": "",
    "id": "1179"
  },
  {
    "raw_code": "def each_batch(of: 1000, load_batch: true)\n          loop do\n            current_scope = scope.dup\n            relation = order.apply_cursor_conditions(current_scope, cursor, keyset_options)\n            relation = relation.reorder(order) unless @in_operator_optimization_options\n            relation = relation.limit(of)\n\n            if load_batch\n              last_record = relation.last\n              break unless last_record\n\n              yield relation\n            else\n              last_record, next_record = relation.offset(of - 1).limit(2)\n              yield relation\n\n              break unless next_record\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1180"
  },
  {
    "raw_code": "def per_page\n          return DEFAULT_PAGE_SIZE if @per_page <= 0\n\n          [@per_page, MAXIMUM_PAGE_SIZE].min\n        end",
    "comment": "Number of records to return per page",
    "label": "",
    "id": "1181"
  },
  {
    "raw_code": "def next(lower_bounds)\n          dup.tap do |next_page|\n            next_page.lower_bounds = lower_bounds&.symbolize_keys\n          end",
    "comment": "Construct a Page for the next page Uses identical order_by/per_page information for the next page",
    "label": "",
    "id": "1182"
  },
  {
    "raw_code": "def initialize(column, value)\n          @column = column\n          @value = value\n        end",
    "comment": "This class builds the WHERE conditions for the keyset pagination library. It produces WHERE conditions for one column at a time.  Requisite 1: Only the last column (columns.last) is non-nullable and distinct. Requisite 2: Only one column is distinct and non-nullable.  Scenario: We want to order by columns named X, Y and Z and build the conditions used in the WHERE clause of a pagination query using a set of cursor values. X is the column definition for a nullable column Y is the column definition for a non-nullable but not distinct column Z is the column definition for a distinct, non-nullable column used as a tie breaker.  Then the method is initially invoked with these arguments: columns = [ColumnDefinition for X, ColumnDefinition for Y, ColumnDefinition for Z] values = { X: x, Y: y, Z: z } => these represent cursor values for pagination (x could be nil since X is nullable) current_conditions is initialized to [] to store the result during the iteration calls invoked within the Order#build_where_values method.  The elements of current_conditions are instances of Arel::Nodes and - will be concatenated using OR or UNION to be used in the WHERE clause.  Example: Let's say we want to build WHERE clause conditions for ORDER BY X DESC NULLS LAST, Y ASC, Z DESC  Iteration 1: columns = [X, Y, Z] At the end, current_conditions should be: [(Z < z)]  Iteration 2: columns = [X, Y] At the end, current_conditions should be: [(Y > y) OR (Y = y AND Z < z)]  Iteration 3: columns = [X] At the end, current_conditions should be: [((X IS NOT NULL AND Y > y) OR (X IS NOT NULL AND Y = y AND Z < z)) OR ((x IS NULL) OR (X IS NULL))]  Parameters:  - columns: instance of ColumnOrderDefinition - value: cursor value for the column",
    "label": "",
    "id": "1183"
  },
  {
    "raw_code": "def not_nullable_conditions(current_conditions)\n          tie_break_conds = current_conditions.map do |conditional|\n            Arel::Nodes::And.new([column_equals_to_value, conditional])\n          end",
    "comment": "WHEN THE COLUMN IS NON-NULLABLE AND DISTINCT Per Assumption 1, only the last column can be non-nullable and distinct (column Z is non-nullable/distinct and comes last in the example). So the Order#build_where_conditions is being called for the first time with current_conditions = [].  At the end of the call, we should expect: current_conditions should be [(Z < z)]  WHEN THE COLUMN IS NON-NULLABLE BUT NOT DISTINCT Let's say Z has been processed and we are about to process the column Y next. (per requisite 1, if a non-nullable but not distinct column is being processed, at the least, the conditional for the non-nullable/distinct column exists)  At the start of the method call: current_conditions = [(Z < z)] comparison_node = (Y < y) eqaulity_node = (Y = y)  We should add a comparison node for the next column Y, (Y < y) then break a tie using the previous conditionals, (Y = y AND Z < z)  At the end of the call, we should expect: current_conditions = [(Y < y), (Y = y AND Z < z)]",
    "label": "",
    "id": "1184"
  },
  {
    "raw_code": "def build_quoted_value\n          return value if value.instance_of?(Arel::Nodes::SqlLiteral)\n\n          Arel::Nodes.build_quoted(value, column.column_expression)\n        end",
    "comment": "Turns the given value to an SQL literal by casting it to the proper format.",
    "label": "",
    "id": "1185"
  },
  {
    "raw_code": "def page\n          @page ||= Page.new(order_by: order_by, per_page: params[:per_page])\n        end",
    "comment": "extracts Paging information from request parameters",
    "label": "",
    "id": "1186"
  },
  {
    "raw_code": "def initialize(scope:, cursor: nil, per_page: 20, cursor_converter: Base64CursorConverter, direction_key: :_kd, keyset_order_options: {})\n          @keyset_scope = build_scope(scope)\n          @order = Gitlab::Pagination::Keyset::Order.extract_keyset_order_object(@keyset_scope)\n          @per_page = per_page\n          @cursor_converter = cursor_converter\n          @direction_key = direction_key\n          @has_another_page = false\n          @at_last_page = false\n          @at_first_page = false\n          @cursor_attributes = decode_cursor_attributes(cursor)\n          @keyset_order_options = keyset_order_options\n\n          set_pagination_helper_flags!\n        end",
    "comment": "scope                  - ActiveRecord::Relation object with order by clause cursor                 - Encoded cursor attributes as String. Empty value will requests the first page. per_page               - Number of items per page. cursor_converter       - Object that serializes and de-serializes the cursor attributes. Implements dump and parse methods. direction_key          - Symbol that will be the hash key of the direction within the cursor. (default: _kd => keyset direction)",
    "label": "",
    "id": "1187"
  },
  {
    "raw_code": "def keyset_relation\n          if paginate_backward?\n            reversed_order\n              .apply_cursor_conditions(keyset_scope, cursor_attributes, keyset_order_options)\n              .reorder(reversed_order)\n              .limit(per_page_plus_one)\n          else\n            order\n              .apply_cursor_conditions(keyset_scope, cursor_attributes, keyset_order_options)\n              .limit(per_page_plus_one)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- This is a reusable module. Defining these scopes on the model would encourage duplication.",
    "label": "",
    "id": "1188"
  },
  {
    "raw_code": "def has_next_page?\n          records\n\n          if at_last_page?\n            false\n          elsif paginate_forward?\n            @has_another_page\n          elsif paginate_backward?\n            true\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord This and has_previous_page? methods are direction aware. In case we paginate backwards, has_next_page? will mean that we have a previous page.",
    "label": "",
    "id": "1189"
  },
  {
    "raw_code": "def self.keyset_aware?(scope)\n          scope.order_values.first.is_a?(self) && scope.order_values.one?\n        end",
    "comment": "Tells whether the given ActiveRecord::Relation has keyset ordering",
    "label": "",
    "id": "1190"
  },
  {
    "raw_code": "def build_where_values(values)\n          return [] if values.blank?\n\n          verify_incoming_values!(values)\n\n          return use_composite_row_comparison(values) if composite_row_comparison_possible?\n\n          column_definitions\n            .map { ColumnConditionBuilder.new(_1, values[_1.attribute_name]) }\n            .reverse\n            .reduce([]) { |where_conditions, column| column.where_conditions(where_conditions) }\n        end",
    "comment": "This methods builds the conditions for the keyset pagination  Example:  |created_at|id| |----------|--| |2020-01-01| 1| |      null| 2| |      null| 3| |2020-02-01| 4|  Note: created_at is not distinct and nullable Order `ORDER BY created_at DESC, id DESC`  We get the following cursor values from the previous page: { id: 4, created_at: '2020-02-01' }  To get the next rows, we need to build the following conditions:  (created_at = '2020-02-01' AND id < 4) OR (created_at < '2020-01-01')  DESC ordering ensures that NULL values are on top so we don't need conditions for NULL values  Another cursor example: { id: 3, created_at: nil }  To get the next rows, we need to build the following conditions:  (id < 3 AND created_at IS NULL) OR (created_at IS NOT NULL)",
    "label": "",
    "id": "1191"
  },
  {
    "raw_code": "def apply_cursor_conditions(scope, values = {}, options = { use_union_optimization: false, in_operator_optimization_options: nil })\n          values ||= {}\n          transformed_values = values.with_indifferent_access\n          scope = apply_custom_projections(scope)\n\n          where_values = build_where_values(transformed_values)\n\n          if options[:use_union_optimization] && where_values.size > 1\n            build_union_query(scope, where_values).reorder(self)\n          elsif options[:in_operator_optimization_options]\n            opts = options[:in_operator_optimization_options]\n\n            Gitlab::Pagination::Keyset::InOperatorOptimization::QueryBuilder.new(\n              **{\n                scope: scope.reorder(self),\n                values: values\n              }.merge(opts)\n            ).execute\n          else\n            scope.where(build_or_query(where_values)) # rubocop: disable CodeReuse/ActiveRecord\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1192"
  },
  {
    "raw_code": "def reversed_order\n          self.class.build(column_definitions.map(&:reverse))\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1193"
  },
  {
    "raw_code": "def use_composite_row_comparison(values)\n          columns = Arel::Nodes::Grouping.new(column_definitions.map(&:column_expression))\n          values = Arel::Nodes::Grouping.new(column_definitions.map do |column_definition|\n            value = values[column_definition.attribute_name]\n            build_quoted(value, column_definition.column_expression)\n          end)\n\n          if column_definitions.first.ascending_order?\n            [columns.gt(values)]\n          else\n            [columns.lt(values)]\n          end",
    "comment": "composite row comparison works with NOT NULL columns and may use only one index scan given a proper index setup Example: (created_at, id) > ('2012-09-18 01:40:01+00', 15)",
    "label": "",
    "id": "1194"
  },
  {
    "raw_code": "def apply_custom_projections(scope)\n          additional_projections = column_definitions.select(&:add_to_projections).map do |column_definition|\n            # avoid mutating the original column_expression\n            column_definition.column_expression.dup.as(column_definition.attribute_name).to_sql\n          end",
    "comment": "Adds extra columns to the SELECT clause",
    "label": "",
    "id": "1195"
  },
  {
    "raw_code": "def initialize(columns, arel_table)\n            @columns = columns.map do |column|\n              OrderByColumnData.new(column, \"order_by_columns_#{column.attribute_name}\", arel_table)\n            end",
    "comment": "This class exposes collection methods for the order by columns  Example: by modeling the `issues.created_at ASC, issues.id ASC` ORDER BY SQL clause, this class will receive two ColumnOrderDefinition objects",
    "label": "",
    "id": "1196"
  },
  {
    "raw_code": "def initialize(column, as, arel_table)\n            super(column.attribute_name.to_s, as, arel_table)\n            @column = column\n          end",
    "comment": "column - a ColumnOrderDefinition object as - custom alias for the column arel_table - relation where the column is located",
    "label": "",
    "id": "1197"
  },
  {
    "raw_code": "def initialize(column, as, arel_table)\n            @original_column_name = column\n            @as = as.to_s\n            @arel_table = arel_table\n          end",
    "comment": "column - name of the DB column as - custom alias for the column arel_table - relation where the column is located",
    "label": "",
    "id": "1198"
  },
  {
    "raw_code": "def projection\n            arel_column.as(as)\n          end",
    "comment": "Generates: `issues.name AS my_alias`",
    "label": "",
    "id": "1199"
  },
  {
    "raw_code": "def arel_column\n            arel_table[original_column_name]\n          end",
    "comment": "Generates: issues.name`",
    "label": "",
    "id": "1200"
  },
  {
    "raw_code": "def arel_column_as\n            arel_table[as]\n          end",
    "comment": "Generates: `issues.my_alias`",
    "label": "",
    "id": "1201"
  },
  {
    "raw_code": "def array_aggregated_column\n            Arel::Nodes::NamedFunction.new('ARRAY_AGG', [column_expression]).as(array_aggregated_column_name)\n          end",
    "comment": "Generates: SELECT ARRAY_AGG(...) AS issues_name_array",
    "label": "",
    "id": "1202"
  },
  {
    "raw_code": "def initialize(scope:, array_scope:, array_mapping_scope:, finder_query: nil, values: {})\n            @scope, success = Gitlab::Pagination::Keyset::SimpleOrderBuilder.build(scope)\n\n            raise(UnsupportedScopeOrder) unless success\n\n            @order = Gitlab::Pagination::Keyset::Order.extract_keyset_order_object(scope)\n            @array_scope = array_scope\n            @array_mapping_scope = array_mapping_scope\n            @values = values\n            @model = @scope.model\n            @table_name = @model.table_name\n            @arel_table = @model.arel_table\n            @finder_strategy = finder_query.present? ? Strategies::RecordLoaderStrategy.new(finder_query, model, order_by_columns) : Strategies::OrderValuesLoaderStrategy.new(model, order_by_columns)\n          end",
    "comment": "This class optimizes slow database queries (PostgreSQL specific) where the IN SQL operator is used with sorting.  Arguments: scope - ActiveRecord::Relation supporting keyset pagination array_scope - ActiveRecord::Relation for the `IN` subselect array_mapping_scope - Lambda for connecting scope with array_scope finder_query - ActiveRecord::Relation for finding one row by the passed in cursor values values - keyset cursor values (optional)  Example ActiveRecord query: Issues in the namespace hierarchy > scope = Issue >  .where(project_id: Group.find(9970).all_projects.select(:id)) >  .order(:created_at, :id) >  .limit(20);  Optimized version:  > scope = Issue.where({}).order(:created_at, :id) # base scope > array_scope = Group.find(9970).all_projects.select(:id) > array_mapping_scope = -> (id_expression) { Issue.where(Issue.arel_table[:project_id].eq(id_expression)) }  # finding the record by id is good enough, we can ignore the created_at_expression > finder_query = -> (created_at_expression, id_expression) { Issue.where(Issue.arel_table[:id].eq(id_expression)) }  > Gitlab::Pagination::Keyset::InOperatorOptimization::QueryBuilder.new( >   scope: scope, >   array_scope: array_scope, >   array_mapping_scope: array_mapping_scope, >   finder_query: finder_query > ).execute.limit(20)",
    "label": "",
    "id": "1203"
  },
  {
    "raw_code": "def build_column_arrays_query\n            q = Arel::SelectManager.new\n              .project(array_scope_columns.array_aggregated_columns + order_by_columns.array_aggregated_columns)\n              .from(array_cte)\n              .join(Arel.sql(\"LEFT JOIN LATERAL (#{initial_keyset_query.to_sql}) #{table_name} ON TRUE\"))\n\n            order_by_columns.each { |c| q.where(c.column_expression.not_eq(nil)) unless c.column.nullable? }\n\n            q.as('array_scope_lateral_query')\n          end",
    "comment": "This query finds the first cursor values for each item in the array CTE.  array_cte:  |project_id| |----------| |         1| |         2| |         3| |         4|  For each project_id, find the first issues row by respecting the created_at, id order.  The `array_mapping_scope` parameter defines how the `array_scope` and the `scope` can be combined.  scope = Issue.where({}) # empty scope array_mapping_scope = Issue.where(project_id: X)  scope.merge(array_mapping_scope) # Issue.where(project_id: X)  X will be replaced with a value from the `array_cte` temporary table.  |created_at|id| |----------|--| |2020-01-15| 2| |2020-01-07| 3| |2020-01-07| 4| |2020-01-10| 5|",
    "label": "",
    "id": "1204"
  },
  {
    "raw_code": "def ensure_one_row(query)\n            q = Arel::SelectManager.new\n            q.projections = order_by_columns.original_column_names_as_tmp_tamble\n\n            null_values = [nil] * order_by_columns.count\n\n            from = Arel::Nodes::Grouping.new(Arel::Nodes::ValuesList.new([null_values])).as('nulls')\n\n            q.from(from)\n            q.join(Arel.sql(\"LEFT JOIN (#{query.to_sql}) record ON TRUE\"))\n            q.limit = 1\n            q\n          end",
    "comment": "NULL guard. This method ensures that NULL values are returned when the passed in scope returns 0 rows. Example query: returns issues.id or NULL  SELECT issues.id FROM (VALUES (NULL)) nulls (id) LEFT JOIN (SELECT id FROM issues WHERE id = 1 LIMIT 1) issues ON TRUE LIMIT 1",
    "label": "",
    "id": "1205"
  },
  {
    "raw_code": "def array_order_query\n            q = Arel::SelectManager.new\n              .project([*order_by_columns.original_column_names_as_arel_string, Arel.sql('position')])\n              .from(\"UNNEST(#{list(order_by_columns.array_aggregated_column_names)}) WITH ORDINALITY AS u(#{list(order_by_columns.original_column_names)}, position)\")\n\n            order_by_columns.each { |c| q.where(Arel.sql(c.original_column_name).not_eq(nil)) unless c.column.nullable? } # ignore rows where all columns are NULL\n\n            q.order(Arel.sql(order_by_without_table_references)).take(1)\n          end",
    "comment": "This subquery finds the cursor values for the next record by sorting the generated cursor arrays in memory and taking the first element. It combines the cursor arrays (UNNEST) together and sorts them according to the originally defined ORDER BY clause.  Example: issues in the group hierarchy with ORDER BY created_at, id  |project_id|  |created_at|id| # 2 arrays combined: issues_created_at_array, issues_id_array |----------|  |----------|--| |         1|  |2020-01-15| 2| |         2|  |2020-01-07| 3| |         3|  |2020-01-07| 4| |         4|  |2020-01-10| 5|  The query will return the cursor values: (2020-01-07, 3) and the array position: 1 From the position, we can tell that the record belongs to the project with id 2.",
    "label": "",
    "id": "1206"
  },
  {
    "raw_code": "def next_cursor_values_query\n            cursor_values = order_by_columns.cursor_values(RECURSIVE_CTE_NAME)\n            array_mapping_scope_columns = array_scope_columns.array_lookup_expressions_by_position(RECURSIVE_CTE_NAME)\n\n            keyset_scope = scope\n              .reselect(*order_by_columns.arel_columns)\n              .merge(array_mapping_scope.call(*array_mapping_scope_columns))\n\n            order\n              .apply_cursor_conditions(keyset_scope, cursor_values, use_union_optimization: true)\n              .reselect(*order_by_columns.map(&:column_for_projection))\n              .limit(1)\n          end",
    "comment": "This subquery finds the next cursor values after the previously determined position (from array_order_query). The current cursor values are passed in as SQL literals since the actual values are encoded into SQL arrays.  Example: issues in the group hierarchy with ORDER BY created_at, id  |project_id|  |created_at|id| # 2 arrays combined: issues_created_at_array, issues_id_array |----------|  |----------|--| |         1|  |2020-01-15| 2| |         2|  |2020-01-07| 3| |         3|  |2020-01-07| 4| |         4|  |2020-01-10| 5|  Assuming that the determined position is 1, the cursor values will be the following: - Filter: project_id = 2 - created_at = 2020-01-07 - id = 3",
    "label": "",
    "id": "1207"
  },
  {
    "raw_code": "def order_by_without_table_references\n            order.column_definitions.each_with_index.map do |column_definition, i|\n              \"#{i + 1} #{column_definition.order_direction_as_sql_string}\"\n            end.join(\", \")\n          end",
    "comment": "Generates an ORDER BY clause by using the column position index and the original order clauses. This method is used to sort the collected arrays in SQL. Example: \"issues\".created_at DESC , \"issues\".id ASC => 1 DESC, 2 ASC",
    "label": "",
    "id": "1208"
  },
  {
    "raw_code": "def to_sort\n        [queue_namespace ? 0 : 1, generated_queue_name]\n      end",
    "comment": "Put namespaced queues first",
    "label": "",
    "id": "1209"
  },
  {
    "raw_code": "def encode_with(coder)\n        coder.represent_map(nil, to_yaml)\n      end",
    "comment": "YAML representation",
    "label": "",
    "id": "1210"
  },
  {
    "raw_code": "def initialize(routing_rules)\n        @rule_evaluators = parse_routing_rules(routing_rules)\n      end",
    "comment": "call-seq: router = WorkerRouter.new([ [\"resource_boundary=cpu\", 'cpu_boundary'], [\"feature_category=pages\", nil], [\"feature_category=source_code_management\", ''], [\"*\", \"default\"] ]) router.route(ACpuBoundaryWorker) # Return \"cpu_boundary\" router.route(JustAPagesWorker)   # Return \"just_a_pages_worker\" router.route(PostReceive)        # Return \"post_receive\" router.route(RandomWorker)       # Return \"default\"  This class is responsible for routing a Sidekiq worker to a certain queue defined in the input routing rules. The input routing rules, as described above, is an order-matter array of tuples [query, queue_name].  - The query syntax follows \"worker matching query\" detailedly denoted in doc/administration/operations/extra_sidekiq_processes.md.  - The queue_name must be a valid Sidekiq queue name. If the queue name is nil, or an empty string, the worker is routed to the queue generated by the name of the worker instead.  Rules are evaluated from first to last, and as soon as we find a match for a given worker we stop processing for that worker (first match wins). If the worker doesn't match any rule, it falls back the queue name generated from the worker name  For further information, please visit: https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1016 ",
    "label": "",
    "id": "1211"
  },
  {
    "raw_code": "def execute\n          # Set source to schedule to clear any missing jobs\n          # See https://github.com/sidekiq-cron/sidekiq-cron/pull/431\n          Sidekiq::Cron::Job.load_from_hash! Gitlab::SidekiqConfig.cron_jobs, source: 'schedule'\n\n          Gitlab.ee do\n            Gitlab::Mirror.configure_cron_job!\n\n            Gitlab::Geo.configure_cron_jobs!\n          end",
    "comment": "We apply Sidekiq job configurations for example during Rails initialization. Jobs have a `status` attribute with one of following values: - `nil`: Job is enabled. - `enabled`: Job is enabled. - `disabled`: Job is disabled. Reapplying configurations with `nil` status won't update a status of `enabled` or `disabled`. After applying the defaults, jobs are disabled or setup up based on the node type (e.g., non-geo, primary geo, or secondary geo).",
    "label": "",
    "id": "1212"
  },
  {
    "raw_code": "def get_feature_category\n        Gitlab::ApplicationContext.current_context_attribute('meta.feature_category') || :not_owned\n      end",
    "comment": "All dummy workers are unowned; get the feature category from the context if available.",
    "label": "",
    "id": "1213"
  },
  {
    "raw_code": "def worker_queues(rails_path = Rails.root.to_s)\n        worker_names(worker_metadatas(rails_path))\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "1214"
  },
  {
    "raw_code": "def initialize(app, options = {})\n        @app = app\n        @paths = Array(options[:paths])\n      end",
    "comment": "Initializes the middleware.  @param app [Rack application] The Rack application. @param options [Hash] The options to customize the middleware behavior. @option options [Array<Regexp>] :paths The regular expressions to match against the path when cookies should be deleted.",
    "label": "",
    "id": "1215"
  },
  {
    "raw_code": "def validate_json_request!(env, request, limits)\n        body = request.body.read\n        request.body.rewind\n\n        return if body.empty?\n\n        if limits[:max_json_size_bytes].to_i > 0 && body.bytesize > limits[:max_json_size_bytes]\n          raise BodySizeExceededError, \"JSON body too large: #{body.bytesize} bytes\"\n        end",
    "comment": "JSON Validation using Oj streaming",
    "label": "",
    "id": "1216"
  },
  {
    "raw_code": "def decorate_params_value(hash_path, value_hash)\n          unless hash_path.is_a?(Hash) && hash_path.count == 1\n            raise \"invalid path: #{hash_path.inspect}\"\n          end",
    "comment": "This function calls itself recursively",
    "label": "",
    "id": "1217"
  },
  {
    "raw_code": "def update_param(key, value)\n          # we make sure we have key in POST otherwise update_params will add it in GET\n          @request.POST[key] ||= value\n\n          # this will force Rack::Request to properly update env keys\n          @request.update_param(key, value)\n\n          # ActionDispatch::Request is based on Rack::Request but it caches params\n          # inside other env keys, here we ensure everything is updated correctly\n          ActionDispatch::Request.new(@request.env).update_param(key, value)\n        end",
    "comment": "update_params ensures that both rails controllers and rack middleware can find workhorse accelerate files in the request",
    "label": "",
    "id": "1218"
  },
  {
    "raw_code": "def not_found_response\n        go_help_page_url = Rails.application.routes.url_helpers.help_page_url('user/project/use_project_as_go_package.md')\n        not_found_message = \"Go package not found or access denied. If you are trying to access a private project, ensure your ~/.netrc file has credentials so the go toolchain can authenticate. See #{go_help_page_url} for details.\"\n\n        [404, { 'Content-Type' => 'text/plain' }, [not_found_message]]\n      end",
    "comment": "not_found_response returns a message that the go cli toolchain displays directly.",
    "label": "",
    "id": "1219"
  },
  {
    "raw_code": "def handle_go_get_request(request)\n        path_info = request.env[\"PATH_INFO\"].delete_prefix('/')\n        project = project_for_path(path_info)\n\n        if project && can_read_project?(request, project)\n          return not_found_response unless project.repository_exists?\n\n          create_go_get_html_response(project.full_path)\n        elsif request.authorization.present?\n          not_found_response\n        else\n          path_segments = path_info.split('/')\n          return not_found_response unless path_segments.length >= 2\n\n          two_segment_path = path_segments.first(2).join('/')\n\n          create_go_get_html_response(two_segment_path)\n        end",
    "comment": "handle_go_get_request responds to `go get` requests by either returning a successful 200 response with meta tags as described in https://go.dev/ref/mod, or a 404 response with a message that the go toolchain will display.  The go toolchain authenticates using basic auth. When credentials are not present, a successful response is always returned as if a project exists (using a two-segment path like `namespace/project`) in order to prevent leaking information about the existence of private projects, and to maintain backwards compatibility for users who have not set up authentication for their go toolchain.",
    "label": "",
    "id": "1220"
  },
  {
    "raw_code": "def create_go_get_html_response(project_full_path)\n        root_path = get_root_path(project_full_path)\n        repo_url = get_repo_url(project_full_path)\n\n        go_import_meta_tag = tag.meta(name: 'go-import', content: \"#{root_path} git #{repo_url}\")\n        head_tag = content_tag :head, go_import_meta_tag\n        body_tag = content_tag :body, \"go get #{root_path}\"\n        html = content_tag :html, head_tag + body_tag\n\n        response = Rack::Response.new(html, 200, { 'Content-Type' => 'text/html' })\n        response.finish\n      end",
    "comment": "create_go_get_html_response creates a HTML document for go get with the expected meta tags.",
    "label": "",
    "id": "1221"
  },
  {
    "raw_code": "def get_root_path(project_full_path)\n        http_url(project_full_path).gsub(%r{\\Ahttps?://}, '')\n      end",
    "comment": "get_root_path returns a root path based on the instance URL that includes a relative part of URL if it was set",
    "label": "",
    "id": "1222"
  },
  {
    "raw_code": "def http_url(project_full_path)\n        Gitlab::Utils.append_path(Gitlab.config.gitlab.url, project_full_path)\n      end",
    "comment": "http_url returns a direct link to the project",
    "label": "",
    "id": "1223"
  },
  {
    "raw_code": "def project_for_path(path_info)\n        project_path_match = \"#{path_info}/\".match(PROJECT_PATH_REGEX)\n        return unless project_path_match\n\n        path = project_path_match[1]\n\n        # A go package path may use a subdirectory. For example a valid package path\n        # is `example.com/namespace/group/group/project/path1/path2/path3`.\n        # So we need to find all potential gitlab project paths from the package path.\n        # For more details on package paths see https://go.dev/ref/mod.\n\n        # Apply maximum upper limit to number of segments (group + 20 subgroups + project = 22 elements)\n        path_segments = path.split('/').take(22) # rubocop: disable CodeReuse/ActiveRecord -- not an active record operation\n\n        project_paths = []\n        begin\n          project_paths << path_segments.join('/')\n          path_segments.pop\n        end while path_segments.length >= 2\n\n        project = Project.where_full_path_in(project_paths).first\n        return project if project\n\n        # It's possible that the project was transferred and has a redirect\n        redirect = RedirectRoute.for_source_type(Project).by_paths(project_paths).first\n        return redirect.source if redirect\n\n        nil\n      end",
    "comment": "project_for_path searches for a project based on the path_info",
    "label": "",
    "id": "1224"
  },
  {
    "raw_code": "def can_read_project?(request, project)\n        return true if project.public?\n        return false unless has_basic_credentials?(request)\n\n        login, password = user_name_and_password(request)\n        auth_result = Gitlab::Auth.find_for_git_client(login, password, project: project, request: request)\n\n        auth_result.success? &&\n          auth_result.authentication_abilities_include?(:read_project) &&\n          auth_result.can_perform_action_on_project?(:read_project, project)\n      end",
    "comment": "can_read_project? checks if the request's credentials have read access to the project",
    "label": "",
    "id": "1225"
  },
  {
    "raw_code": "def read_only?\n          Gitlab::Database.read_only?\n        end",
    "comment": "Overridden in EE module",
    "label": "",
    "id": "1226"
  },
  {
    "raw_code": "def allowlisted_routes\n          workhorse_passthrough_route? || internal_route? || lfs_batch_route? ||\n            compare_git_revisions_route? || sidekiq_route? || session_route? ||\n            graphql_query?\n        end",
    "comment": "Overridden in EE module",
    "label": "",
    "id": "1227"
  },
  {
    "raw_code": "def workhorse_passthrough_route?\n          # Calling route_hash may be expensive. Only do it if we think there's a possible match\n          return false unless request.post? &&\n            request_path.end_with?('.git/git-upload-pack')\n\n          ALLOWLISTED_GIT_READ_ONLY_ROUTES[route_hash[:controller]]&.include?(\n            route_hash[:action]\n          )\n        end",
    "comment": "URL for requests passed through gitlab-workhorse to rails-web https://gitlab.com/gitlab-org/gitlab-workhorse/-/merge_requests/12",
    "label": "",
    "id": "1228"
  },
  {
    "raw_code": "def lfs_batch_route?\n          # Calling route_hash may be expensive. Only do it if we think there's a possible match\n          return unless request_path.end_with?('/info/lfs/objects/batch')\n\n          ALLOWLISTED_GIT_LFS_BATCH_ROUTES[route_hash[:controller]]&.include?(\n            route_hash[:action]\n          )\n        end",
    "comment": "Batch upload requests are blocked in: https://gitlab.com/gitlab-org/gitlab/blob/master/app/controllers/repositories/lfs_api_controller.rb#L106",
    "label": "",
    "id": "1229"
  },
  {
    "raw_code": "def handle_read_only_error\n          forbidden_read_only_response\n        end",
    "comment": "overridden in EE module",
    "label": "",
    "id": "1230"
  },
  {
    "raw_code": "def initialize(project)\n        @project = project\n\n        @already_enqueued_cache_key =\n          format(ALREADY_ENQUEUED_CACHE_KEY, project: project.id, collection: collection_method)\n        @job_waiter_cache_key =\n          format(JOB_WAITER_CACHE_KEY, project: project.id, collection: collection_method)\n        @job_waiter_remaining_cache_key = format(JOB_WAITER_REMAINING_CACHE_KEY, project: project.id,\n          collection: collection_method)\n        @page_keyset = Gitlab::Import::PageKeyset.new(project, collection_method, 'bitbucket-importer')\n      end",
    "comment": "project - An instance of `Project`.",
    "label": "",
    "id": "1231"
  },
  {
    "raw_code": "def each_object_to_import\n        repo = project.import_source\n\n        options = collection_options.merge(next_url: page_keyset.current)\n\n        client.each_page(collection_method, representation_type, repo, options) do |page|\n          page.items.each do |object|\n            job_waiter.jobs_remaining = Gitlab::Cache::Import::Caching.increment(job_waiter_remaining_cache_key)\n\n            object = object.to_hash\n\n            next if already_enqueued?(object)\n\n            yield object\n\n            # We mark the object as imported immediately so we don't end up\n            # scheduling it multiple times.\n            mark_as_enqueued(object)\n          end",
    "comment": "The method that will be called for traversing through all the objects to import, yielding them to the supplied block.",
    "label": "",
    "id": "1232"
  },
  {
    "raw_code": "def collection_options\n        {}\n      end",
    "comment": "Any options to be passed to the method used for retrieving the data to import.",
    "label": "",
    "id": "1233"
  },
  {
    "raw_code": "def id_for_already_enqueued_cache(object)\n        raise NotImplementedError\n      end",
    "comment": "Returns the ID to use for the cache used for checking if an object has already been enqueued or not.  object - The object we may want to import.",
    "label": "",
    "id": "1234"
  },
  {
    "raw_code": "def sidekiq_worker_class\n        raise NotImplementedError\n      end",
    "comment": "The Sidekiq worker class used for scheduling the importing of objects in parallel.",
    "label": "",
    "id": "1235"
  },
  {
    "raw_code": "def collection_method\n        raise NotImplementedError\n      end",
    "comment": "The name of the method to call to retrieve the data to import.",
    "label": "",
    "id": "1236"
  },
  {
    "raw_code": "def representation_type\n        raise NotImplementedError\n      end",
    "comment": "The name of the method to call to retrieve the representation object",
    "label": "",
    "id": "1237"
  },
  {
    "raw_code": "def mark_as_enqueued(object)\n        id = id_for_already_enqueued_cache(object)\n\n        Gitlab::Cache::Import::Caching.set_add(already_enqueued_cache_key, id)\n      end",
    "comment": "Marks the given object as \"already enqueued\".",
    "label": "",
    "id": "1238"
  },
  {
    "raw_code": "def concurrent_import_jobs_limit\n          100\n        end",
    "comment": "To avoid overloading Gitaly, we use a smaller limit for pull requests than the one defined in the application settings.",
    "label": "",
    "id": "1239"
  },
  {
    "raw_code": "def total_value_count_estimate(body)\n        body.count(\"{[,:\")\n      end",
    "comment": ": => Number of key-value pairs , => Number of elements in arrays (off by one since [1, 2, 3] has just 2 commas) [ => Number of arrays { => Number of objects",
    "label": "",
    "id": "1240"
  },
  {
    "raw_code": "def acceptable_error?\n          ACCEPTABLE_ERROR_CODES.include?(code)\n        end",
    "comment": "In some cases, we treat error response as acceptable:  A GPG key that wasn't issued by BeyondIdentity returns 404 status code but users should be able to add those GPG keys to their profile.",
    "label": "",
    "id": "1241"
  },
  {
    "raw_code": "def parse(project, payload, monitoring_tool: nil, integration: nil)\n          payload_class = payload_class_for(monitoring_tool: monitoring_tool || payload&.dig('monitoring_tool'))\n\n          payload_class.new(project: project, payload: payload, integration: integration)\n        end",
    "comment": "Instantiates an instance of a subclass of Gitlab::AlertManagement::Payload::Base. This can be used to create new alerts or read content from the payload of an existing AlertManagement::Alert  @param project [Project] @param payload [Hash] @param monitoring_tool [String] @param integration [AlertManagement::HttpIntegration]",
    "label": "",
    "id": "1242"
  },
  {
    "raw_code": "def self.attribute(key, paths:, type: nil, fallback: -> { nil })\n          define_method(key) do\n            strong_memoize(key) do\n              paths = Array(paths).first.is_a?(String) ? [Array(paths)] : paths\n              value = value_for_paths(paths)\n              value = parse_value(value, type) if value\n\n              value.presence || fallback.call\n            end",
    "comment": "Defines a method which allows access to a given value within an alert payload  @param key [Symbol] Name expected to be used to reference value @param paths [String, Array<String>, Array<Array<String>>,] List of (nested) keys at value can be found, the first to yield a result will be used @param type [Symbol] If value should be converted to another type, that should be specified here @param fallback [Proc] Block to be executed to yield a value if a value cannot be idenitied at any provided paths Example) attribute :title paths: [['title'], ['details', 'title']] fallback: Proc.new { 'New Alert' }  The above sample definition will define a method called #title which will return the value from the payload under the key `title` if available, otherwise looking under `details.title`. If neither returns a value, the return value will be `'New Alert'`",
    "label": "",
    "id": "1243"
  },
  {
    "raw_code": "def alert_params\n          {\n            description: truncate(description, ::AlertManagement::Alert::DESCRIPTION_MAX_LENGTH),\n            ended_at: ends_at,\n            environment: environment,\n            fingerprint: gitlab_fingerprint,\n            hosts: truncate_hosts(Array(hosts).flatten),\n            monitoring_tool: truncate(monitoring_tool, ::AlertManagement::Alert::TOOL_MAX_LENGTH),\n            payload: payload,\n            project_id: project.id,\n            service: truncate(service, ::AlertManagement::Alert::SERVICE_MAX_LENGTH),\n            severity: severity,\n            started_at: starts_at,\n            title: truncate(title, ::AlertManagement::Alert::TITLE_MAX_LENGTH)\n          }.transform_values(&:presence).compact\n        end",
    "comment": "Attributes of an AlertManagement::Alert as read directly from a payload. Prefer accessing AlertManagement::Alert directly for read operations.",
    "label": "",
    "id": "1244"
  },
  {
    "raw_code": "def full_query\n          return unless generator_url\n\n          uri = URI(generator_url)\n\n          Rack::Utils.parse_query(uri.query).fetch('g0.expr')\n        rescue URI::InvalidURIError, KeyError\n        end",
    "comment": "Parses `g0.expr` from `generatorURL`.  Example: http://localhost:9090/graph?g0.expr=vector%281%29&g0.tab=1",
    "label": "",
    "id": "1245"
  },
  {
    "raw_code": "def find_thread_unsafe(jid)\n        @jobs.dig(jid, :thread)\n      end",
    "comment": "This method needs to be thread-safe This is why it passes thread in block, to ensure that we do process this thread",
    "label": "",
    "id": "1246"
  },
  {
    "raw_code": "def not_created_instances\n        pending_instances_count = wanted_instances - filtered_pods_by_track.count\n\n        return [] if pending_instances_count <= 0\n\n        Array.new(pending_instances_count, deployment_instance(pod_name: 'Not provided', pod_status: 'Pending'))\n      end",
    "comment": "These are replicas that did not get created yet, So they still do not have any associated pod, these are marked as pending instances.",
    "label": "",
    "id": "1247"
  },
  {
    "raw_code": "def initialize(cluster, project:)\n        @cluster = cluster\n        @project = project\n      end",
    "comment": " Ideally we would just use an environment record here instead of passing a project and name/slug separately, but we need to be able to look up namespaces before the environment has been persisted.",
    "label": "",
    "id": "1248"
  },
  {
    "raw_code": "def generate_slug(name)\n        Gitlab::Slug::Environment.new(name).generate\n      end",
    "comment": " Environment slug can be predicted given an environment name, so even if the environment isn't persisted yet we still know what to look for.",
    "label": "",
    "id": "1249"
  },
  {
    "raw_code": "def self.kubeclient_error_status(message)\n        if message&.match?(/timed out|timeout/i)\n          :unreachable\n        else\n          :authentication_failure\n        end",
    "comment": "KubeClient uses the same error class For connection errors (eg. timeout) and for Kubernetes errors.",
    "label": "",
    "id": "1250"
  },
  {
    "raw_code": "def initialize(api_prefix, **kubeclient_options)\n        @api_prefix = api_prefix\n        @kubeclient_options = DEFAULT_KUBECLIENT_OPTIONS\n          .deep_merge(kubeclient_options)\n          .merge(http_max_redirects: 0)\n\n        validate_url!\n      end",
    "comment": "We disable redirects through 'http_max_redirects: 0', so that KubeClient does not follow redirects and expose internal services.",
    "label": "",
    "id": "1251"
  },
  {
    "raw_code": "def create_or_update_role_binding(resource)\n        update_role_binding(resource)\n      end",
    "comment": "Note that we cannot update roleRef as that is immutable",
    "label": "",
    "id": "1252"
  },
  {
    "raw_code": "def service_acount_token_type\n        'kubernetes.io/service-account-token'\n      end",
    "comment": "as per https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#to-create-additional-api-tokens",
    "label": "",
    "id": "1253"
  },
  {
    "raw_code": "def validations_for_path\n        []\n      end",
    "comment": "Method overwritten in EE to inject custom validations",
    "label": "",
    "id": "1254"
  },
  {
    "raw_code": "def lfs_file_locks_validation\n        ->(paths) do\n          lfs_lock = project.lfs_file_locks.where(path: paths).where.not(user_id: user_access.user.id).take\n\n          if lfs_lock\n            return format(_(\"'%{lock_path}' is locked in Git LFS by @%{lock_user_name}\"),\n              lock_path: lfs_lock.path, lock_user_name: lfs_lock.user.username)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1255"
  },
  {
    "raw_code": "def commits\n        strong_memoize(:commits) do\n          newrevs = @changes.filter_map do |change|\n            newrev = change[:newrev]\n\n            next if blank_rev?(newrev)\n\n            newrev\n          end",
    "comment": "All commits which have been newly introduced via any of the given changes. This set may also contain commits which are not referenced by any of the new revisions.",
    "label": "",
    "id": "1256"
  },
  {
    "raw_code": "def commits_for(oldrev, newrev)\n        commits_by_id = commits.index_by(&:id)\n\n        result = []\n        pending = Set[newrev]\n\n        # We go up the parent chain of our newrev and collect all commits which\n        # are new. In case a commit's ID cannot be found in the set of new\n        # commits, then it must already be a preexisting commit.\n        while pending.any?\n          rev = pending.first\n          pending.delete(rev)\n\n          # Remove the revision from commit candidates such that we don't walk\n          # it multiple times. If the hash doesn't contain the revision, then\n          # we have either already walked the commit or it's not new.\n          commit = commits_by_id.delete(rev)\n          next if commit.nil?\n\n          # Only add the parent ID to the pending set if we actually know its\n          # commit to guards us against readding an ID which we have already\n          # queued up before. Furthermore, we stop walking as soon as we hit\n          # `oldrev` such that we do not include any commits in our checks\n          # which have been \"over-pushed\" by the client.\n          commit.parent_ids.each do |parent_id|\n            pending.add(parent_id) if commits_by_id.has_key?(parent_id) && parent_id != oldrev\n          end",
    "comment": "All commits which have been newly introduced via the given revision.",
    "label": "",
    "id": "1257"
  },
  {
    "raw_code": "def commitish_ref?(ref)\n        Gitlab::Git.branch_ref?(ref) || Gitlab::Git.tag_ref?(ref)\n      end",
    "comment": "refs/notes/commits contains commits added via `git-notes`. We currently have no features that check notes so we can skip them. To future-proof we are skipping anything that isn't a branch or tag ref as those are the only refs that can contain commits.",
    "label": "",
    "id": "1258"
  },
  {
    "raw_code": "def log_timed(log_message, start = Time.now)\n        check_timeout_reached\n\n        timed = true\n\n        yield\n\n        append_message(log_message + time_suffix_message(start: start))\n      rescue GRPC::DeadlineExceeded, TimeoutError\n        args = { cancelled: true }\n        args[:start] = start if timed\n\n        append_message(log_message + time_suffix_message(**args))\n\n        raise TimeoutError\n      end",
    "comment": "Adds trace of method being tracked with the correspondent time it took to run it. We make use of the start default argument on unit tests related to this method ",
    "label": "",
    "id": "1259"
  },
  {
    "raw_code": "def append_message(message)\n        log << message\n      end",
    "comment": "We always want to append in-place on the log",
    "label": "",
    "id": "1260"
  },
  {
    "raw_code": "def self.legacy_message_key(user, repository)\n        return unless repository.project\n\n        \"#{REDIRECT_NAMESPACE}:#{user.id}:#{repository.project.id}\"\n      end",
    "comment": "TODO: Remove in the next release https://gitlab.com/gitlab-org/gitlab/-/issues/292030",
    "label": "",
    "id": "1261"
  },
  {
    "raw_code": "def signed_by_gitlab?(commit)\n        return false unless updated_from_web? && commit.has_signature?\n\n        commit_signatures[commit.id][:signer] == :SIGNER_SYSTEM\n      end",
    "comment": "If a commit is created from Web and signed by GitLab, we can skip the committer check because it's equal to GitLab <noreply@gitlab.com>",
    "label": "",
    "id": "1262"
  },
  {
    "raw_code": "def creation?\n        super && @root_ref && (@branch_name != @default_branch)\n      end",
    "comment": "If the `root_ref` is not present means that this is the first commit to the repository and when the default branch is going to be created. We allow the first branch creation no matter the name because it can be even an imported snippet from an instance with a different default branch.",
    "label": "",
    "id": "1263"
  },
  {
    "raw_code": "def self.legacy_message_key(user, repository)\n        return unless repository.project\n\n        \"#{PROJECT_CREATED}:#{user.id}:#{repository.project.id}\"\n      end",
    "comment": "TODO: Remove in the next release https://gitlab.com/gitlab-org/gitlab/-/issues/292030",
    "label": "",
    "id": "1264"
  },
  {
    "raw_code": "def file_size_limit\n        nil\n      end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- This is fully overriden in EE,Lint/MissingCopEnableDirective :nocov:",
    "label": "",
    "id": "1265"
  },
  {
    "raw_code": "def ensure_hash(ambiguous_param)\n        case ambiguous_param\n        when String\n          if ambiguous_param.present?\n            ensure_hash(Gitlab::Json.parse(ambiguous_param))\n          else\n            {}\n          end",
    "comment": "Handle form data, JSON body, or a blank value",
    "label": "",
    "id": "1266"
  },
  {
    "raw_code": "def self.force(value)\n        case value\n        when ::Gitlab::Graphql::Lazy\n          value.force\n        when ::BatchLoader::GraphQL\n          value.sync\n        when ::Gitlab::Graphql::Deferred\n          value.execute\n        when ::GraphQL::Execution::Lazy\n          value.value # part of the private api, but we can force this as well\n        when ::Concurrent::Promise\n          value.execute if value.state == :unscheduled\n\n          value.value # value.value(10.seconds)\n        else\n          value\n        end",
    "comment": "Force evaluation of a (possibly) lazy value",
    "label": "",
    "id": "1267"
  },
  {
    "raw_code": "def apply_to_graphql_name(graphql_name)\n          return graphql_name unless deprecation = self::OLD_GRAPHQL_NAME_MAP[graphql_name]\n\n          self.map_graphql_name(deprecation.new_name)\n        end",
    "comment": "Returns the new `graphql_name` (Type#graphql_name) of a deprecated GID, or the `graphql_name` argument given if no deprecation applies.",
    "label": "",
    "id": "1268"
  },
  {
    "raw_code": "def from_query(query)\n        operation_name = query.selected_operation_name\n\n        return UNKNOWN unless operation_name\n\n        @operation_hash[operation_name] || UNKNOWN\n      end",
    "comment": "Returns the known operation from the given ::GraphQL::Query object",
    "label": "",
    "id": "1269"
  },
  {
    "raw_code": "def init_gitlab_deprecation(kwargs)\n        if kwargs[:deprecation_reason].present?\n          raise ArgumentError, <<~ERROR\n            Use `deprecated` property instead of `deprecation_reason`. See\n            #{Rails.application.routes.url_helpers.help_page_url('development/api_graphql_styleguide.md', anchor: 'deprecating-schema-items')}\n          ERROR\n        end",
    "comment": "Set deprecation, mutate the arguments",
    "label": "",
    "id": "1270"
  },
  {
    "raw_code": "def copy_field_description(type, field_name)\n          type.fields[field_name.to_s.camelize(:lower)].description\n        end",
    "comment": "Returns the `description` for property of field `field_name` on type. This can be used to ensure, for example, that mutation argument descriptions are always identical to the corresponding query field descriptions.  E.g.: argument :name, GraphQL::Types::String, description: copy_field_description(Types::UserType, :name)",
    "label": "",
    "id": "1271"
  },
  {
    "raw_code": "def has_previous_page\n          if after\n            # If `after` is specified, that points to a specific record,\n            # even if it's the first one.  Since we're asking for `after`,\n            # then the specific record we're pointing to is in the\n            # previous page\n            true\n          elsif last\n            limited_nodes\n            !!@has_previous_page_cache\n          else\n            # Key thing to remember.  When `before` is specified (and no `last`),\n            # the spec says return _all_ edges minus anything after the `before`.\n            # Which means the returned list starts at the very first record.\n            # Then the max_page kicks in, and returns the first max_page items.\n            # Because of this, `has_previous_page` will be false\n            false\n          end",
    "comment": "rubocop: disable Naming/PredicateName -- methods required by PageInfo https://relay.dev/graphql/connections.htm#sec-undefined.PageInfo.Fields",
    "label": "",
    "id": "1272"
  },
  {
    "raw_code": "def nodes\n          limited_nodes.take(limit_value)\n        end",
    "comment": "rubocop: enable Naming/PredicateName",
    "label": "",
    "id": "1273"
  },
  {
    "raw_code": "def dup\n          self.class.new(\n            items.dup,\n            context: context,\n            first: first,\n            after: after,\n            max_page_size: max_page_size,\n            last: last,\n            before: before\n          )\n        end",
    "comment": "Part of the implied interface for default objects for BatchLoader: objects must be clonable",
    "label": "",
    "id": "1274"
  },
  {
    "raw_code": "def cursor_for(item)\n          encode_cursor(item)\n        end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord -- requires AR methods to build pagination conditions",
    "label": "",
    "id": "1275"
  },
  {
    "raw_code": "def build_cursor_conditions(cursor_data, direction:)\n          sort_field, sort_order = extract_sort_info.values_at(:field, :order)\n          return [] if sort_field != cursor_data['sort_field']\n\n          sort_value = cursor_data['sort_value']\n\n          sort_attr = Arel::Nodes::SqlLiteral.new(sort_field)\n\n          # Start with the primary sort condition\n          or_conditions = [build_primary_sort_condition(sort_attr, sort_value, sort_order, direction)]\n\n          # Add tie-breaking conditions for group fields\n          group_field_conditions = build_group_field_conditions(\n            cursor_data, direction, sort_attr, sort_value, sort_field\n          )\n\n          or_conditions.concat(group_field_conditions)\n\n          # Build nested OR conditions for stable ordering\n          or_conditions.reduce do |accumulated, condition|\n            if Gitlab.next_rails?\n              Arel::Nodes::Or.new([accumulated, condition])\n            else\n              Arel::Nodes::Or.new(accumulated, condition)\n            end",
    "comment": "currently this method supports only one order",
    "label": "",
    "id": "1276"
  },
  {
    "raw_code": "def has_previous_page\n            strong_memoize(:has_previous_page) do\n              if after\n                # If `after` is specified, that points to a specific record,\n                # even if it's the first one.  Since we're asking for `after`,\n                # then the specific record we're pointing to is in the\n                # previous page\n                true\n              elsif last\n                limited_nodes\n                !!@has_previous_page\n              else\n                # Key thing to remember.  When `before` is specified (and no `last`),\n                # the spec says return _all_ edges minus anything after the `before`.\n                # Which means the returned list starts at the very first record.\n                # Then the max_page kicks in, and returns the first max_page items.\n                # Because of this, `has_previous_page` will be false\n                false\n              end",
    "comment": "rubocop: disable Naming/PredicateName https://relay.dev/graphql/connections.htm#sec-undefined.PageInfo.Fields",
    "label": "",
    "id": "1277"
  },
  {
    "raw_code": "def cursor_for(node)\n            order = Gitlab::Pagination::Keyset::Order.extract_keyset_order_object(items)\n            encode(order.cursor_attributes_for_node(node).to_json)\n          end",
    "comment": "rubocop: enable Naming/PredicateName",
    "label": "",
    "id": "1278"
  },
  {
    "raw_code": "def limited_nodes\n            strong_memoize(:limited_nodes) do\n              if first && last\n                raise Gitlab::Graphql::Errors::ArgumentError, \"Can only provide either `first` or `last`, not both\"\n              end",
    "comment": "Apply `first` and `last` to `sliced_nodes`",
    "label": "",
    "id": "1279"
  },
  {
    "raw_code": "def changed_in_milestone(format: :plain)\n          verb = if experiment?\n                   'Introduced'\n                 else\n                   'Deprecated'\n                 end",
    "comment": "Returns 'Deprecated in GitLab <milestone>' for proper deprecations. Returns 'Introduced in GitLab <milestone>' for :experiment deprecations. Formatted to markdown or plain format.",
    "label": "",
    "id": "1280"
  },
  {
    "raw_code": "def value_with_count(value, (previous_count, previous_accounted_for))\n          newly_accounted_for = accounted_for - previous_accounted_for\n          value = force(value)\n          count = [current_gitaly_call_count - (previous_count + newly_accounted_for), 0].max\n\n          [value, count]\n        end",
    "comment": "Resolutions are not nested nicely (due to laziness), so we have to know not just how many calls were made before resolution started, but also how many were accounted for by fields with the correct settings in between.  e.g. the following is not just plausible, but common:  enter A.user (lazy) enter A.x leave A.x enter A.calls_gitaly leave A.calls_gitaly (accounts for 1 call) leave A.user  In this circumstance we need to mark the calls made by A.calls_gitaly as accounted for, even though they were made after we yielded in A.user",
    "label": "",
    "id": "1281"
  },
  {
    "raw_code": "def current_wal_locations\n          wal_locations_by_db_name&.stringify_keys\n        end",
    "comment": "We stringify keys since otherwise the graphql-ruby serializer will inject additional metadata to keep track of which keys used to be symbols.",
    "label": "",
    "id": "1282"
  },
  {
    "raw_code": "def execute_multiplex(multiplex:)\n          start_time = ::Gitlab::Metrics::System.monotonic_time\n          super\n        rescue StandardError => e\n          raise\n        ensure\n          duration_s = ::Gitlab::Metrics::System.monotonic_time - start_time\n\n          multiplex.queries.each do |query|\n            export_query_info(query: query, duration_s: duration_s, exception: e)\n          end",
    "comment": "All queries pass through a multiplex, even if only one query is executed https://github.com/rmosolgo/graphql-ruby/blob/43e377b5b743a9102381d6ad3adaaed13ff5b6dd/lib/graphql/schema.rb#L1303  Instrumenting the multiplex ensures that all queries have been fully exectued meaning that `query.result` is present. We need `query.result` to know if the query was successful or not in metrics and to include the errors in the logs.",
    "label": "",
    "id": "1283"
  },
  {
    "raw_code": "def find\n          BatchLoader::GraphQL.for([model_id.to_i, preloads]).batch(key: model_class) do |for_params, loader, args|\n            model = args[:key]\n            keys_by_id = for_params.group_by(&:first)\n            ids = for_params.map(&:first)\n            preloads = for_params.flat_map(&:second).uniq\n            results = model.where(id: ids)\n            results = results.preload(*preloads) unless preloads.empty?\n            results = results.index_by(&:id)\n\n            keys_by_id.each do |id, keys|\n              keys.each { |k| loader.call(k, results[id] || default_value) }\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1284"
  },
  {
    "raw_code": "def inherited(klass)\n            GitlabSchema.lazy_resolve(klass, :load)\n          end",
    "comment": "Automatically register the inheriting classes to GitlabSchema as lazy objects.",
    "label": "",
    "id": "1285"
  },
  {
    "raw_code": "def load\n          case reflection.macro\n          when :has_many\n            relation_proxy\n          when :has_one\n            relation_proxy.last\n          else\n            raise 'Not supported association type!'\n          end",
    "comment": "Returns an instance of `RelationProxy` for the object (parent model). The returned object behaves like an Active Record relation to support keyset pagination.",
    "label": "",
    "id": "1286"
  },
  {
    "raw_code": "def relation(**)\n          base_relation\n        end",
    "comment": "Implement this one if you want to filter the relation",
    "label": "",
    "id": "1287"
  },
  {
    "raw_code": "def placeholder_record\n          model.new(reflection.active_record_primary_key => 0)\n        end",
    "comment": "This will only work for HasMany and HasOne associations for now",
    "label": "",
    "id": "1288"
  },
  {
    "raw_code": "def lateral_relation\n            original_relation\n              .unscope(where: foreign_key) # unscoping the where condition generated for the placeholder_record.\n              .where(klass.arel_table[foreign_key].eq(active_record.arel_table[active_record_primary_key]))\n          end",
    "comment": "This only works for HasMany and HasOne.",
    "label": "",
    "id": "1289"
  },
  {
    "raw_code": "def method_missing(method_name, ...)\n            result = registry.public_send(method_name, ...) # rubocop:disable GitlabSecurity/PublicSend\n\n            return self if result == registry\n\n            result\n          end",
    "comment": "Delegate everything to registry",
    "label": "",
    "id": "1290"
  },
  {
    "raw_code": "def on_leave_field(node, _parent, visitor)\n            return if skip_node?(node, visitor)\n\n            node_name = node.name\n            node_visits[node_name] ||= 0\n            node_visits[node_name] -= 1\n          end",
    "comment": "Visitors are all defined on the AST::Analyzer base class We override them for custom analyzers.",
    "label": "",
    "id": "1291"
  },
  {
    "raw_code": "def recursion_threshold\n            RECURSION_THRESHOLD\n          end",
    "comment": "separated into a method for use in allow_high_graphql_recursion",
    "label": "",
    "id": "1292"
  },
  {
    "raw_code": "def self.default_threshold\n        100\n      end",
    "comment": "The maximum number of SQL queries that can be executed in a request. For the sake of keeping things simple we hardcode this value here, it's not supposed to be changed very often anyway.",
    "label": "",
    "id": "1293"
  },
  {
    "raw_code": "def self.threshold\n        default_threshold\n      end",
    "comment": "Deprecated, use default_threshold",
    "label": "",
    "id": "1294"
  },
  {
    "raw_code": "def self.run\n        previous_transaction = current\n\n        transaction = new\n        Thread.current[THREAD_KEY] = transaction\n\n        [transaction, yield]\n      ensure\n        Thread.current[THREAD_KEY] = previous_transaction\n      end",
    "comment": "Starts a new transaction and returns it and the blocks' return value.  Example:  transaction, retval = Transaction.run do 10 end  retval # => 10",
    "label": "",
    "id": "1295"
  },
  {
    "raw_code": "def act_upon_results\n        return unless threshold_exceeded?\n\n        error = ThresholdExceededError.new(error_message)\n\n        raise(error) if raise_error?\n      end",
    "comment": "Sends a notification based on the number of executed SQL queries.",
    "label": "",
    "id": "1296"
  },
  {
    "raw_code": "def ignorable?(sql)\n        return true if sql&.include?(GEO_NODES_LOAD)\n        return true if sql&.include?(LICENSES_LOAD)\n        return true if SCHEMA_INTROSPECTION.match?(sql)\n        return true if SAVEPOINT.match?(sql)\n        return true if SET.match?(sql)\n        return true if SHOW.match?(sql)\n\n        false\n      end",
    "comment": "queries can be safely ignored if they are amoritized in regular usage (i.e. only requested occasionally and otherwise cached).",
    "label": "",
    "id": "1297"
  },
  {
    "raw_code": "def link_method_call(method_name, value = nil, &url_proc)\n        regex = method_call_regex(method_name, value)\n\n        link_regex(regex, &url_proc)\n      end",
    "comment": "Links package names in a method call or assignment string argument.  Example: link_method_call('gem') # Will link `package` in `gem \"package\"`, `gem(\"package\")` and `gem = \"package\"`  link_method_call('gem', 'specific_package') # Will link `specific_package` in `gem \"specific_package\"`  link_method_call('github', /[^\\/\"]+\\/[^\\/\"]+/) # Will link `user/repo` in `github \"user/repo\"`, but not `github \"package\"`  link_method_call(%w[add_dependency add_development_dependency]) # Will link `spec.add_dependency \"package\"` and `spec.add_development_dependency \"package\"`  link_method_call('name') # Will link `package` in `self.name = \"package\"`",
    "label": "",
    "id": "1298"
  },
  {
    "raw_code": "def link_regex(regex, &url_proc)\n        highlighted_lines.map!.with_index do |rich_line, i|\n          marker = StringRegexMarker.new((plain_lines[i].chomp! || plain_lines[i]), rich_line.html_safe)\n\n          marker.mark(regex, group: :name) do |text, left:, right:, mode:|\n            url = yield(text)\n            url ? link_tag(text, url) : text\n          end",
    "comment": "Links package names based on regex.  Example: link_regex(/(github:|:github =>)\\s*['\"](?<name>[^'\"]+)['\"]/) # Will link `user/repo` in `github: \"user/repo\"` or `:github => \"user/repo\"`",
    "label": "",
    "id": "1299"
  },
  {
    "raw_code": "def link_dependencies\n        highlighted_lines.map!.with_index do |rich_line, i|\n          plain_line = plain_lines[i].chomp\n          match = REGEX.match(plain_line)\n          next rich_line unless match\n\n          i, j = match.offset(:name)\n          marker = StringRangeMarker.new(plain_line, rich_line.html_safe)\n          marker.mark([i..(j - 1)]) do |text, left:, right:, mode:|\n            url = package_url(text, match[:version])\n            url ? link_tag(text, url) : text\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1300"
  },
  {
    "raw_code": "def link_dependencies\n        highlighted_lines.map!.with_index do |rich_line, i|\n          plain_line = plain_lines[i].chomp\n          match = REGEX.match(plain_line)\n          next rich_line unless match\n\n          i0, j0 = match.offset(:name)\n          i2, j2 = match.offset(:checksum)\n\n          marker = StringRangeMarker.new(plain_line, rich_line.html_safe)\n          marker.mark([i0..(j0 - 1), i2..(j2 - 1)]) do |text, left:, right:, mode:|\n            if left\n              url = package_url(text, match[:version])\n              url ? link_tag(text, url) : text\n\n            elsif right\n              link_tag(text, \"https://sum.golang.org/lookup/#{match[:name]}@#{match[:version]}\")\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1301"
  },
  {
    "raw_code": "def link_json(key, value = nil, link: :value, &url_proc)\n        key = regexp_for_value(key, default: /[^\" ]+/)\n        value = regexp_for_value(value, default: /[^\" ]+/)\n\n        if link == :value\n          value = /(?<name>#{value})/\n        else\n          key = /(?<name>#{key})/\n        end",
    "comment": "Links package names in a JSON key or values.  Example: link_json('name') # Will link `package` in `\"name\": \"package\"`  link_json('name', 'specific_package') # Will link `specific_package` in `\"name\": \"specific_package\"`  link_json('name', /[^\\/]+\\/[^\\/]+/) # Will link `user/repo` in `\"name\": \"user/repo\"`, but not `\"name\": \"package\"`  link_json('specific_package', '1.0.1', link: :key) # Will link `specific_package` in `\"specific_package\": \"1.0.1\"`",
    "label": "",
    "id": "1302"
  },
  {
    "raw_code": "def parse(keyword:)\n          plain_lines.each_with_object([]) do |line, packages|\n            name = fetch(line, method_call_regex(keyword))\n\n            next unless name\n\n            git_ref = fetch(line, GIT_REGEX)\n            github_ref = fetch(line, GITHUB_REGEX)\n\n            packages << Gitlab::DependencyLinker::Package.new(name, git_ref, github_ref)\n          end",
    "comment": "Returns a list of Gitlab::DependencyLinker::Package  keyword - The package definition keyword, e.g. `:gem` for Gemfile parsing, `:pod` for Podfile.",
    "label": "",
    "id": "1303"
  },
  {
    "raw_code": "def binary_path\n        @binary_path ||= content_match ? search_result_path : parsed_content[:binary_path]\n      end",
    "comment": "binary_path is used for running filters on all matches. For grepped results (which use content_match), we get the path from the beginning of the grepped result which is faster than parsing the whole snippet",
    "label": "",
    "id": "1304"
  },
  {
    "raw_code": "def initialize(found_blob)\n        super\n\n        @wiki ||= found_blob.project.wiki\n      end",
    "comment": "@param found_blob [Gitlab::Search::FoundBlob]",
    "label": "",
    "id": "1305"
  },
  {
    "raw_code": "def search(term)\n        ids = latest_ids\n        query_items_by_ids(term, ids)\n      end",
    "comment": "Searches for recently viewed items matching the given term  @param term [String, nil] Search term to filter items by title, or nil for no filtering @return [Array<ActiveRecord::Base>] Array of matching items ordered by recency",
    "label": "",
    "id": "1306"
  },
  {
    "raw_code": "def latest_with_timestamps\n        timestamps = latest_ids_with_timestamps\n        ids = timestamps.keys.map(&:to_i)\n        items = query_items_by_ids(nil, ids)\n\n        items.index_with { |item| Time.zone.at(timestamps[item.id.to_s]) }\n      end",
    "comment": "Returns the most recently viewed items with their view timestamps  @return [Hash<ActiveRecord::Base, Time>] Hash where keys are recently viewed ActiveRecord objects and values are the timestamp of the last visit",
    "label": "",
    "id": "1307"
  },
  {
    "raw_code": "def self.data\n        {\n          sast: {\n            name: _('Static Application Security Testing (SAST)'),\n            short_name: _('SAST'),\n            description: _('Analyze your source code for vulnerabilities.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path('user/application_security/sast/_index.md'),\n            configuration_help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/sast/_index.md', anchor: 'configuration'),\n            type: 'sast'\n          },\n          sast_advanced: {\n            name: _('GitLab Advanced SAST'),\n            short_name: _('Advanced SAST'),\n            description: _('Analyze your source code for vulnerabilities with the GitLab Advanced SAST analyzer.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/sast/gitlab_advanced_sast.md'),\n            configuration_help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/sast/gitlab_advanced_sast.md',\n              anchor: 'configuration'),\n            type: 'sast_advanced'\n          },\n          sast_iac: {\n            name: _('Infrastructure as Code (IaC) Scanning'),\n            short_name: s_('ciReport|SAST IaC'),\n            description: _('Analyze your infrastructure as code configuration files for known vulnerabilities.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path('user/application_security/iac_scanning/_index.md'),\n            configuration_help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/iac_scanning/_index.md',\n              anchor: 'configuration'),\n            type: 'sast_iac'\n          },\n          dast: {\n            badge: {\n              text: _('Available on demand'),\n              tooltip_text: _(\n                'On-demand scans run outside of the DevOps cycle and find vulnerabilities in your projects'),\n              variant: 'neutral'\n            },\n            secondary: {\n              type: 'dast_profiles',\n              name: _('DAST profiles'),\n              description: s_('SecurityConfiguration|Manage profiles for use by DAST scans.'),\n              configuration_text: s_('SecurityConfiguration|Manage profiles')\n            },\n            name: _('Dynamic Application Security Testing (DAST)'),\n            short_name: s_('ciReport|DAST'),\n            description: s_('ciReport|Analyze a deployed version of your web application for known ' \\\n                            'vulnerabilities by examining it from the outside in. DAST works ' \\\n                            'by simulating external attacks on your application while it is running.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path('user/application_security/dast/_index.md'),\n            configuration_help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/dast/_index.md', anchor: 'enable-automatic-dast-run'),\n            type: 'dast',\n            anchor: 'dast'\n          },\n          dependency_scanning: {\n            name: _('Dependency Scanning'),\n            description: _('Analyze your dependencies for known vulnerabilities.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/dependency_scanning/_index.md'),\n            configuration_help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/dependency_scanning/_index.md', anchor: 'configuration'),\n            type: 'dependency_scanning',\n            anchor: 'dependency-scanning'\n          },\n          container_scanning: {\n            name: _('Container Scanning'),\n            description: _('Check your Docker images for known vulnerabilities.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/container_scanning/_index.md'),\n            configuration_help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/container_scanning/_index.md', anchor: 'configuration'),\n            type: 'container_scanning'\n          },\n          container_scanning_for_registry: {\n            name: _('Container Scanning For Registry'),\n            description: _('Run container scanning job whenever a container image with the latest tag is pushed.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/container_scanning/_index.md', anchor: 'container-scanning-for-registry'),\n            type: 'container_scanning_for_registry'\n          },\n          license_information_source: {\n            name: _('License information source'),\n            description: _('Define the preferred source for license information.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/compliance/license_scanning_of_cyclonedx_files/_index.md',\n              anchor: 'use-cyclonedx-report-as-a-source-of-license-information'),\n            type: 'license_information_source'\n          },\n          secret_push_protection: {\n            name: _('Secret push protection'),\n            description: _('Block secrets such as keys and API tokens from being pushed to your repositories. ' \\\n                           'Secret push protection is triggered when commits are pushed to a repository. ' \\\n                           'If any secrets are detected, the push is blocked.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/secret_detection/secret_push_protection/_index.md'),\n            type: 'secret_push_protection'\n          },\n          secret_detection: {\n            name: _('Pipeline Secret Detection'),\n            description: _('Analyze your source code and Git history for secrets by using CI/CD pipelines.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/secret_detection/pipeline/_index.md'),\n            configuration_help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/secret_detection/pipeline/_index.md', anchor: 'configuration'),\n            type: 'secret_detection'\n          },\n          api_fuzzing: {\n            name: _('API Fuzzing'),\n            description: _('Find bugs in your code with API fuzzing.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/api_fuzzing/_index.md'),\n            type: 'api_fuzzing'\n          },\n          coverage_fuzzing: {\n            name: _('Coverage Fuzzing'),\n            description: _('Find bugs in your code with coverage-guided fuzzing.'),\n            help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/coverage_fuzzing/_index.md'),\n            configuration_help_path: Gitlab::Routing.url_helpers.help_page_path(\n              'user/application_security/coverage_fuzzing/_index.md', anchor: 'enable-coverage-guided-fuzz-testing'),\n            type: 'coverage_fuzzing',\n            secondary: {\n              type: 'corpus_management',\n              name: _('Corpus Management'),\n              description: s_('SecurityConfiguration|Manage corpus files used as seed ' \\\n                              'inputs with coverage-guided fuzzing.'),\n              configuration_text: s_('SecurityConfiguration|Manage corpus')\n            }\n          }\n        }.freeze\n      end",
    "comment": "rubocop: disable Metrics/AbcSize -- Generate dynamic translation as per https://docs.gitlab.com/ee/development/i18n/externalization.html#keep-translations-dynamic",
    "label": "",
    "id": "1308"
  },
  {
    "raw_code": "def self.data\n        {\n          site: {\n            DAST_ACTIVE_SCAN_TIMEOUT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"3h\",\n              name: s_(\"DastProfiles|Active scan timeout\"),\n              description: s_(\n                \"DastProfiles|The maximum amount of time to wait for the active scan phase of the scan to complete. \" \\\n                  \"Defaults to 3h.\"\n              )\n            },\n            DAST_ACTIVE_SCAN_WORKER_COUNT: {\n              additional: true,\n              type: \"number\",\n              example: 3,\n              name: s_(\"DastProfiles|Active scan worker count\"),\n              description: s_(\"DastProfiles|The number of active checks to run in parallel. Defaults to 3.\")\n            },\n            DAST_AUTH_AFTER_LOGIN_ACTIONS: {\n              additional: true,\n              auth: true,\n              type: \"string\",\n              example: \"select(option=id:accept-yes),click(on=css:.continue)\",\n              name: s_(\"DastProfiles|After-login actions\"),\n              description: format(s_(\n                \"DastProfiles|A comma-separated list of actions to take after login but before login verification. \" \\\n                  \"Supports `click` and `select` actions. \" \\\n                  \"See [Taking additional actions after submitting the login form](%{documentation_link}).\"),\n                documentation_link: authentication_actions_documentation_link\n              )\n            },\n            DAST_AUTH_BEFORE_LOGIN_ACTIONS: {\n              additional: true,\n              auth: true,\n              type: \"selector\",\n              example: \"css:.user,id:show-login-form\",\n              name: s_(\"DastProfiles|Before-login actions\"),\n              description: s_(\n                \"DastProfiles|A comma-separated list of selectors representing elements to click on \" \\\n                  \"prior to entering the DAST_AUTH_USERNAME and DAST_AUTH_PASSWORD into the login form.\"\n              )\n            },\n            DAST_AUTH_CLEAR_INPUT_FIELDS: {\n              additional: true,\n              auth: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Clear input fields\"),\n              description: s_(\n                \"DastProfiles|Disables clearing of username and password fields before attempting manual login. \" \\\n                  \"Set to false by default.\"\n              )\n            },\n            DAST_AUTH_COOKIE_NAMES: {\n              additional: true,\n              auth: true,\n              type: \"string\",\n              example: \"sessionID,groupName\",\n              name: s_(\"DastProfiles|Cookie names\"),\n              description: s_(\n                \"DastProfiles|Set to a comma-separated list of cookie names to specify which cookies \" \\\n                  \"are used for authentication.\"\n              )\n            },\n            DAST_AUTH_FIRST_SUBMIT_FIELD: {\n              additional: true,\n              auth: true,\n              type: \"selector\",\n              example: \"css:input[type=submit]\",\n              name: s_(\"DastProfiles|First submit field\"),\n              description: s_(\n                \"DastProfiles|A selector describing the element that is clicked on to submit the username form \" \\\n                  \"of a multi-page login process.\"\n              )\n            },\n            DAST_AUTH_NEGOTIATE_DELEGATION: {\n              additional: true,\n              auth: true,\n              type: \"string\",\n              example: \"*.example.com,example.com,*.EXAMPLE.COM,EXAMPLE.COM\",\n              name: s_(\"DastProfiles|Authentication delegation servers\"),\n              description: s_(\n                \"DastProfiles|Which servers should be allowed for integrated authentication and delegation. \" \\\n                  \"This property sets two Chromium policies: \" \\\n                  \"[AuthServerAllowlist](https://chromeenterprise.google/policies/#AuthServerAllowlist) and \" \\\n                  \"[AuthNegotiateDelegateAllowlist]\" \\\n                  \"(https://chromeenterprise.google/policies/#AuthNegotiateDelegateAllowlist). \" \\\n                  \"[Introduced](https://gitlab.com/gitlab-org/gitlab/-/issues/502476) in GitLab 17.6.\"\n              )\n            },\n            DAST_AUTH_OTP_FIELD: {\n              additional: true,\n              auth: true,\n              type: \"selector\",\n              example: \"name:otp\",\n              name: s_(\"DastProfiles|OTP field\"),\n              description: s_(\n                \"DastProfiles|A selector describing the element used to enter the one-time password on the login form.\"\n              )\n            },\n            DAST_AUTH_OTP_KEY: {\n              additional: false,\n              auth: true,\n              type: \"String\",\n              example: \"I5UXITDBMIQEIQKTKQFA====\",\n              name: s_(\"DastProfiles|OTP secret key\"),\n              description: s_(\n                \"DastProfiles|The Base32 encoded secret key to use when generating a one-time password to \" \\\n                  \"authenticate to the website.\")\n            },\n            DAST_AUTH_OTP_SUBMIT_FIELD: {\n              additional: true,\n              auth: true,\n              type: \"selector\",\n              example: \"css:input[type=submit]\",\n              name: s_(\"DastProfiles|OTP submit field\"),\n              description: s_(\n                \"DastProfiles|A selector describing the element that is clicked on to submit the OTP form \" \\\n                  \"when it is separate from the username.\"\n              )\n            },\n            DAST_AUTH_PASSWORD: {\n              additional: false,\n              auth: true,\n              type: \"String\",\n              example: \"P@55w0rd!\",\n              name: s_(\"DastProfiles|Password\"),\n              description: s_(\"DastProfiles|The password to authenticate to in the website.\")\n            },\n            DAST_AUTH_PASSWORD_FIELD: {\n              additional: false,\n              auth: true,\n              type: \"selector\",\n              example: \"name:password\",\n              name: s_(\"DastProfiles|Password field\"),\n              description: s_(\n                \"DastProfiles|A selector describing the element used to enter the password on the login form.\"\n              )\n            },\n            DAST_AUTH_SUBMIT_FIELD: {\n              additional: false,\n              auth: true,\n              type: \"selector\",\n              example: \"css:input[type=submit]\",\n              name: s_(\"DastProfiles|Submit field\"),\n              description: s_(\n                \"DastProfiles|A selector describing the element clicked on to submit the login form \" \\\n                  \"for a single-page login form, or the password form for a multi-page login form.\"\n              )\n            },\n            DAST_AUTH_SUCCESS_IF_AT_URL: {\n              additional: true,\n              auth: true,\n              type: \"URL\",\n              example: \"https://www.site.com/welcome*\",\n              name: s_(\"DastProfiles|Success URL\"),\n              description: s_(\n                \"DastProfiles|A URL that is compared to the URL in the browser to determine if authentication \" \\\n                  \"has succeeded after the login form is submitted. Wildcard `*` can be used to match a dynamic URL.\"\n              )\n            },\n            DAST_AUTH_SUCCESS_IF_ELEMENT_FOUND: {\n              additional: true,\n              auth: true,\n              type: \"selector\",\n              example: \"css:.user-avatar\",\n              name: s_(\"DastProfiles|Success element\"),\n              description: s_(\n                \"DastProfiles|A selector describing an element whose presence is used to determine if \" \\\n                  \"authentication has succeeded after the login form is submitted.\"\n              )\n            },\n            DAST_AUTH_SUCCESS_IF_NO_LOGIN_FORM: {\n              additional: true,\n              auth: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Success without login form\"),\n              description: s_(\n                \"DastProfiles|Verifies successful authentication by checking for the absence of a login form \" \\\n                  \"after the login form has been submitted. This success check is enabled by default.\"\n              )\n            },\n            DAST_AUTH_TYPE: {\n              additional: true,\n              auth: true,\n              type: \"string\",\n              example: \"basic-digest\",\n              name: s_(\"DastProfiles|Authentication type\"),\n              description: s_(\"DastProfiles|The authentication type to use.\")\n            },\n            DAST_AUTH_URL: {\n              additional: false,\n              auth: true,\n              type: \"URL\",\n              example: \"https://www.site.com/login\",\n              name: s_(\"DastProfiles|Authentication URL\"),\n              description: s_(\n                \"DastProfiles|The URL of the page containing the login form on the target website. \" \\\n                  \"DAST_AUTH_USERNAME and DAST_AUTH_PASSWORD are submitted with the login form to create \" \\\n                  \"an authenticated scan.\"\n              )\n            },\n            DAST_AUTH_USERNAME: {\n              additional: false,\n              auth: true,\n              type: \"string\",\n              example: \"user@email.com\",\n              name: s_(\"DastProfiles|Username\"),\n              description: s_(\"DastProfiles|The username to authenticate to in the website.\")\n            },\n            DAST_AUTH_USERNAME_FIELD: {\n              additional: false,\n              auth: true,\n              type: \"selector\",\n              example: \"name:username\",\n              name: s_(\"DastProfiles|Username field\"),\n              description: s_(\n                \"DastProfiles|A selector describing the element used to enter the username on the login form.\"\n              )\n            },\n            DAST_CRAWL_EXTRACT_ELEMENT_TIMEOUT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"5s\",\n              name: s_(\"DastProfiles|Extract element timeout\"),\n              description: s_(\n                \"DastProfiles|The maximum amount of time to allow the browser to extract newly found elements \" \\\n                  \"or navigations. Defaults to `5s`.\"\n              )\n            },\n            DAST_CRAWL_MAX_ACTIONS: {\n              additional: true,\n              type: \"number\",\n              example: \"10000\",\n              name: s_(\"DastProfiles|Maximum action count\"),\n              description: s_(\n                \"DastProfiles|The maximum number of actions that the crawler performs. \" \\\n                  \"Example actions include selecting a link, or filling out a form. \" \\\n                  \"Defaults to `10000`.\"\n              )\n            },\n            DAST_CRAWL_MAX_DEPTH: {\n              additional: true,\n              type: \"number\",\n              example: \"10\",\n              name: s_(\"DastProfiles|Maximum action depth\"),\n              description: s_(\n                \"DastProfiles|The maximum number of chained actions that the crawler takes. \" \\\n                  \"For example, `Click, Form Fill, Click` is a depth of three. \" \\\n                  \"Defaults to `10`.\"\n              )\n            },\n            DAST_CRAWL_SEARCH_ELEMENT_TIMEOUT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"3s\",\n              name: s_(\"DastProfiles|Element search timeout\"),\n              description: s_(\n                \"DastProfiles|The maximum amount of time to allow the browser to search for new elements \" \\\n                  \"or user actions. Defaults to `3s`.\"\n              )\n            },\n            DAST_CRAWL_TIMEOUT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"5m\",\n              name: s_(\"DastProfiles|Timeout\"),\n              description: s_(\n                \"DastProfiles|The maximum amount of time to wait for the crawl phase of the scan to complete. \" \\\n                  \"Defaults to `24h`.\"\n              )\n            },\n            DAST_CRAWL_WORKER_COUNT: {\n              additional: true,\n              type: \"number\",\n              example: \"3\",\n              name: s_(\"DastProfiles|Worker count\"),\n              description: s_(\n                \"DastProfiles|The maximum number of concurrent browser instances to use. \" \\\n                  \"For instance runners on GitLab.com, we recommended a maximum of three. \" \\\n                  \"Private runners with more resources may benefit from a higher number, \" \\\n                  \"but are likely to produce little benefit after five to seven instances. \" \\\n                  \"The default value is dynamic, equal to the number of usable logical CPUs.\"\n              )\n            },\n            DAST_PAGE_DOM_READY_TIMEOUT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"7s\",\n              name: s_(\"DastProfiles|DOM ready timeout\"),\n              description: s_(\n                \"DastProfiles|The maximum amount of time to wait for a browser to consider a page loaded \" \\\n                  \"and ready for analysis after a navigation completes. Defaults to `6s`.\"\n              )\n            },\n            DAST_PAGE_DOM_STABLE_WAIT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"200ms\",\n              name: s_(\"DastProfiles|DOM stable timeout\"),\n              description: s_(\n                \"DastProfiles|Define how long to wait for updates to the DOM before checking a page is stable. \" \\\n                  \"Defaults to `500ms`.\"\n              )\n            },\n            DAST_PAGE_ELEMENT_READY_TIMEOUT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"600ms\",\n              name: s_(\"DastProfiles|Page ready timeout\"),\n              description: s_(\n                \"DastProfiles|The maximum amount of time to wait for an element before determining it is \" \\\n                  \"ready for analysis. Defaults to `300ms`.\"\n              )\n            },\n            DAST_PAGE_IS_LOADING_ELEMENT: {\n              additional: true,\n              type: \"selector\",\n              example: \"css:#page-is-loading\",\n              name: s_(\"DastProfiles|Loading element\"),\n              description: s_(\n                \"DastProfiles|Selector that, when no longer visible on the page, indicates to the analyzer \" \\\n                  \"that the page has finished loading and the scan can continue. \" \\\n                  \"Cannot be used with `DAST_PAGE_IS_READY_ELEMENT`.\"\n              )\n            },\n            DAST_PAGE_IS_READY_ELEMENT: {\n              additional: true,\n              type: \"selector\",\n              example: \"css:#page-is-ready\",\n              name: s_(\"DastProfiles|Ready element\"),\n              description: s_(\n                \"DastProfiles|Selector that when detected as visible on the page, indicates to the analyzer \" \\\n                  \"that the page has finished loading and the scan can continue. \" \\\n                  \"Cannot be used with `DAST_PAGE_IS_LOADING_ELEMENT`.\"\n              )\n            },\n            DAST_PAGE_MAX_RESPONSE_SIZE_MB: {\n              additional: true,\n              type: \"number\",\n              example: \"15\",\n              name: s_(\"DastProfiles|Maximum response size (MB)\"),\n              description: s_(\n                \"DastProfiles|The maximum size of a HTTP response body. \" \\\n                  \"Responses with bodies larger than this are blocked by the browser. \" \\\n                  \"Defaults to `10` MB.\"\n              )\n            },\n            DAST_PAGE_READY_AFTER_ACTION_TIMEOUT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"7s\",\n              name: s_(\"DastProfiles|Page ready timeout (after action)\"),\n              description: s_(\n                \"DastProfiles|The maximum amount of time to wait for a browser to consider a page loaded \" \\\n                  \"and ready for analysis. Defaults to `7s`.\"\n              )\n            },\n            DAST_PAGE_READY_AFTER_NAVIGATION_TIMEOUT: {\n              additional: true,\n              type: \"Duration string\",\n              example: \"15s\",\n              name: s_(\"DastProfiles|Page ready timeout (after navigation)\"),\n              description: s_(\n                \"DastProfiles|The maximum amount of time to wait for a browser to navigate from one page \" \\\n                  \"to another. Defaults to `15s`.\"\n              )\n            },\n            DAST_PASSIVE_SCAN_WORKER_COUNT: {\n              additional: true,\n              type: \"int\",\n              example: \"5\",\n              name: s_(\"DastProfiles|Passive scan worker count\"),\n              description: s_(\n                \"DastProfiles|Number of workers that passive scan in parallel. \" \\\n                  \"Defaults to the number of available CPUs.\"\n              )\n            },\n            DAST_PKCS12_CERTIFICATE_BASE64: {\n              additional: true,\n              type: \"string\",\n              example: \"ZGZkZ2p5NGd...\",\n              name: s_(\"DastProfiles|PKCS12 certificate\"),\n              description: s_(\n                \"DastProfiles|The PKCS12 certificate used for sites that require Mutual TLS. \" \\\n                  \"Must be encoded as base64 text.\"\n              )\n            },\n            DAST_PKCS12_PASSWORD: {\n              additional: true,\n              type: \"string\",\n              example: \"password\",\n              name: s_(\"DastProfiles|PKCS12 password\"),\n              description: format(s_(\n                \"DastProfiles|The password of the certificate used in `DAST_PKCS12_CERTIFICATE_BASE64`. \" \\\n                  \"Create sensitive [custom CI/CI variables](%{documentation_link}) using the GitLab UI.\"),\n                documentation_link: ci_variables_documentation_link\n              )\n            },\n            DAST_REQUEST_ADVERTISE_SCAN: {\n              additional: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Advertise scan\"),\n              description: format(s_(\n                \"DastProfiles|Set to `true` to add a `Via: GitLab DAST %{version}` header to every request sent, \" \\\n                  \"advertising that the request was sent as part of a GitLab DAST scan. Default: `false`.\"\n              ), version: \"<version>\")\n            },\n            DAST_REQUEST_COOKIES: {\n              additional: true,\n              type: \"dictionary\",\n              example: \"abtesting_group:3,region:locked\",\n              name: s_(\"DastProfiles|Request cookies\"),\n              description: s_(\"DastProfiles|A cookie name and value to be added to every request.\")\n            },\n            DAST_REQUEST_HEADERS: {\n              additional: false,\n              type: \"String\",\n              example: \"Cache-control:no-cache\",\n              name: s_(\"DastProfiles|Request headers\"),\n              description: s_(\n                \"DastProfiles|Set to a comma-separated list of request header names and values. \" \\\n                  \"The following headers are not supported: `content-length`, `cookie2`, `keep-alive`, `hosts`, \" \\\n                  \"`trailer`, `transfer-encoding`, and all headers with a `proxy-` prefix.\"\n              )\n            },\n            DAST_SCOPE_ALLOW_HOSTS: {\n              additional: true,\n              type: \"List of strings\",\n              example: \"site.com,another.com\",\n              name: s_(\"DastProfiles|Allowed hosts\"),\n              description: s_(\n                \"DastProfiles|Hostnames included in this variable are considered in scope when crawled. \" \\\n                  \"By default the `DAST_TARGET_URL` hostname is included in the allowed hosts list. \" \\\n                  \"Headers set using `DAST_REQUEST_HEADERS` are added to every request made to these hostnames.\"\n              )\n            },\n            DAST_SCOPE_EXCLUDE_ELEMENTS: {\n              additional: true,\n              type: \"selector\",\n              example: \"a[href='2.html'],css:.no-follow\",\n              name: s_(\"DastProfiles|Excluded elements\"),\n              description: s_(\"DastProfiles|Comma-separated list of selectors that are ignored when scanning.\")\n            },\n            DAST_SCOPE_EXCLUDE_HOSTS: {\n              additional: true,\n              type: \"List of strings\",\n              example: \"site.com,another.com\",\n              name: s_(\"DastProfiles|Excluded hosts\"),\n              description: s_(\n                \"DastProfiles|Hostnames included in this variable are considered excluded and connections \" \\\n                  \"are forcibly dropped.\"\n              )\n            },\n            DAST_SCOPE_EXCLUDE_URLS: {\n              auth: true,\n              additional: false,\n              type: \"URLs\",\n              example: \"https://site.com/.*/sign-out\",\n              name: s_(\"DastProfiles|Excluded URLs\"),\n              description: s_(\n                \"DastProfiles|The URLs to skip during the authenticated scan; comma-separated. \" \\\n                  \"Regular expression syntax can be used to match multiple URLs. \" \\\n                  \"For example, `.*` matches an arbitrary character sequence.\"\n              )\n            },\n            DAST_SCOPE_IGNORE_HOSTS: {\n              additional: true,\n              type: \"List of strings\",\n              example: \"site.com,another.com\",\n              name: s_(\"DastProfiles|Ignored hosts\"),\n              description: s_(\n                \"DastProfiles|Hostnames included in this variable are accessed, not attacked, \" \\\n                  \"and not reported against.\"\n              )\n            },\n            DAST_TARGET_CHECK_SKIP: {\n              additional: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Skip target check\"),\n              description: s_(\n                \"DastProfiles|Set to `true` to prevent DAST from checking that the target is available \" \\\n                  \"before scanning. Default: `false`.\"\n              )\n            },\n            DAST_TARGET_CHECK_TIMEOUT: {\n              additional: true,\n              type: \"number\",\n              example: \"60\",\n              name: s_(\"DastProfiles|Target check timeout\"),\n              description: s_(\"DastProfiles|Time limit in seconds to wait for target availability. Default: `60s`.\")\n            },\n            DAST_TARGET_PATHS_FILE: {\n              additional: true,\n              type: \"string\",\n              example: \"/builds/project/urls.txt\",\n              name: s_(\"DastProfiles|Target paths file\"),\n              description: s_(\n                \"DastProfiles|Scan only these paths instead of crawling the whole site. \" \\\n                  \"Set to a file path containing a list of URL paths relative to `DAST_TARGET_URL`. \" \\\n                  \"The file must be plain text with one path per line. When this is set, \" \\\n                  \"`DAST_CRAWL_MAX_DEPTH` defaults to 1. To prevent this, set `DAST_OVERRIDE_MAX_DEPTH: false`.\"\n              )\n            },\n            DAST_TARGET_PATHS: {\n              additional: true,\n              type: \"string\",\n              example: \"/page1.html,/category1/page3.html\",\n              name: s_(\"DastProfiles|Target paths\"),\n              description: s_(\n                \"DastProfiles|Scan only these paths instead of crawling the whole site. \" \\\n                  \"Set to a comma-separated list of URL paths relative to `DAST_TARGET_URL`. When this is set, \" \\\n                  \"`DAST_CRAWL_MAX_DEPTH` defaults to 1. To prevent this, set `DAST_OVERRIDE_MAX_DEPTH: false`.\"\n              )\n            },\n            DAST_TARGET_URL: {\n              additional: false,\n              type: \"URL\",\n              example: \"https://site.com\",\n              name: s_(\"DastProfiles|Target URL\"),\n              description: s_(\"DastProfiles|The URL of the website to scan.\")\n            },\n            DAST_USE_CACHE: {\n              additional: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Use cache\"),\n              description: s_(\n                \"DastProfiles|Set to `false` to disable caching. \" \\\n                  \"Default: `true`. \" \\\n                  \"**Note**: Disabling cache can cause OOM events or DAST job timeouts.\"\n              )\n            },\n            DAST_CRAWL_GROUPED_URLS: {\n              additional: true,\n              type: \"string\",\n              example: \"https://example.com/hello/*,https://example.com/world/*/details\",\n              name: s_(\"DastProfiles|Grouped URLs\"),\n              description: s_(\n                \"DastProfiles|(Experimental) Set to a comma-separated list of wildcard URL patterns \" \\\n                  \"with at least one `*`. \" \\\n                  \"To reduce scan time, the scanner groups and analyzes only one matched URL per pattern.\"\n              )\n            }\n          },\n          scanner: {\n            DAST_AUTH_REPORT: {\n              additional: true,\n              auth: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Generate authentication report\"),\n              description: s_(\n                \"DastProfiles|Set to `true` to generate a report detailing steps taken during the \" \\\n                  \"authentication process. You must also define `gl-dast-debug-auth-report.html` as a \" \\\n                  \"CI job artifact to be able to access the generated report. \" \\\n                  \"The report's content aids when debugging authentication failures. Defaults to `false`.\"\n              )\n            },\n            DAST_CHECKS_TO_EXCLUDE: {\n              additional: true,\n              type: \"string\",\n              example: \"552.2,78.1\",\n              name: s_(\"DastProfiles|Excluded checks\"),\n              description: format(s_(\n                \"DastProfiles|Comma-separated list of check identifiers to exclude from the scan. \" \\\n                  \"For identifiers, see [vulnerability checks](%{documentation_link}).\"),\n                documentation_link: vulnerability_checks_documentation_link\n              )\n            },\n            DAST_CHECKS_TO_RUN: {\n              additional: true,\n              type: \"List of strings\",\n              example: \"16.1,16.2,16.3\",\n              name: s_(\"DastProfiles|Included checks\"),\n              description: format(s_(\n                \"DastProfiles|Comma-separated list of check identifiers to use for the scan. \" \\\n                  \"For identifiers, see [vulnerability checks](%{documentation_link}).\"),\n                documentation_link: vulnerability_checks_documentation_link\n              )\n            },\n            DAST_CRAWL_GRAPH: {\n              additional: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Generate graph\"),\n              description: s_(\n                \"DastProfiles|Set to `true` to generate an SVG graph of navigation paths visited during crawl phase \" \\\n                  \"of the scan. You must also define `gl-dast-crawl-graph.svg` as a CI job artifact to be able to \" \\\n                  \"access the generated graph. Defaults to `false`.\"\n              )\n            },\n            DAST_FULL_SCAN: {\n              additional: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Full scan\"),\n              description: s_(\"DastProfiles|Set to `true` to run both passive and active checks. Default is `false`.\")\n            },\n            DAST_LOG_BROWSER_OUTPUT: {\n              additional: true,\n              type: \"boolean\",\n              example: true,\n              name: s_(\"DastProfiles|Log browser output\"),\n              description: s_(\"DastProfiles|Set to `true` to log Chromium `STDOUT` and `STDERR`.\")\n            },\n            DAST_LOG_CONFIG: {\n              additional: true,\n              type: \"List of strings\",\n              example: \"brows:debug,auth:debug\",\n              name: s_(\"DastProfiles|Log levels\"),\n              description: s_(\n                \"DastProfiles|A list of modules and their intended logging level for use in the console log.\")\n            },\n            DAST_LOG_DEVTOOLS_CONFIG: {\n              additional: true,\n              type: \"string\",\n              example: \"Default:messageAndBody,truncate:2000\",\n              name: s_(\"DastProfiles|Log messages\"),\n              description: s_(\"DastProfiles|Set to log protocol messages between DAST and the Chromium browser.\")\n            },\n            DAST_LOG_FILE_CONFIG: {\n              additional: true,\n              type: \"List of strings\",\n              example: \"brows:debug,auth:debug\",\n              name: s_(\"DastProfiles|Log file levels\"),\n              description: s_(\n                \"DastProfiles|A list of modules and their intended logging level for use in the file log.\")\n            },\n            DAST_LOG_FILE_PATH: {\n              additional: true,\n              type: \"string\",\n              example: \"/output/browserker.log\",\n              name: s_(\"DastProfiles|Log file path\"),\n              description: s_(\"DastProfiles|Set to the path of the file log. Default is `gl-dast-scan.log`.\")\n            },\n            SECURE_ANALYZERS_PREFIX: {\n              additional: true,\n              type: \"URL\",\n              example: \"registry.organization.com\",\n              name: s_(\"DastProfiles|Docker registry\"),\n              description: s_(\"DastProfiles|Set the Docker registry base address from which to download the analyzer.\")\n            },\n            SECURE_LOG_LEVEL: {\n              additional: true,\n              type: \"string\",\n              example: \"debug\",\n              name: s_(\"DastProfiles|Default log level\"),\n              description: format(s_(\n                \"DastProfiles|Set the default level for the file log. \" \\\n                  \"See [SECURE_LOG_LEVEL](%{documentation_link}).\"),\n                documentation_link: secure_log_level_documentation_link\n              )\n            }\n          }\n        }.freeze\n      end",
    "comment": "rubocop: disable Metrics/AbcSize -- Generate dynamic translation as per https://docs.gitlab.com/ee/development/i18n/externalization.html#keep-translations-dynamic",
    "label": "",
    "id": "1309"
  },
  {
    "raw_code": "def self.additional_site_variables\n        data[:site].merge(data[:scanner]).filter { |_, variable| variable[:additional] }\n      end",
    "comment": "rubocop: enable Metrics/AbcSize",
    "label": "",
    "id": "1310"
  },
  {
    "raw_code": "def initialize(project, client, parallel: true)\n        @project = project\n        @client = client\n        @parallel = parallel\n        @page_keyset = Gitlab::Import::PageKeyset.new(project, collection_method, ::Import::SOURCE_GITHUB)\n        @already_imported_cache_key = format(ALREADY_IMPORTED_CACHE_KEY, project: project.id,\n          collection: collection_method)\n        @job_waiter_cache_key = format(JOB_WAITER_CACHE_KEY, project: project.id, collection: collection_method)\n        @job_waiter_remaining_cache_key = format(JOB_WAITER_REMAINING_CACHE_KEY, project: project.id,\n          collection: collection_method)\n      end",
    "comment": "project - An instance of `Project`. client - An instance of `Gitlab::GithubImport::Client`. parallel - When set to true the objects will be imported in parallel.",
    "label": "",
    "id": "1311"
  },
  {
    "raw_code": "def sequential_import\n        each_object_to_import do |object|\n          repr = object_representation(object)\n\n          importer_class.new(repr, project, client).execute\n        end",
    "comment": "Imports all the objects in sequence in the current thread.",
    "label": "",
    "id": "1312"
  },
  {
    "raw_code": "def parallel_import\n        raise 'Batch settings must be defined for parallel import' if parallel_import_batch.blank?\n\n        spread_parallel_import\n      end",
    "comment": "Imports all objects in parallel by scheduling a Sidekiq job for every individual object.",
    "label": "",
    "id": "1313"
  },
  {
    "raw_code": "def each_object_to_import\n        repo = project.import_source\n\n        # URL to resume the pagination from in case the job is interrupted.\n        resume_url = page_keyset.current\n\n        client.each_page(collection_method, resume_url, repo, collection_options) do |page|\n          page.objects.each do |object|\n            object = object.to_h\n\n            next if already_imported?(object)\n\n            if increment_object_counter?(object)\n              Gitlab::GithubImport::ObjectCounter.increment(project, object_type, :fetched)\n            end",
    "comment": "The method that will be called for traversing through all the objects to import, yielding them to the supplied block.",
    "label": "",
    "id": "1314"
  },
  {
    "raw_code": "def already_imported?(object)\n        id = id_for_already_imported_cache(object)\n\n        Gitlab::Cache::Import::Caching.set_includes?(already_imported_cache_key, id)\n      end",
    "comment": "Returns true if the given object has already been imported, false otherwise.  object - The object to check.",
    "label": "",
    "id": "1315"
  },
  {
    "raw_code": "def mark_as_imported(object)\n        id = id_for_already_imported_cache(object)\n\n        Gitlab::Cache::Import::Caching.set_add(already_imported_cache_key, id)\n      end",
    "comment": "Marks the given object as \"already imported\".",
    "label": "",
    "id": "1316"
  },
  {
    "raw_code": "def id_for_already_imported_cache(object)\n        raise NotImplementedError\n      end",
    "comment": "Returns the ID to use for the cache used for checking if an object has already been imported or not.  object - The object we may want to import.",
    "label": "",
    "id": "1317"
  },
  {
    "raw_code": "def representation_class\n        raise NotImplementedError\n      end",
    "comment": "The class used for converting API responses to Hashes when performing the import.",
    "label": "",
    "id": "1318"
  },
  {
    "raw_code": "def importer_class\n        raise NotImplementedError\n      end",
    "comment": "The class to use for importing objects when importing them sequentially.",
    "label": "",
    "id": "1319"
  },
  {
    "raw_code": "def sidekiq_worker_class\n        raise NotImplementedError\n      end",
    "comment": "The Sidekiq worker class used for scheduling the importing of objects in parallel.",
    "label": "",
    "id": "1320"
  },
  {
    "raw_code": "def collection_method\n        raise NotImplementedError\n      end",
    "comment": "The name of the method to call to retrieve the data to import.",
    "label": "",
    "id": "1321"
  },
  {
    "raw_code": "def collection_options\n        {}\n      end",
    "comment": "Any options to be passed to the method used for retrieving the data to import.",
    "label": "",
    "id": "1322"
  },
  {
    "raw_code": "def already_imported_ids\n        Gitlab::Cache::Import::Caching.values_from_set(already_imported_cache_key)\n      end",
    "comment": "Returns the set used to track \"already imported\" objects. Items are the values returned by `#id_for_already_imported_cache`.",
    "label": "",
    "id": "1323"
  },
  {
    "raw_code": "def add(record, issue_event)\n        json = issue_event.to_hash.to_json\n\n        if json.bytesize > MAX_EVENT_SIZE\n          Logger.warn(\n            message: 'Event too large to cache',\n            project_id: project.id,\n            github_identifiers: issue_event.github_identifiers\n          )\n\n          return\n        end",
    "comment": "Add issue event as JSON to the cache  @param record [ActiveRecord::Model] Model that responds to :iid @param event [GitLab::GitHubImport::Representation::IssueEvent]",
    "label": "",
    "id": "1324"
  },
  {
    "raw_code": "def events(record)\n        events = Gitlab::Cache::Import::Caching.values_from_list(events_cache_key(record)).map do |event|\n          Representation::IssueEvent.from_json_hash(Gitlab::Json.parse(event))\n        end",
    "comment": "Reads issue events from cache  @param record [ActiveRecord::Model] Model that responds to :iid @retun [Array<GitLab::GitHubImport::Representation::IssueEvent>] List of issue events",
    "label": "",
    "id": "1325"
  },
  {
    "raw_code": "def delete(record)\n        Gitlab::Cache::Import::Caching.del(events_cache_key(record))\n      end",
    "comment": "Deletes the cache  @param record [ActiveRecord::Model] Model that responds to :iid",
    "label": "",
    "id": "1326"
  },
  {
    "raw_code": "def initialize(project, client)\n        @project = project\n        @client = client\n      end",
    "comment": "project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1327"
  },
  {
    "raw_code": "def author_id_for(object, author_key: :author)\n        user_info = case author_key\n                    when :actor\n                      object[:actor]\n                    when :review_requester\n                      object[:review_requester]\n                    else\n                      object ? object[:author] : nil\n                    end",
    "comment": "Returns the GitLab user ID of an object's author.  If the object has no author ID we'll use the ID of the GitLab ghost user. object - An instance of `Hash` or a `Github::Representer`",
    "label": "",
    "id": "1328"
  },
  {
    "raw_code": "def user_id_for(user, ghost: true)\n        if user.nil? || user[:login].nil? || user[:login] == 'ghost'\n          return ghost ? GithubImport.ghost_user_id : nil\n        end",
    "comment": "Returns the GitLab user ID for a GitHub user. Can return nil if `ghost` is `false`. The `ghost: false` argument is used to avoid assigning ghost users as assignees or reviewers.  @param user [Gitlab::GithubImport::Representation::User, Hash] @param ghost [Boolean] Determines what to do if user is nil or is the GitHub ghost. If `true`, ID of the GitLab ghost is returned. If `false`, nil is returned. @return [Integer, NilClass]",
    "label": "",
    "id": "1329"
  },
  {
    "raw_code": "def source_user(user)\n        source_user = source_user_mapper.find_source_user(user[:id])\n\n        return source_user if source_user\n\n        source_user_mapper.find_or_create_source_user(\n          source_name: fetch_source_name_from_github(user[:login]),\n          source_username: user[:login],\n          source_user_identifier: user[:id]\n        )\n      end",
    "comment": "Returns the GitLab user ID from placeholder or reassigned_to user.",
    "label": "",
    "id": "1330"
  },
  {
    "raw_code": "def source_user_accepted?(user)\n        return true unless user_mapping_enabled?\n        return true if map_to_personal_namespace_owner?\n\n        source_user(user).accepted_status?\n      end",
    "comment": "Returns true if GitLab user has accepted their reassignment status or if UCM is not enabled",
    "label": "",
    "id": "1331"
  },
  {
    "raw_code": "def fetch_source_name_from_github(username)\n        in_lock(lease_key(username), sleep_sec: 0.2.seconds, retries: 30) do |retried|\n          if retried\n            source_name = read_source_name_from_cache(username)\n\n            next source_name if source_name.present?\n          end",
    "comment": "Retrieves the name of the user associated with a specified GitHub username.  To prevent multiple concurrent requests for the same user, a exclusive lock is used. The name is cached to avoid multiple calls to GitHub.  @param [String] username GitHub username @return [String] name of the user",
    "label": "",
    "id": "1332"
  },
  {
    "raw_code": "def find(id, username)\n        email = email_for_github_username(username)\n        cached, found_id = find_from_cache(id, email)\n\n        return found_id if found_id\n\n        # We only want to query the database if necessary. If previous lookups\n        # didn't yield a user ID we won't query the database again until the\n        # keys expire.\n        find_id_from_database(id, email) unless cached\n      end",
    "comment": "Returns the GitLab ID for the given GitHub ID or username.  id - The ID of the GitHub user. username - The username of the GitHub user.",
    "label": "",
    "id": "1333"
  },
  {
    "raw_code": "def find_from_cache(id, email = nil)\n        id_exists, id_for_github_id = cached_id_for_github_id(id)\n\n        return [id_exists, id_for_github_id] if id_for_github_id\n\n        # Just in case no Email address could be retrieved (for whatever reason)\n        return [false] unless email\n\n        cached_id_for_github_email(email)\n      end",
    "comment": "Finds a user ID from the cache for a given GitHub ID or Email.",
    "label": "",
    "id": "1334"
  },
  {
    "raw_code": "def find_id_from_database(id, email)\n        id_for_github_id(id) || id_for_github_email(email)\n      end",
    "comment": "Finds a GitLab user ID from the database for a given GitHub user ID or Email.",
    "label": "",
    "id": "1335"
  },
  {
    "raw_code": "def email_for_github_username(username)\n        email = read_email_from_cache(username)\n\n        if email.blank? && !email_fetched_for_project?(username)\n          in_lock(lease_key(username), sleep_sec: 0.2.seconds, retries: 30) do |retried|\n            # when retried, check the cache again as the other process that had the lease may have fetched the email\n            if retried\n              email = read_email_from_cache(username)\n\n              # early return if the other process fetched a non-empty email. If the email is empty, we'll attempt to\n              # fetch it again in the lines below, but using the ETAG cached by the other process which won't count to\n              # the rate limit.\n              next email if email.present?\n            end",
    "comment": "Find the public email of a given username in GitHub. The email is cached to avoid multiple calls to GitHub. The cache is shared among all projects. If the email was not found, a blank email is cached. If the email is blank, we attempt to fetch it from GitHub using an ETAG request once for every project. @param username [String] The username of the GitHub user.  @return [String] If public email is found @return [Nil] If public email or username does not exist",
    "label": "",
    "id": "1336"
  },
  {
    "raw_code": "def id_for_github_id(id)\n        gitlab_id =\n          if project.github_enterprise_import?\n            nil\n          else\n            query_id_for_github_id(id)\n          end",
    "comment": "If importing from github.com, queries and caches the GitLab user ID for a GitHub user ID, if one was found.  When importing from Github Enterprise, do not query user by Github ID since we only have users' Github ID from github.com.",
    "label": "",
    "id": "1337"
  },
  {
    "raw_code": "def id_for_github_email(email)\n        gitlab_id = query_id_for_github_email(email) || nil\n\n        Gitlab::Cache::Import::Caching.write(ID_FOR_EMAIL_CACHE_KEY % email, gitlab_id)\n      end",
    "comment": "Queries and caches the GitLab user ID for a GitHub email, if one was found.",
    "label": "",
    "id": "1338"
  },
  {
    "raw_code": "def read_id_from_cache(key)\n        value = Gitlab::Cache::Import::Caching.read(key)\n        exists = !value.nil?\n        number = value.to_i\n\n        # The cache key may be empty to indicate a previously looked up user for\n        # which we couldn't find an ID.\n        [exists, number > 0 ? number : nil]\n      end",
    "comment": "Reads an ID from the cache.  The return value is an Array with two values:  1. A boolean indicating if the key was present or not. 2. The ID as an Integer, or nil in case no ID could be found.",
    "label": "",
    "id": "1339"
  },
  {
    "raw_code": "def read_source_name_from_cache(username)\n        Gitlab::Cache::Import::Caching.read(source_name_cache_key(username))\n      end",
    "comment": "Reads source name from internal cache for the given username  @param [String] username The username of the GitHub user. @return [String|nil] Return the cached source name or nil",
    "label": "",
    "id": "1340"
  },
  {
    "raw_code": "def cache_source_name(username, source_name)\n        Gitlab::Cache::Import::Caching.write(source_name_cache_key(username), source_name)\n      end",
    "comment": "Caches the source name associated to the username  @param [String] username The username of the GitHub user. @param [String] source_name The source_name to value to be cached.",
    "label": "",
    "id": "1341"
  },
  {
    "raw_code": "def read_email_from_cache(username)\n        Gitlab::Cache::Import::Caching.read(email_cache_key(username))\n      end",
    "comment": "Retrieves the email associated with the given username from the cache.  The return value can be an email, an empty string, or nil.  If an empty string is returned, it indicates that the user's email was fetched but not set on GitHub. If nil is returned, it indicates that the user's email wasn't fetched or the cache has expired. If an email is returned, it means the user has a public email set, and it has been successfully cached.",
    "label": "",
    "id": "1342"
  },
  {
    "raw_code": "def cache_email!(username, email)\n        return unless email\n\n        Gitlab::Cache::Import::Caching.write(email_cache_key(username), email)\n      end",
    "comment": "Caches the email associated to the username  An empty email is cached when the user email isn't set on GitHub. This is done to prevent UserFinder from fetching the user's email again when the user's email isn't set on GitHub",
    "label": "",
    "id": "1343"
  },
  {
    "raw_code": "def get_assets_download_redirection_url\n        return file_url unless file_url.starts_with?(github_assets_url_regex)\n\n        options[:follow_redirects] = false\n        response = ::Import::Clients::HTTP.get(file_url, options)\n\n        if response.redirection?\n          response.headers[:location]\n        else\n          file_url\n        end",
    "comment": "Github /assets redirection link will redirect to aws which has its own authorization. Keeping our bearer token will cause request rejection eg. Only one auth mechanism allowed; only the X-Amz-Algorithm query parameter, Signature query string parameter or the Authorization header should be specified.",
    "label": "",
    "id": "1344"
  },
  {
    "raw_code": "def initialize(text, author = nil, exists = false, project: nil, client: nil)\n        @text = text\n        @author = author\n        @exists = exists\n        @project = project\n        @web_endpoint = client&.web_endpoint || ::Octokit::Default.web_endpoint\n      end",
    "comment": "text - The Markdown text as a String. author - An instance of `Gitlab::GithubImport::Representation::User` exists - Boolean that indicates the user exists in the GitLab database. project - An instance of `Project`.",
    "label": "",
    "id": "1345"
  },
  {
    "raw_code": "def convert_ref_links(text, project, web_endpoint)\n        web_endpoint = web_endpoint.chop if web_endpoint.end_with?('/')\n        matcher_options = { github_url: web_endpoint, import_source: project.import_source }\n        issue_ref_matcher = ISSUE_REF_MATCHER % matcher_options\n        pull_ref_matcher = PULL_REF_MATCHER % matcher_options\n\n        url_helpers = Rails.application.routes.url_helpers\n        text.gsub(issue_ref_matcher, url_helpers.project_issues_url(project))\n            .gsub(pull_ref_matcher, url_helpers.project_merge_requests_url(project))\n      end",
    "comment": "Links like `https://domain.github.com/<namespace>/<project>/pull/<iid>` needs to be converted",
    "label": "",
    "id": "1346"
  },
  {
    "raw_code": "def initialize(token, host: nil, per_page: DEFAULT_PER_PAGE, parallel: true)\n        @host = host\n        @octokit = ::Octokit::Client.new(\n          access_token: token,\n          per_page: per_page,\n          api_endpoint: api_endpoint,\n          web_endpoint: web_endpoint\n        )\n\n        @octokit.connection_options[:ssl] = { verify: verify_ssl }\n\n        @parallel = parallel\n      end",
    "comment": "token - The GitHub API token to use.  host - The GitHub hostname. If nil, github.com will be used.  per_page - The number of objects that should be displayed per page.  parallel - When set to true hitting the rate limit will result in a dedicated error being raised. When set to `false` we will instead just `sleep()` until the rate limit is reset. Setting this value to `true` for parallel importing is crucial as otherwise hitting the rate limit will result in a thread being blocked in a `sleep()` call for up to an hour.",
    "label": "",
    "id": "1347"
  },
  {
    "raw_code": "def user(username, options = {})\n        with_rate_limit do\n          user = octokit.user(username, options)\n\n          next if octokit.last_response&.status == 304\n\n          user.to_h\n        end",
    "comment": "Returns the details of a GitHub user. 304 (Not Modified) status means the user is cached - API won't return user data.  @param username[String] the username of the user. @param options[Hash] the optional parameters.",
    "label": "",
    "id": "1348"
  },
  {
    "raw_code": "def repository(name)\n        with_rate_limit { octokit.repo(name).to_h }\n      end",
    "comment": "Returns the details of a GitHub repository.  name - The path (in the form `owner/repository`) of the repository.",
    "label": "",
    "id": "1349"
  },
  {
    "raw_code": "def each_page(method, resume_url, *args, &block)\n        return to_enum(__method__, method, resume_url, *args) unless block\n\n        collection = with_rate_limit do\n          if resume_url.present?\n            octokit.get(resume_url)\n          else\n            octokit.public_send(method, *args)\n          end",
    "comment": "Fetches data from the GitHub API and yields a Page object for every page of data, without loading all of them into memory.  @param method [Symbol] The Octokit method to use for getting the data @param resume_url [String, nil] The GitHub link header URL to resume pagination. When nil, the method will be invoked from the first page @param args [Array] Arguments to pass to the Octokit method @yield [Page] Each page of data from the API @return [Enumerator] When no block is given  rubocop: disable GitlabSecurity/PublicSend",
    "label": "",
    "id": "1350"
  },
  {
    "raw_code": "def each_object(method, *args, &block)\n        return to_enum(__method__, method, *args) unless block\n\n        each_page(method, nil, *args) do |page|\n          page.objects.each do |object|\n            yield object.to_h\n          end",
    "comment": "Iterates over all of the objects for the given method (e.g. `:labels`).  method - The method to send to Octokit for querying data. args - Any arguments to pass to the Octokit method.",
    "label": "",
    "id": "1351"
  },
  {
    "raw_code": "def with_rate_limit\n        return with_retry { yield } unless rate_limiting_enabled?\n\n        request_count_counter.increment\n\n        raise_or_wait_for_rate_limit('Internal threshold reached') unless requests_remaining?\n\n        begin\n          with_retry { yield }\n        rescue ::Octokit::TooManyRequests => e\n          raise_or_wait_for_rate_limit(e.response_body)\n\n          # This retry will only happen when running in sequential mode as we'll\n          # raise an error in parallel mode.\n          retry\n        end",
    "comment": "Yields the supplied block, responding to any rate limit errors.  The exact strategy used for handling rate limiting errors depends on whether we are running in parallel mode or not. For more information see `#rate_or_wait_for_rate_limit`.",
    "label": "",
    "id": "1352"
  },
  {
    "raw_code": "def requests_remaining?\n        if requests_limit == SEARCH_MAX_REQUESTS_PER_MINUTE\n          return remaining_requests > SEARCH_RATE_LIMIT_THRESHOLD\n        end",
    "comment": "Returns `true` if we're still allowed to perform API calls. Search API has rate limit of 30, use lowered threshold when search is used.",
    "label": "",
    "id": "1353"
  },
  {
    "raw_code": "def increment(project, object_type, operation, value: 1)\n          integer = value.to_i\n\n          return if integer <= 0\n\n          validate_operation!(operation)\n\n          increment_project_counter(project, object_type, operation, integer)\n          increment_global_counter(object_type, operation, integer)\n\n          project.import_state&.expire_etag_cache\n        end",
    "comment": "Increments the project and the global counters if the given value is >= 1",
    "label": "",
    "id": "1354"
  },
  {
    "raw_code": "def increment_global_counter(object_type, operation, value)\n          key = GLOBAL_COUNTER_KEY % {\n            operation: operation,\n            object_type: object_type\n          }\n          description = GLOBAL_COUNTER_DESCRIPTION % {\n            operation: operation,\n            object_type: object_type.to_s.humanize\n          }\n\n          Gitlab::Metrics.counter(key.to_sym, description).increment(by: value)\n        end",
    "comment": "Global counters are long lived, in Prometheus, and it's used to report the health of the Github Importer in the Grafana Dashboard https://dashboards.gitlab.net/d/2zgM_rImz/github-importer?orgId=1",
    "label": "",
    "id": "1355"
  },
  {
    "raw_code": "def increment_project_counter(project, object_type, operation, value)\n          counter_key = PROJECT_COUNTER_KEY % {\n            project: project.id,\n            operation: operation,\n            object_type: object_type\n          }\n\n          add_counter_to_list(project, operation, counter_key)\n\n          CACHING.increment_by(counter_key, value, timeout: IMPORT_CACHING_TIMEOUT)\n        end",
    "comment": "Project counters are short lived, in Redis, and it's used to report how successful a project import was with the #summary method.",
    "label": "",
    "id": "1356"
  },
  {
    "raw_code": "def initialize(project, object)\n        @project = project\n        @object = object\n      end",
    "comment": "project - An instance of `Project`. object - The object to look up or set a database ID for.",
    "label": "",
    "id": "1357"
  },
  {
    "raw_code": "def database_id\n        val = Gitlab::Cache::Import::Caching.read_integer(cache_key, timeout: timeout)\n\n        return if val == CACHE_OBJECT_NOT_FOUND\n        return val if val.present?\n\n        object_id = cache_key_type.safe_constantize&.find_by(project_id: project.id, iid: cache_key_iid)&.id ||\n          CACHE_OBJECT_NOT_FOUND\n\n        cache_database_id(object_id)\n        object_id == CACHE_OBJECT_NOT_FOUND ? nil : object_id\n      end",
    "comment": "Returns the database ID for the object.  This method will return `nil` if no ID could be found.",
    "label": "",
    "id": "1358"
  },
  {
    "raw_code": "def cache_database_id(database_id)\n        Gitlab::Cache::Import::Caching.write(cache_key, database_id, timeout: timeout)\n      end",
    "comment": "Associates the given database ID with the current object.  database_id - The ID of the corresponding database row.",
    "label": "",
    "id": "1359"
  },
  {
    "raw_code": "def cache_key_type\n        if object.respond_to?(:issuable_type)\n          object.issuable_type\n        elsif object.respond_to?(:noteable_type)\n          object.noteable_type\n        else\n          raise(\n            TypeError,\n            \"Instances of #{object.class} are not supported\"\n          )\n        end",
    "comment": "Returns the identifier to use for cache keys.  For issues and pull requests this will be \"Issue\" or \"MergeRequest\" respectively. For diff notes this will return \"MergeRequest\", for regular notes it will either return \"Issue\" or \"MergeRequest\" depending on what type of object the note belongs to.",
    "label": "",
    "id": "1360"
  },
  {
    "raw_code": "def each_associated(_parent_record, associated)\n        associated = associated.to_h\n\n        return if already_imported?(associated)\n\n        Gitlab::GithubImport::ObjectCounter.increment(project, object_type, :fetched)\n\n        yield(associated)\n\n        mark_as_imported(associated)\n      end",
    "comment": "Sometimes we need to add some extra info from parent to associated record that is not available by default in Github API response object. For example: lib/gitlab/github_import/importer/single_endpoint_issue_events_importer.rb:26",
    "label": "",
    "id": "1361"
  },
  {
    "raw_code": "def initialize(project, client)\n        @project = project\n        @client = client\n        @validation_errors = []\n      end",
    "comment": "project - An instance of `Project`. client - An instance of `Gitlab::GithubImport::Client`.",
    "label": "",
    "id": "1362"
  },
  {
    "raw_code": "def build_database_rows(enum)\n        errors = []\n        rows = enum.each_with_object([]) do |(object, _), result|\n          next if already_imported?(object)\n\n          attrs = build_attributes(object)\n          build_record = model.new(attrs)\n\n          if build_record.invalid?\n            github_identifiers = github_identifiers(object)\n\n            log_error(github_identifiers, build_record.errors.full_messages)\n            errors << {\n              validation_errors: build_record.errors,\n              external_identifiers: github_identifiers\n            }\n            next\n          end",
    "comment": "Builds and returns an Array of objects to bulk insert into the database and array of validation errors if object is invalid.  enum - An Enumerable that returns the objects to turn into database rows.",
    "label": "",
    "id": "1363"
  },
  {
    "raw_code": "def bulk_insert(rows, batch_size: 100)\n        inserted_ids = []\n        rows.each_slice(batch_size) do |slice|\n          inserted_ids += ApplicationRecord.legacy_bulk_insert(model.table_name, slice, return_ids: true) # rubocop:disable Gitlab/BulkInsert -- required\n\n          log_and_increment_counter(slice.size, :imported)\n        end",
    "comment": "Bulk inserts the given rows into the database.",
    "label": "",
    "id": "1364"
  },
  {
    "raw_code": "def self.symbolize_hash(raw_hash = nil)\n        hash = raw_hash.deep_symbolize_keys\n\n        TIMESTAMP_KEYS.each do |key|\n          hash[key] = Time.parse(hash[key]) if hash[key].is_a?(String)\n        end",
    "comment": "Converts a Hash with String based keys to one that can be used by the various Representation classes.  Example:  Representation.symbolize_hash('number' => 10) # => { number: 10 }",
    "label": "",
    "id": "1365"
  },
  {
    "raw_code": "def parallel_import_batch\n        { size: Gitlab::CurrentSettings.concurrent_github_import_jobs_limit, delay: 1.minute }\n      end",
    "comment": "Default batch settings for parallel import (can be redefined in Importer/Worker classes)",
    "label": "",
    "id": "1366"
  },
  {
    "raw_code": "def initialize(project)\n        @project = project\n      end",
    "comment": "project - An instance of `Project`.",
    "label": "",
    "id": "1367"
  },
  {
    "raw_code": "def id_for(name)\n        cache_key = cache_key_for(name)\n        val = Gitlab::Cache::Import::Caching.read_integer(cache_key)\n\n        return if val == CACHE_OBJECT_NOT_FOUND\n        return val if val.present?\n\n        object_id = project.labels.with_title(name).pick(:id) || CACHE_OBJECT_NOT_FOUND\n\n        Gitlab::Cache::Import::Caching.write(cache_key, object_id)\n        object_id == CACHE_OBJECT_NOT_FOUND ? nil : object_id\n      end",
    "comment": "Returns the label ID for the given name.",
    "label": "",
    "id": "1368"
  },
  {
    "raw_code": "def build_cache\n        mapping = @project\n          .labels\n          .pluck(:id, :name)\n          .each_with_object({}) do |(id, name), hash|\n            hash[cache_key_for(name)] = id\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1369"
  },
  {
    "raw_code": "def cache_key_for(name)\n        format(CACHE_KEY, project: project.id, name: name)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1370"
  },
  {
    "raw_code": "def initialize(project)\n        @project = project\n      end",
    "comment": "project - An instance of `Project`",
    "label": "",
    "id": "1371"
  },
  {
    "raw_code": "def id_for(issuable)\n        return unless issuable.milestone_number\n\n        milestone_iid = issuable.milestone_number\n        cache_key = cache_key_for(milestone_iid)\n\n        val = Gitlab::Cache::Import::Caching.read_integer(cache_key)\n\n        return if val == CACHE_OBJECT_NOT_FOUND\n        return val if val.present?\n\n        object_id = project.milestones.by_iid(milestone_iid).pick(:id) || CACHE_OBJECT_NOT_FOUND\n\n        Gitlab::Cache::Import::Caching.write(cache_key, object_id)\n        object_id == CACHE_OBJECT_NOT_FOUND ? nil : object_id\n      end",
    "comment": "issuable - An instance of `Gitlab::GithubImport::Representation::Issue` or `Gitlab::GithubImport::Representation::PullRequest`.",
    "label": "",
    "id": "1372"
  },
  {
    "raw_code": "def build_cache\n        mapping = @project\n          .milestones\n          .pluck(:id, :iid)\n          .each_with_object({}) do |(id, iid), hash|\n            hash[cache_key_for(iid)] = id\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1373"
  },
  {
    "raw_code": "def cache_key_for(iid)\n        format(CACHE_KEY, project: project.id, iid: iid)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1374"
  },
  {
    "raw_code": "def initialize(collaborator, project, client)\n          @collaborator = collaborator\n          @project = project\n          @client = client\n          @members_finder = ::MembersFinder.new(project, project.creator)\n        end",
    "comment": "collaborator - An instance of `Gitlab::GithubImport::Representation::Collaborator` project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1375"
  },
  {
    "raw_code": "def existing_labels\n          @existing_labels ||= project.labels.pluck(:title).to_set\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1376"
  },
  {
    "raw_code": "def execute\n          rows, validation_errors = build_labels\n\n          bulk_insert(rows)\n          bulk_insert_failures(validation_errors) if validation_errors.any?\n          build_labels_cache\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1377"
  },
  {
    "raw_code": "def initialize(replay_event, project, client)\n          @project = project\n          @client = client\n          @replay_event = replay_event\n        end",
    "comment": "replay_event - An instance of `Gitlab::GithubImport::Representation::ReplayEvent`. project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1378"
  },
  {
    "raw_code": "def existing_tags\n          @existing_tags ||= project.releases.pluck(:tag).to_set\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1379"
  },
  {
    "raw_code": "def github_users\n          @github_users ||= []\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1380"
  },
  {
    "raw_code": "def execute\n          rows, validation_errors = build_releases\n\n          inserted_ids = bulk_insert(rows)\n\n          inserted_ids.zip(github_users).each do |id, user|\n            # `id` is the GitLab Release ID we just inserted.\n            # `user` is the GitHub user object.\n            push_references_by_ids(project, [id], Release, :author_id, user[:id])\n          end",
    "comment": "Note: if you're going to replace `legacy_bulk_insert` with something that triggers callback to generate HTML version - you also need to regenerate it in Gitlab::GithubImport::Importer::NoteAttachmentsImporter.",
    "label": "",
    "id": "1381"
  },
  {
    "raw_code": "def initialize(note, project, client)\n          @note = note\n          @project = project\n          @client = client\n        end",
    "comment": "note - An instance of `Gitlab::GithubImport::Representation::DiffNote` project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1382"
  },
  {
    "raw_code": "def import_with_legacy_diff_note\n          log_diff_note_creation('LegacyDiffNote')\n          # It's possible that during an import we'll insert tens of thousands\n          # of diff notes. If we were to use the Note/LegacyDiffNote model here\n          # we'd also have to run additional queries for both validations and\n          # callbacks, putting a lot of pressure on the database.\n          #\n          # To work around this we're using bulk_insert with a single row. This\n          # allows us to efficiently insert data (even if it's just 1 row)\n          # without having to use all sorts of hacks to disable callbacks.\n          attributes = {\n            noteable_type: note.noteable_type,\n            system: false,\n            type: 'LegacyDiffNote',\n            discussion_id: note.discussion_id,\n            noteable_id: merge_request_id,\n            project_id: project.id,\n            namespace_id: project.project_namespace_id,\n            author_id: author_id,\n            note: note_body,\n            commit_id: note.original_commit_id,\n            line_code: note.line_code,\n            created_at: note.created_at,\n            updated_at: note.updated_at,\n            st_diff: note.diff_hash.to_yaml,\n            imported_from: ::Import::HasImportSource::IMPORT_SOURCES[:github]\n          }\n\n          diff_note = LegacyDiffNote.new(attributes.merge(importing: true))\n          diff_note.validate!\n\n          ids = ApplicationRecord.legacy_bulk_insert(LegacyDiffNote.table_name, [attributes], return_ids: true)\n\n          push_references_by_ids(project, ids, LegacyDiffNote, :author_id, note[:author]&.id)\n        end",
    "comment": "rubocop:disable Gitlab/BulkInsert",
    "label": "",
    "id": "1383"
  },
  {
    "raw_code": "def import_with_diff_note\n          log_diff_note_creation('DiffNote')\n\n          record = ::Import::Github::Notes::CreateService.new(project, author, {\n            noteable_type: note.noteable_type,\n            system: false,\n            type: 'DiffNote',\n            noteable_id: merge_request_id,\n            project_id: project.id,\n            note: note_body,\n            discussion_id: note.discussion_id,\n            commit_id: note.original_commit_id,\n            created_at: note.created_at,\n            updated_at: note.updated_at,\n            position: note.diff_position,\n            imported_from: ::Import::SOURCE_GITHUB\n          }).execute(importing: true)\n\n          raise DiffNoteCreationError, record unless record.persisted?\n\n          push_reference(project, record, :author_id, note.author&.id)\n        end",
    "comment": "rubocop:enabled Gitlab/BulkInsert",
    "label": "",
    "id": "1384"
  },
  {
    "raw_code": "def merge_request_id\n          @merge_request_id ||= GithubImport::IssuableFinder.new(project, note).database_id\n        end",
    "comment": "Returns the ID of the merge request this note belongs to.",
    "label": "",
    "id": "1385"
  },
  {
    "raw_code": "def initialize(note, project, client)\n          @note = note\n          @project = project\n          @client = client\n          @user_finder = GithubImport::UserFinder.new(project, client)\n        end",
    "comment": "note - An instance of `Gitlab::GithubImport::Representation::Note`. project - An instance of `Project`. client - An instance of `Gitlab::GithubImport::Client`.",
    "label": "",
    "id": "1386"
  },
  {
    "raw_code": "def find_noteable_id\n          GithubImport::IssuableFinder.new(project, note).database_id\n        end",
    "comment": "Returns the ID of the issue or merge request to create the note for.",
    "label": "",
    "id": "1387"
  },
  {
    "raw_code": "def self.import_if_issue(issue, project, client)\n          new(issue, project, client).execute unless issue.pull_request?\n        end",
    "comment": "Imports an issue if it's a regular issue and not a pull request.",
    "label": "",
    "id": "1388"
  },
  {
    "raw_code": "def initialize(issue, project, client)\n          @issue = issue\n          @project = project\n          @client = client\n          @user_finder = GithubImport::UserFinder.new(project, client)\n          @milestone_finder = MilestoneFinder.new(project)\n          @issuable_finder = GithubImport::IssuableFinder.new(project, issue)\n        end",
    "comment": "issue - An instance of `Gitlab::GithubImport::Representation::Issue`. project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1389"
  },
  {
    "raw_code": "def issue_assignee_map\n          @map ||= issue.assignees.each_with_object({}) do |assignee, map|\n            gitlab_user_id = user_finder.user_id_for(assignee, ghost: false)\n            next unless gitlab_user_id\n\n            map[gitlab_user_id] = assignee[:id]\n          end",
    "comment": "Returns a Hash of { GitLabUserId => GitHubUserId } that can be used for both importing and pushing user references.",
    "label": "",
    "id": "1390"
  },
  {
    "raw_code": "def create_issue\n          author_id, author_found = user_finder.author_id_for(issue)\n\n          description = MarkdownText.format(issue.description, issue.author, author_found, project: project,\n            client: client)\n          assignee_ids = issue_assignee_map.keys\n\n          attributes = {\n            iid: issue.iid,\n            title: issue.truncated_title,\n            author_id: author_id,\n            assignee_ids: assignee_ids,\n            project_id: project.id,\n            namespace_id: project.project_namespace_id,\n            description: description,\n            milestone_id: milestone_finder.id_for(issue),\n            state_id: ::Issue.available_states[issue.state],\n            created_at: issue.created_at,\n            updated_at: issue.updated_at,\n            work_item_type_id: issue.work_item_type_id,\n            imported_from: ::Import::SOURCE_GITHUB\n          }\n\n          project.issues.create!(attributes.merge(importing: true))\n        end",
    "comment": "Creates a new GitLab issue for the current GitHub issue.",
    "label": "",
    "id": "1391"
  },
  {
    "raw_code": "def each_object_to_import\n          repo = project.import_source\n\n          protected_branches = client.branches(repo).select { |branch| branch.dig(:protection, :enabled) }\n          protected_branches.each do |protected_branch|\n            next if already_imported?(protected_branch)\n\n            object = client.branch_protection(repo, protected_branch[:name])\n            next if object.nil?\n\n            yield object\n\n            Gitlab::GithubImport::ObjectCounter.increment(project, object_type, :fetched)\n            mark_as_imported(protected_branch)\n          end",
    "comment": "The method that will be called for traversing through all the objects to import, yielding them to the supplied block.",
    "label": "",
    "id": "1392"
  },
  {
    "raw_code": "def each_associated(parent_record, associated)\n          associated = associated.to_h\n\n          compose_associated_id!(parent_record, associated)\n\n          return if already_imported?(associated) || importer_class::SUPPORTED_EVENTS.exclude?(associated[:event])\n\n          cache_event(parent_record, associated)\n\n          increment_object_counter(associated[:event])\n\n          pull_request = parent_record.is_a? MergeRequest\n          associated[:issue] = { number: parent_record.iid, pull_request: pull_request }\n          yield(associated)\n\n          mark_as_imported(associated)\n        end",
    "comment": "In single endpoint there is no issue info to which associated related To make it possible to identify issue in separated worker we need to patch Sawyer instances here with issue number",
    "label": "",
    "id": "1393"
  },
  {
    "raw_code": "def each_associated_page(&block)\n          issues_collection.each_batch(of: BATCH_SIZE, column: :iid) { |batch| process_batch(batch, &block) }\n          merge_requests_collection.each_batch(of: BATCH_SIZE, column: :iid) { |batch| process_batch(batch, &block) }\n        end",
    "comment": "In Github Issues and MergeRequests uses the same API to get their events. Even more - they have commonly uniq iid",
    "label": "",
    "id": "1394"
  },
  {
    "raw_code": "def compose_associated_id!(issuable, event)\n          return if event[:event] != 'cross-referenced'\n\n          event[:id] = \"cross-reference##{issuable.iid}-in-#{event.dig(:source, :issue, :id)}\"\n        end",
    "comment": "Cross-referenced events on Github doesn't have id.",
    "label": "",
    "id": "1395"
  },
  {
    "raw_code": "def initialize(issue, project, client)\n          @issue = issue\n          @project = project\n          @client = client\n        end",
    "comment": "issue - An instance of `Gitlab::GithubImport::Representation::Issue`. project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1396"
  },
  {
    "raw_code": "def initialize(issue_event, project, client)\n          @issue_event = issue_event\n          @project = project\n          @client = client\n        end",
    "comment": "issue_event - An instance of `Gitlab::GithubImport::Representation::IssueEvent`. project - An instance of `Project`. client - An instance of `Gitlab::GithubImport::Client`.",
    "label": "",
    "id": "1397"
  },
  {
    "raw_code": "def each_object_to_import\n          repo = project.import_source\n\n          direct_collaborators = client.collaborators(repo, affiliation: 'direct')\n          outside_collaborators = client.collaborators(repo, affiliation: 'outside')\n          collaborators_to_import = direct_collaborators.to_a - outside_collaborators.to_a\n\n          collaborators_to_import.each do |collaborator|\n            next if already_imported?(collaborator)\n\n            yield collaborator\n\n            Gitlab::GithubImport::ObjectCounter.increment(project, object_type, :fetched)\n            mark_as_imported(collaborator)\n          end",
    "comment": "The method that will be called for traversing through all the objects to import, yielding them to the supplied block.",
    "label": "",
    "id": "1398"
  },
  {
    "raw_code": "def initialize(lfs_object, project, _)\n          @lfs_object = lfs_object\n          @project = project\n        end",
    "comment": "lfs_object - An instance of `Gitlab::GithubImport::Representation::LfsObject`. project - An instance of `Project`.",
    "label": "",
    "id": "1399"
  },
  {
    "raw_code": "def parallel_import_batch\n          { size: 200, delay: 1.minute }\n        end",
    "comment": "To avoid overloading Gitaly, we use a smaller limit for pull requests than the one defined in the application settings.",
    "label": "",
    "id": "1400"
  },
  {
    "raw_code": "def initialize(issue, project, client)\n          @issue = issue\n          @project = project\n          @client = client\n          @label_finder = LabelFinder.new(project)\n        end",
    "comment": "issue - An instance of `Gitlab::GithubImport::Representation::Issue` project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1401"
  },
  {
    "raw_code": "def initialize(protected_branch, project, client)\n          @protected_branch = protected_branch\n          @project = project\n          @client = client\n          @user_finder = GithubImport::UserFinder.new(project, client)\n          @gitlab_user_id_to_github_user_id = {}\n        end",
    "comment": "protected_branch - An instance of `Gitlab::GithubImport::Representation::ProtectedBranch`. project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1402"
  },
  {
    "raw_code": "def merge_access_level\n          gitlab_access = gitlab_access_level_for(:merge)\n\n          return gitlab_access if gitlab_access == Gitlab::Access::NO_ACCESS\n\n          [gitlab_access, GITHUB_DEFAULT_MERGE_ACCESS_LEVEL].max\n        end",
    "comment": "Gets the strictest merge_access_level between GitHub and GitLab",
    "label": "",
    "id": "1403"
  },
  {
    "raw_code": "def gitlab_access_level_for(action)\n          if default_branch?\n            action == :push ? default_branch_push_access_level : default_branch_merge_access_level\n          elsif protected_on_gitlab?\n            non_default_branch_access_level_for(action)\n          else\n            gitlab_default_access_level_for(action)\n          end",
    "comment": "action - :push/:merge",
    "label": "",
    "id": "1404"
  },
  {
    "raw_code": "def initialize(pull_request, project, client)\n          @pull_request = pull_request\n          @project = project\n          @client = client\n          @user_finder = GithubImport::UserFinder.new(project, client)\n          @milestone_finder = MilestoneFinder.new(project)\n          @issuable_finder =\n            GithubImport::IssuableFinder.new(project, pull_request)\n        end",
    "comment": "pull_request - An instance of `Gitlab::GithubImport::Representation::PullRequest`. project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1405"
  },
  {
    "raw_code": "def create_merge_request\n          author_id, author_found = user_finder.author_id_for(pull_request)\n\n          description = MarkdownText.format(pull_request.description, pull_request.author, author_found, project: project, client: client)\n\n          attributes = {\n            iid: pull_request.iid,\n            title: pull_request.truncated_title,\n            description: description,\n            source_project_id: project.id,\n            target_project_id: project.id,\n            source_branch: pull_request.formatted_source_branch,\n            target_branch: pull_request.target_branch,\n            state_id: ::MergeRequest.available_states[pull_request.state],\n            milestone_id: milestone_finder.id_for(pull_request),\n            author_id: author_id,\n            created_at: pull_request.created_at,\n            updated_at: pull_request.updated_at,\n            imported_from: ::Import::HasImportSource::IMPORT_SOURCES[:github]\n          }\n\n          mr = project.merge_requests.new(attributes.merge(importing: true))\n          mr.validate!\n\n          create_merge_request_without_hooks(project, attributes, pull_request.iid)\n        end",
    "comment": "Creates the merge request and returns its ID.  This method will return `nil` if the merge request could not be created, otherwise it will return an Array containing the following values:  1. A MergeRequest instance. 2. A boolean indicating if the MR already exists.",
    "label": "",
    "id": "1406"
  },
  {
    "raw_code": "def create_source_branch_if_not_exists(merge_request)\n          return unless merge_request.open?\n\n          source_branch = pull_request.formatted_source_branch\n\n          return if project.repository.branch_exists?(source_branch)\n\n          project.repository.add_branch(project.creator, source_branch, pull_request.source_branch_sha)\n        rescue Gitlab::Git::PreReceiveError, Gitlab::Git::CommandError => e\n          Gitlab::ErrorTracking.track_exception(e,\n            source_branch: source_branch,\n            project_id: merge_request.project.id,\n            merge_request_id: merge_request.id)\n        end",
    "comment": "An imported merge request will not be mergeable unless the source branch exists. For pull requests from forks, the source branch will be in the form of \"github/fork/{project-name}/{source_branch}\". This branch will never exist, so we create it here.  Note that we only create the branch if the merge request is still open. For projects that have many pull requests, we assume that if it's closed the branch has already been deleted.",
    "label": "",
    "id": "1407"
  },
  {
    "raw_code": "def import_wiki?\n          client_repository[:has_wiki] &&\n            !project.wiki_repository_exists? &&\n            Gitlab::GitalyClient::RemoteService.exists?(wiki_url)\n        end",
    "comment": "Returns true if we should import the wiki for the project. rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1408"
  },
  {
    "raw_code": "def execute\n          imported =\n            # It's possible a repository has already been imported when running\n            # this code, e.g. because we had to retry this job after\n            # `import_wiki?` raised a rate limit error. In this case we'll skip\n            # re-importing the main repository.\n            if project.empty_repo?\n              import_repository\n            else\n              true\n            end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Imports the repository data.  This method will return true if the data was imported successfully or the repository had already been imported before.",
    "label": "",
    "id": "1409"
  },
  {
    "raw_code": "def existing_milestones\n          @existing_milestones ||= project.milestones.pluck(:iid).to_set\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1410"
  },
  {
    "raw_code": "def execute\n          rows, validation_errors = build_milestones\n\n          bulk_insert(rows)\n          bulk_insert_failures(validation_errors) if validation_errors.any?\n          build_milestones_cache\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1411"
  },
  {
    "raw_code": "def initialize(note_text, project, client)\n          @note_text = note_text\n          @project = project\n          @client = client\n          @web_endpoint = client.web_endpoint\n        end",
    "comment": "note_text - An instance of `Gitlab::GithubImport::Representation::NoteText`. project - An instance of `Project`. client - An instance of `Gitlab::GithubImport::Client`.",
    "label": "",
    "id": "1412"
  },
  {
    "raw_code": "def convert_project_content_link(attachment_url, import_source)\n          path_without_domain = attachment_url.gsub(web_endpoint, '')\n          path_without_import_source = path_without_domain.gsub(import_source, '').delete_prefix('/')\n          path_with_blob_prefix = \"/-#{path_without_import_source}\"\n\n          ::Gitlab::Routing.url_helpers.project_url(project) + path_with_blob_prefix\n        end",
    "comment": "From: https://github.com/login/test-import-attachments-source/blob/main/example.md To: https://gitlab.com/login/test-import-attachments-target/-/blob/main/example.md",
    "label": "",
    "id": "1413"
  },
  {
    "raw_code": "def download_attachment(attachment)\n          downloader = ::Gitlab::GithubImport::AttachmentsDownloader.new(attachment.url, options: options,\n            web_endpoint: web_endpoint)\n\n          file = downloader.perform\n\n          # for ghe imports skip file attachments\n          # in these cases the AttachmentsDownloader returns the redirect url\n          # so we return the original attachment.url\n          if web_endpoint != ::Octokit::Default.web_endpoint && file.is_a?(String) && file.starts_with?(github_file_url_regex) # rubocop:disable Layout/LineLength,Lint/RedundantCopDisableDirective -- minor infraction\n            return attachment.url\n          end",
    "comment": "in: an instance of Gitlab::GithubImport::Markdown::Attachment out: gitlab attachment markdown url",
    "label": "",
    "id": "1414"
  },
  {
    "raw_code": "def each_object_to_import\n            collection.each_batch(of: BATCH_SIZE, column: ordering_column) do |batch|\n              batch.each do |record|\n                next if already_imported?(record)\n\n                if has_attachments?(record)\n                  Gitlab::GithubImport::ObjectCounter.increment(project, object_type, :fetched)\n\n                  yield record\n                end",
    "comment": "The method that will be called for traversing through all the objects to import, yielding them to the supplied block.",
    "label": "",
    "id": "1415"
  },
  {
    "raw_code": "def initialize(review, project, client)\n            @review = review\n            @project = project\n            @client = client\n            @merge_request = project.merge_requests.find_by_iid(review.merge_request_iid)\n            @user_finder = GithubImport::UserFinder.new(project, client)\n          end",
    "comment": "review - An instance of `Gitlab::GithubImport::Representation::PullRequestReview` project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1416"
  },
  {
    "raw_code": "def initialize(pull_request, project, client)\n            @pull_request = pull_request\n            @project = project\n            @client = client\n            @merged_by = pull_request.merged_by\n          end",
    "comment": "pull_request - An instance of `Gitlab::GithubImport::Representation::PullRequest` project - An instance of `Project` client - An instance of `Gitlab::GithubImport::Client`",
    "label": "",
    "id": "1417"
  },
  {
    "raw_code": "def init_mentioned_in(record_class, iid)\n            db_id = fetch_mentioned_in_db_id(record_class, iid)\n            return if db_id.nil?\n\n            record = record_class.new(id: db_id, iid: iid)\n            record.project = project\n            record.namespace = project.project_namespace if record.respond_to?(:namespace)\n            record.readonly!\n            record\n          end",
    "comment": "record_class - Issue/MergeRequest",
    "label": "",
    "id": "1418"
  },
  {
    "raw_code": "def fetch_mentioned_in_db_id(record_class, number)\n            sawyer_mentioned_in_adapter = Struct.new(:iid, :issuable_type, keyword_init: true)\n            mentioned_in_adapter = sawyer_mentioned_in_adapter.new(\n              iid: number, issuable_type: record_class.name\n            )\n\n            issuable_db_id(mentioned_in_adapter)\n          end",
    "comment": "record_class - Issue/MergeRequest",
    "label": "",
    "id": "1419"
  },
  {
    "raw_code": "def initialize(project, client)\n            @project = project\n            @client = client\n            @user_finder = UserFinder.new(project, client)\n          end",
    "comment": "project - An instance of `Project`. client - An instance of `Gitlab::GithubImport::Client`.",
    "label": "",
    "id": "1420"
  },
  {
    "raw_code": "def execute(issue_event)\n            raise NotImplementedError\n          end",
    "comment": "issue_event - An instance of `Gitlab::GithubImport::Representation::IssueEvent`.",
    "label": "",
    "id": "1421"
  },
  {
    "raw_code": "def event_outside_cutoff?(issue_event)\n            issue_event.created_at < PruneOldEventsWorker::CUTOFF_DATE.ago && PruneOldEventsWorker.pruning_enabled?\n          end",
    "comment": "`PruneOldEventsWorker` deletes Event records older than a cutoff date. Before importing Events, check if they would be pruned.",
    "label": "",
    "id": "1422"
  },
  {
    "raw_code": "def from_markdown(markdown_node, web_endpoint)\n            case markdown_node.type\n            when :html, :inline_html\n              from_inline_html(markdown_node, web_endpoint)\n            when :image\n              from_markdown_image(markdown_node, web_endpoint)\n            when :link\n              from_markdown_link(markdown_node, web_endpoint)\n            when :text, :paragraph\n              from_markdown_text(markdown_node, web_endpoint)\n            end",
    "comment": "markdown_node - CommonMarker::Node",
    "label": "",
    "id": "1423"
  },
  {
    "raw_code": "def from_markdown_text(markdown_node, web_endpoint)\n            text = markdown_node.to_plaintext.strip\n\n            url = URI.extract(text, %w[http https]).first\n            return if url.nil?\n\n            return unless github_url?(url, web_endpoint, media: true)\n            return unless whitelisted_type?(url, web_endpoint, media: true)\n\n            # we don't have the :alt or :name so we use a default name\n            new(\"media_attachment\", url, web_endpoint)\n          end",
    "comment": "this checks for any attachment links that appear as plain text without a filetype suffix e.g. \"https://github.com/user-attachments/assets/75334fd4\" each markdown_node will only ever have a single url as embedded media on GitHub is always on its own line",
    "label": "",
    "id": "1424"
  },
  {
    "raw_code": "def expose_attribute(*names)\n            names.each do |name|\n              name = name.to_sym\n\n              define_method(name) { attributes[name] }\n            end",
    "comment": "Defines getter methods for the given attribute names.  Example:  expose_attribute :iid, :title",
    "label": "",
    "id": "1425"
  },
  {
    "raw_code": "def self.from_api_response(review, additional_data = {})\n          user = Representation::User.from_api_response(review[:user]) if review[:user]\n\n          new(\n            merge_request_id: review[:merge_request_id],\n            merge_request_iid: review[:merge_request_iid],\n            author: user,\n            note: review[:body],\n            review_type: review[:state],\n            submitted_at: review[:submitted_at],\n            review_id: review[:id]\n          )\n        end",
    "comment": "Builds a PullRequestReview from a GitHub API response.  review - An instance of `Hash` containing the note details.",
    "label": "",
    "id": "1426"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          hash = Representation.symbolize_hash(raw_hash)\n\n          hash[:author] &&= Representation::User.from_json_hash(hash[:author])\n          hash[:submitted_at] = Time.parse(hash[:submitted_at]).in_time_zone if hash[:submitted_at].present?\n\n          new(hash)\n        end",
    "comment": "Builds a new note using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1427"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the raw note details. The keys of this Hash must be Symbols.",
    "label": "",
    "id": "1428"
  },
  {
    "raw_code": "def self.from_db_record(record)\n          check_record_class!(record)\n\n          record_type = record.class.name\n          # only column for note is different along MODELS_ALLOWLIST\n          text = record.is_a?(::Note) ? record.note : record.description\n          new(\n            record_db_id: record.id,\n            record_type: record_type,\n            text: text,\n            iid: record.try(:iid),\n            tag: record.try(:tag),\n            noteable_type: record.try(:noteable_type)\n          )\n        end",
    "comment": "Builds a note text representation from DB record of Note or Release.  record - An instance of `Note`, `Release`, `Issue`, `MergeRequest` model",
    "label": "",
    "id": "1429"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the event details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1430"
  },
  {
    "raw_code": "def self.from_api_response(branch_protection, _additional_object_data = {})\n          branch_name = branch_protection[:url].match(%r{/branches/(\\S{1,255})/protection$})[1]\n\n          allowed_to_push_users = branch_protection.dig(:required_pull_request_reviews,\n            :bypass_pull_request_allowances,\n            :users)\n          allowed_to_push_users &&= allowed_to_push_users.map do |u|\n            Representation::User.from_api_response(u)\n          end",
    "comment": "Builds a Branch Protection info from a GitHub API response. Resource structure details: https://docs.github.com/en/rest/branches/branch-protection#get-branch-protection branch_protection - An instance of `Hash` containing the protection details.",
    "label": "",
    "id": "1431"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          hash = Representation.symbolize_hash(raw_hash)\n\n          hash[:allowed_to_push_users].map! do |u|\n            Representation::User.from_json_hash(u)\n          end",
    "comment": "Builds a new Protection using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1432"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the raw Protection details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1433"
  },
  {
    "raw_code": "def self.from_api_response(collaborator, _additional_data = {})\n          new(\n            id: collaborator[:id],\n            login: collaborator[:login],\n            role_name: collaborator[:role_name]\n          )\n        end",
    "comment": "Builds a user from a GitHub API response.  collaborator - An instance of `Hash` containing the user & role details.",
    "label": "",
    "id": "1434"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          new(Representation.symbolize_hash(raw_hash))\n        end",
    "comment": "Builds a user using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1435"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the user details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1436"
  },
  {
    "raw_code": "def self.from_api_response(issue, additional_data = {})\n          user =\n            if issue[:user]\n              Representation::User.from_api_response(issue[:user])\n            end",
    "comment": "Builds an issue from a GitHub API response.  issue - An instance of `Hash` containing the issue details.",
    "label": "",
    "id": "1437"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          hash = Representation.symbolize_hash(raw_hash)\n\n          hash[:state] = hash[:state].to_sym\n          hash[:assignees].map! { |u| Representation::User.from_json_hash(u) }\n          hash[:author] &&= Representation::User.from_json_hash(hash[:author])\n\n          new(hash)\n        end",
    "comment": "Builds a new issue using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1438"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A hash containing the raw issue details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1439"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the event details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1440"
  },
  {
    "raw_code": "def from_api_response(event, additional_data = {})\n            new(\n              id: event[:id],\n              actor: user_representation(event[:actor] || event[:user]),\n              event: event[:event],\n              commit_id: event[:commit_id],\n              label_title: event.dig(:label, :name),\n              old_title: event.dig(:rename, :from),\n              new_title: event.dig(:rename, :to),\n              milestone_title: event.dig(:milestone, :title),\n              issue: event[:issue],\n              source: event[:source],\n              assignee: user_representation(event[:assignee]),\n              requested_reviewer: user_representation(event[:requested_reviewer]),\n              review_requester: user_representation(event[:review_requester]),\n              created_at: event[:created_at],\n              updated_at: event[:updated_at],\n              submitted_at: event[:submitted_at],\n              state: event[:state],\n              body: event[:body]\n            )\n          end",
    "comment": "Builds an event from a GitHub API response.  event - An instance of `Hash` containing the event details.",
    "label": "",
    "id": "1441"
  },
  {
    "raw_code": "def from_json_hash(raw_hash)\n            hash = Representation.symbolize_hash(raw_hash)\n            hash[:actor] = user_representation(hash[:actor], source: :hash)\n            hash[:assignee] = user_representation(hash[:assignee], source: :hash)\n            hash[:requested_reviewer] = user_representation(hash[:requested_reviewer], source: :hash)\n            hash[:review_requester] = user_representation(hash[:review_requester], source: :hash)\n\n            new(hash)\n          end",
    "comment": "Builds an event using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1442"
  },
  {
    "raw_code": "def self.from_api_response(note, additional_data = {})\n          matches = note[:html_url].match(NOTEABLE_TYPE_REGEX)\n\n          if !matches || !matches[:type]\n            raise(\n              ArgumentError,\n              \"The note URL #{note[:html_url].inspect} is not supported\"\n            )\n          end",
    "comment": "Builds a note from a GitHub API response.  note - An instance of `Hash` containing the note details.",
    "label": "",
    "id": "1443"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          hash = Representation.symbolize_hash(raw_hash)\n\n          hash[:author] &&= Representation::User.from_json_hash(hash[:author])\n\n          new(hash)\n        end",
    "comment": "Builds a new note using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1444"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the raw note details. The keys of this Hash must be Symbols.",
    "label": "",
    "id": "1445"
  },
  {
    "raw_code": "def self.from_api_response(pr, additional_data = {})\n          assignee = Representation::User.from_api_response(pr[:assignee]) if pr[:assignee]\n          user = Representation::User.from_api_response(pr[:user]) if pr[:user]\n          merged_by = Representation::User.from_api_response(pr[:merged_by]) if pr[:merged_by]\n\n          hash = {\n            iid: pr[:number],\n            title: pr[:title],\n            description: pr[:body],\n            source_branch: pr.dig(:head, :ref),\n            target_branch: pr.dig(:base, :ref),\n            source_branch_sha: pr.dig(:head, :sha),\n            target_branch_sha: pr.dig(:base, :sha),\n            source_repository_id: pr.dig(:head, :repo, :id),\n            target_repository_id: pr.dig(:base, :repo, :id),\n            source_repository_owner: pr.dig(:head, :user, :login),\n            state: pr[:state] == 'open' ? :opened : :closed,\n            milestone_number: pr.dig(:milestone, :number),\n            author: user,\n            assignee: assignee,\n            created_at: pr[:created_at],\n            updated_at: pr[:updated_at],\n            merged_at: pr[:merged_at],\n            merged_by: merged_by\n          }\n\n          new(hash)\n        end",
    "comment": "Builds a PR from a GitHub API response.  issue - An instance of `Hash` containing the PR details.",
    "label": "",
    "id": "1446"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          hash = Representation.symbolize_hash(raw_hash)\n\n          hash[:state] = hash[:state].to_sym\n          hash[:author] &&= Representation::User.from_json_hash(hash[:author])\n\n          # Assignees are optional so we only convert it from a Hash if one was\n          # set.\n          hash[:assignee] &&= Representation::User.from_json_hash(hash[:assignee])\n          hash[:merged_by] &&= Representation::User.from_json_hash(hash[:merged_by])\n\n          new(hash)\n        end",
    "comment": "Builds a new PR using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1447"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the raw PR details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1448"
  },
  {
    "raw_code": "def formatted_source_branch\n          if cross_project? && source_repository_owner\n            \"github/fork/#{source_repository_owner}/#{source_branch}\"\n          elsif source_branch == target_branch\n            # Sometimes the source and target branch are the same, but GitLab\n            # doesn't support this. This can happen when both the user and\n            # source repository have been deleted, and the PR was submitted from\n            # the fork's master branch.\n            \"#{source_branch}-#{iid}\"\n          else\n            source_branch\n          end",
    "comment": "Returns a formatted source branch.  For cross-project pull requests the branch name will be in the format `github/fork/owner-name/branch-name`.",
    "label": "",
    "id": "1449"
  },
  {
    "raw_code": "def self.from_api_response(lfs_object, additional_data = {})\n          new(\n            oid: lfs_object.oid,\n            link: lfs_object.link,\n            size: lfs_object.size,\n            headers: lfs_object.headers\n          )\n        end",
    "comment": "Builds a lfs_object",
    "label": "",
    "id": "1450"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          new(Representation.symbolize_hash(raw_hash))\n        end",
    "comment": "Builds a new lfs_object using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1451"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the raw lfs_object details. The keys of this Hash must be Symbols.",
    "label": "",
    "id": "1452"
  },
  {
    "raw_code": "def self.from_api_response(user, additional_data = {})\n          new(\n            id: user[:id],\n            login: user[:login]\n          )\n        end",
    "comment": "Builds a user from a GitHub API response.  user - An instance of `Hash` containing the user details.",
    "label": "",
    "id": "1453"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          new(Representation.symbolize_hash(raw_hash))\n        end",
    "comment": "Builds a user using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1454"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A Hash containing the user details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1455"
  },
  {
    "raw_code": "def to_hash\n          hash = {}\n\n          attributes.each do |key, value|\n            hash[key] = convert_value_for_to_hash(value)\n          end",
    "comment": "Converts the current representation to a Hash. The keys of this Hash will be Symbols.",
    "label": "",
    "id": "1456"
  },
  {
    "raw_code": "def convert_value_for_to_hash(value)\n          if value.is_a?(Array)\n            value.map { |v| convert_value_for_to_hash(v) }\n          elsif value.respond_to?(:to_hash)\n            value.to_hash\n          elsif value.respond_to?(:strftime) || value.is_a?(Symbol)\n            value.to_s\n          else\n            value\n          end",
    "comment": "This method allow objects to be safely passed directly to Sidekiq without errors. It returns JSON datatypes: string, integer, float, boolean, null(nil), array and hash.",
    "label": "",
    "id": "1457"
  },
  {
    "raw_code": "def self.from_api_response(note, additional_data = {})\n          matches = note[:html_url].match(NOTEABLE_ID_REGEX)\n\n          unless matches\n            raise(\n              ArgumentError,\n              \"The note URL #{note[:html_url].inspect} is not supported\"\n            )\n          end",
    "comment": "Builds a diff note from a GitHub API response.  note - An instance of `Hash` containing the note details.",
    "label": "",
    "id": "1458"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          hash = Representation.symbolize_hash(raw_hash)\n\n          hash[:author] &&= Representation::User.from_json_hash(hash[:author])\n\n          new(hash)\n        end",
    "comment": "Builds a new note using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1459"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n\n          @note_formatter = DiffNotes::SuggestionFormatter.new(\n            note: attributes[:note],\n            start_line: attributes[:start_line],\n            end_line: attributes[:end_line]\n          )\n        end",
    "comment": "attributes - A Hash containing the raw note details. The keys of this Hash must be Symbols.",
    "label": "",
    "id": "1460"
  },
  {
    "raw_code": "def diff_hash\n          {\n            diff: diff_hunk,\n            new_path: file_path,\n            old_path: file_path,\n\n            # These fields are not displayed for LegacyDiffNote notes, so it\n            # doesn't really matter what we set them to.\n            a_mode: '100644',\n            b_mode: '100644',\n            new_file: false\n          }\n        end",
    "comment": "Returns a Hash that can be used to populate `notes.st_diff`, removing the need for requesting Git data for every diff note. Used when importing with LegacyDiffNote",
    "label": "",
    "id": "1461"
  },
  {
    "raw_code": "def diff_position\n          position_params = {\n            diff_refs: merge_request.diff_refs,\n            old_path: file_path,\n            new_path: file_path\n          }\n\n          Gitlab::Diff::Position.new(position_params.merge(diff_line_params))\n        end",
    "comment": "Used when importing with DiffNote",
    "label": "",
    "id": "1462"
  },
  {
    "raw_code": "def from_api_response(review_requests, _additional_data = {})\n              review_requests = Representation.symbolize_hash(review_requests)\n              users = review_requests[:users].map do |user_data|\n                Representation::User.from_api_response(user_data)\n              end",
    "comment": "Builds a list of requested reviewers from a GitHub API response.  review_requests - An instance of `Hash` containing the review requests details.",
    "label": "",
    "id": "1463"
  },
  {
    "raw_code": "def initialize(attributes)\n            @attributes = attributes\n          end",
    "comment": "attributes - A Hash containing the review details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1464"
  },
  {
    "raw_code": "def formatted_note\n            @formatted_note ||=\n              if contains_suggestion?\n                note.gsub(\n                  GITHUB_SUGGESTION,\n                  \"\\\\k<suggestion>:#{suggestion_range}\\\\k<eol>\"\n                )\n              else\n                note\n              end",
    "comment": "Returns a tuple with: - a boolean indicating if the note has suggestions - the note with the suggestion formatted for Gitlab",
    "label": "",
    "id": "1465"
  },
  {
    "raw_code": "def suggestion_range\n            \"-#{line_count}+0\"\n          end",
    "comment": "Github always saves the comment on the _last_ line of the range. Therefore, the diff hunk will always be related to lines before the comment itself.",
    "label": "",
    "id": "1466"
  },
  {
    "raw_code": "def all_relation\n        Upload.all.preload(:model)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1467"
  },
  {
    "raw_code": "def local?(upload)\n        upload.local?\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1468"
  },
  {
    "raw_code": "def run_batches(&blk)\n        all_relation.in_batches(of: batch_size, start: start, finish: finish) do |batch| # rubocop: disable Cop/InBatches\n          range = batch.first.id..batch.last.id\n          failures = run_batch_for(batch)\n\n          yield(range, failures)\n        end",
    "comment": "Yields a Range of IDs and a Hash of failed verifications (object => error)",
    "label": "",
    "id": "1469"
  },
  {
    "raw_code": "def verify_remote(object)\n        return failure(object, 'Remote object does not exist') unless remote_object_exists?(object)\n\n        success\n      end",
    "comment": "We don't calculate checksum for remote objects, so just check existence",
    "label": "",
    "id": "1470"
  },
  {
    "raw_code": "def all_relation\n        raise NotImplementedError\n      end",
    "comment": "This should return an ActiveRecord::Relation suitable for calling #in_batches on",
    "label": "",
    "id": "1471"
  },
  {
    "raw_code": "def local?(_object)\n        raise NotImplementedError\n      end",
    "comment": "Should return true if the object is stored locally",
    "label": "",
    "id": "1472"
  },
  {
    "raw_code": "def expected_checksum(_object)\n        raise NotImplementedError\n      end",
    "comment": "The checksum we expect the object to have",
    "label": "",
    "id": "1473"
  },
  {
    "raw_code": "def actual_checksum(_object)\n        raise NotImplementedError\n      end",
    "comment": "The freshly-recalculated checksum of the object",
    "label": "",
    "id": "1474"
  },
  {
    "raw_code": "def remote_object_exists?(object)\n        raise NotImplementedError\n      end",
    "comment": "Be sure to perform a hard check of the remote object (don't just check DB value)",
    "label": "",
    "id": "1475"
  },
  {
    "raw_code": "def compared_with(new_value)\n        delta  = new_value - @value\n        @value = new_value\n\n        delta\n      end",
    "comment": "new_value - The value to compare with as a Numeric.  Returns a new Numeric (depending on the type of `new_value`).",
    "label": "",
    "id": "1476"
  },
  {
    "raw_code": "def initialize(name, module_name, method_name, transaction)\n        @module_name = module_name\n        @method_name = method_name\n        @transaction = transaction\n        @name = name\n        @labels = { module: @module_name, method: @method_name }\n        @real_time = 0.0\n        @cpu_time = 0.0\n        @call_count = 0\n      end",
    "comment": "name - The full name of the method (including namespace) such as `User#sign_in`. ",
    "label": "",
    "id": "1477"
  },
  {
    "raw_code": "def measure\n        start_real = System.monotonic_time\n        start_cpu = System.cpu_time\n        retval = yield\n\n        real_time = System.monotonic_time - start_real\n        cpu_time = System.cpu_time - start_cpu\n\n        @real_time += real_time\n        @cpu_time += cpu_time\n        @call_count += 1\n\n        if above_threshold? && transaction\n          label_keys = labels.keys\n          transaction.observe(:gitlab_method_call_duration_seconds, real_time, labels) do\n            docstring 'Method calls real duration'\n            label_keys label_keys\n            buckets [0.01, 0.05, 0.1, 0.5, 1]\n          end",
    "comment": "Measures the real and CPU execution time of the supplied block.",
    "label": "",
    "id": "1478"
  },
  {
    "raw_code": "def above_threshold?\n        real_time.in_milliseconds >= ::Gitlab::Metrics.method_call_threshold\n      end",
    "comment": "Returns true if the total runtime of this method exceeds the method call threshold.",
    "label": "",
    "id": "1479"
  },
  {
    "raw_code": "def add_event(event_name, tags = {})\n        event_name = \"gitlab_transaction_event_#{event_name}_total\".to_sym\n        metric = self.class.prometheus_metric(event_name, :counter) do\n          label_keys tags.keys\n        end",
    "comment": "Tracks a business level event  Business level events including events such as Git pushes, Emails being sent, etc.  event_name - The name of the event (e.g. \"git_push\"). tags - A set of tags to attach to the event.",
    "label": "",
    "id": "1480"
  },
  {
    "raw_code": "def method_call_for(name, module_name, method_name)\n        unless method = @methods[name]\n          @methods[name] = method = MethodCall.new(name, module_name, method_name, self)\n        end",
    "comment": "Returns a MethodCall object for the given name.",
    "label": "",
    "id": "1481"
  },
  {
    "raw_code": "def increment(name, value = 1, labels = {}, &block)\n        counter = self.class.prometheus_metric(name, :counter, &block)\n\n        counter.increment(filter_labels(labels), value)\n      end",
    "comment": "Increment counter metric  It will initialize the metric if metric is not found  block - if provided can be used to initialize metric with custom options (docstring, labels, with_feature)  Example: ``` transaction.increment(:mestric_name, 1, { docstring: 'Custom title', base_labels: {sane: 'yes'} } ) do  transaction.increment(:mestric_name, 1) do docstring 'Custom title' label_keys %i(sane) end ```",
    "label": "",
    "id": "1482"
  },
  {
    "raw_code": "def set(name, value, labels = {}, &block)\n        gauge = self.class.prometheus_metric(name, :gauge, &block)\n\n        gauge.set(filter_labels(labels), value)\n      end",
    "comment": "Set gauge metric  It will initialize the metric if metric is not found  block - if provided, it can be used to initialize metric with custom options (docstring, labels, with_feature, multiprocess_mode) - multiprocess_mode is :all by default  Example: ``` transaction.set(:mestric_name, 1) do multiprocess_mode :livesum end ```",
    "label": "",
    "id": "1483"
  },
  {
    "raw_code": "def observe(name, value, labels = {}, &block)\n        histogram = self.class.prometheus_metric(name, :histogram, &block)\n\n        histogram.observe(filter_labels(labels), value)\n      end",
    "comment": "Observe histogram metric  It will initialize the metric if metric is not found  block - if provided, it can be used to initialize metric with custom options (docstring, labels, with_feature, buckets)  Example: ``` transaction.observe(:mestric_name, 1) do buckets [100, 1000, 10000, 100000, 1000000, 10000000] end ```",
    "label": "",
    "id": "1484"
  },
  {
    "raw_code": "def call(env)\n        trans = Gitlab::Metrics::WebTransaction.new(env)\n\n        begin\n          retval = trans.run { @app.call(env) }\n\n        rescue Exception => error # rubocop: disable Lint/RescueException\n          trans.add_event(:rails_exception)\n\n          raise error\n        end",
    "comment": "env - A Hash containing Rack environment details.",
    "label": "",
    "id": "1485"
  },
  {
    "raw_code": "def transaction(event)\n          observe(:gitlab_database_transaction_seconds, event) do\n            buckets TRANSACTION_DURATION_BUCKET\n          end",
    "comment": "This event is published from ActiveRecordBaseTransactionMetrics and used to record a database transaction duration when calling ApplicationRecord.transaction {} block.",
    "label": "",
    "id": "1486"
  },
  {
    "raw_code": "def count\n            Gitlab::SafeRequestStore[COUNTER].to_i\n          end",
    "comment": "@return [Integer] the total number of LDAP requests",
    "label": "",
    "id": "1487"
  },
  {
    "raw_code": "def duration\n            Gitlab::SafeRequestStore[DURATION].to_f\n          end",
    "comment": "@return [Float] the total duration spent on LDAP requests",
    "label": "",
    "id": "1488"
  },
  {
    "raw_code": "def payload\n            {\n              net_ldap_count: count,\n              net_ldap_duration_s: duration\n            }\n          end",
    "comment": "Used in Gitlab::InstrumentationHelper to merge the LDAP stats into the log output  @return [Hash<Integer, Float>] a hash of the stored statistics",
    "label": "",
    "id": "1489"
  },
  {
    "raw_code": "def observe_event(event)\n          add_to_request_store(event)\n          expose_metrics(event)\n        end",
    "comment": "Called when an event is triggered in ActiveSupport::Notifications  This method is aliased to the various events triggered by the Net::LDAP library, as the method will be called by those names when triggered.  It stores statistics in the request for output to logs, and also resubmits the event data into Prometheus for monitoring purposes.",
    "label": "",
    "id": "1490"
  },
  {
    "raw_code": "def add_to_request_store(event)\n          return unless Gitlab::SafeRequestStore.active?\n\n          Gitlab::SafeRequestStore[COUNTER] = self.class.count + 1\n          Gitlab::SafeRequestStore[DURATION] = self.class.duration + convert_to_seconds(event.duration)\n        end",
    "comment": "Track these events as statistics for the current requests, for logging purposes",
    "label": "",
    "id": "1491"
  },
  {
    "raw_code": "def expose_metrics(event)\n          return unless current_transaction\n\n          # event.name will be, for example, `search.net_ldap`\n          # and so we only want the first part, which is the\n          # true name of the event\n          labels = { name: event.name.split(\".\").first }\n          duration = convert_to_seconds(event.duration)\n\n          current_transaction.increment(:gitlab_net_ldap_total, 1, labels) do\n            docstring 'Net::LDAP calls'\n            label_keys labels.keys\n          end",
    "comment": "Converts the observed events into Prometheus metrics",
    "label": "",
    "id": "1492"
  },
  {
    "raw_code": "def normalize_source(source)\n          return SOURCE_DIRECT if source.blank?\n\n          normalized_source = source.gsub('streamed from ', '')\n\n          if normalized_source.start_with?(SOURCE_GRAPHQL_EVENT)\n            # Take at most two levels of topic namespacing.\n            normalized_source.split(':').reject(&:empty?).take(2).join(':') # rubocop: disable CodeReuse/ActiveRecord\n          elsif normalized_source.start_with?(SOURCE_GRAPHQL_SUBSCRIPTION)\n            SOURCE_GRAPHQL_SUBSCRIPTION\n          else\n            SOURCE_OTHER\n          end",
    "comment": "Since transmission sources can have high dimensionality when they carry IDs, we need to collapse them. If it's not a well-know broadcast, we report it as \"other\".",
    "label": "",
    "id": "1493"
  },
  {
    "raw_code": "def graphql_event_broadcasting_from(payload_data)\n          # Depending on whether the query result was passed in-process from a direct\n          # execution (e.g. in response to a subcription request) or cross-process by\n          # going through PubSub, we might encounter either string or symbol keys.\n          # We do not use deep_transform_keys here because the payload can be large\n          # and performance would be affected.\n          query_result = payload_data[:result] || payload_data['result'] || {}\n          query_result_data = query_result['data'] || {}\n          gql_operation = query_result_data.each_key.first\n\n          return unless gql_operation\n\n          \"#{SOURCE_GRAPHQL_EVENT}:#{gql_operation}\"\n        end",
    "comment": "When possible tries to query operation name. This will only return data for GraphQL subscription broadcasts.",
    "label": "",
    "id": "1494"
  },
  {
    "raw_code": "def web_transaction_completed(_event)\n          return unless Gitlab::SafeRequestStore.active?\n\n          LOG_COUNTERS.keys.each { |result| increment_prometheus_for_result_label(result) }\n        end",
    "comment": "we want to update Prometheus counter after the controller/action are set",
    "label": "",
    "id": "1495"
  },
  {
    "raw_code": "def self.parameter_filter\n          @parameter_filter ||= ActiveSupport::ParameterFilter.new(Rails.application.config.filter_parameters)\n        end",
    "comment": "Rubocop requires this be public",
    "label": "",
    "id": "1496"
  },
  {
    "raw_code": "def docstring(docstring = nil)\n          @docstring = docstring unless docstring.nil?\n\n          @docstring\n        end",
    "comment": "Documentation describing metric in metrics endpoint '/-/metrics'",
    "label": "",
    "id": "1497"
  },
  {
    "raw_code": "def multiprocess_mode(mode = nil)\n          @multiprocess_mode = mode unless mode.nil?\n\n          @multiprocess_mode\n        end",
    "comment": "Gauge aggregation mode for multiprocess metrics - :all (default) returns each gauge for every process - :livesum all process'es gauges summed up - :max maximum value of per process gauges - :min minimum value of per process gauges",
    "label": "",
    "id": "1498"
  },
  {
    "raw_code": "def buckets(buckets = nil)\n          @buckets = buckets unless buckets.nil?\n\n          @buckets\n        end",
    "comment": "Measurement buckets for histograms",
    "label": "",
    "id": "1499"
  },
  {
    "raw_code": "def base_labels\n          @base_labels ||= @label_keys.product([nil]).to_h\n\n          @base_labels\n        end",
    "comment": "Base labels are merged with per metric labels",
    "label": "",
    "id": "1500"
  },
  {
    "raw_code": "def with_feature(name = nil)\n          @with_feature = name unless name.nil?\n\n          @with_feature\n        end",
    "comment": "Use feature toggle to control whether certain metric is enabled/disabled",
    "label": "",
    "id": "1501"
  },
  {
    "raw_code": "def initialize(**options)\n          super(Settings.monitoring.web_exporter, log_enabled: true, log_file: 'web_exporter.log', **options)\n        end",
    "comment": "This exporter is always run on master process",
    "label": "",
    "id": "1502"
  },
  {
    "raw_code": "def initialize(settings, log_enabled:, log_file:, gc_requests: false, **options)\n          super(**options)\n\n          @settings = settings\n          @gc_requests = gc_requests\n\n          # log_enabled does not exist for all exporters\n          log_sink = log_enabled ? File.join(Rails.root, 'log', log_file) : File::NULL\n          @logger = WEBrick::Log.new(log_sink)\n          @logger.time_format = \"[%Y-%m-%dT%H:%M:%S.%L%z]\"\n        end",
    "comment": "@param settings [Hash] SettingsLogic hash containing the `*_exporter` config @param log_enabled [Boolean] whether to log HTTP requests @param log_file [String] path to where the server log should be located @param gc_requests [Boolean] whether to run a major GC after each scraper request",
    "label": "",
    "id": "1503"
  },
  {
    "raw_code": "def load_ca_certs_bundle(ca_certs_string)\n          return [] unless ca_certs_string\n\n          ca_certs_string.scan(CERT_REGEX).map do |ca_cert_string|\n            OpenSSL::X509::Certificate.new(ca_cert_string)\n          end",
    "comment": "In Ruby OpenSSL v3.0.0, this can be replaced by OpenSSL::X509::Certificate.load https://github.com/ruby/openssl/issues/254",
    "label": "",
    "id": "1504"
  },
  {
    "raw_code": "def sample\n          try_obtain_lease do\n            # Keep reporting the metrics while the lease is valid\n            # to ensure we have continuous data. Also check if the\n            # sampler is still running because a SIGTERM will cause\n            # the sleep to be interrupted and the loop to run again\n            # until the condition is false.\n            while running && exclusive_lease.same_uuid?\n              report_metrics\n\n              # Ensure that we don't sleep if the state changed\n              # after reporting metrics.\n              break unless running\n\n              Kernel.sleep(DEFAULT_SAMPLING_INTERVAL_SECONDS)\n            end",
    "comment": "The sleep ensures that: 1. Process A runs sampler and takes the lease 2. Other processes running sampler will not be able to take the lease, so they will be no-ops 3. While the lease still exists (for 5 minutes): a. The sampler writes the metrics b. The sampler sleeps for 30s c. We hope scrapes happen here (occur every minute), so we expect 4 or 5 scrapes for 1 sampler 4. Reset metrics to 0 5. The first other process picks up the lease, goto 1  Therefore we ensure that on every scrape, 1 process would report the correct data while the process that previously held lease report 0.",
    "label": "",
    "id": "1505"
  },
  {
    "raw_code": "def lease_timeout\n          LEASE_TIMEOUT\n        end",
    "comment": "Used by ExclusiveLeaseGuard",
    "label": "",
    "id": "1506"
  },
  {
    "raw_code": "def initialize(interval: nil, logger: Logger.new($stdout), warmup: false, **options)\n          interval ||= ENV[interval_env_key]&.to_i\n          interval ||= self.class::DEFAULT_SAMPLING_INTERVAL_SECONDS\n          interval_half = interval.to_f / 2\n\n          @interval = interval\n          @interval_steps = (-interval_half..interval_half).step(0.1).to_a\n\n          @logger = logger\n          @warmup = warmup\n\n          super(**options)\n        end",
    "comment": "interval - The sampling interval in seconds. warmup   - When true, takes a single sample eagerly before entering the sampling loop. This can be useful to ensure that all metrics files exist after `start` returns, since prometheus-client-mmap creates them lazily upon first access.",
    "label": "",
    "id": "1507"
  },
  {
    "raw_code": "def sleep_interval\n          while step = @interval_steps.sample\n            next if step == @last_step\n\n            @last_step = step\n\n            return @interval + @last_step\n          end",
    "comment": "Returns the sleep interval with a random adjustment.  The random adjustment is put in place to ensure we:  1. Don't generate samples at the exact same interval every time (thus potentially missing anything that happens in between samples). 2. Don't sample data at the same interval two times in a row.",
    "label": "",
    "id": "1508"
  },
  {
    "raw_code": "def bypass_session!(admin_id)\n          Gitlab::SafeRequestStore[CURRENT_REQUEST_BYPASS_SESSION_ADMIN_ID_RS_KEY] = admin_id\n          # Bypassing the session invalidates the cached value of admin_mode?\n          # Any new calls need to be re-computed.\n          uncache_admin_mode_state(admin_id)\n\n          Gitlab::AppLogger.debug(\"Bypassing session in admin mode for: #{admin_id}\")\n\n          return unless block_given?\n\n          begin\n            yield\n          ensure\n            reset_bypass_session!(admin_id)\n          end",
    "comment": "Admin mode activation requires storing a flag in the user session. Using this method when scheduling jobs in sessionless environments (e.g. Sidekiq, API) will bypass the session check for a user that was already in admin mode  If passed a block, it will surround the block execution and reset the session bypass at the end; otherwise you must remember to call '.reset_bypass_session!'",
    "label": "",
    "id": "1509"
  },
  {
    "raw_code": "def with_current_admin(admin)\n          return yield unless new(admin).admin_mode?\n\n          Gitlab::SafeRequestStore[CURRENT_REQUEST_ADMIN_MODE_USER_RS_KEY] = admin\n\n          Gitlab::AppLogger.debug(\"Admin mode active for: #{admin.username}\")\n\n          yield\n        ensure\n          Gitlab::SafeRequestStore.delete(CURRENT_REQUEST_ADMIN_MODE_USER_RS_KEY)\n        end",
    "comment": "Store in the current request the provided user model (only if in admin mode) and yield",
    "label": "",
    "id": "1510"
  },
  {
    "raw_code": "def optionally_run_in_admin_mode(user)\n          raise NonSidekiqEnvironmentError unless Gitlab::Runtime.sidekiq?\n\n          return yield unless Gitlab::CurrentSettings.admin_mode && user.can_access_admin_area?\n\n          bypass_session!(user.id) do\n            with_current_admin(user) do\n              yield\n            end",
    "comment": "Execute the given block with admin privileges if the user is an admin and admin mode is enabled. Otherwise, execute the block with regular user permissions.",
    "label": "",
    "id": "1511"
  },
  {
    "raw_code": "def admin_mode_rs_key\n        @admin_mode_rs_key ||= { res: :current_user_mode, user: user.id, method: :admin_mode? }\n      end",
    "comment": "RequestStore entry to cache #admin_mode? result",
    "label": "",
    "id": "1512"
  },
  {
    "raw_code": "def admin_mode_requested_rs_key\n        @admin_mode_requested_rs_key ||= { res: :current_user_mode, user: user.id, method: :admin_mode_requested? }\n      end",
    "comment": "RequestStore entry to cache #admin_mode_requested? result",
    "label": "",
    "id": "1513"
  },
  {
    "raw_code": "def privileged_runtime?\n        Gitlab::Runtime.rake? || Gitlab::Runtime.rails_runner? || Gitlab::Runtime.console?\n      end",
    "comment": "Runtimes which imply shell access get admin mode automatically, see Gitlab::Runtime",
    "label": "",
    "id": "1514"
  },
  {
    "raw_code": "def initialize(request)\n        @request = request\n      end",
    "comment": "@param [ActionDispatch::Request] request",
    "label": "",
    "id": "1515"
  },
  {
    "raw_code": "def valid_token_for_user!\n        user_public_key = signing_key_for_user!\n        openssh_public_key = convert_public_key_to_openssh_key!(user_public_key)\n\n        payload, header = decode_json_token!(user_public_key, openssh_public_key)\n        raise Gitlab::Auth::DpopValidationError, 'Unable to decode JWT' if payload.nil? || header.nil?\n\n        jwk = header['jwk']\n\n        begin\n          raise Gitlab::Auth::DpopValidationError, 'JWK contains private key' if JWT::JWK::RSA.import(jwk).private?\n\n          unless openssh_public_key.to_s == OpenSSL::PKey.read(JWT::JWK::RSA.import(jwk).public_key.to_pem).to_s\n            raise 'Failed to parse JWK: invalid JWK'\n          end",
    "comment": "Check that the DPoP is signed with a SSH key belonging to the user",
    "label": "",
    "id": "1516"
  },
  {
    "raw_code": "def algorithm_for_dpop_validation(key)\n        SUPPORTED_JWS_ALGORITHMS.each do |key_algorithm, jwt_algorithm|\n          return jwt_algorithm if key.start_with?(key_algorithm)\n        end",
    "comment": "Finds the algorithm from the public key to decode the JWT in valid_for_user!",
    "label": "",
    "id": "1517"
  },
  {
    "raw_code": "def valid_access_token_hash!\n        expected_hash = Base64.urlsafe_encode64(\n          Digest::SHA256.digest(personal_access_token_plaintext),\n          padding: false\n        )\n\n        return if ActiveSupport::SecurityUtils.secure_compare(token.payload['ath'], expected_hash)\n\n        raise Gitlab::Auth::DpopValidationError, 'Incorrect access token hash in JWT'\n      end",
    "comment": "Check that the DPoP contains a hash of the PAT being used. Users can have multiple PATs, so we still need to check that they created this DPoP for this particular PAT.",
    "label": "",
    "id": "1518"
  },
  {
    "raw_code": "def self.link_from_oauth_token(oauth_token)\n        fabricate(oauth_token.user).tap do |identity|\n          identity.link!(oauth_token.scope_user) if identity&.composite?\n        end",
    "comment": "TODO: why is this called 3 times in doorkeeper_access_spec.rb specs?",
    "label": "",
    "id": "1519"
  },
  {
    "raw_code": "def authorization_required?\n        false\n      end",
    "comment": "Require user authorization to link identity. False by default, enabled in specific subclasses.",
    "label": "",
    "id": "1520"
  },
  {
    "raw_code": "def identity\n        @identity ||= current_user.identities\n                                  .with_extern_uid(provider, uid)\n                                  .first_or_initialize(extern_uid: uid)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1521"
  },
  {
    "raw_code": "def find_user_from_warden\n        current_request.env['warden']&.authenticate if verified_request?\n      end",
    "comment": "Check the Rails session for valid authentication details",
    "label": "",
    "id": "1522"
  },
  {
    "raw_code": "def find_user_from_web_access_token(request_format, scopes: [:api])\n        return unless access_token && valid_web_access_format?(request_format)\n\n        validate_and_save_access_token!(scopes: scopes)\n\n        ::PersonalAccessTokens::LastUsedService.new(access_token).execute\n\n        access_token.user || raise(UnauthorizedError)\n      end",
    "comment": "We allow private access tokens with `api` scope to be used by web requests on RSS feeds or ICS files for backwards compatibility. It is also used by GraphQL/API requests. And to allow accessing /archive programatically as it was a big pain point for users https://gitlab.com/gitlab-org/gitlab/-/issues/28978. Used for release downloading as well",
    "label": "",
    "id": "1523"
  },
  {
    "raw_code": "def deploy_token_from_request\n        return unless route_authentication_setting[:deploy_token_allowed]\n        return unless Gitlab::ExternalAuthorization.allow_deploy_tokens_and_deploy_keys?\n\n        self.current_token = current_request.env[DEPLOY_TOKEN_HEADER].presence || parsed_oauth_token\n\n        if has_basic_credentials?(current_request)\n          _, self.current_token = user_name_and_password(current_request)\n        end",
    "comment": "This returns a deploy token, not a user since a deploy token does not belong to a user.  deploy tokens are accepted with deploy token headers and basic auth headers",
    "label": "",
    "id": "1524"
  },
  {
    "raw_code": "def verified_request?\n        Gitlab::RequestForgeryProtection.verified?(current_request.env)\n      end",
    "comment": "Check if the request is GET/HEAD, or if CSRF token is valid.",
    "label": "",
    "id": "1525"
  },
  {
    "raw_code": "def find_user_for_git_or_lfs_request\n        return unless git_or_lfs_request?\n\n        find_user_from_lfs_token || find_user_from_basic_auth_password\n      end",
    "comment": "To prevent Rack Attack from incorrectly rate limiting authenticated Git activity, we need to authenticate the user from other means (e.g. HTTP Basic Authentication) only if the request originated from a Git or Git LFS request. Repositories::GitHttpClientController or Repositories::LfsApiController normally does the authentication, but Rack Attack runs before those controllers.",
    "label": "",
    "id": "1526"
  },
  {
    "raw_code": "def unique_by_namespace(slug)\n        path = Namespaces::RandomizedSuffixPath.new(slug).to_s\n        Gitlab::Utils::Uniquify.new.string(path) do |s|\n          Namespace.all.find_by_path_or_name(s)\n        end",
    "comment": "decomposed from Namespace.clean_path",
    "label": "",
    "id": "1527"
  },
  {
    "raw_code": "def two_factor_authentication_required?\n        return false if allow_2fa_bypass_for_provider\n\n        Gitlab::CurrentSettings.require_two_factor_authentication? ||\n          current_user&.require_two_factor_authentication_from_group? ||\n          (Gitlab::CurrentSettings.require_admin_two_factor_authentication && current_user&.can_access_admin_area?)\n      end",
    "comment": "-- Admin mode does not matter in the context of verifying for two factor statuses",
    "label": "",
    "id": "1528"
  },
  {
    "raw_code": "def find_user_for_graphql_api_request\n        find_user_from_web_access_token(:api, scopes: graphql_authorization_scopes) ||\n          find_user_from_personal_access_token_for_api_or_git\n      end",
    "comment": "Use a minimal subset of find_user_from_any_authentication_method so only token types allowed for GraphQL can authenticate users. CI_JOB_TOKENs are not allowed for now, since their access is too broad.  Overridden in EE",
    "label": "",
    "id": "1529"
  },
  {
    "raw_code": "def graphql_authorization_scopes\n        [:api, :read_api]\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "1530"
  },
  {
    "raw_code": "def initialize(*args)\n          if args.length > 1\n            initialize_array(args)\n          else\n            initialize_string(args[0])\n          end",
    "comment": " Initialize a DN, escaping as required. Pass in attributes in name/value pairs. If there is a left over argument, it will be appended to the dn without escaping (useful for a base string).  Most uses of this class will be to escape a DN, rather than to parse it, so storing the dn as an escaped String and parsing parts as required with a state machine seems sensible.",
    "label": "",
    "id": "1531"
  },
  {
    "raw_code": "def each_pair\n          state = :key\n          key = StringIO.new\n          value = StringIO.new\n          hex_buffer = \"\"\n\n          @dn.each_char.with_index do |char, dn_index|\n            case state\n            when :key then\n              case char\n              when 'a'..'z', 'A'..'Z' then\n                state = :key_normal\n                key << char\n              when '0'..'9' then\n                state = :key_oid\n                key << char\n              when ' ' then state = :key\n              else raise(MalformedError, \"Unrecognized first character of an RDN attribute type name \\\"#{char}\\\"\")\n              end",
    "comment": " Parse a DN into key value pairs using ASN from https://www.rfc-editor.org/rfc/rfc2253 section 3. rubocop:disable Metrics/AbcSize rubocop:disable Metrics/CyclomaticComplexity rubocop:disable Metrics/PerceivedComplexity",
    "label": "",
    "id": "1532"
  },
  {
    "raw_code": "def to_a\n          a = []\n          self.each_pair { |key, value| a << key << value } unless @dn.empty?\n          a\n        end",
    "comment": " Returns the DN as an array in the form expected by the constructor.",
    "label": "",
    "id": "1533"
  },
  {
    "raw_code": "def to_s\n          @dn\n        end",
    "comment": " Return the DN as an escaped string.",
    "label": "",
    "id": "1534"
  },
  {
    "raw_code": "def to_normalized_s\n          self.class.new(*to_a).to_s.downcase\n        end",
    "comment": " Return the DN as an escaped and normalized string.",
    "label": "",
    "id": "1535"
  },
  {
    "raw_code": "def self.escape(string)\n          escaped = string.gsub(ESCAPE_RE) { |char| \"\\\\\" + char }\n          escaped.gsub(HEX_ESCAPE_RE) { |char| HEX_ESCAPES[char] }\n        end",
    "comment": " Escape a string for use in a DN value",
    "label": "",
    "id": "1536"
  },
  {
    "raw_code": "def method_missing(method, *args, &block)\n          @dn.send(method, *args, &block)\n        end",
    "comment": " Proxy all other requests to the string object, because a DN is mainly used within the library as a string rubocop:disable GitlabSecurity/PublicSend",
    "label": "",
    "id": "1537"
  },
  {
    "raw_code": "def respond_to?(sym, include_private = false)\n          @dn.respond_to?(sym, include_private)\n        end",
    "comment": " Redefined to be consistent with redefined `method_missing` behavior",
    "label": "",
    "id": "1538"
  },
  {
    "raw_code": "def find_user\n          find_by_uid_and_provider || find_by_email || build_new_user\n        end",
    "comment": "instance methods",
    "label": "",
    "id": "1539"
  },
  {
    "raw_code": "def sync_ssh_keys\n          options['sync_ssh_keys']\n        end",
    "comment": "The LDAP attribute in which the ssh keys are stored",
    "label": "",
    "id": "1540"
  },
  {
    "raw_code": "def self.normalize_uid(uid)\n          ::Gitlab::Auth::Ldap::DN.normalize_value(uid)\n        rescue ::Gitlab::Auth::Ldap::DN::FormatError => e\n          Gitlab::AppLogger.info(\"Returning original UID \\\"#{uid}\\\" due to error during normalization attempt: #{e.message}\")\n\n          uid\n        end",
    "comment": "Returns the UID in a normalized form.  1. Excess spaces are stripped 2. The string is downcased (for case-insensitivity)",
    "label": "",
    "id": "1541"
  },
  {
    "raw_code": "def attribute_value(attribute)\n          attributes = Array(config.attributes[attribute.to_s])\n          selected_attr = attributes.find { |attr| entry.respond_to?(attr) }\n\n          return unless selected_attr\n\n          entry.public_send(selected_attr) # rubocop:disable GitlabSecurity/PublicSend\n        end",
    "comment": "Using the LDAP attributes configuration, find and return the first attribute with a value. For example, by default, when given 'email', this method looks for 'mail', 'email' and 'userPrincipalName' and returns the first with a value.",
    "label": "",
    "id": "1542"
  },
  {
    "raw_code": "def access_token_create_response\n            # Returns '201 CREATED' on successful creation of a new access token.\n            strong_memoize(:access_token_create_response) do\n              post(\n                url: url('/login'),\n                body: {\n                  client_id: ::Gitlab.config.forti_token_cloud.client_id,\n                  client_secret: ::Gitlab.config.forti_token_cloud.client_secret\n                }.to_json\n              )\n            end",
    "comment": "TODO: Cache the access token: https://gitlab.com/gitlab-org/gitlab/-/issues/292437",
    "label": "",
    "id": "1543"
  },
  {
    "raw_code": "def login(login, password)\n          raise NotImplementedError\n        end",
    "comment": "Implementation must return user object if login successful",
    "label": "",
    "id": "1544"
  },
  {
    "raw_code": "def gitlab_username_claim\n          provider_args['gitlab_username_claim']&.to_sym\n        end",
    "comment": "Allow for configuring a custom username claim per provider from the auth hash or use the canonical username or nickname fields",
    "label": "",
    "id": "1545"
  },
  {
    "raw_code": "def generate_username(email)\n          return unless valid_email_username_length?(email)\n\n          username = mb_chars_unicode_normalize(email.match(/^[^@]*/)[0])\n          username if valid_email_username_length?(username)\n        end",
    "comment": "Get the first part of the email address (before @) In addition in removes illegal characters Perform length validation twice: - Before normalization to prevent normalizing excessively long strings - After normalization to ensure certain normalized multibyte characters don't exceed length.",
    "label": "",
    "id": "1546"
  },
  {
    "raw_code": "def valid_email_username_length?(email_or_username)\n          return true if email_or_username.length <= 254\n\n          errors[:identity_provider_email] = _(\"must be 254 characters or less.\")\n          false\n        end",
    "comment": "RFC 3606 and RFC 2821 restrict total email length to 254 characters. Do not allow longer emails to be passed in because unicode normalization can be intensive.",
    "label": "",
    "id": "1547"
  },
  {
    "raw_code": "def find_by_uid_and_provider(uid, provider)\n            identity = ::Identity.with_extern_uid(provider, uid).take\n\n            return unless identity\n            raise IdentityWithUntrustedExternUidError unless identity.trusted_extern_uid?\n\n            identity.user\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1548"
  },
  {
    "raw_code": "def find_by_email\n          return unless auth_hash.has_attribute?(:email)\n\n          ::User.find_by(email: auth_hash.email.downcase)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1549"
  },
  {
    "raw_code": "def auto_link_ldap_user?\n          Gitlab.config.omniauth.auto_link_ldap_user\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1550"
  },
  {
    "raw_code": "def documentation_link\n          ::Gitlab::Auth::OAuth::Provider\n            .config_for(provider.to_s)\n            &.dig('step_up_auth', scope.to_s, 'documentation_link')\n        end",
    "comment": "Returns the documentation link for this provider's step-up authentication configuration  @return [String, nil] the documentation link URL or nil if not configured",
    "label": "",
    "id": "1551"
  },
  {
    "raw_code": "def enabled_by_config?(scope: STEP_UP_AUTH_SCOPE_ADMIN_MODE)\n            oauth_providers.any? do |provider|\n              enabled_for_provider?(provider_name: provider, scope: scope)\n            end",
    "comment": "Checks if step-up authentication is enabled for the step-up auth scope 'admin_mode'  @return [Boolean] true if any OAuth provider requires step-up auth for admin mode",
    "label": "",
    "id": "1552"
  },
  {
    "raw_code": "def enabled_for_provider?(provider_name:, scope: STEP_UP_AUTH_SCOPE_ADMIN_MODE)\n            has_required_claims?(provider_name, scope) ||\n              has_included_claims?(provider_name, scope)\n          end",
    "comment": "Checks if step-up authentication configuration exists for a provider name  @param oauth_provider_name [String] the name of the OAuth provider @param scope [Symbol] the scope to check configuration for (default: :admin_mode) @return [Boolean] true if configuration exists",
    "label": "",
    "id": "1553"
  },
  {
    "raw_code": "def succeeded?(session, scope: STEP_UP_AUTH_SCOPE_ADMIN_MODE)\n            step_up_auth_flows(session)\n              .select do |step_up_auth_flow|\n                step_up_auth_flow.scope.to_s == scope.to_s\n              end",
    "comment": "Verifies if step-up authentication has succeeded for any provider with the step-up auth scope 'admin_mode'  @param session [Hash] the session hash containing authentication state @return [Boolean] true if step-up authentication is authenticated",
    "label": "",
    "id": "1554"
  },
  {
    "raw_code": "def conditions_fulfilled?(oauth_extra_metadata:, provider:, scope: STEP_UP_AUTH_SCOPE_ADMIN_MODE)\n            conditions = []\n\n            if has_required_claims?(provider, scope)\n              conditions << required_conditions_fulfilled?(oauth_extra_metadata: oauth_extra_metadata,\n                provider: provider, scope: scope)\n            end",
    "comment": "Validates if all step-up authentication conditions are met  @param oauth [OAuth2::AccessToken] the OAuth object to validate @param scope [Symbol] the scope to validate conditions for (default: :admin_mode) @return [Boolean] true if all conditions are fulfilled",
    "label": "",
    "id": "1555"
  },
  {
    "raw_code": "def slice_relevant_id_token_claims(oauth_raw_info:, provider:, scope: STEP_UP_AUTH_SCOPE_ADMIN_MODE)\n            relevant_id_token_claims = [\n              *get_id_token_claims_required_conditions(provider, scope)&.keys,\n              *get_id_token_claims_included_conditions(provider, scope)&.keys\n            ]\n            oauth_raw_info.slice(*relevant_id_token_claims)\n          end",
    "comment": "Slices the relevant ID token claims from the provided OAuth raw information.  @param oauth_raw_info [Hash] The raw information received from the OAuth provider. @param provider [String] The name of the OAuth provider. @param scope [String] The scope of the authentication request, default is STEP_UP_AUTH_SCOPE_ADMIN_MODE. @return [Hash] A hash containing only the relevant ID token claims.",
    "label": "",
    "id": "1556"
  },
  {
    "raw_code": "def failed_step_up_auth_flows(session, scope: STEP_UP_AUTH_SCOPE_ADMIN_MODE)\n            (step_up_auth_flows(session) || []).select do |flow|\n              flow.enabled_by_config? &&\n                flow.failed? &&\n                flow.scope.to_s == scope.to_s\n            end",
    "comment": "Returns step-up authentication flows that have failed  @param session [Hash] the session hash containing authentication state @param scope [Symbol] the scope to check (default: :admin_mode) @return [Array<StepUpAuthenticationFlow>] list of failed authentication flows",
    "label": "",
    "id": "1557"
  },
  {
    "raw_code": "def self.ok(ok_value)\n        new(ok_value: ok_value)\n      end",
    "comment": "The .ok and .err factory class methods are the only way to create a Result  \"self.ok\" corresponds to Ok(T) in Rust: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Ok  @param [Object, #new] ok_value @return [Result] noinspection MissingYardParamTag -- RubyMine does not recognize \"duck type\" Types (https://rubydoc.info/gems/yard/file/docs/Tags.md#duck-types). This has been reported to JetBrains - issue link pending",
    "label": "",
    "id": "1558"
  },
  {
    "raw_code": "def self.err(err_value)\n        new(err_value: err_value)\n      end",
    "comment": "\"self.err\" corresponds to Err(E) in Rust: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Err  @param [Object, #new] err_value @return [Result] noinspection MissingYardParamTag -- RubyMine does not recognize \"duck type\" Types (https://rubydoc.info/gems/yard/file/docs/Tags.md#duck-types). This has been reported to JetBrains - issue link pending",
    "label": "",
    "id": "1559"
  },
  {
    "raw_code": "def initialize(ok_value: nil, err_value: nil)\n        if (!ok_value.nil? && !err_value.nil?) || (ok_value.nil? && err_value.nil?)\n          raise(ArgumentError, 'Do not directly use private constructor, use Result.ok or Result.err')\n        end",
    "comment": "@param [Object, nil] ok_value @param [Object, nil] err_value @return [Object]",
    "label": "",
    "id": "1560"
  },
  {
    "raw_code": "def unwrap\n        ok? ? value : raise(\"Called Result#unwrap on an 'err' Result\")\n      end",
    "comment": "\"#unwrap\" corresponds to \"unwrap\" in Rust.  @return [Object] @raise [RuntimeError] if called on an \"err\" Result",
    "label": "",
    "id": "1561"
  },
  {
    "raw_code": "def unwrap_err\n        err? ? value : raise(\"Called Result#unwrap_err on an 'ok' Result\")\n      end",
    "comment": "\"#unwrap\" corresponds to \"unwrap\" in Rust.  @return [Object] @raise [RuntimeError] if called on an \"ok\" Result",
    "label": "",
    "id": "1562"
  },
  {
    "raw_code": "def ok?\n        # We don't make `@ok` an attr_reader, because we don't want to confusingly shadow the class method `.ok`\n        @ok\n      end",
    "comment": "The `ok?` attribute is true if the Result was constructed with .ok, and false if it was constructed with .err  \"#ok?\" corresponds to \"is_ok\" in Rust. @return [Boolean]",
    "label": "",
    "id": "1563"
  },
  {
    "raw_code": "def err?\n        !ok?\n      end",
    "comment": "The `err?` attribute is false if the Result was constructed with .ok, and true if it was constructed with .err \"#err?\" corresponds to \"is_err\" in Rust.  @return [Boolean]",
    "label": "",
    "id": "1564"
  },
  {
    "raw_code": "def and_then(lambda_or_singleton_method)\n        validate_lambda_or_singleton_method(callee: lambda_or_singleton_method, invoking_method: __method__)\n\n        # Return/passthough the Result itself if it is an err\n        return self if err?\n\n        # If the Result is ok, call the lambda or singleton method with the contained value\n        result = lambda_or_singleton_method.call(value)\n\n        unless result.is_a?(Result)\n          err_msg = \"Result##{__method__} expects a lambda or singleton method object which returns a 'Result' \" \\\n            \"type, but instead received '#{lambda_or_singleton_method.inspect}' which returned '#{result.class}'. \" \\\n            \"Check that the previous method calls in the '#and_then' chain are correct.\"\n          raise(TypeError, err_msg)\n        end",
    "comment": "`and_then` is a functional way to chain together operations which may succeed or have errors. It is passed a lambda or class (singleton) method object, and must return a Result object representing \"ok\" or \"err\".  If the Result object it is called on is \"ok\", then the passed lambda or singleton method is called with the value contained in the Result.  If the Result object it is called on is \"err\", then it is returned without calling the passed lambda or method.  It only supports being passed a lambda, or a class (singleton) method object which responds to `call` with a single argument (arity of 1). If multiple values are needed, pass a hash or array. Note that passing `Proc` objects is NOT supported, even though the YARD annotation contains `Proc` (because the type of a lambda is also `Proc`).  Passing instance methods to `and_then` is not supported, because the methods in the chain should be stateless \"pure functions\", and should not be persisting or referencing any instance state anyway.  \"#and_then\" corresponds to \"and_then\" in Rust: https://doc.rust-lang.org/std/result/enum.Result.html#method.and_then  @param [Proc, Method] lambda_or_singleton_method @return [Result] @raise [TypeError]",
    "label": "",
    "id": "1565"
  },
  {
    "raw_code": "def map(lambda_or_singleton_method)\n        validate_lambda_or_singleton_method(callee: lambda_or_singleton_method, invoking_method: __method__)\n\n        # Return/passthrough the Result itself if it is an err\n        return self if err?\n\n        # If the Result is ok, call the lambda or singleton method with the contained value\n        mapped_value = lambda_or_singleton_method.call(value)\n\n        if mapped_value.is_a?(Result)\n          err_msg = \"Result##{__method__} expects a lambda or singleton method object which returns an unwrapped \" \\\n            \"value, not a 'Result', but instead received '#{lambda_or_singleton_method.inspect}' which returned \" \\\n            \"a 'Result'.\"\n          raise(TypeError, err_msg)\n        end",
    "comment": "`map` is similar to `and_then`, but it is used for \"single track\" methods which always succeed, and have no possibility of returning an error (but they may still raise exceptions, which is unrelated to the Result handling). The passed lambda or singleton method must return a value, not a Result.  If the Result object it is called on is \"ok\", then the passed lambda or singleton method is called with the value contained in the Result.  If the Result object it is called on is \"err\", then it is returned without calling the passed lambda or method.  \"#map\" corresponds to \"map\" in Rust: https://doc.rust-lang.org/std/result/enum.Result.html#method.map  @param [Proc, Method] lambda_or_singleton_method @return [Result] @raise [TypeError]",
    "label": "",
    "id": "1566"
  },
  {
    "raw_code": "def map_err(lambda_or_singleton_method)\n        validate_lambda_or_singleton_method(callee: lambda_or_singleton_method, invoking_method: __method__)\n\n        # Return/passthrough the Result itself if it is an ok\n        return self if ok?\n\n        # If the Result is err, call the lambda or singleton method with the contained value\n        mapped_value = lambda_or_singleton_method.call(value)\n\n        if mapped_value.is_a?(Result)\n          err_msg = \"Result##{__method__} expects a lambda or singleton method object which returns an unwrapped \" \\\n            \"value, not a 'Result', but instead received '#{lambda_or_singleton_method.inspect}' which returned \" \\\n            \"a 'Result'.\"\n          raise(TypeError, err_msg)\n        end",
    "comment": "`map_err` is the inverse of `map`. It behaves identically, but it only processes `err` values instead of `ok` values.  \"#map_err\" corresponds to \"map_err\" in Rust: https://doc.rust-lang.org/std/result/enum.Result.html#method.map_err  @param [Proc, Method] lambda_or_singleton_method @return [Result] @raise [TypeError]",
    "label": "",
    "id": "1567"
  },
  {
    "raw_code": "def inspect_ok(lambda_or_singleton_method, enforce_immutability: true)\n        validate_lambda_or_singleton_method(callee: lambda_or_singleton_method, invoking_method: __method__)\n\n        # Return/passthrough the Result itself if it is an err\n        return self if err?\n\n        # If the Result is ok, call the lambda or singleton method with the contained value\n        call_and_optionally_enforce_value_is_not_mutated(\n          callee: lambda_or_singleton_method,\n          value: value,\n          invoking_method: __method__,\n          enforce_immutability: enforce_immutability\n        )\n\n        # Return/passthrough the original Result\n        self\n      end",
    "comment": "`inspect_ok` is similar to `map`, becuase it receives the wrapped `ok` value, but it does not allow modification of the value like `map`. The original result is always returned from `inspect_ok`.  The passed lambda or singleton method must return, `nil`, to enforce the fact that the return value is ignored, and the original Result is always returned. This corresponds to the `void` type in YARD/RBS type annotations, and the `unit` type in Rust (https://doc.rust-lang.org/std/primitive.unit.html).  If the passed method does not return `nil`, an error will be raised.  \"#inspect_ok\" corresponds to \"inspect\" in Rust: https://doc.rust-lang.org/std/result/enum.Result.html#method.inspect  If enforce_immutability is true, the passed method will not be allowed to mutate the value passed to it.  Note that we could not call this method `inspect` to match the Rust Result.inspect function, because that would conflict with the Kernel#inspect method in Ruby.  @param [Proc, Method] lambda_or_singleton_method @param [Boolean] enforce_immutability enforces immutability of the value passed to lambda_or_singleton_method @return [Result] @raise [TypeError]",
    "label": "",
    "id": "1568"
  },
  {
    "raw_code": "def inspect_err(lambda_or_singleton_method, enforce_immutability: true)\n        validate_lambda_or_singleton_method(callee: lambda_or_singleton_method, invoking_method: __method__)\n\n        # Return/passthrough the Result itself if it is an ok\n        return self if ok?\n\n        # If the Result is err, call the lambda or singleton method with the contained value\n        call_and_optionally_enforce_value_is_not_mutated(\n          callee: lambda_or_singleton_method,\n          value: value,\n          invoking_method: __method__,\n          enforce_immutability: enforce_immutability\n        )\n\n        # Return/passthrough the original Result\n        self\n      end",
    "comment": "`inspect_err` is the inverse of `inspect_ok`. It behaves identically, but it only processes `err` values instead of `ok` values.  The passed lambda or singleton method must return, `nil`, to enforce the fact that the return value is ignored, and the original Result is always returned. This corresponds to the `void` type in YARD/RBS type annotations, and the `unit` type in Rust (https://doc.rust-lang.org/std/primitive.unit.html).  If the passed method does not return `nil`, an error will be raised.  If enforce_immutablity is true, the passed method will not be allowed to mutate the value passed to it.  \"#inspect_err\" corresponds to \"inspect_err\" in Rust: https://doc.rust-lang.org/std/result/enum.Result.html#method.inspect_err  @param [Proc, Method] lambda_or_singleton_method @param [Boolean] enforce_immutability enforces immutability of the value passed to lambda_or_singleton_method @return [Result] @raise [TypeError]",
    "label": "",
    "id": "1569"
  },
  {
    "raw_code": "def to_h\n        ok? ? { ok: value } : { err: value }\n      end",
    "comment": "`to_h` supports destructuring of a result object, for example: `result => { ok: }; puts ok`  @return [Hash]",
    "label": "",
    "id": "1570"
  },
  {
    "raw_code": "def deconstruct_keys(keys)\n        raise(ArgumentError, 'Use either :ok or :err for pattern matching') unless [[:ok], [:err]].include?(keys)\n\n        to_h\n      end",
    "comment": "`deconstruct_keys` supports pattern matching on a Result object with a `case` statement. See specs for examples.  @param [Array] keys @return [Hash] @raise [ArgumentError]",
    "label": "",
    "id": "1571"
  },
  {
    "raw_code": "def ==(other)\n        # NOTE: The underlying `@ok` instance variable is a boolean, so we only need to check `ok?`, not `err?` too\n        self.class == other.class && other.ok? == ok? && other.instance_variable_get(:@value) == value\n      end",
    "comment": "@param [Result] other @return [Boolean]",
    "label": "",
    "id": "1572"
  },
  {
    "raw_code": "def value # rubocop:disable Style/TrivialAccessors -- We are not using attr_reader here, so we can avoid nilability type errors in RubyMine\n        # TODO: We are not using attr_reader here, so we can avoid nilability type errors in RubyMine.\n        #   This will be reported to JetBrains and tracked on\n        #   https://handbook.gitlab.com/handbook/tools-and-tips/editors-and-ides/jetbrains-ides/tracked-jetbrains-issues/,\n        #   this comment should then be updated with the issue link on that page.\n        #   Note that we don't use `noinspection` to suppress this error because there's several instances where\n        #   this error leaks to other classes through the call stack.\n        @value\n      end",
    "comment": "The `value` attribute will contain either the ok_value or the err_value  @return [Object] noinspection RubyMismatchedReturnType",
    "label": "",
    "id": "1573"
  },
  {
    "raw_code": "def validate_lambda_or_singleton_method(callee:, invoking_method:)\n        is_lambda = callee.is_a?(Proc) && callee.lambda?\n        is_singleton_method =\n          callee.is_a?(Method) && callee.owner.singleton_class?\n\n        unless is_lambda || is_singleton_method\n          err_msg = \"Result##{invoking_method} expects a lambda or singleton method object, \" \\\n            \"but instead received '#{callee.inspect}'.\"\n          raise(TypeError, err_msg)\n        end",
    "comment": "@param [Proc, Method] callee @param [Symbol] invoking_method @return [void] @raise [TypeError]",
    "label": "",
    "id": "1574"
  },
  {
    "raw_code": "def call_and_optionally_enforce_value_is_not_mutated(callee:, value:, invoking_method:, enforce_immutability:)\n        return_value_from_call =\n          if enforce_immutability\n            # If enforce_immutability is true, deep clone and freeze the value before passing it,\n            # so that the callee can't mutate the original value\n            begin\n              callee.call(deep_clone_and_freeze(value))\n            rescue FrozenError => e\n              err_msg = \"ERROR: #{callee} must not modify the passed value argument, because it was invoked via \" \\\n                \"Result##{invoking_method}. Ensure that no side effects are being performed which modify any \" \\\n                \"properties of nested objects, such as lazy memoization. \" \\\n                \"Alternately, you can pass 'enforce_immutability: false' to Result##{invoking_method}.\"\n              raise e.exception(\"#{e.message}. #{err_msg}\")\n            end",
    "comment": "@param [Proc, Method] callee @param [Object] value @param [Symbol] invoking_method @param [Boolean] enforce_immutability enforces immutability of the value passed to lambda_or_singleton_method @return [void] @raise [RuntimeError]",
    "label": "",
    "id": "1575"
  },
  {
    "raw_code": "def deep_clone_and_freeze(object)\n        case object\n        when Array\n          object.map { |entry| deep_clone_and_freeze(entry) }.freeze\n        when Hash\n          object.transform_values { |entry| deep_clone_and_freeze(entry) }.freeze\n        else\n          object.clone(freeze: true)\n        end",
    "comment": "@param [Object] object @return [Object]",
    "label": "",
    "id": "1576"
  },
  {
    "raw_code": "def validate_return_value_is_void(return_value:, invoking_method:)\n        return if return_value.nil?\n\n        err_msg = \"The method passed to Result##{invoking_method} must always return 'nil' (void). This enforces \" \\\n          \"that the return value is never used or modified. The existing 'Result' object is always passed along the \" \\\n          \"chain unchanged. The return value received was '#{return_value.inspect}' instead of 'nil'.\"\n        raise(TypeError, err_msg)\n      end",
    "comment": "@param [Proc, Method] return_value @param [Symbol] invoking_method @return [void] @raise [TypeError]",
    "label": "",
    "id": "1577"
  },
  {
    "raw_code": "def initialize(content = {})\n        raise ArgumentError, 'content must be a Hash' unless content.is_a?(Hash)\n\n        @content = content\n      end",
    "comment": "@param [Hash] content @return [Message] raise [ArgumentError] if content is not a Hash",
    "label": "",
    "id": "1578"
  },
  {
    "raw_code": "def ==(other)\n        self.class == other.class && content == other.content\n      end",
    "comment": "@param [Gitlab::Fp::Message] other @return [TrueClass, FalseClass]",
    "label": "",
    "id": "1579"
  },
  {
    "raw_code": "def to_s\n        inspect\n      end",
    "comment": "@return [String]",
    "label": "",
    "id": "1580"
  },
  {
    "raw_code": "def self.extended(base)\n        base.class_eval do\n          private_class_method :retrieve_single_public_singleton_method, :public_singleton_methods_to_ignore\n        end",
    "comment": "@param [Class] base @return void",
    "label": "",
    "id": "1581"
  },
  {
    "raw_code": "def retrieve_single_public_singleton_method(fp_module_or_class)\n        fp_class_singleton_methods = fp_module_or_class.singleton_methods(false)\n        public_singleton_methods = fp_class_singleton_methods - public_singleton_methods_to_ignore\n\n        # Note: Intentionally using Array#[] instead of Array#first here, because there appears to be a bug\n        #       in the type declaration, that doesn't indicate that #first should have `implicitly-returns-nil`\n        #       behavior. See https://github.com/ruby/rbs/pull/1226, this probably needs to be fixed for #first too.\n        #       Until then, we use #[] to avoid type inspection warnings in RubyMine.\n        return public_singleton_methods[0] if public_singleton_methods.size == 1\n\n        fp_doc_link =\n          \"https://gitlab.com/gitlab-org/gitlab/-/blob/master/ee/lib/remote_development/README.md#functional-patterns\"\n\n        rop_doc_link =\n          \"https://gitlab.com/gitlab-org/gitlab/-/blob/master/ee/lib/remote_development/README.md#railway-oriented-programming-and-the-result-class\"\n\n        if public_singleton_methods.size > 1\n          err_msg =\n            \"Railway Oriented Programming (ROP) pattern violation in class `#{fp_module_or_class}`. \" \\\n              \"Expected exactly one (1) public entry point singleton/class method to be present \" \\\n              \"in a class which is used with the ROP pattern, but \" \\\n              \"#{public_singleton_methods.size} \" \\\n              \"public singleton methods were found: #{public_singleton_methods.sort.join(', ')}. \" \\\n              \"You can make the non-entry-point method(s) private via `private_class_method :method_name`. \" \\\n              \"See #{fp_doc_link} and #{rop_doc_link} for more information.\"\n          raise(ArgumentError, err_msg)\n        end",
    "comment": "@param [Class] fp_module_or_class @raise [RuntimeError] @return [Symbol]",
    "label": "",
    "id": "1582"
  },
  {
    "raw_code": "def public_singleton_methods_to_ignore\n        # Singleton methods added by other libraries that we need to ignore.\n        Module.singleton_methods(false) +\n          Class.singleton_methods(false) +\n          [:_] # NOTE: `_` (from GettextI18nRails) is ignored because we mock it globally in fast_spec_helper\n      end",
    "comment": "@return [Array<Symbol>]",
    "label": "",
    "id": "1583"
  },
  {
    "raw_code": "def self.extended(base)\n        base.class_eval do\n          private_class_method :generate_error_response_from_message\n        end",
    "comment": "@param [Class] base @return void",
    "label": "",
    "id": "1584"
  },
  {
    "raw_code": "def generate_error_response_from_message(message:, reason:)\n        details_string =\n          case message.content\n          in {}\n            nil\n          in { details: String => error_details }\n            error_details\n          in { details: Array => error_details }\n            # NOTE: This join string intentionally has two spaces, so that it is a unique pattern which can be\n            # split and parsed on the client if desired, to be presented in separate HTML elements.\n            # We may eventually return an array of error messages, but this is a workaround for now.\n            error_details.join(\",  \")\n          in { errors: ActiveModel::Errors => errors }\n            errors.full_messages.join(\",  \")\n          else\n            raise \"Unexpected message content, add a case to pattern match it and convert it to a String.\"\n          end",
    "comment": "@param [Gitlab::Fp::Message] message @param [Symbol] reason @return [Hash]",
    "label": "",
    "id": "1585"
  },
  {
    "raw_code": "def initialize(result:)\n        msg = \"Failed to pattern match #{result.ok? ? \"'ok'\" : \"'err'\"} Result \" \\\n          \"containing message of type: #{(result.ok? ? result.unwrap : result.unwrap_err).class.name}\"\n\n        super(msg)\n      end",
    "comment": "@param [Gitlab::Fp::Result] result @return [void]",
    "label": "",
    "id": "1586"
  },
  {
    "raw_code": "def get(setting_names, options = {})\n          unless setting_names.is_a?(Array) && setting_names.all?(Symbol)\n            raise \"setting_names arg must be an Array of Symbols\"\n          end",
    "comment": "@param [Array<Symbol>] setting_names @param [Hash] options @return [Object] @raise [RuntimeError]",
    "label": "",
    "id": "1587"
  },
  {
    "raw_code": "def get_single_setting(setting_name, options = {})\n          get([setting_name], options).fetch(setting_name)\n        end",
    "comment": "@param [Symbol] setting_name @param [Hash] options @return [Object] @raise [RuntimeError]",
    "label": "",
    "id": "1588"
  },
  {
    "raw_code": "def self.resolve(setting_names, dependencies)\n          result = []\n          visited = Set.new\n          queue = setting_names.clone\n\n          until queue.empty?\n            setting_name = queue.shift\n\n            # Go to the next item in the queue if we've already seen this one\n            next if visited.include?(setting_name)\n\n            visited.add(setting_name)\n            result.push(setting_name)\n\n            setting_dependencies = dependencies[setting_name]\n            queue.push(*setting_dependencies) if setting_dependencies\n          end",
    "comment": "@param [Array] setting_names An array of setting names @param [Hash] dependencies A hash of setting names to array of dependent setting names @return [Array] An array of setting names + all recursive dependencies",
    "label": "",
    "id": "1589"
  },
  {
    "raw_code": "def self.process(context)\n          return Gitlab::Fp::Result.ok(context) if Rails.env.production?\n\n          context => {\n            env_var_prefix: String => env_var_prefix,\n            env_var_failed_message_class: Class => env_var_failed_message_class,\n          }\n\n          err_result = nil\n          context[:settings].each_key do |setting_name|\n            env_var_name = \"#{env_var_prefix}_#{setting_name.to_s.upcase}\"\n            env_var_value_string = ENV[env_var_name]\n\n            # If there is no matching ENV var, break the loop and go to the next setting\n            next unless env_var_value_string\n\n            begin\n              env_var_value = cast_value(\n                env_var_name: env_var_name,\n                env_var_value_string: env_var_value_string,\n                setting_type: context[:setting_types][setting_name]\n              )\n            rescue RuntimeError => e\n              # err_result will be set to a non-nil Gitlab::Fp::Result.err if casting fails\n              err_result = Gitlab::Fp::Result.err(env_var_failed_message_class.new(details: e.message))\n            end",
    "comment": "@param [Hash] context @return [Gitlab::Fp::Result]",
    "label": "",
    "id": "1590"
  },
  {
    "raw_code": "def self.cast_value(env_var_name:, env_var_value_string:, setting_type:)\n          # noinspection RubyIfCanBeCaseInspection -- This cannot be a case statement - see discussion here https://gitlab.com/gitlab-org/gitlab/-/merge_requests/148287#note_1849160293\n          if setting_type == String\n            env_var_value_string\n          elsif setting_type == Integer\n            # NOTE: The following line works because String#to_i does not raise exceptions for non-integer values\n            unless env_var_value_string.to_i.to_s == env_var_value_string\n              raise \"ENV var '#{env_var_name}' value could not be cast to Integer type.\"\n            end",
    "comment": "@param [String] env_var_name @param [String] env_var_value_string @param [Class] setting_type @return [Object] @raise [RuntimeError]",
    "label": "",
    "id": "1591"
  },
  {
    "raw_code": "def self.parse_json(env_var_name:, value:)\n          # noinspection InvalidCallToProtectedPrivateMethod - See https://handbook.gitlab.com/handbook/tools-and-tips/editors-and-ides/jetbrains-ides/code-inspection/why-are-there-noinspection-comments/\n          Oj.load(value, mode: :rails, symbol_keys: true)\n        rescue EncodingError => e\n          raise \"ENV var '#{env_var_name}' value was not valid parseable JSON. Parse error was: '#{e.message}'\"\n        end",
    "comment": "@param [String] env_var_name @param [String] value @return [Object, Array] @raise [EncodingError]",
    "label": "",
    "id": "1592"
  },
  {
    "raw_code": "def self.parse(\n          module_name:,\n          requested_setting_names:,\n          default_settings:,\n          mutually_dependent_settings_groups: []\n        )\n          settings = {}\n          setting_types = {}\n\n          default_settings.each do |setting_name, setting_value_and_type|\n            next unless requested_setting_names.include?(setting_name)\n\n            unless setting_value_and_type.is_a?(Array) && setting_value_and_type.length == 2\n              raise \"#{module_name} Setting entry for '#{setting_name}' must \" \\\n                \"be a two-element array containing the value and type.\"\n            end",
    "comment": "@param [String] module_name @param [Array] requested_setting_names @param [Hash] default_settings @param [Array] mutually_dependent_settings_groups @return [Array<(Hash, Hash)>] settings, setting_types @raise [RuntimeError]",
    "label": "",
    "id": "1593"
  },
  {
    "raw_code": "def self.validate_mutually_dependent_settings(\n          requested_setting_names:,\n          mutually_dependent_settings:,\n          default_setting_names:\n        )\n          # No blank line here (satisfies Layout/EmptyLinesAroundMethodBody)\n\n          if (mutually_dependent_settings - default_setting_names).any?\n            raise \"Unknown mutually dependent setting(s): \" \\\n              \"#{(mutually_dependent_settings - default_setting_names).join(', ')}\"\n          end",
    "comment": "@param [array] requested_setting_names @param [array] mutually_dependent_settings @param [array] default_setting_names @return boolean @raise [RuntimeError]",
    "label": "",
    "id": "1594"
  },
  {
    "raw_code": "def self.validate_setting_type(setting_value, setting_type, setting_name, module_name)\n          return true if setting_type == :Boolean && (setting_value == true || setting_value == false)\n\n          # noinspection RubyMismatchedArgumentType -- RubyMine type checker doesn't recognize guard clause above\n          return true if setting_type != :Boolean && setting_value.is_a?(setting_type)\n\n          # NOTE: We are raising an exception here instead of returning a Result.err, because this is\n          # a coding syntax error in the 'default_settings', not a user or data error.\n          raise \"#{module_name} Setting '#{setting_name}' has a type of '#{setting_value.class}', \" \\\n            \"which does not match declared type of '#{setting_type}'.\"\n        end",
    "comment": "@param [Object] setting_value @param [Class, Symbol] setting_type @param [Symbol] setting_name @param [String] module_name @return boolean @raise [RuntimeError]",
    "label": "",
    "id": "1595"
  },
  {
    "raw_code": "def build(event)\n        [\n          timestamps_data,\n          event_data(event),\n          user_data,\n          event_specific_user_data(event)\n        ].reduce(:merge)\n      end",
    "comment": "Sample data { :created_at=>\"2021-04-02T10:00:26Z\", :updated_at=>\"2021-04-02T10:00:26Z\", :event_name=>\"user_create\", :name=>\"John Doe\", :email=>\"john@example.com\", :user_id=>1, :username=>\"johndoe\" }",
    "label": "",
    "id": "1596"
  },
  {
    "raw_code": "def build(event)\n        [\n          timestamps_data,\n          project_member_data,\n          event_data(event)\n        ].reduce(:merge)\n      end",
    "comment": "Sample data { :created_at=>\"2021-03-02T10:43:17Z\", :updated_at=>\"2021-03-02T10:43:17Z\", :project_name=>\"gitlab\", :project_path=>\"gitlab\", :project_path_with_namespace=>\"namespace1/gitlab\", :project_id=>1, :user_username=>\"johndoe\", :user_name=>\"John Doe\", :user_email=>\"john@example.com\", :user_id=>2, :access_level=>\"Developer\", :project_visibility=>\"internal\", :event_name=>\"user_update_for_team\" }",
    "label": "",
    "id": "1597"
  },
  {
    "raw_code": "def build(event)\n        [\n          timestamps_data,\n          event_data(event),\n          group_data,\n          event_specific_group_data(event)\n        ].reduce(:merge)\n      end",
    "comment": "Sample data { :created_at=>\"2021-01-20T09:40:12Z\", :updated_at=>\"2021-01-20T09:40:12Z\", :event_name=>\"group_rename\", :name=>\"group1\", :path=>\"group1\", :full_path=>\"group1\", :group_id=>1, :old_path=>\"old-path\", :old_full_path=>\"old-path\" }",
    "label": "",
    "id": "1598"
  },
  {
    "raw_code": "def build(event, include_deprecated_owner: false)\n        [\n          event_data(event),\n          timestamps_data,\n          project_data(include_deprecated_owner: include_deprecated_owner),\n          event_specific_project_data(event)\n        ].reduce(:merge)\n      end",
    "comment": "Sample data { event_name: \"project_rename\", created_at: \"2021-04-19T07:05:36Z\", updated_at: \"2021-04-19T07:05:36Z\", name: \"my_project\", path: \"my_project\", path_with_namespace: \"namespace2/my_project\", project_id: 1, owner_name: \"John\", owner_email: \"user1@example.org\", owners: [name: \"John\", email: \"user1@example.org\"], project_visibility: \"internal\", old_path_with_namespace: \"old-path-with-namespace\" }",
    "label": "",
    "id": "1599"
  },
  {
    "raw_code": "def build(event)\n        [\n          event_data(event),\n          timestamps_data,\n          key_data,\n          user_data\n        ].reduce(:merge)\n      end",
    "comment": "Sample data { event_name: \"key_create\", created_at: \"2021-04-19T06:13:24Z\", updated_at: \"2021-04-19T06:13:24Z\", key: \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQClDn/5BaESHlSb3NxQtiUc0BXgK6lsqdAUIdS3lwZ2gbACDhtoLYnc+qhZ4b8gWzE+2A8RmkvLe98T7noRoW4DAYs67NSqMs/kXd2ESPNV8qqv0u7tCxPz+c7DaYp2oC/avlxVQ2AeULZLCEwalYZ7irde0EZMeTwNIRu5s88gOw== dummy@gitlab.com\", id: 1, username: \"johndoe\" }",
    "label": "",
    "id": "1600"
  },
  {
    "raw_code": "def build(event)\n        [\n          timestamps_data,\n          group_member_data,\n          event_data(event)\n        ].reduce(:merge)\n      end",
    "comment": "Sample data { :event_name=>\"user_add_to_group\", :group_name=>\"GitLab group\", :group_path=>\"gitlab\", :group_id=>1, :user_username=>\"robert\", :user_name=>\"Robert Mills\", :user_email=>\"robert@example.com\", :user_id=>14, :group_access=>\"Guest\", :created_at=>\"2020-11-04T10:12:10Z\", :updated_at=>\"2020-11-04T10:12:10Z\", :expires_at=>\"2020-12-04T10:12:10Z\" }",
    "label": "",
    "id": "1601"
  },
  {
    "raw_code": "def self.render(pdf, data: {})\n          new(pdf, data).render\n        end",
    "comment": "rubocop:enable  Layout/LineLength",
    "label": "",
    "id": "1602"
  },
  {
    "raw_code": "def render\n          return :noop if @data.blank?\n\n          @pdf.bounding_box([0, @y], width: @pdf.bounds.right, height: @height) do\n            # draw the slightly off-white background\n            @pdf.save_graphics_state\n            @pdf.fill_color \"F9F9F9\"\n            @pdf.fill_rectangle [0 - 10, @pdf.bounds.top], @pdf.bounds.right + 10, @height\n            @pdf.restore_graphics_state\n\n            @pdf.move_down 10\n\n            # rubocop:disable Layout/LineLength -- long text for title\n            # draw the title\n            @pdf.text_box(\n              s_('Vulnerability History'),\n              at: [0, @pdf.cursor],\n              width: @pdf.bounds.right, height: 20,\n              align: :left, style: :bold, size: 14)\n\n            @pdf.move_down 20\n\n            @pdf.text_box(\n              s_(\"Historical view of open vulnerabilities in the default branch. Excludes vulnerabilities that were resolved or dismissed.\"),\n              at: [0, @pdf.cursor],\n              width: @pdf.bounds.right, height: 20,\n              align: :left, size: 10)\n            # rubocop:enable  Layout/LineLength\n\n            @pdf.move_down 10\n\n            @pdf.bounding_box([0, @pdf.cursor], width: @pdf.bounds.right, height: @height - 40) do\n              # SVG from the frontend\n              @pdf.svg @data, width: @pdf.bounds.right, height: @svg_height\n\n              @pdf.move_down 20\n              legend_y = @pdf.cursor\n\n              # draw a divider line\n              @pdf.save_graphics_state\n              @pdf.stroke_color \"dddddd\"\n              @pdf.stroke_line([0, legend_y + 20], [@pdf.bounds.right, legend_y + 20])\n              @pdf.restore_graphics_state\n\n              # draw the SVG chart's legend\n              legend_line_width = 8\n              legend_line_right_padding = 5\n              legend_item_width = legend_line_width + legend_line_right_padding + 40\n              start_x = @pdf.bounds.left + 30\n\n              SEVERITY_LEGEND.each_with_index do |severity, index|\n                x_position = start_x + (index * legend_item_width)\n\n                @pdf.save_graphics_state\n                @pdf.stroke_color severity[:color]\n                @pdf.line_width = 2\n                @pdf.stroke_line([x_position, legend_y], [x_position + legend_line_width, legend_y])\n\n                @pdf.fill_color '000000'\n                x_position += legend_line_width + legend_line_right_padding\n\n                @pdf.text_box(\n                  severity[:name],\n                  at: [x_position, legend_y + 7],\n                  width: legend_item_width, height: 15,\n                  valign: :center, style: :bold, size: 8)\n                @pdf.restore_graphics_state\n              end",
    "comment": "rubocop:disable Metrics/AbcSize --- this is the nature of the dsl.",
    "label": "",
    "id": "1603"
  },
  {
    "raw_code": "def sort_projects(projects)\n          return projects if projects.blank?\n\n          projects.sort_by do |project|\n            severities = project[\"vulnerabilitySeveritiesCount\"]\n            [\n              -severities[\"critical\"],\n              -severities[\"high\"],\n              -severities[\"medium\"],\n              -severities[\"low\"],\n              -severities[\"info\"],\n              -severities[\"unknown\"]\n            ]\n          end.first(5)\n        end",
    "comment": "TODO: Once the below issue is resolved, we can likely delete this sorting as the projects should arrive to us sorted: https://gitlab.com/gitlab-org/gitlab/-/issues/545479",
    "label": "",
    "id": "1604"
  },
  {
    "raw_code": "def batch_clean(start_id: nil, stop_id: nil, dry_run: true, sleep_time: nil, uploader: nil, since: nil)\n        relation = Upload.where('lower(path) like ? or lower(path) like ? or lower(path) like ?',\n          '%.jpg', '%.jpeg', '%.tiff')\n        relation = relation.where(uploader: uploader) if uploader\n        relation = relation.where('created_at > ?', since) if since\n\n        logger.info \"running in dry run mode, no images will be rewritten\" if dry_run\n\n        find_params = {\n          start: start_id.present? ? start_id.to_i : nil,\n          finish: stop_id.present? ? stop_id.to_i : Upload.last&.id,\n          batch_size: 1000\n        }\n\n        relation.find_each(**find_params) do |upload|\n          clean(upload.retrieve_uploader, dry_run: dry_run)\n          sleep sleep_time if sleep_time\n        rescue StandardError => err\n          logger.error \"failed to sanitize #{upload_ref(upload)}: #{err.message}\"\n          logger.debug err.backtrace.join(\"\\n \")\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1605"
  },
  {
    "raw_code": "def clean(uploader, dry_run: true)\n        Dir.mktmpdir('gitlab-exif') do |tmpdir|\n          src_path = fetch_upload_to_file(uploader, tmpdir)\n\n          to_remove = extra_tags(src_path)\n\n          if to_remove.empty?\n            logger.info \"#{upload_ref(uploader.upload)}: only whitelisted tags present, skipping\"\n            break\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1606"
  },
  {
    "raw_code": "def resolve!(_placeholders = {})\n        self\n      end",
    "comment": "Present for compatibility with license templates, which can replace text like `[fullname]` with a user-specified string. This is a no-op for other templates",
    "label": "",
    "id": "1607"
  },
  {
    "raw_code": "def categories\n          {}\n        end",
    "comment": "Set categories as sub directories Example: { \"category_name_1\" => \"directory_path_1\", \"category_name_2\" => \"directory_name_2\" } Default is no category with all files in base dir of each class",
    "label": "",
    "id": "1608"
  },
  {
    "raw_code": "def finder(project = nil)\n          raise NotImplementedError\n        end",
    "comment": "Defines which strategy will be used to get templates files RepoTemplateFinder - Finds templates on project repository, templates are filtered per project GlobalTemplateFinder - Finds templates on gitlab installation source, templates can be used in all projects",
    "label": "",
    "id": "1609"
  },
  {
    "raw_code": "def repository_template_names(project)\n          template_names_by_category(self.all(project))\n        end",
    "comment": "`repository_template_names` - reads through Gitaly the actual templates names within a given project's repository. This is only used by issue and merge request templates, that need to call this once and then cache the returned value.  `template_names` - is an alias to `repository_template_names`. It would read through Gitaly the actual template names within a given project's repository for all file templates other than `issue` and `merge request` description templates, which would instead overwrite the `template_names` method to return a redis cached version, by reading cached values from `repository.issue_template_names_hash` and `repository.merge_request_template_names_hash` methods.",
    "label": "",
    "id": "1610"
  },
  {
    "raw_code": "def via(pool)\n            in_via_state = Thread.current[:inside_sidekiq_via_scope]\n            Thread.current[:inside_sidekiq_via_scope] = true\n\n            super(pool)\n          ensure\n            Thread.current[:inside_sidekiq_via_scope] = in_via_state\n          end",
    "comment": "Sets inside_sidekiq_via_scope state to true to avoid error when validation is called",
    "label": "",
    "id": "1611"
  },
  {
    "raw_code": "def allow_unrouted_sidekiq_calls\n          currently_allowed = Thread.current[:allow_unrouted_sidekiq_calls]\n          Thread.current[:allow_unrouted_sidekiq_calls] = true\n\n          yield\n        ensure\n          Thread.current[:allow_unrouted_sidekiq_calls] = currently_allowed\n        end",
    "comment": "Used to allow Sidekiq API or Sidekiq.redis for spec set-ups and components that does not require sharding such as CronJobs (performed using Sidekiq.redis).",
    "label": "",
    "id": "1612"
  },
  {
    "raw_code": "def enabled\n          validate_sidekiq_shard_awareness = Thread.current[:validate_sidekiq_shard_awareness]\n          Thread.current[:validate_sidekiq_shard_awareness] = true\n\n          yield\n        ensure\n          Thread.current[:validate_sidekiq_shard_awareness] = validate_sidekiq_shard_awareness\n        end",
    "comment": "This allows us to perform validation within the scope of GitLab application logic without needing to modify/patch Sidekiq internals such as job fetching, cron-polling, and housekeeping.",
    "label": "",
    "id": "1613"
  },
  {
    "raw_code": "def method_missing(*args, &block)\n        validate! if Thread.current[:validate_sidekiq_shard_awareness]\n\n        super(*args, &block)\n      end",
    "comment": "This is used to patch the Sidekiq::RedisClientAdapter to validate all Redis commands are routed rubocop:disable Style/MissingRespondToMissing -- already defined in the module we are patching",
    "label": "",
    "id": "1614"
  },
  {
    "raw_code": "def increment(key, amount = 1, options = {})\n        # Our code below that prevents calling EXPIRE after every INCR assumes\n        # we always increment by 1. This is true in Rack::Attack as of v6.6.1.\n        # This guard should alert us if Rack::Attack changes its behavior in a\n        # future version.\n        raise InvalidAmount unless amount == 1\n\n        with do |redis|\n          key = namespace(key)\n          new_value = redis.incr(key)\n          expires_in = options[:expires_in]\n          redis.expire(key, expires_in) if new_value == 1 && expires_in\n          new_value\n        end",
    "comment": "The increment method gets called very often. The implementation below aims to minimize the number of Redis calls we make.",
    "label": "",
    "id": "1615"
  },
  {
    "raw_code": "def initialize(project:, chat_name:, name:, arguments:, channel:, response_url:)\n        @project = project\n        @chat_name = chat_name\n        @name = name\n        @arguments = arguments\n        @channel = channel\n        @response_url = response_url\n      end",
    "comment": "project - The Project to schedule the command for. chat_name - The ChatName belonging to the user that scheduled the command. name - The name of the chat command to run. arguments - The arguments (as a String) to pass to the command. channel - The channel the message was sent from. response_url - The URL to send the response back to.",
    "label": "",
    "id": "1616"
  },
  {
    "raw_code": "def try_create_pipeline\n        return unless valid?\n\n        create_pipeline\n      end",
    "comment": "Tries to create a new pipeline.  This method will return a pipeline that _may_ be persisted, or `nil` if the pipeline could not be created.",
    "label": "",
    "id": "1617"
  },
  {
    "raw_code": "def build_environment_variables(pipeline)\n        pipeline.variables.build(\n          [{ key: 'CHAT_INPUT', value: arguments },\n            { key: 'CHAT_CHANNEL', value: channel },\n            { key: 'CHAT_USER_ID', value: chat_name.chat_id }]\n        )\n      end",
    "comment": "pipeline - The `Ci::Pipeline` to create the environment variables for.",
    "label": "",
    "id": "1618"
  },
  {
    "raw_code": "def build_chat_data(pipeline)\n        pipeline.build_chat_data(\n          project_id: project.id,\n          chat_name_id: chat_name.id,\n          response_url: response_url\n        )\n      end",
    "comment": "pipeline - The `Ci::Pipeline` to create the chat data for.",
    "label": "",
    "id": "1619"
  },
  {
    "raw_code": "def self.responder_for(build)\n        response_url = build.pipeline.chat_data&.response_url\n        return unless response_url\n\n        if response_url.start_with?('https://hooks.slack.com/')\n          Gitlab::Chat::Responder::Slack.new(build)\n        else\n          Gitlab::Chat::Responder::Mattermost.new(build)\n        end",
    "comment": "Returns an instance of the responder to use for generating chat responses.  This method will return `nil` if no formatter is available for the given build.  build - A `Ci::Build` that executed a chat command.",
    "label": "",
    "id": "1620"
  },
  {
    "raw_code": "def initialize(build)\n        @build = build\n      end",
    "comment": "build - The `Ci::Build` to obtain the output from.",
    "label": "",
    "id": "1621"
  },
  {
    "raw_code": "def to_s\n        offset, length = read_offset_and_length\n\n        trace.read do |stream|\n          stream.seek(offset)\n\n          output = stream\n            .stream\n            .read(length)\n            .force_encoding(Encoding.default_external)\n\n          without_executed_command_line(output)\n        end",
    "comment": "Returns a `String` containing the output of the build.  The output _does not_ include the command that was executed.",
    "label": "",
    "id": "1622"
  },
  {
    "raw_code": "def without_executed_command_line(output)\n        # If `output.split(\"\\n\")` produces an empty Array then the slicing that\n        # follows it will produce a nil. For example:\n        #\n        #     \"\\n\".split(\"\\n\")        # => []\n        #     \"\\n\".split(\"\\n\")[1..] # => nil\n        #\n        # To work around this we only \"join\" if we're given an Array.\n        if (converted = output.split(\"\\n\")[1..])\n          converted.join(\"\\n\")\n        else\n          ''\n        end",
    "comment": "Removes the line containing the executed command from the build output.  output - A `String` containing the output of a trace section.",
    "label": "",
    "id": "1623"
  },
  {
    "raw_code": "def find_build_trace_section(name)\n        trace_sections.find { |s| s[:name] == name }\n      end",
    "comment": "Returns the trace section for the given name, or `nil` if the section could not be found.  name - The name of the trace section to find.",
    "label": "",
    "id": "1624"
  },
  {
    "raw_code": "def read_offset_and_length\n        section = find_build_trace_section(PRIMARY_SECTION) ||\n          find_build_trace_section(FALLBACK_SECTION) ||\n          find_build_trace_section(LEGACY_SECTION)\n\n        unless section\n          raise(\n            MissingBuildSectionError,\n            \"The build_script trace section could not be found for build #{build.id}\"\n          )\n        end",
    "comment": "Returns the offset to seek to and the number of bytes to read relative to the offset.",
    "label": "",
    "id": "1625"
  },
  {
    "raw_code": "def initialize(build)\n          @build = build\n        end",
    "comment": "build - The `Ci::Build` that was executed.",
    "label": "",
    "id": "1626"
  },
  {
    "raw_code": "def send_response(output)\n          response = Integrations::Clients::HTTP.post(\n            pipeline.chat_data.response_url,\n            {\n              headers: { Accept: 'application/json' },\n              body: output.to_json\n            }\n          )\n\n          unless response.success?\n            Gitlab::AppLogger.warn(\n              message: 'Posting chat response failed',\n              error_message: response.message,\n              code: response.code\n            )\n          end",
    "comment": "Sends a response back to Slack  output - The output to send back to Slack, as a Hash.",
    "label": "",
    "id": "1627"
  },
  {
    "raw_code": "def success(output)\n          if output.empty?\n            Gitlab::AppLogger.info(message: 'Chat pipeline successful, but output is empty')\n            return\n          end",
    "comment": "Sends the output for a build that completed successfully.  output - The output produced by the chat command.",
    "label": "",
    "id": "1628"
  },
  {
    "raw_code": "def failure\n          send_response(\n            text: message_text(\"<#{build_url}|Sorry, the build failed!>\"),\n            response_type: RESPONSE_TYPE\n          )\n        end",
    "comment": "Sends the output for a build that failed.",
    "label": "",
    "id": "1629"
  },
  {
    "raw_code": "def scheduled_output\n          # We return an empty message so that Slack still shows the input\n          # command, without polluting the channel with standard \"The job has\n          # been scheduled\" (or similar) responses.\n          { text: '' }\n        end",
    "comment": "Returns the output to send back after a command has been scheduled.",
    "label": "",
    "id": "1630"
  },
  {
    "raw_code": "def send_response(body)\n          Integrations::Clients::HTTP.post(\n            pipeline.chat_data.response_url,\n            {\n              headers: { 'Content-Type': 'application/json' },\n              body: body.to_json\n            }\n          )\n        end",
    "comment": "Sends a response back to Mattermost  body - The message payload to send back to Mattermost, as a Hash.",
    "label": "",
    "id": "1631"
  },
  {
    "raw_code": "def success(output)\n          return if output.empty?\n\n          send_response(\n            response_type: :in_channel,\n            attachments: [\n              {\n                color: SUCCESS_COLOR,\n                text: \"ChatOps job started by #{user_ref} completed successfully\",\n                fields: [\n                  {\n                    short: true,\n                    title: \"ID\",\n                    value: build_ref.to_s\n                  },\n                  {\n                    short: true,\n                    title: \"Name\",\n                    value: build.name\n                  },\n                  {\n                    short: false,\n                    title: \"Output\",\n                    value: success_message(output)\n                  }\n                ]\n              }\n            ]\n          )\n        end",
    "comment": "Sends the output for a build that completed successfully.  output - The output produced by the chat command.",
    "label": "",
    "id": "1632"
  },
  {
    "raw_code": "def failure\n          send_response(\n            response_type: :in_channel,\n            attachments: [\n              {\n                color: FAILURE_COLOR,\n                text: \"ChatOps job started by #{user_ref} failed!\",\n                fields: [\n                  {\n                    short: true,\n                    title: \"ID\",\n                    value: build_ref.to_s\n                  },\n                  {\n                    short: true,\n                    title: \"Name\",\n                    value: build.name\n                  }\n                ]\n              }\n            ]\n          )\n        end",
    "comment": "Sends the output for a build that failed.",
    "label": "",
    "id": "1633"
  },
  {
    "raw_code": "def scheduled_output\n          {\n            response_type: :ephemeral,\n            text: \"Your ChatOps job #{build_ref} has been created!\"\n          }\n        end",
    "comment": "Returns the output to send back after a command has been scheduled.",
    "label": "",
    "id": "1634"
  },
  {
    "raw_code": "def collect\n          created_at_arel = ::Ci::Pipeline.arel_table['created_at']\n          pipelines_by_interval = project.all_pipelines\n            .where(created_at_arel.gteq(@from))\n            .where(created_at_arel.lteq(@to))\n            .group(\"date_trunc('#{interval}', #{::Ci::Pipeline.table_name}.created_at)\")\n\n          count_by_status = totals_by_status(pipelines_by_interval)\n          totals_count =\n            pipelines_by_interval\n              .count(:created_at)\n              .transform_keys { |date| date.strftime(@format) }\n\n          current = @from\n          while current <= @to\n            label = current.strftime(@format)\n            @labels       << label\n            @totals[:all] << (totals_count[label] || 0)\n            @selected_statuses.each do |status|\n              @totals[status] << (count_by_status.dig(status, label) || 0)\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1635"
  },
  {
    "raw_code": "def totals_by_status(pipelines_by_interval)\n          return {} unless @totals.slice(*SELECTABLE_STATUSES).any?\n\n          analytics_status_grouping = COMPLETED_STATUSES.index_with { |status| status }\n\n          counts_by_status = pipelines_by_interval\n            .group(:status) # rubocop: disable CodeReuse/ActiveRecord -- this grouping is very specific to this chart\n            .count(:created_at)\n\n          # Convert hash layout from [2024-05-14 00:00:00 UTC, \"failed\"]=>1 to {:failed=>{\"14 May\"=>1}\n          counts_by_status\n            .group_by { |(_date, status),| analytics_status_grouping.fetch(status.to_sym, :other) }\n            .transform_values do |values|\n              values.to_h.transform_keys { |date,| date.strftime(@format) }\n            end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1636"
  },
  {
    "raw_code": "def legacy_validate(content, dry_run: false, ref: project&.default_branch)\n        if dry_run\n          simulate_pipeline_creation(content, ref)\n        else\n          legacy_static_validation(content)\n        end",
    "comment": "Our goal is to remove the `sha` dependency and the custom `YamlProcessor` usage from the CI linting logic. This legacy method is aimed to be removed with https://gitlab.com/gitlab-org/gitlab/-/issues/543727.",
    "label": "",
    "id": "1637"
  },
  {
    "raw_code": "def unsafe_set(data)\n        write('w+b') do |stream|\n          stream.set(data.dup)\n        end",
    "comment": "Like `set` it writes the whole trace but doesn't hide secrets! This is solely used in spec factories to avoid calling unstubbed ApplicationSetting(ci_job_token_signing_key) attribute outside spec examples (like in `let_it_be`).",
    "label": "",
    "id": "1638"
  },
  {
    "raw_code": "def validate_job!(name, job)\n        validate_job_stage!(name, job)\n        validate_job_dependencies!(name, job)\n        validate_job_needs!(name, job)\n        validate_dynamic_child_pipeline_dependencies!(name, job)\n        validate_job_environment!(name, job)\n        validate_job_pages_publish!(name, job)\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "1639"
  },
  {
    "raw_code": "def releases\n        return unless enabled?\n        return if backoff_active?\n\n        Rails.cache.fetch(\n          cache_key,\n          skip_nil: true,\n          expires_in: RELEASES_VALIDITY_PERIOD,\n          race_condition_ttl: 10.seconds\n        ) do\n          response = Gitlab::HTTP.try_get(runner_releases_url)\n          @releases_by_minor = nil\n\n          unless response&.success?\n            @backoff_expire_time = next_backoff.from_now\n            break nil\n          end",
    "comment": "Returns a sorted list of the publicly available GitLab Runner releases ",
    "label": "",
    "id": "1640"
  },
  {
    "raw_code": "def releases_by_minor\n        return unless releases\n\n        @releases_by_minor ||= releases.group_by(&:without_patch).transform_values(&:max)\n      end",
    "comment": "Returns a hash with the latest runner version per minor release ",
    "label": "",
    "id": "1641"
  },
  {
    "raw_code": "def parse_natural_with_timestamp(starts_at, cadence)\n          case cadence[:unit]\n          when 'day' # Currently supports only 'every 1 day'.\n            \"#{starts_at.min} #{starts_at.hour} * * *\"\n          when 'week' # Currently supports only 'every 1 week'.\n            \"#{starts_at.min} #{starts_at.hour} * * #{starts_at.wday}\"\n          when 'month'\n            unless [1, 3, 6, 12].include?(cadence[:duration])\n              raise NotImplementedError, \"The cadence #{cadence} is not supported\"\n            end",
    "comment": "This method generates compatible expressions that can be parsed by Fugit::Nat.parse to generate a cron line. It takes start date of the cron and cadence in the following format: cadence = { unit: 'day/week/month/year' duration: 1 }",
    "label": "",
    "id": "1642"
  },
  {
    "raw_code": "def try_parse_cron(cron, cron_timezone)\n        Fugit::Cron.parse(\"#{cron} #{cron_timezone}\")\n      end",
    "comment": "NOTE: cron_timezone can only accept timezones listed in TZInfo::Timezone. Aliases of Timezones from ActiveSupport::TimeZone are NOT accepted, because Fugit::Cron only supports TZInfo::Timezone.  For example, those codes have the same effect. Time.zone = 'Pacific Time (US & Canada)' (ActiveSupport::TimeZone) Time.zone = 'America/Los_Angeles' (TZInfo::Timezone)  However, try_parse_cron only accepts the latter format. try_parse_cron('* * * * *', 'Pacific Time (US & Canada)') -> Doesn't work try_parse_cron('* * * * *', 'America/Los_Angeles') -> Works If you want to know more, please take a look https://github.com/rails/rails/blob/master/activesupport/lib/active_support/values/time_zone.rb",
    "label": "",
    "id": "1643"
  },
  {
    "raw_code": "def initialize(\n        config,\n        project: nil, pipeline: nil, sha: nil, ref: nil, user: nil, parent_pipeline: nil, source: nil,\n        pipeline_config: nil, logger: nil, inject_edge_stages: true, pipeline_policy_context: nil, inputs: {}\n      )\n        @logger = logger || ::Gitlab::Ci::Pipeline::Logger.new(project: project)\n        @source_ref_path = pipeline&.source_ref_path\n        @project = project\n        @inject_edge_stages = inject_edge_stages\n        @pipeline_policy_context = pipeline_policy_context\n\n        @context = self.logger.instrument(:config_build_context, once: true) do\n          pipeline ||= ::Ci::Pipeline.new(project: project, sha: sha, ref: ref, user: user, source: source)\n\n          build_context(project: project, pipeline: pipeline, sha: sha, user: user, parent_pipeline: parent_pipeline, pipeline_config: pipeline_config, pipeline_policy_context: pipeline_policy_context)\n        end",
    "comment": "rubocop: disable Metrics/ParameterLists",
    "label": "",
    "id": "1644"
  },
  {
    "raw_code": "def valid?\n        @root.valid?\n      end",
    "comment": "rubocop: enable Metrics/ParameterLists",
    "label": "",
    "id": "1645"
  },
  {
    "raw_code": "def variables\n        Gitlab::Ci::Config::FeatureFlags.with_actor(@project) do\n          root.variables_value\n        end",
    "comment": " Temporary method that should be removed after refactoring  rubocop:disable Gitlab/NoCodeCoverageComment -- This is an existing method and probably never called :nocov:",
    "label": "",
    "id": "1646"
  },
  {
    "raw_code": "def variables_with_data\n        Gitlab::Ci::Config::FeatureFlags.with_actor(@project) do\n          root.variables_entry.value_with_data\n        end",
    "comment": ":nocov: rubocop:enable Gitlab/NoCodeCoverageComment",
    "label": "",
    "id": "1647"
  },
  {
    "raw_code": "def rescue_errors\n        RESCUE_ERRORS\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "1648"
  },
  {
    "raw_code": "def project_claims\n        ::JSONWebToken::UserProjectTokenClaims\n         .new(project: source_project, user: user)\n         .generate\n      end",
    "comment": "Claims for the source project of a merge request pipeline, or else claims of the project where the pipeline is running.",
    "label": "",
    "id": "1649"
  },
  {
    "raw_code": "def job_project_claims\n        ::JSONWebToken::UserProjectTokenClaims\n         .new(project: project, user: user)\n         .project_claims(key_prefix: 'job_')\n      end",
    "comment": "Claims related to the project running the job. These claims match project_claims unless this is a merge request pipeline for a fork project.",
    "label": "",
    "id": "1650"
  },
  {
    "raw_code": "def source_project\n        pipeline.merge_request_from_forked_project? ? pipeline.merge_request.source_project : project\n      end",
    "comment": "Source project of a merge request pipeline, or else project where the pipeline is running.",
    "label": "",
    "id": "1651"
  },
  {
    "raw_code": "def cache_prefix(cache, index)\n          files = cache.dig(:key, :files) if cache.is_a?(Hash) && cache[:key].is_a?(Hash)\n\n          return index if files.blank?\n\n          filenames = files.map { |file| file.split('.').first }.join('_')\n\n          \"#{index}_#{filenames}\"\n        end",
    "comment": "The below method fixes a bug related to incorrect caches being used For more details please see: https://gitlab.com/gitlab-org/gitlab/-/issues/388374",
    "label": "",
    "id": "1652"
  },
  {
    "raw_code": "def options_retry\n    strong_memoize(:options_retry) do\n      value = @build.options&.dig(:retry)\n      value = value.is_a?(Integer) ? { max: value } : value.to_h\n      value.with_indifferent_access\n    end",
    "comment": "The format of the retry option changed in GitLab 11.5: Before it was integer only, after it is a hash. New builds are created with the new format, but builds created before GitLab 11.5 and saved in database still have the old integer only format. This method returns the retry option normalized as a hash in 11.5+ format.",
    "label": "",
    "id": "1653"
  },
  {
    "raw_code": "def conflicting_ci_namespace_requested?(namespace_record)\n            build.expanded_kubernetes_namespace.present? &&\n              namespace_record.namespace != build.expanded_kubernetes_namespace\n          end",
    "comment": " A namespace can only be specified via gitlab-ci.yml for unmanaged clusters, as we currently have no way of preventing a job requesting a namespace it shouldn't have access to.  To make this clear, we fail the build instead of silently using a namespace other than the one explicitly specified.  Support for managed clusters will be added in https://gitlab.com/gitlab-org/gitlab/issues/38054",
    "label": "",
    "id": "1654"
  },
  {
    "raw_code": "def top_level_glob?(glob)\n          glob.exclude?('/') && glob.exclude?('**')\n        end",
    "comment": "matches glob patterns that only match files in the top level directory",
    "label": "",
    "id": "1655"
  },
  {
    "raw_code": "def exact_glob?(glob)\n          glob.exclude?('*') && glob.exclude?('?') && glob.exclude?('[') && glob.exclude?('{')\n        end",
    "comment": "matches glob patterns that have no metacharacters for File#fnmatch?",
    "label": "",
    "id": "1656"
  },
  {
    "raw_code": "def extension_glob?(glob)\n          without_nested = without_wildcard_nested_pattern(glob)\n\n          without_nested.start_with?('.') && without_nested.exclude?('/') && exact_glob?(without_nested)\n        end",
    "comment": "matches glob patterns like **/*.js or **/*.so.1 to optimize with path.end_with?('.js')",
    "label": "",
    "id": "1657"
  },
  {
    "raw_code": "def build_attributes\n            pipeline_attributes.merge(\n              yaml_variables: @yaml_variables)\n          end",
    "comment": "Remove this method with FF `read_from_new_ci_destinations`",
    "label": "",
    "id": "1658"
  },
  {
    "raw_code": "def seed_environment\n            return unless attributes[:environment].present?\n\n            # The initial `environment` parameter is `expanded_environment_name` for a build.\n            # The `expanded_environment_name` method uses `metadata&.expanded_environment_name` first to check\n            # but we don't need it here because `metadata.expanded_environment_name` is only set in\n            # `app/services/environments/create_for_job_service.rb` which is after the pipeline creation.\n            ExpandVariables.expand(attributes[:environment], -> { simple_variables.sort_and_expand_all })\n          end",
    "comment": "Copied from `app/models/concerns/ci/deployable.rb#expanded_environment_name` See: https://gitlab.com/gitlab-org/gitlab/-/issues/479126",
    "label": "",
    "id": "1659"
  },
  {
    "raw_code": "def seed_kubernetes_namespace\n            return unless attributes[:environment].present?\n\n            namespace = attributes[:options]&.dig(:environment, :kubernetes, :namespace)\n\n            return unless namespace.present?\n\n            ExpandVariables.expand(namespace, -> { simple_variables })\n          end",
    "comment": "Copied from `app/models/concerns/ci/deployable.rb#expanded_kubernetes_namespace` See: https://gitlab.com/gitlab-org/gitlab/-/issues/479126",
    "label": "",
    "id": "1660"
  },
  {
    "raw_code": "def extract_project_path(path)\n          return if path.start_with?('/') # invalid project full path.\n\n          index = path.rindex('/') # find index of last `/` in the path\n          return unless index\n\n          path[0..index - 1]\n        end",
    "comment": "Given a path like \"my-org/sub-group/the-project/the-component\" we expect that the last `/` is the separator between the project full path and the component name.",
    "label": "",
    "id": "1661"
  },
  {
    "raw_code": "def with_actor(actor)\n            previous = Thread.current[ACTOR_KEY]\n\n            # When actor is `nil` the method `Thread.current[]=` does not\n            # create the ACTOR_KEY. Instead, we want to still save an explicit\n            # value to know that we are within the `with_actor` block.\n            Thread.current[ACTOR_KEY] = actor || NO_ACTOR_VALUE\n\n            yield\n          ensure\n            Thread.current[ACTOR_KEY] = previous\n          end",
    "comment": "Cache a feature flag actor as thread local variable so we can have it available later with #enabled?",
    "label": "",
    "id": "1662"
  },
  {
    "raw_code": "def enabled?(feature_flag)\n            ::Feature.enabled?(feature_flag, current_actor)\n          end",
    "comment": "Use this to check if a feature flag is enabled",
    "label": "",
    "id": "1663"
  },
  {
    "raw_code": "def expand(config)\n              config.flat_map do |config|\n                values = config.values\n\n                values[0]\n                  .product(*values.from(1))\n                  .map { |vals| config.keys.zip(vals).to_h }\n              end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1664"
  },
  {
    "raw_code": "def compose_jobs!\n            factory = logger.instrument(:config_root_compose_jobs_factory, once: true) do\n              ::Gitlab::Config::Entry::Factory.new(Entry::Jobs)\n                .value(jobs_config)\n                .with(key: :jobs, parent: self,\n                  description: 'Jobs definition for this pipeline')\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1665"
  },
  {
    "raw_code": "def filter_jobs!\n            return unless @config.is_a?(Hash)\n\n            @jobs_config = @config\n              .except(*self.class.reserved_nodes_names)\n              .select do |name, config|\n              Entry::Jobs.find_type(name, config).present? || ALLOWED_KEYS.exclude?(name)\n            end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1666"
  },
  {
    "raw_code": "def errors\n            []\n          end",
    "comment": " This method is here because `Context` will be responsible for validating specs, inputs and defaults. ",
    "label": "",
    "id": "1667"
  },
  {
    "raw_code": "def evaluate!\n            data_access, *functions = data.split('|').map(&:strip)\n            access = Interpolation::Access.new(data_access, ctx)\n\n            return @errors.concat(access.errors) unless access.valid?\n            return @errors.push('too many functions in interpolation block') if functions.count > MAX_FUNCTIONS\n\n            result = Interpolation::FunctionsStack.new(functions, ctx).evaluate(access.value)\n\n            if result.success?\n              @value = result.value\n            else\n              @errors.concat(result.errors)\n            end",
    "comment": "We expect the block data to be a string with one or more entities delimited by pipes: <access> | <function1> | <function2> | ... <functionN>",
    "label": "",
    "id": "1668"
  },
  {
    "raw_code": "def replace!(&block)\n            recursive_replace(@config, Visitor.new, &block)\n          rescue TooManyNodesError\n            @errors.push('config too large')\n            nil\n          rescue NodeTooLargeError\n            @errors.push('config node too large')\n            nil\n          end",
    "comment": " The replace! method will yield a block and replace each of the hash config nodes with the return value of the block.  It returns `nil` if there were errors found during the process. ",
    "label": "",
    "id": "1669"
  },
  {
    "raw_code": "def initialize(\n            project: nil, pipeline: nil, sha: nil, user: nil, parent_pipeline: nil, variables: nil,\n            pipeline_config: nil, logger: nil, pipeline_policy_context: nil\n          )\n            @project = project\n            @pipeline = pipeline\n            @sha = sha\n            @user = user\n            @parent_pipeline = parent_pipeline\n            @variables = variables || Ci::Variables::Collection.new\n            @pipeline_config = pipeline_config\n            @pipeline_policy_context = pipeline_policy_context\n            @expandset = []\n            @parallel_requests = []\n            @execution_deadline = 0\n            @logger = logger || Gitlab::Ci::Pipeline::Logger.new(project: project)\n            @max_includes = Gitlab::CurrentSettings.current_application_settings.ci_max_includes\n            @max_total_yaml_size_bytes =\n              Gitlab::CurrentSettings.current_application_settings.ci_max_total_yaml_size_bytes\n            @total_file_size_in_bytes = 0\n            yield self if block_given?\n          end",
    "comment": "rubocop:disable Metrics/ParameterLists -- all arguments needed",
    "label": "",
    "id": "1670"
  },
  {
    "raw_code": "def top_level_worktree_paths\n            strong_memoize(:top_level_worktree_paths) do\n              project.repository.tree(sha).blobs.map(&:path)\n            end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "1671"
  },
  {
    "raw_code": "def internal_include?\n            !!pipeline_config&.internal_include_prepended?\n          end",
    "comment": "Some Ci::ProjectConfig sources prepend the config content with an \"internal\" `include`, which becomes the first included file. When running a pipeline, we pass pipeline_config into the context of the first included file, which we use in this method to determine if the file is an \"internal\" one.",
    "label": "",
    "id": "1672"
  },
  {
    "raw_code": "def get_project_name(project_name)\n              if project_name.is_a?(Array)\n                project_name.first\n              else\n                project_name\n              end",
    "comment": "TODO: To be removed after we deprecate usage of array in `project` keyword. https://gitlab.com/gitlab-org/gitlab/-/issues/365975",
    "label": "",
    "id": "1673"
  },
  {
    "raw_code": "def preload_context\n              # no-op\n            end",
    "comment": "This method is overridden to load context into the memoized result or to lazily load context via BatchLoader",
    "label": "",
    "id": "1674"
  },
  {
    "raw_code": "def init_with(coder)\n              @data = {\n                tag: coder.tag,       # This is the custom YAML tag, like !reference or !flatten\n                style: coder.style,\n                seq: coder.seq,       # This holds Array data\n                scalar: coder.scalar, # This holds data of basic types, like String.\n                map: coder.map        # This holds Hash data.\n              }\n            end",
    "comment": "Only one of the `seq`, `scalar`, `map` fields is available.",
    "label": "",
    "id": "1675"
  },
  {
    "raw_code": "def double_quote_pattern\n          /(\\A|#{delimiter})\\s*\"(.*?)\"\\s*(?=#{delimiter}\\s*|\\z)/\n        end",
    "comment": "(             # Tag start delimiter ($1) \\A       |  # Either string start or #{delimiter}        # a delimiter ) \\s*\"          # quote (\") optionally preceded by whitespace (.*?)         # Tag ($2) \"\\s*          # quote (\") optionally followed by whitespace (?=           # Tag end delimiter (not consumed; is zero-length lookahead) #{delimiter}\\s*  |  # Either a delimiter optionally followed by whitespace or \\z          # string end )",
    "label": "",
    "id": "1676"
  },
  {
    "raw_code": "def single_quote_pattern\n          /(\\A|#{delimiter})\\s*'(.*?)'\\s*(?=#{delimiter}\\s*|\\z)/\n        end",
    "comment": "(             # Tag start delimiter ($1) \\A       |  # Either string start or #{delimiter}        # a delimiter ) \\s*'          # quote (') optionally preceded by whitespace (.*?)         # Tag ($2) '\\s*          # quote (') optionally followed by whitespace (?=           # Tag end delimiter (not consumed; is zero-length lookahead) #{delimiter}\\s*  | d # Either a delimiter optionally followed by whitespace or \\z          # string end )",
    "label": "",
    "id": "1677"
  },
  {
    "raw_code": "def create_tags(tags)\n          existing_tag_records = ::Ci::Tag.where(name: tags).to_a\n          missing_tags = detect_missing_tags(tags, existing_tag_records)\n          return existing_tag_records if missing_tags.empty?\n\n          missing_tags\n            .map { |tag| { name: tag } }\n            .each_slice(TAGS_BATCH_SIZE) do |tags_attributes|\n              ::Ci::Tag.insert_all!(tags_attributes)\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1678"
  },
  {
    "raw_code": "def build_taggings(tag_records_by_name)\n          accumulator = []\n\n          taggables.each do |taggable|\n            tag_list = tag_list_by_taggable[taggable]\n            next unless tag_list\n\n            tags = tag_records_by_name.values_at(*tag_list)\n            tags.each do |tag|\n              accumulator << new_taggings_record(tag, taggable)\n            end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1679"
  },
  {
    "raw_code": "def external_project_path?\n          ci_config_path =~ /\\A.+(yml|yaml)@.+\\z/\n        end",
    "comment": "Example: path/to/.gitlab-ci.yml@another-group/another-project",
    "label": "",
    "id": "1680"
  },
  {
    "raw_code": "def extract_location_tokens\n          path_file, path_project = ci_config_path.split('@', 2)\n\n          if path_project.include? \":\"\n            project, ref = path_project.split(':', 2)\n            [path_file, project, ref]\n          else\n            [path_file, path_project]\n          end",
    "comment": "Example: path/to/.gitlab-ci.yml@another-group/another-project:refname",
    "label": "",
    "id": "1681"
  },
  {
    "raw_code": "def internal_include_prepended?\n          false\n        end",
    "comment": "Indicates if we are prepending the content with an \"internal\" `include`",
    "label": "",
    "id": "1682"
  },
  {
    "raw_code": "def content\n          nil\n        end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- overridden and tested in EE :nocov:",
    "label": "",
    "id": "1683"
  },
  {
    "raw_code": "def internal_include_prepended?\n          true\n        end",
    "comment": ":nocov: rubocop:enable Gitlab/NoCodeCoverageComment",
    "label": "",
    "id": "1684"
  },
  {
    "raw_code": "def content\n          nil\n        end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- overridden and tested in EE :nocov:",
    "label": "",
    "id": "1685"
  },
  {
    "raw_code": "def source\n          :security_policies_default_source\n        end",
    "comment": ":nocov: rubocop:enable Gitlab/NoCodeCoverageComment",
    "label": "",
    "id": "1686"
  },
  {
    "raw_code": "def internal_include_prepended?\n          true\n        end",
    "comment": "Bridge.yaml_for_downstream injects an `include`",
    "label": "",
    "id": "1687"
  },
  {
    "raw_code": "def enabled?\n          ::Gitlab::SafeRequestStore.fetch(:ci_pipeline_archived_access) do\n            ::Feature.enabled?(:ci_pipeline_archived_access, :current_request, type: :ops)\n          end",
    "comment": "Tracks interactions with pipelines that are archived or will be archived (90+ days old). Helps assess impact of archival policies on user workflows before implementation. Primarily for GitLab.com data collection. ",
    "label": "",
    "id": "1688"
  },
  {
    "raw_code": "def from_periods(periods)\n          process_duration(process_periods(periods))\n        end",
    "comment": "periods should be sorted by `first`",
    "label": "",
    "id": "1689"
  },
  {
    "raw_code": "def self_and_downstreams_builds_of_pipeline(pipeline)\n          ::Ci::Build\n            .select(:id, :type, :started_at, :finished_at)\n            .in_pipelines(\n              pipeline.self_and_downstreams.select(:id)\n            )\n            .with_status(STATUSES)\n            .latest\n            .where.not(started_at: nil)\n            .order(:started_at)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1690"
  },
  {
    "raw_code": "def process_duration(periods)\n          periods.sum(&:duration)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1691"
  },
  {
    "raw_code": "def preload_commit_authors\n          @pipeline.commit.try(:lazy_author)\n        end",
    "comment": "This also preloads the author of every commit. We're using \"lazy_author\" here since \"author\" immediately loads the data on the first call.",
    "label": "",
    "id": "1692"
  },
  {
    "raw_code": "def preload_ref_commits\n          @pipeline.lazy_ref_commit\n        end",
    "comment": "This preloads latest commits for given refs and therefore makes it much less expensive to check if a pipeline is a latest one for given branch.",
    "label": "",
    "id": "1693"
  },
  {
    "raw_code": "def preload_stages_warnings\n          @pipeline.stages.each { |stage| stage.number_of_warnings }\n        end",
    "comment": "This preloads the number of warnings for every stage, ensuring that Ci::Stage#has_warnings? doesn't execute any additional queries.",
    "label": "",
    "id": "1694"
  },
  {
    "raw_code": "def preload_persisted_environments\n          @pipeline.scheduled_actions.each { |action| action.persisted_environment }\n          @pipeline.manual_actions.each { |action| action.persisted_environment }\n        end",
    "comment": "This batch loads the associated environments of multiple actions (builds) that can't use `preload` due to the indirect relationship.",
    "label": "",
    "id": "1695"
  },
  {
    "raw_code": "def tag_ref?\n            return true if full_git_ref_name_unavailable?\n\n            Gitlab::Git.tag_ref?(origin_ref).present?\n          end",
    "comment": "Verifies that origin_ref is a fully qualified tag reference (refs/tags/<tag-name>)  Fallbacks to `true` for backward compatibility reasons if origin_ref is a short ref",
    "label": "",
    "id": "1696"
  },
  {
    "raw_code": "def branch_ref?\n            return true if full_git_ref_name_unavailable?\n\n            Gitlab::Git.branch_ref?(origin_ref).present?\n          end",
    "comment": "Verifies that origin_ref is a fully qualified branch reference (refs/heads/<branch-name>)  Fallbacks to `true` for backward compatibility reasons if origin_ref is a short ref",
    "label": "",
    "id": "1697"
  },
  {
    "raw_code": "def pipeline_execution_policy_build?(_build)\n            false\n          end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "1698"
  },
  {
    "raw_code": "def scan_execution_policy_build?(_build)\n            false\n          end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "1699"
  },
  {
    "raw_code": "def force_pipeline_creation_to_continue?\n            false\n          end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- method is tested in EE :nocov: Overridden in EE",
    "label": "",
    "id": "1700"
  },
  {
    "raw_code": "def perform!\n              # to be overridden in EE\n            end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- EE module is tested :nocov:",
    "label": "",
    "id": "1701"
  },
  {
    "raw_code": "def perform!\n              # to be overridden in EE\n            end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- EE module is tested :nocov:",
    "label": "",
    "id": "1702"
  },
  {
    "raw_code": "def tokens_rpn\n            output = []\n            operators = []\n\n            @tokens.each do |token|\n              case token.type\n              when :value\n                output.push(token)\n              when :logical_operator\n                output.push(operators.pop) while token.lexeme.consume?(operators.last&.lexeme)\n\n                operators.push(token)\n              when :parenthesis_open\n                operators.push(token)\n              when :parenthesis_close\n                output.push(operators.pop) while token.lexeme.consume?(operators.last&.lexeme)\n\n                raise ParseError, 'Unmatched parenthesis' unless operators.last\n\n                operators.pop if operators.last.lexeme.type == :parenthesis_open\n              end",
    "comment": "Parse the expression into Reverse Polish Notation (See: Shunting-yard algorithm) Taken from: https://en.wikipedia.org/wiki/Shunting-yard_algorithm#The_algorithm_in_detail",
    "label": "",
    "id": "1703"
  },
  {
    "raw_code": "def initialize(left, right)\n              raise OperatorError, 'Invalid left operand' unless left.respond_to? :evaluate\n              raise OperatorError, 'Invalid right operand' unless right.respond_to? :evaluate\n\n              @left = left\n              @right = right\n            end",
    "comment": "This operator class is design to handle single operators that take two arguments. Expression::Parser was originally designed to read infix operators, and so the two operands are called \"left\" and \"right\" here. If we wish to implement an Operator that takes a greater or lesser number of arguments, a structural change or additional Operator superclass will likely be needed.",
    "label": "",
    "id": "1704"
  },
  {
    "raw_code": "def scoped_user_id_attribute\n            user_identity = ::Gitlab::Auth::Identity.fabricate(@pipeline.user)\n\n            return {} unless user_identity&.composite? && user_identity.linked?\n\n            attrs = { scoped_user_id: user_identity.scoped_user.id }\n\n            if Feature.enabled?(:stop_writing_builds_metadata, @pipeline.project)\n              attrs\n            else\n              attrs.merge(options: attrs)\n            end",
    "comment": "Scoped user is present when the user creating the pipeline supports composite identity. For example: a service account like GitLab Duo. The scoped user is used to further restrict the permissions of the CI job token associated to the `job.user`.",
    "label": "",
    "id": "1705"
  },
  {
    "raw_code": "def allow_failure_criteria_attributes\n            return {} if rules_attributes[:allow_failure].nil?\n            return {} unless @seed_attributes.dig(:options, :allow_failure_criteria)\n\n            { options: { allow_failure_criteria: nil } }\n          end",
    "comment": "If a job uses `allow_failure:exit_codes` and `rules:allow_failure` we need to prevent the exit codes from being persisted because they would break the behavior defined by `rules:allow_failure`.",
    "label": "",
    "id": "1706"
  },
  {
    "raw_code": "def upload_attributes\n          strong_memoize(:upload_attributes) do\n            ::Ci::JobArtifact.find(trace_artifact.id).file.file.attributes\n          end",
    "comment": "Carrierwave caches attributes for the local file and does not replace them with the ones from object store after the upload completes. We need to force it to fetch them directly from the object store.",
    "label": "",
    "id": "1707"
  },
  {
    "raw_code": "def trace_chunks\n          strong_memoize(:trace_chunks) do\n            ::Ci::BuildTraceChunk.with_read_consistency(build) do\n              build.trace_chunks.persisted\n                .select(::Ci::BuildTraceChunk.metadata_attributes)\n            end",
    "comment": " Trace chunks will be persisted in a database if an object store is not configured - in that case we do not want to load entire raw data of all the chunks into memory.  We ignore `raw_data` attribute instead, and rely on internal build trace chunk database adapter to handle `ActiveModel::MissingAttributeError` exception.  Alternative solution would be separating chunk data from chunk metadata on the database level too. ",
    "label": "",
    "id": "1708"
  },
  {
    "raw_code": "def truncate(offset)\n          raise ArgumentError, 'Outside of file' if offset > size || offset < 0\n          return if offset == size # Skip the following process as it doesn't affect anything\n\n          @tell = offset\n          @size = offset\n\n          # remove all next chunks\n          trace_chunks.where('chunk_index > ?', chunk_index).fast_destroy_all\n\n          # truncate current chunk\n          current_chunk.truncate(chunk_offset)\n        ensure\n          invalidate_chunk_cache\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1709"
  },
  {
    "raw_code": "def flush\n          # no-op\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1710"
  },
  {
    "raw_code": "def in_range?\n          @chunk_range&.include?(tell)\n        end",
    "comment": " The below methods are not implemented in IO class ",
    "label": "",
    "id": "1711"
  },
  {
    "raw_code": "def current_chunk\n          @chunks_cache[chunk_index] ||= trace_chunks.find_by(chunk_index: chunk_index)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1712"
  },
  {
    "raw_code": "def next_chunk\n          @chunks_cache[chunk_index] = begin\n            ::Ci::BuildTraceChunk\n              .safe_find_or_create_by(build: build, chunk_index: chunk_index)\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1713"
  },
  {
    "raw_code": "def trace_chunks\n          ::Ci::BuildTraceChunk.where(build: build)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1714"
  },
  {
    "raw_code": "def calculate_size\n          trace_chunks.order(chunk_index: :desc).first.try(&:end_offset).to_i\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1715"
  },
  {
    "raw_code": "def value_with_jitter\n          value + jitter\n        end",
    "comment": "This formula generates an increasing delay between executions 9.6, 19.2, 28.8, 38.4, 48.0 + a random amount of time to change the order of execution for the jobs. With maximum value for each call to rand(4), this sums up to 6.8 days and with minimum values is 6 days. ",
    "label": "",
    "id": "1716"
  },
  {
    "raw_code": "def on_0(_) { reset: true } end\n\n        def on_1(_) { enable: STYLE_SWITCHES[:bold] } end\n\n        def on_3(_) { enable: STYLE_SWITCHES[:italic] } end\n\n        def on_4(_) { enable: STYLE_SWITCHES[:underline] } end\n\n        def on_8(_) { enable: STYLE_SWITCHES[:conceal] } end\n\n        def on_9(_) { enable: STYLE_SWITCHES[:cross] } end\n\n        def on_21(_) { disable: STYLE_SWITCHES[:bold] } end\n\n        def on_22(_) { disable: STYLE_SWITCHES[:bold] } end\n\n        def on_23(_) { disable: STYLE_SWITCHES[:italic] } end\n\n        def on_24(_) { disable: STYLE_SWITCHES[:underline] } end\n\n        def on_28(_) { disable: STYLE_SWITCHES[:conceal] } end\n\n        def on_29(_) { disable: STYLE_SWITCHES[:cross] } end\n\n        def on_30(_) { fg: fg_color(0) } end\n\n        def on_31(_) { fg: fg_color(1) } end\n\n        def on_32(_) { fg: fg_color(2) } end\n\n        def on_33(_) { fg: fg_color(3) } end\n\n        def on_34(_) { fg: fg_color(4) } end\n\n        def on_35(_) { fg: fg_color(5) } end\n\n        def on_36(_) { fg: fg_color(6) } end\n\n        def on_37(_) { fg: fg_color(7) } end\n\n        def on_38(stack) { fg: fg_color_256(stack) } end\n\n        def on_39(_) { fg: nil } end\n\n        def on_40(_) { bg: bg_color(0) } end\n\n        def on_41(_) { bg: bg_color(1) } end\n\n        def on_42(_) { bg: bg_color(2) } end\n\n        def on_43(_) { bg: bg_color(3) } end\n\n        def on_44(_) { bg: bg_color(4) } end\n\n        def on_45(_) { bg: bg_color(5) } end\n\n        def on_46(_) { bg: bg_color(6) } end\n\n        def on_47(_) { bg: bg_color(7) } end\n\n        def on_48(stack) { bg: bg_color_256(stack) } end\n\n        def on_49(_) { bg: nil } end\n\n        def on_90(_) { fg: fg_color(0, 'l') } end\n\n        def on_91(_) { fg: fg_color(1, 'l') } end\n\n        def on_92(_) { fg: fg_color(2, 'l') } end\n\n        def on_93(_) { fg: fg_color(3, 'l') } end\n\n        def on_94(_) { fg: fg_color(4, 'l') } end\n\n        def on_95(_) { fg: fg_color(5, 'l') } end\n\n        def on_96(_) { fg: fg_color(6, 'l') } end\n\n        def on_97(_) { fg: fg_color(7, 'l') } end\n\n        def on_99(_) { fg: fg_color(9, 'l') } end\n\n        def on_100(_) { bg: bg_color(0, 'l') } end\n\n        def on_101(_) { bg: bg_color(1, 'l') } end\n\n        def on_102(_) { bg: bg_color(2, 'l') } end\n\n        def on_103(_) { bg: bg_color(3, 'l') } end\n\n        def on_104(_) { bg: bg_color(4, 'l') } end\n\n        def on_105(_) { bg: bg_color(5, 'l') } end\n\n        def on_106(_) { bg: bg_color(6, 'l') } end\n\n        def on_107(_) { bg: bg_color(7, 'l') } end\n\n        def on_109(_) { fg: bg_color(9, 'l') } end\n        # rubocop:enable Style/SingleLineMethods\n\n        def fg_color(color_index, prefix = nil)\n          term_color_class(color_index, ['fg', prefix])\n        end\n\n        def fg_color_256(command_stack)\n          xterm_color_class(command_stack, 'fg')\n        end\n\n        def bg_color(color_index, prefix = nil)\n          term_color_class(color_index, ['bg', prefix])\n        end\n\n        def bg_color_256(command_stack)\n          xterm_color_class(command_stack, 'bg')\n        end\n\n        def term_color_class(color_index, prefix)\n          color_name = COLOR[color_index]\n          return if color_name.nil?\n\n          color_class(['term', prefix, color_name])\n        end\n\n        def xterm_color_class(command_stack, prefix)\n          # the 38 and 48 commands have to be followed by \"5\" and the color index\n          return unless command_stack.length >= 2\n          return unless command_stack[0] == \"5\"\n\n          command_stack.shift # ignore the \"5\" command\n          color_index = command_stack.shift.to_i\n\n          return unless color_index >= 0\n          return unless color_index <= 255\n\n          color_class([\"xterm\", prefix, color_index])\n        end\n\n        def color_class(segments)\n          [segments].flatten.compact.join('-')\n        end\n      end\n    end\n  end\nend",
    "comment": "rubocop:disable Style/SingleLineMethods",
    "label": "",
    "id": "1717"
  },
  {
    "raw_code": "def to_runner_variables\n          self.map(&:to_runner_variable)\n        end",
    "comment": "This method should only be called via runner_variables->to_runner_variables because this is an expensive operation by initializing new objects in `to_runner_variable`.",
    "label": "",
    "id": "1718"
  },
  {
    "raw_code": "def scoped_variables(job, environment:, dependencies:)\n          Gitlab::Ci::Variables::Collection.new.tap do |variables|\n            if pipeline.only_workload_variables?\n              # predefined_project_variables includes things like $CI_PROJECT_PATH which are used by the runner to clone\n              # the repo\n              variables.concat(project.predefined_project_variables)\n\n              # yaml_variables is how we inject dynamic configuration into a workload\n              variables.concat(job.yaml_variables)\n\n              variables.concat(\n                user_defined_variables(options: job.options, environment: environment, job_variables: job.manual_variables)\n              )\n              next\n            end",
    "comment": "When adding new variables, consider either adding or commenting out them in the following methods: - unprotected_scoped_variables - scoped_variables_for_pipeline_seed",
    "label": "",
    "id": "1719"
  },
  {
    "raw_code": "def environment_tier_from_job_options(options, environment)\n          options.dig(:environment, :deployment_tier) || persisted_environment(environment).try(:tier)\n        end",
    "comment": "We use the `environment` parameter instead of `options[:environment]` because `environment` is expanded.",
    "label": "",
    "id": "1720"
  },
  {
    "raw_code": "def environment_url_from_job_options(options, environment)\n          options.dig(:environment, :url) || persisted_environment(environment).try(:external_url)\n        end",
    "comment": "We use the `environment` parameter instead of `options[:environment]` because `environment` is expanded.",
    "label": "",
    "id": "1721"
  },
  {
    "raw_code": "def errors\n            strong_memoize(:errors) do\n              # Check for cyclic dependencies and build error message in that case\n              cyclic_vars = each_strongly_connected_component.filter_map do |component|\n                component.map { |v| v[:key] }.inspect if component.size > 1\n              end",
    "comment": "errors sorts an array of variables, ignoring unknown variable references, and returning an error string if a circular variable reference is found",
    "label": "",
    "id": "1722"
  },
  {
    "raw_code": "def to_runner_variable\n            @variable.reject do |hash_key, hash_value|\n              (hash_key == :file || hash_key == :raw) && hash_value == false\n            end",
    "comment": " If `file: true` or `raw: true` has been provided we expose it, otherwise we don't expose `file` and `raw` attributes at all (stems from what the runner expects).  This method should only be called via runner_variables->to_runner_variables->to_runner_variable because this is an expensive operation by initializing a new object. ",
    "label": "",
    "id": "1723"
  },
  {
    "raw_code": "def to_s\n            return to_hash_variable.to_s unless depends_on\n\n            \"#{to_hash_variable}, depends_on=#{depends_on}\"\n          end",
    "comment": "This is for debugging purposes only.",
    "label": "",
    "id": "1724"
  },
  {
    "raw_code": "def sanitize_json_data\n            return unless json_data.gsub!(/(?<!\\\\)(?:\\\\\\\\)*\\\\u0000/, '\\\\\\\\\\u0000')\n\n            report.add_warning('Parsing', 'Report artifact contained unicode null characters which are escaped during the ingestion.')\n          end",
    "comment": "PostgreSQL can not save texts with unicode null character that's why we are escaping that character.",
    "label": "",
    "id": "1725"
  },
  {
    "raw_code": "def introspect_parser\n            Thread.current[:introspect_parser] ||= Oj::Introspect.new(filter: \"remediations\")\n          end",
    "comment": "New Oj parsers are not thread safe, therefore, we need to initialize them for each thread.",
    "label": "",
    "id": "1726"
  },
  {
    "raw_code": "def create_scan_primary_identifiers\n            return unless scan_data.is_a?(Hash) && scan_data['primary_identifiers']\n\n            scan_data['primary_identifiers'].map do |identifier|\n              ::Gitlab::Ci::Reports::Security::Identifier.new(\n                external_type: identifier['type'],\n                external_id: identifier['value'],\n                name: identifier['name'],\n                url: identifier['url'])\n            end",
    "comment": "TODO: primary_identifiers should be initialized on the scan itself but we do not currently parse scans through `MergeReportsService`",
    "label": "",
    "id": "1727"
  },
  {
    "raw_code": "def type; end\n\n            def required_attributes_present?\n              self.class::REQUIRED_ATTRIBUTES.all? do |keys|\n                data.dig(*keys).present?\n              end\n            end",
    "comment": "Implement in child class returns a symbol of the source type",
    "label": "",
    "id": "1728"
  },
  {
    "raw_code": "def matched_full_path\n              merge_request_file_paths.find do |full_path|\n                full_path.include?(current_path)\n              end",
    "comment": "Jacoco reports only provide the relative path so we need to match the files against the ones changed on the MR and provide the full path in our reports",
    "label": "",
    "id": "1729"
  },
  {
    "raw_code": "def limited_tests\n          strong_memoize(:limited_tests) do\n            # rubocop: disable CodeReuse/ActiveRecord\n            TestSummary.new(\n              new_failures: new_failures.take(max_tests),\n              existing_failures: existing_failures.take(max_tests(new_failures)),\n              resolved_failures: resolved_failures.take(max_tests(new_failures, existing_failures)),\n              new_errors: new_errors.take(max_tests),\n              existing_errors: existing_errors.take(max_tests(new_errors)),\n              resolved_errors: resolved_errors.take(max_tests(new_errors, existing_errors))\n            )\n            # rubocop: enable CodeReuse/ActiveRecord\n          end",
    "comment": "This is used to limit the presented test cases but does not affect total count of tests in the summary",
    "label": "",
    "id": "1730"
  },
  {
    "raw_code": "def build_ids\n          @build_report_results.pluck(:build_id)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1731"
  },
  {
    "raw_code": "def location_fingerprint\n            location_fingerprints.first\n          end",
    "comment": "Returns either the max priority signature hex or the location fingerprint",
    "label": "",
    "id": "1732"
  },
  {
    "raw_code": "def signature_hexes\n            return [] unless @vulnerability_finding_signatures_enabled && signatures.present?\n\n            signatures.sort_by { |sig| -sig.priority }.map(&:signature_hex)\n          end",
    "comment": "Returns the signature hexes in reverse priority order",
    "label": "",
    "id": "1733"
  },
  {
    "raw_code": "def status_tooltip\n          label\n        end",
    "comment": "Hint that appears on all the pipeline graph tooltips and builds on the right sidebar in Job detail view",
    "label": "",
    "id": "1734"
  },
  {
    "raw_code": "def badge_tooltip\n          subject.status\n        end",
    "comment": "Hint that appears on the build badges",
    "label": "",
    "id": "1735"
  },
  {
    "raw_code": "def initialize(all_jobs, with_allow_failure: true, dag: false, project: nil)\n          unless all_jobs.respond_to?(:pluck)\n            raise ArgumentError, \"all_jobs needs to respond to `.pluck`\"\n          end",
    "comment": "This class accepts an array of arrays/hashes/or objects `with_allow_failure` will be removed when deleting ci_remove_ensure_stage_service",
    "label": "",
    "id": "1736"
  },
  {
    "raw_code": "def status\n          return if none?\n\n          strong_memoize(:status) do\n            if @dag && any_skipped_or_ignored?\n              # The DAG job is skipped if one of the needs does not run at all.\n              'skipped'\n            elsif only_of?(:skipped, :ignored)\n              'skipped'\n            elsif only_of?(:success, :skipped, :success_with_warnings, :ignored)\n              'success'\n            elsif only_of?(:created, :success_with_warnings, :ignored)\n              'created'\n            elsif only_of?(:preparing, :success_with_warnings, :ignored)\n              'preparing'\n            elsif only_of?(:canceled, :success, :skipped, :success_with_warnings, :ignored)\n              'canceled'\n            elsif only_of?(:pending, :created, :skipped, :success_with_warnings, :ignored)\n              'pending'\n            elsif any_of?(:running, :pending)\n              'running'\n            elsif any_of?(:waiting_for_resource)\n              'waiting_for_resource'\n            elsif any_of?(:waiting_for_callback)\n              'waiting_for_callback'\n            elsif any_of?(:manual)\n              'manual'\n            elsif any_of?(:scheduled)\n              'scheduled'\n            elsif any_of?(:preparing)\n              'preparing'\n            elsif any_of?(:created)\n              'running'\n            elsif any_of?(:canceling)\n              'canceling'\n            else\n              'failed'\n            end",
    "comment": "The status calculation is order dependent, 1. In some cases we assume that that status is exact if the we only have given statues, 2. In other cases we assume that status is of that type based on what statuses are no longer valid based on the data set that we have  This method is used for three cases: 1. When it is called for a stage or a pipeline (with `all_jobs` from all jobs in a stage or a pipeline), then, the returned status is assigned to the stage or pipeline. 2. When it is called for a job (with `all_jobs` from all previous jobs or all needed jobs), then, the returned status is used to determine if the job is processed or not. 3. When it is called for a group (of jobs that are related), then, the returned status is used to show the overall status of the group. rubocop: disable Metrics/CyclomaticComplexity rubocop: disable Metrics/PerceivedComplexity",
    "label": "",
    "id": "1737"
  },
  {
    "raw_code": "def warnings?\n          @status_set.include?(:success_with_warnings)\n        end",
    "comment": "rubocop: enable Metrics/CyclomaticComplexity rubocop: enable Metrics/PerceivedComplexity",
    "label": "",
    "id": "1738"
  },
  {
    "raw_code": "def illustration\n            {\n              image: 'illustrations/empty-state/empty-job-not-triggered-md.svg',\n              size: '',\n              title: _('This job is preparing to start'),\n              content: _('This job is performing tasks that must complete before it can start')\n            }\n          end",
    "comment": " TODO: image is shared with 'pending' until we get a dedicated one ",
    "label": "",
    "id": "1739"
  },
  {
    "raw_code": "def self.matches?(build, user)\n            false\n          end",
    "comment": "Extended in EE",
    "label": "",
    "id": "1740"
  },
  {
    "raw_code": "def self.matches?(_bridge, _user)\n            false\n          end",
    "comment": "Extended in EE",
    "label": "",
    "id": "1741"
  },
  {
    "raw_code": "def illustration\n            {\n              image: 'illustrations/empty-state/empty-job-pending-md.svg',\n              size: '',\n              title: _('This job is waiting for resource: ') + subject.resource_group.key\n            }\n          end",
    "comment": " TODO: image is shared with 'pending' until we get a dedicated one ",
    "label": "",
    "id": "1742"
  },
  {
    "raw_code": "def allow_development_tooling(directives)\n          return unless Rails.env.development?\n\n          allow_webpack_dev_server(directives)\n          allow_letter_opener(directives)\n          allow_snowplow_micro(directives) if Gitlab::Tracking.snowplow_micro_enabled?\n        end",
    "comment": "connect_src with 'self' includes https/wss variations of the origin, however, safari hasn't covered this yet and we need to explicitly add support for websocket origins until Safari catches up with the specs",
    "label": "",
    "id": "1743"
  },
  {
    "raw_code": "def allow_framed_gitlab_paths(directives)\n          ['/admin/', '/assets/', '/-/speedscope/index.html', '/-/sandbox/'].map do |path|\n            append_to_directive(directives, 'frame_src', Gitlab::Utils.append_path(Gitlab.config.gitlab.url, path))\n          end",
    "comment": "Using 'self' in the CSP introduces several CSP bypass opportunities for this reason we list the URLs where GitLab frames itself instead",
    "label": "",
    "id": "1744"
  },
  {
    "raw_code": "def csp_level_3_backport(directives)\n          # See https://gitlab.com/gitlab-org/gitlab/-/issues/343579\n          # frame-src was deprecated in CSP level 2 in favor of child-src\n          # CSP level 3 \"undeprecated\" frame-src and browsers fall back on child-src if it's missing\n          # However Safari seems to read child-src first so we'll just keep both equal\n          append_to_directive(directives, 'child_src', directives['frame_src'])\n\n          # Safari also doesn't support worker-src and only checks child-src\n          # So for compatibility until it catches up to other browsers we need to\n          # append worker-src's content to child-src\n          append_to_directive(directives, 'child_src', directives['worker_src'])\n        end",
    "comment": "The follow contains workarounds to patch Safari's lack of support for CSP Level 3",
    "label": "",
    "id": "1745"
  },
  {
    "raw_code": "def title\n          raise NotImplementedError, \"Expected #{self.name} to implement title\"\n        end",
    "comment": ":nocov: the class including this concern is expected to test this method.",
    "label": "",
    "id": "1746"
  },
  {
    "raw_code": "def value\n          raise NotImplementedError, \"Expected #{self.name} to implement value\"\n        end",
    "comment": ":nocov: :nocov: the class including this concern is expected to test this method.",
    "label": "",
    "id": "1747"
  },
  {
    "raw_code": "def links\n          []\n        end",
    "comment": ":nocov:",
    "label": "",
    "id": "1748"
  },
  {
    "raw_code": "def commits_count\n          return unless ref\n\n          @commits_count ||= gitaly_commit_client.commit_count(ref, after: @options[:from], before: @options[:to])\n        end",
    "comment": "Don't use the `Gitlab::Git::Repository#log` method, because it enforces a limit. Since we need a commit count, we _can't_ enforce a limit, so the easiest way forward is to replicate the relevant portions of the `log` function here.",
    "label": "",
    "id": "1749"
  },
  {
    "raw_code": "def self.matches_syntax?(pattern)\n        pattern.is_a?(String) && pattern.match(PATTERN).present?\n      end",
    "comment": "Checks if pattern matches a regexp pattern but does not enforce it's validity",
    "label": "",
    "id": "1750"
  },
  {
    "raw_code": "def self.valid?(pattern)\n        !!self.fabricate(pattern)\n      end",
    "comment": "The regexp can match the pattern `/.../`, but may not be fabricatable: it can be invalid or incomplete: `/match ( string/`",
    "label": "",
    "id": "1751"
  },
  {
    "raw_code": "def sanitize(message)\n        return message if message.blank?\n\n        safe_messages = message.split(\"\\n\").map do |msg|\n          if (match = msg.match(SAFE_MESSAGE_REGEX))\n            match[:safe_message].presence\n          end",
    "comment": "In gitaly-ruby we override this method to do nothing, so that sanitization happens in gitlab-rails only.",
    "label": "",
    "id": "1752"
  },
  {
    "raw_code": "def with_commit_in_source_tmp(commit_id, &blk)\n        tmp_ref = \"refs/#{::Repository::REF_TMP}/#{SecureRandom.hex}\"\n\n        yield commit_id if source_repo.fetch_source_branch!(target_repo, commit_id, tmp_ref)\n      ensure\n        source_repo.delete_refs(tmp_ref) # best-effort\n      end",
    "comment": "Fetch the ref into source_repo from target_repo, using a temporary ref name that will be deleted once the method completes. This is a no-op if fetching the source branch fails",
    "label": "",
    "id": "1753"
  },
  {
    "raw_code": "def self.collect_ref(project_id, ref_name)\n        refs_to_preload[project_id] ||= Set.new\n        refs_to_preload[project_id] << ref_name\n      end",
    "comment": "Collects a ref for later batch processing",
    "label": "",
    "id": "1754"
  },
  {
    "raw_code": "def self.preload_refs_for_project(project)\n        return unless project\n        return unless refs_to_preload.key?(project.id)\n\n        refs_to_preload[project.id].each do |ref_name|\n          project.repository.lazy_ref_exists?(ref_name)\n        end",
    "comment": "Populates batch loading of all collected refs for the given project",
    "label": "",
    "id": "1755"
  },
  {
    "raw_code": "def self.refs_to_preload\n        if Gitlab::SafeRequestStore.active?\n          Gitlab::SafeRequestStore.fetch(:refs_to_preload) { {} }\n        else\n          # Needed for tests. SafeRequestStore is disabled by default in the test environment.\n          # We would need to modify dozens of tests enabling :request_store before else block can be removed.\n          Thread.current[:refs_to_preload] ||= {}\n        end",
    "comment": "Thread-local storage for refs pending batch processing",
    "label": "",
    "id": "1756"
  },
  {
    "raw_code": "def parse\n        reindex_by_path(get_submodules_by_name)\n      end",
    "comment": "Parses the contents of a .gitmodules file and returns a hash of submodule information, indexed by path.",
    "label": "",
    "id": "1757"
  },
  {
    "raw_code": "def sha\n        if unknown_refs.any?\n          raise ReferenceNotFoundError, \"Can't find merge base for unknown refs: #{unknown_refs.inspect}\"\n        end",
    "comment": "Returns the SHA of the first common ancestor",
    "label": "",
    "id": "1758"
  },
  {
    "raw_code": "def commit\n        return unless sha\n\n        @commit ||= @repository.commit_by(oid: sha)\n      end",
    "comment": "Returns the merge base as a Gitlab::Git::Commit",
    "label": "",
    "id": "1759"
  },
  {
    "raw_code": "def unknown_refs\n        @unknown_refs ||= Hash[@refs.zip(commits_for_refs)]\n                            .select { |ref, commit| commit.nil? }.keys\n      end",
    "comment": "Returns the refs passed on initialization that aren't found in the repository, and thus cannot be used to find a merge base.",
    "label": "",
    "id": "1760"
  },
  {
    "raw_code": "def delay\n        return 0 if @jobs_enqueued < JOBS_THRESHOLD\n\n        (@jobs_enqueued / PROCESS_COMMIT_MAX_JOBS_PER_S).seconds\n      end",
    "comment": "The number of seconds to delay ProcessCommitWorker to ensure a maximum of PROCESS_COMMIT_MAX_JOBS_PER_S jobs executed per second.",
    "label": "",
    "id": "1761"
  },
  {
    "raw_code": "def initialize(repository)\n        @repository = repository\n      end",
    "comment": "'repository' is a Gitlab::Git::Repository",
    "label": "",
    "id": "1762"
  },
  {
    "raw_code": "def parse(raw_change)\n        @blob_id, @blob_size, @raw_operation, raw_paths = raw_change.split(' ', 4)\n        @blob_size = @blob_size.to_i\n        @operation = extract_operation\n        @old_path, @new_path = extract_paths(raw_paths)\n      end",
    "comment": "Input data has the following format:  When a file has been modified: 7e3e39ebb9b2bf433b4ad17313770fbe4051649c 669 M\\tfiles/ruby/popen.rb  When a file has been renamed: 85bc2f9753afd5f4fc5d7c75f74f8d526f26b4f3 107 R060\\tfiles/js/commit.js.coffee\\tfiles/js/commit.coffee",
    "label": "",
    "id": "1763"
  },
  {
    "raw_code": "def repository\n        @repository ||= Gitlab::Git::Repository.new(storage, relative_path, GL_REPOSITORY, gl_project_path)\n      end",
    "comment": "Allows for reusing other RPCs by 'tricking' Gitaly to think its a repository",
    "label": "",
    "id": "1764"
  },
  {
    "raw_code": "def attributes(file_path)\n        absolute_path = File.join('/', file_path)\n\n        patterns.each do |pattern, attrs|\n          return attrs if File.fnmatch?(pattern, absolute_path)\n        end",
    "comment": "Returns all the Git attributes for the given path.  file_path - A path to a file for which to get the attributes.  Returns a Hash.",
    "label": "",
    "id": "1765"
  },
  {
    "raw_code": "def patterns\n        @patterns ||= parse_data\n      end",
    "comment": "Returns a Hash containing the file patterns and their attributes.",
    "label": "",
    "id": "1766"
  },
  {
    "raw_code": "def parse_attributes(string)\n        values = {}\n        dash = '-'\n        equal = '='\n        binary = 'binary'\n\n        string.split(/\\s+/).each do |chunk|\n          # Data such as \"foo = bar\" should be treated as \"foo\" and \"bar\" being\n          # separate boolean attributes.\n          next if chunk == equal\n\n          key = chunk\n\n          # Input: \"-foo\"\n          if chunk.start_with?(dash)\n            key = chunk.byteslice(1, chunk.length - 1)\n            value = false\n\n          # Input: \"foo=bar\"\n          elsif chunk.include?(equal)\n            key, value = chunk.split(equal, 2)\n\n          # Input: \"foo\"\n          else\n            value = true\n          end",
    "comment": "Parses an attribute string.  These strings can be in the following formats:  text      # => { \"text\" => true } -text     # => { \"text\" => false } key=value # => { \"key\" => \"value\" }  string - The string to parse.  Returns a Hash containing the attributes and their values.",
    "label": "",
    "id": "1767"
  },
  {
    "raw_code": "def each_line\n        @data.each_line do |line|\n          break unless line.valid_encoding?\n\n          yield line.strip\n        end",
    "comment": "Iterates over every line in the attributes file.",
    "label": "",
    "id": "1768"
  },
  {
    "raw_code": "def parse_data\n        pairs = []\n        comment = '#'\n\n        each_line do |line|\n          next if line.start_with?(comment) || line.empty?\n\n          pattern, attrs = line.split(/\\s+/, 2)\n\n          parsed = attrs ? parse_attributes(attrs) : {}\n\n          absolute_pattern = pattern.starts_with?('/') ? pattern : File.join('**/', pattern)\n\n          pairs << [absolute_pattern, parsed]\n        end",
    "comment": "Parses the Git attributes file contents.",
    "label": "",
    "id": "1769"
  },
  {
    "raw_code": "def self.set(gl_repository, relative_path, env)\n        return unless Gitlab::SafeRequestStore.active?\n\n        raise \"missing gl_repository\" if gl_repository.blank?\n\n        Gitlab::SafeRequestStore[:gitlab_git_env] ||= {}\n        Gitlab::SafeRequestStore[:gitlab_git_env][gl_repository] = allowlist_git_env(env)\n        Gitlab::SafeRequestStore[:gitlab_git_relative_path] = relative_path\n      end",
    "comment": "set stores the quarantining variables into request store.  relative_path is sent from Gitaly to Rails when invoking internal API. In production it points to the transaction's snapshot repository. Tests should pass the original relative path of the repository as Gitaly is stubbed out from the invokation loop and doesn't create a transaction snapshot.",
    "label": "",
    "id": "1770"
  },
  {
    "raw_code": "def self.get_relative_path\n        return unless Gitlab::SafeRequestStore.active?\n\n        Gitlab::SafeRequestStore.fetch(:gitlab_git_relative_path)\n      end",
    "comment": "get_relative_path returns the relative path of the repository this hook call is triggered for. This is the repository's relative path in the transaction's snapshot and is passed back to Gitaly in quarantined calls.",
    "label": "",
    "id": "1771"
  },
  {
    "raw_code": "def where(options)\n          repo = options.delete(:repo)\n          raise 'Gitlab::Git::Repository is required' unless repo.respond_to?(:log)\n\n          repo.log(options)\n        end",
    "comment": "Get commits collection  Ex. Commit.where( repo: repo, ref: 'master', path: 'app/models', limit: 10, offset: 5, ) ",
    "label": "",
    "id": "1772"
  },
  {
    "raw_code": "def find(repo, commit_id = \"HEAD\")\n          # Already a commit?\n          return commit_id if commit_id.is_a?(Gitlab::Git::Commit)\n\n          # This saves us an RPC round trip.\n          return unless valid?(commit_id)\n\n          commit = find_commit(repo, commit_id)\n\n          decorate(repo, commit) if commit\n        rescue Gitlab::Git::CommandError, Gitlab::Git::Repository::NoRepository, ArgumentError\n          nil\n        end",
    "comment": "Get single commit  Ex. Commit.find(repo, '29eda46b')  Commit.find(repo, 'master')  Gitaly migration: https://gitlab.com/gitlab-org/gitaly/issues/321",
    "label": "",
    "id": "1773"
  },
  {
    "raw_code": "def last(repo)\n          find(repo)\n        end",
    "comment": "Get last commit for HEAD  Ex. Commit.last(repo) ",
    "label": "",
    "id": "1774"
  },
  {
    "raw_code": "def last_for_path(repo, ref, path = nil, literal_pathspec: false)\n          # This is not where..first from ActiveRecord\n          where(\n            repo: repo,\n            ref: ref,\n            path: path,\n            limit: 1,\n            literal_pathspec: literal_pathspec\n          ).first\n        end",
    "comment": "Get last commit for specified path and ref  Ex. Commit.last_for_path(repo, '29eda46b', 'app/models')  Commit.last_for_path(repo, 'master', 'Gemfile') ",
    "label": "",
    "id": "1775"
  },
  {
    "raw_code": "def between(repo, base, head, limit: nil)\n          # In either of these cases, we are guaranteed to return no commits, so\n          # shortcut the RPC call\n          return [] if Gitlab::Git.blank_ref?(base) || Gitlab::Git.blank_ref?(head)\n\n          wrapped_gitaly_errors do\n            revisions = [head, \"^#{base}\"] # base..head\n            client = repo.gitaly_commit_client\n\n            # We must return the commits in chronological order but using both\n            # limit and reverse in the Gitaly RPC would return the oldest N,\n            # rather than newest N, commits, so reorder in Ruby with limit\n            if limit\n              client.list_commits(revisions, pagination_params: { limit: limit }).reverse!\n            else\n              client.list_commits(revisions, reverse: true)\n            end",
    "comment": "Get commits between two revspecs See also #repository.commits_between  Ex. Commit.between(repo, '29eda46b', 'master') # all commits, ordered oldest to newest Commit.between(repo, '29eda46b', 'master', limit: 100) # 100 newest commits, ordered oldest to newest ",
    "label": "",
    "id": "1776"
  },
  {
    "raw_code": "def find_all(repo, options = {})\n          wrapped_gitaly_errors do\n            Gitlab::GitalyClient::CommitService.new(repo).find_all_commits(options)\n          end",
    "comment": "Prefer `list_all` since it deprecates this RPC and `FindCommits`.  Returns commits collection  Ex. Commit.find_all( repo, ref: 'master', max_count: 10, skip: 5, order: :date )  +options+ is a Hash of optional arguments to git :ref is the ref from which to begin (SHA1 or name) :max_count is the maximum number of commits to fetch :skip is the number of commits to skip :order is the commits order and allowed value is :none (default), :date, :topo, or any combination of them (in an array). Commit ordering types are documented here: https://git-scm.com/docs/git-log#_commit_ordering",
    "label": "",
    "id": "1777"
  },
  {
    "raw_code": "def list_all(repo, options = {})\n          wrapped_gitaly_errors do\n            Gitlab::GitalyClient::CommitService.new(repo).list_commits(options[:revisions], options.except(:revisions))\n          end",
    "comment": "ListCommits lists all commits reachable via a set of references by doing a graph walk. This deprecates FindAllCommits and FindCommits (except Follow is not yet supported). Any unknown revisions will cause the RPC to fail.",
    "label": "",
    "id": "1778"
  },
  {
    "raw_code": "def batch_by_oid(repo, oids)\n          wrapped_gitaly_errors do\n            repo.gitaly_commit_client.list_commits_by_oid(oids)\n          end",
    "comment": "Only to be used when the object ids will not necessarily have a relation to each other. The last 10 commits for a branch for example, should go through .where",
    "label": "",
    "id": "1779"
  },
  {
    "raw_code": "def different_committer?\n        author_name != committer_name || author_email != committer_email\n      end",
    "comment": "Was this commit committed by a different person than the original author?",
    "label": "",
    "id": "1780"
  },
  {
    "raw_code": "def diff_from_parent(options = {})\n        @repository.gitaly_commit_client.diff_from_parent(self, options)\n      end",
    "comment": "Returns a diff object for the changes from this commit's first parent. If there is no parent, then the diff is between this commit and an empty repo. See Repository#diff for keys allowed in the +options+ hash.",
    "label": "",
    "id": "1781"
  },
  {
    "raw_code": "def ref_names(repo)\n        refs(repo).map do |ref|\n          strip_ref_prefix(ref)\n        end",
    "comment": "Get ref names collection  Ex. commit.ref_names(repo) ",
    "label": "",
    "id": "1782"
  },
  {
    "raw_code": "def parse_commit_trailers(trailers)\n        trailers.each_with_object({}) do |trailer, hash|\n          (hash[trailer.key] ||= []) << encode!(trailer.value)\n        end",
    "comment": "Turn the commit trailers into a hash of key: [value, value] arrays",
    "label": "",
    "id": "1783"
  },
  {
    "raw_code": "def init_date_from_gitaly(author)\n        return date_in_utc(author) if author.timezone.blank?\n\n        Time.strptime(\"#{author.date.seconds} #{author.timezone}\", '%s %z')\n      rescue ArgumentError\n        date_in_utc(author)\n      end",
    "comment": "Gitaly provides a UNIX timestamp in author.date.seconds, and a timezone offset in author.timezone. If the latter isn't present, assume UTC.",
    "label": "",
    "id": "1784"
  },
  {
    "raw_code": "def refs(repo)\n        repo.refs_hash[id]\n      end",
    "comment": "Get a collection of Gitlab::Git::Ref objects for this commit.  Ex. commit.ref(repo) ",
    "label": "",
    "id": "1785"
  },
  {
    "raw_code": "def tree_entries(\n          repository:,\n          sha:,\n          path: nil,\n          recursive: false,\n          skip_flat_paths: true,\n          rescue_not_found:  true,\n          pagination_params: nil\n        )\n          path = nil if path == '' || path == '/'\n\n          wrapped_gitaly_errors do\n            repository.gitaly_commit_client.tree_entries(\n              repository, sha, path, recursive, skip_flat_paths, pagination_params)\n          end",
    "comment": "Get list of tree objects for repository based on commit sha and path",
    "label": "",
    "id": "1786"
  },
  {
    "raw_code": "def find_id_by_path(repository, root_id, path)\n          root_tree = repository.lookup(root_id)\n          path_arr = path.split('/')\n\n          entry = root_tree.find do |entry|\n            entry[:name] == path_arr[0] && entry[:type] == :tree\n          end",
    "comment": "Recursive search of tree id for path  Ex. blog/            # oid: 1a app/           # oid: 2a models/      # oid: 3a views/       # oid: 4a   Tree.find_id_by_path(repo, '1a', 'app/models') # => '3a' ",
    "label": "",
    "id": "1787"
  },
  {
    "raw_code": "def batch(repository, blob_references, blob_size_limit: MAX_DATA_DISPLAY_SIZE)\n          blob_references.each_slice(BATCH_SIZE).flat_map do |refs|\n            repository.gitaly_blob_client.get_blobs(refs, blob_size_limit).to_a\n          end",
    "comment": "Returns an array of Blob instances, specified in blob_references as [[commit_sha, path], [commit_sha, path], ...]. If blob_size_limit < 0 then the full blob contents are returned. If blob_size_limit >= 0 then each blob will contain no more than limit bytes in its data attribute.  Keep in mind that this method may allocate a lot of memory. It is up to the caller to limit the number of blobs and blob_size_limit. ",
    "label": "",
    "id": "1788"
  },
  {
    "raw_code": "def batch_metadata(repository, blob_references)\n          batch(repository, blob_references, blob_size_limit: 0)\n        end",
    "comment": "Returns an array of Blob instances just with the metadata, that means the data attribute has no content.",
    "label": "",
    "id": "1789"
  },
  {
    "raw_code": "def batch_lfs_pointers(repository, blob_ids)\n          wrapped_gitaly_errors do\n            repository.gitaly_blob_client.batch_lfs_pointers(blob_ids.to_a)\n          end",
    "comment": "Find LFS blobs given an array of sha ids Returns array of Gitlab::Git::Blob Does not guarantee blob data will be set",
    "label": "",
    "id": "1790"
  },
  {
    "raw_code": "def load_all_data!(repository)\n        return if @data == '' # don't mess with submodule blobs\n\n        # Even if we return early, recalculate whether this blob is binary in\n        # case a blob was initialized as text but the full data isn't\n        @binary = nil\n\n        return if @loaded_all_data\n\n        @data = repository.gitaly_blob_client.get_blob(oid: id, limit: -1).data\n        @loaded_all_data = true\n        @loaded_size = @data.bytesize\n      end",
    "comment": "Load all blob data (not just the first MAX_DATA_DISPLAY_SIZE bytes) into memory as a Ruby string.",
    "label": "",
    "id": "1791"
  },
  {
    "raw_code": "def lfs_pointer?\n        self.class.size_could_be_lfs?(size) && has_lfs_version_key? && lfs_oid.present? && lfs_size.present?\n      end",
    "comment": "Valid LFS object pointer is a text file consisting of version oid size see https://github.com/github/git-lfs/blob/v1.1.0/docs/spec.md#the-pointer",
    "label": "",
    "id": "1792"
  },
  {
    "raw_code": "def handle_error(exception)\n        error = Gitlab::GitalyClient.unwrap_detailed_error(exception)\n        handle_detailed_error(error, exception) || handle_default_error(exception)\n      end",
    "comment": "handle_error first tries to handle the error as a detailed error, if no mapping found, it falls back to handling it as a default error according to the error code.",
    "label": "",
    "id": "1793"
  },
  {
    "raw_code": "def self.extract_branch_name(str)\n        str.delete_prefix('refs/heads/')\n      end",
    "comment": "Extract branch name from full ref path  Ex. Ref.extract_branch_name('refs/heads/master') #=> 'master'",
    "label": "",
    "id": "1794"
  },
  {
    "raw_code": "def initialize(repo, commit)\n        @id = commit.id\n\n        additions, deletions = fetch_stats(repo, commit)\n\n        @additions = additions.to_i\n        @deletions = deletions.to_i\n        @total = @additions + @deletions\n      end",
    "comment": "Instantiate a CommitStats object  Gitaly migration: https://gitlab.com/gitlab-org/gitaly/issues/323",
    "label": "",
    "id": "1795"
  },
  {
    "raw_code": "def flipper_id\n        \"Repository:#{@relative_path}\"\n      end",
    "comment": "Support Feature Flag Repository actor",
    "label": "",
    "id": "1796"
  },
  {
    "raw_code": "def root_ref(head_only: false)\n        wrapped_gitaly_errors do\n          gitaly_ref_client.default_branch_name(head_only: head_only)\n        end",
    "comment": "Default branch in the repository",
    "label": "",
    "id": "1797"
  },
  {
    "raw_code": "def branch_names\n        refs = list_refs([Gitlab::Git::BRANCH_REF_PREFIX])\n\n        refs.map { |ref| Gitlab::Git.branch_name(ref.name) }\n      end",
    "comment": "Returns an Array of branch names sorted by name ASC",
    "label": "",
    "id": "1798"
  },
  {
    "raw_code": "def branches\n        wrapped_gitaly_errors do\n          gitaly_ref_client.branches\n        end",
    "comment": "Returns an Array of Branches",
    "label": "",
    "id": "1799"
  },
  {
    "raw_code": "def find_branch(name)\n        wrapped_gitaly_errors do\n          gitaly_ref_client.find_branch(name)\n        end",
    "comment": "Directly find a branch with a simple name (e.g. master) ",
    "label": "",
    "id": "1800"
  },
  {
    "raw_code": "def branch_count\n        branch_names.count\n      end",
    "comment": "Returns the number of valid branches",
    "label": "",
    "id": "1801"
  },
  {
    "raw_code": "def tag_count\n        tag_names.count\n      end",
    "comment": "Returns the number of valid tags",
    "label": "",
    "id": "1802"
  },
  {
    "raw_code": "def tag_names\n        refs = list_refs([Gitlab::Git::TAG_REF_PREFIX])\n\n        refs.map { |ref| Gitlab::Git.tag_name(ref.name) }\n      end",
    "comment": "Returns an Array of tag names",
    "label": "",
    "id": "1803"
  },
  {
    "raw_code": "def tags(sort_by: nil, pagination_params: nil)\n        wrapped_gitaly_errors do\n          gitaly_ref_client.tags(sort_by: sort_by, pagination_params: pagination_params)\n        end",
    "comment": "Returns an Array of Tags ",
    "label": "",
    "id": "1804"
  },
  {
    "raw_code": "def ref_exists?(ref_name)\n        wrapped_gitaly_errors do\n          gitaly_ref_exists?(ref_name)\n        end",
    "comment": "Returns true if the given ref name exists  Ref names must start with `refs/`.",
    "label": "",
    "id": "1805"
  },
  {
    "raw_code": "def tag_exists?(name)\n        wrapped_gitaly_errors do\n          gitaly_ref_exists?(\"refs/tags/#{name}\")\n        end",
    "comment": "Returns true if the given tag exists  name - The name of the tag as a String.",
    "label": "",
    "id": "1806"
  },
  {
    "raw_code": "def branch_exists?(name)\n        wrapped_gitaly_errors do\n          gitaly_ref_exists?(\"refs/heads/#{name}\")\n        end",
    "comment": "Returns true if the given branch exists  name - The name of the branch as a String.",
    "label": "",
    "id": "1807"
  },
  {
    "raw_code": "def ref_names\n        branch_names + tag_names\n      end",
    "comment": "Returns an Array of branch and tag names",
    "label": "",
    "id": "1808"
  },
  {
    "raw_code": "def archive_prefix(ref, sha, project_path, append_sha:, path:)\n        ref = ref.gsub(%r{^refs/(?:heads|tags)/}, '')\n\n        append_sha = (ref != sha) if append_sha.nil?\n\n        formatted_ref = ref.tr('/', '-')\n\n        prefix_segments = [project_path, formatted_ref]\n        prefix_segments << sha if append_sha\n        prefix_segments << path.tr('/', '-').gsub(%r{^/|/$}, '') if path\n\n        prefix_segments.join('-')\n      end",
    "comment": "This is both the filename of the archive (missing the extension) and the name of the top-level member of the archive under which all files go",
    "label": "",
    "id": "1809"
  },
  {
    "raw_code": "def archive_file_path(storage_path, sha, name, format = \"tar.gz\")\n        # Build file path\n        return unless name\n\n        extension =\n          case format\n          when \"tar.bz2\", \"tbz\", \"tbz2\", \"tb2\", \"bz2\"\n            \"tar.bz2\"\n          when \"tar\"\n            \"tar\"\n          when \"zip\"\n            \"zip\"\n          else\n            # everything else should fall back to tar.gz\n            \"tar.gz\"\n          end",
    "comment": "The full path on disk where the archive should be stored. This is used to cache the archive between requests.  The path is a global namespace, so needs to be globally unique. This is achieved by including `gl_repository` in the path.  Archives relating to a particular ref when the SHA is not present in the filename must be invalidated when the ref is updated to point to a new SHA. This is achieved by including the SHA in the path.  As this is a full path on disk, it is not \"cloud native\". This should be resolved by either removing the cache, or moving the implementation into Gitaly and removing the ArchivePath parameter from the git-archive senddata response.",
    "label": "",
    "id": "1810"
  },
  {
    "raw_code": "def size\n        if Feature.enabled?(:use_repository_info_for_repository_size)\n          repository_info_size_megabytes\n        else\n          kilobytes = gitaly_repository_client.repository_size\n          (kilobytes.to_f / 1024).round(2)\n        end",
    "comment": "Return repo size in megabytes",
    "label": "",
    "id": "1811"
  },
  {
    "raw_code": "def recent_objects_size\n        wrapped_gitaly_errors do\n          recent_size_in_bytes = gitaly_repository_client.repository_info.objects.recent_size\n\n          Gitlab::Utils.bytes_to_megabytes(recent_size_in_bytes)\n        end",
    "comment": "Return repository recent objects size in mebibytes  This differs from the #size method in that it does not include the size of: - stale objects - cruft packs of unreachable objects  see: https://gitlab.com/gitlab-org/gitaly/-/blob/257ee33ca268d48c8f99dcbfeaaf7d8b19e07f06/internal/gitaly/service/repository/repository_info.go#L41-62",
    "label": "",
    "id": "1812"
  },
  {
    "raw_code": "def object_directory_size\n        gitaly_repository_client.get_object_directory_size.to_f * 1024\n      end",
    "comment": "Return git object directory size in bytes",
    "label": "",
    "id": "1813"
  },
  {
    "raw_code": "def log(options)\n        raise ArgumentError, 'Invalid message_regex pattern' unless valid_message_regex?(options[:message_regex])\n\n        options = DEFAULT_LOG_OPTIONS.merge(options)\n\n        limit = options[:limit]\n        if limit == 0 || !limit.is_a?(Integer)\n          raise ArgumentError, \"invalid Repository#log limit: #{limit.inspect}\"\n        end",
    "comment": "Build an array of commits.  Usage. repo.log( ref: 'master', path: 'app/models', limit: 10, offset: 5, after: Time.new(2016, 4, 21, 14, 32, 10), message_regex: 'project' )",
    "label": "",
    "id": "1814"
  },
  {
    "raw_code": "def blobs(revisions, with_paths: false, dynamic_timeout: nil)\n        revisions = revisions.reject { |rev| rev.blank? || Gitlab::Git.blank_ref?(rev) }\n\n        return [] if revisions.blank?\n\n        wrapped_gitaly_errors do\n          gitaly_blob_client.list_blobs(revisions, limit: REV_LIST_COMMIT_LIMIT,\n            with_paths: with_paths, dynamic_timeout: dynamic_timeout)\n        end",
    "comment": "List blobs reachable via a set of revisions. Supports the pseudo-revisions `--not` and `--all`. Uses the minimum of GitalyClient.medium_timeout and dynamic timeout if the dynamic timeout is set, otherwise it'll always use the medium timeout.",
    "label": "",
    "id": "1815"
  },
  {
    "raw_code": "def count_commits_between(from, to, options = {})\n        count_commits(from: from, to: to, **options)\n      end",
    "comment": "Counts the amount of commits between `from` and `to`.",
    "label": "",
    "id": "1816"
  },
  {
    "raw_code": "def raw_changes_between(old_rev, new_rev)\n        @raw_changes_between ||= {}\n\n        return [] if new_rev.blank? || Gitlab::Git.blank_ref?(new_rev)\n\n        @raw_changes_between[[old_rev, new_rev]] ||= wrapped_gitaly_errors do\n          gitaly_repository_client.raw_changes_between(old_rev, new_rev)\n            .each_with_object([]) do |msg, arr|\n            msg.raw_changes.each { |change| arr << ::Gitlab::Git::RawDiffChange.new(change) }\n          end",
    "comment": "old_rev and new_rev are commit ID's the result of this method is an array of Gitlab::Git::RawDiffChange",
    "label": "",
    "id": "1817"
  },
  {
    "raw_code": "def merge_base(...)\n        wrapped_gitaly_errors do\n          gitaly_repository_client.find_merge_base(...)\n        end",
    "comment": "Returns the SHA of the most recent common ancestor of +from+ and +to+",
    "label": "",
    "id": "1818"
  },
  {
    "raw_code": "def ancestor?(from, to)\n        gitaly_commit_client.ancestor?(from, to)\n      end",
    "comment": "Returns true is +from+ is direct ancestor to +to+, otherwise false",
    "label": "",
    "id": "1819"
  },
  {
    "raw_code": "def diff_blobs(...)\n        wrapped_gitaly_errors do\n          gitaly_diff_client.diff_blobs(...)\n        end",
    "comment": "Returns an array of DiffBlob objects that represent a diff between two blobs in a repository. For each diff generated, the pre-image and post-image blob IDs should be obtained using `find_changed_paths` method.",
    "label": "",
    "id": "1820"
  },
  {
    "raw_code": "def diff_blobs_with_raw_info(...)\n        wrapped_gitaly_errors do\n          gitaly_diff_client.diff_blobs_with_raw_info(...)\n        end",
    "comment": "Returns an array of DiffBlob objects that represent diffs between pairs of blobs in a repository using raw changed path information. More efficient for large batches of files compared to diff_blobs.",
    "label": "",
    "id": "1821"
  },
  {
    "raw_code": "def diff(from, to, options = {}, *paths)\n        iterator = gitaly_commit_client.diff(from, to, options.merge(paths: paths))\n\n        Gitlab::Git::DiffCollection.new(iterator, options)\n      end",
    "comment": "Return an array of Diff objects that represent the diff between +from+ and +to+.  See Diff::filter_diff_options for the allowed diff options.  The +options+ hash can also include :break_rewrites to split larger rewrites into delete/add pairs.",
    "label": "",
    "id": "1822"
  },
  {
    "raw_code": "def refs_hash\n        return @refs_hash if @refs_hash\n\n        @refs_hash = Hash.new { |h, k| h[k] = [] }\n\n        (tags + branches).each do |ref|\n          next unless ref.target && ref.name && ref.dereferenced_target&.id\n\n          @refs_hash[ref.dereferenced_target.id] << ref.name\n        end",
    "comment": "Get refs hash which key is the commit id and value is a Gitlab::Git::Tag or Gitlab::Git::Branch Note that both inherit from Gitlab::Git::Ref",
    "label": "",
    "id": "1823"
  },
  {
    "raw_code": "def refs_by_oid(oid:, limit: 0, ref_patterns: nil)\n        wrapped_gitaly_errors do\n          gitaly_ref_client.find_refs_by_oid(oid: oid, limit: limit, ref_patterns: ref_patterns) || []\n        end",
    "comment": "Returns matching refs for OID  Limit of 0 means there is no limit.",
    "label": "",
    "id": "1824"
  },
  {
    "raw_code": "def submodule_url_for(ref, path)\n        wrapped_gitaly_errors do\n          gitaly_submodule_url_for(ref, path)\n        end",
    "comment": "Returns url for submodule  Ex. @repository.submodule_url_for('master', 'rack') # => git@localhost:rack.git ",
    "label": "",
    "id": "1825"
  },
  {
    "raw_code": "def submodule_urls_for(ref)\n        wrapped_gitaly_errors do\n          gitaly_submodule_urls_for(ref)\n        end",
    "comment": "Returns path to url mappings for submodules  Ex. @repository.submodule_urls_for('master') # => { 'rack' => 'git@localhost:rack.git' } ",
    "label": "",
    "id": "1826"
  },
  {
    "raw_code": "def commit_count(ref)\n        wrapped_gitaly_errors do\n          gitaly_commit_client.commit_count(ref)\n        end",
    "comment": "Return total commits count accessible from passed ref",
    "label": "",
    "id": "1827"
  },
  {
    "raw_code": "def diverging_commit_count(from, to, max_count: 0)\n        wrapped_gitaly_errors do\n          gitaly_commit_client.diverging_commit_count(from, to, max_count: max_count)\n        end",
    "comment": "Return total diverging commits count",
    "label": "",
    "id": "1828"
  },
  {
    "raw_code": "def clean(options = {})\n        strategies = [:remove_untracked]\n        strategies.push(:force) if options[:f]\n        strategies.push(:remove_ignored) if options[:x]\n\n        # TODO: implement this method\n      end",
    "comment": "Mimic the `git clean` command and recursively delete untracked files. Valid keys that can be passed in the +options+ hash are:  :d - Remove untracked directories :f - Remove untracked directories that are managed by a different repository :x - Remove ignored files  The value in +options+ must evaluate to true for an option to take effect.  Examples:  repo.clean(d: true, f: true) # Enable the -d and -f options  repo.clean(d: false, x: true) # -x is enabled, -d is not",
    "label": "",
    "id": "1829"
  },
  {
    "raw_code": "def delete_branch(branch_name)\n        branch_name = \"#{Gitlab::Git::BRANCH_REF_PREFIX}#{branch_name}\" unless branch_name.start_with?(\"refs/\")\n\n        delete_refs(branch_name)\n      rescue CommandError => e\n        raise DeleteBranchError, e\n      end",
    "comment": "Delete the specified branch from the repository Note: No Git hooks are executed for this action",
    "label": "",
    "id": "1830"
  },
  {
    "raw_code": "def update_refs(ref_list)\n        wrapped_gitaly_errors do\n          gitaly_ref_client.update_refs(ref_list: ref_list) if ref_list.any?\n        end",
    "comment": "Update a list of references from X -> Y  Ref list is expected to be an array of hashes in the form: old_sha: new_sha reference:  When new_sha is Gitlab::Git::SHA1_BLANK_SHA, then this will be deleted",
    "label": "",
    "id": "1831"
  },
  {
    "raw_code": "def create_branch(ref, start_point = \"HEAD\")\n        write_ref(ref, start_point)\n      end",
    "comment": "Create a new branch named **ref+ based on **stat_point+, HEAD by default Note: No Git hooks are executed for this action  Examples: create_branch(\"feature\") create_branch(\"other-feature\", \"master\")",
    "label": "",
    "id": "1832"
  },
  {
    "raw_code": "def ls_files(ref)\n        gitaly_commit_client.ls_files(ref)\n      end",
    "comment": "Returns result like \"git ls-files\" , recursive and full file path  Ex. repo.ls_files('master') ",
    "label": "",
    "id": "1833"
  },
  {
    "raw_code": "def attributes(path)\n        info_attributes.attributes(path)\n      end",
    "comment": "Returns the Git attributes for the given file path.  See `Gitlab::Git::Attributes` for more information.",
    "label": "",
    "id": "1834"
  },
  {
    "raw_code": "def attributes_at(ref)\n        AttributesAtRefParser.new(self, ref)\n      end",
    "comment": "Returns parsed .gitattributes for a given ref  This only parses the root .gitattributes file, it does not traverse subfolders to find additional .gitattributes files  This method is around 30 times slower than `attributes`, which uses `$GIT_DIR/info/attributes`. Consider caching AttributesAtRefParser and reusing that for multiple calls instead of this method.",
    "label": "",
    "id": "1835"
  },
  {
    "raw_code": "def list_refs(...)\n        wrapped_gitaly_errors do\n          gitaly_ref_client.list_refs(...)\n        end",
    "comment": "peel_tags slows down the request by a factor of 3-4",
    "label": "",
    "id": "1836"
  },
  {
    "raw_code": "def commit(ref = 'HEAD')\n        Gitlab::Git::Commit.find(self, ref)\n      end",
    "comment": "Refactoring aid; allows us to copy code from app/models/repository.rb",
    "label": "",
    "id": "1837"
  },
  {
    "raw_code": "def fetch_remote(\n        url,\n        refmap: nil, ssh_auth: nil, forced: false, no_tags: false, prune: true,\n        http_authorization_header: \"\", resolved_address: \"\")\n        wrapped_gitaly_errors do\n          gitaly_repository_client.fetch_remote(\n            url,\n            refmap: refmap,\n            ssh_auth: ssh_auth,\n            forced: forced,\n            no_tags: no_tags,\n            prune: prune,\n            timeout: GITLAB_PROJECTS_TIMEOUT,\n            http_authorization_header: http_authorization_header,\n            resolved_address: resolved_address\n          )\n        end",
    "comment": "Fetch remote for repository  remote - remote name url - URL of the remote to fetch. `remote` is not used in this case. refmap - if url is given, determines which references should get fetched where ssh_auth - SSH known_hosts data and a private key to use for public-key authentication forced - should we use --force flag? no_tags - should we use --no-tags flag? prune - should we use --prune flag? resolved_address - resolved IP address for provided URL",
    "label": "",
    "id": "1838"
  },
  {
    "raw_code": "def batch_blobs(items, blob_size_limit: Gitlab::Git::Blob::MAX_DATA_DISPLAY_SIZE)\n        Gitlab::Git::Blob.batch(self, items, blob_size_limit: blob_size_limit)\n      end",
    "comment": "Items should be of format [[commit_id, path], [commit_id1, path1]]",
    "label": "",
    "id": "1839"
  },
  {
    "raw_code": "def commit_files(\n        user, branch_name:, message:, actions:,\n        author_email: nil, author_name: nil,\n        start_branch_name: nil, start_sha: nil, start_repository: nil,\n        force: false, sign: true, target_sha: nil\n      )\n        wrapped_gitaly_errors do\n          gitaly_operation_client.user_commit_files(user, branch_name,\n            message, actions, author_email, author_name, start_branch_name,\n            start_repository, force, start_sha, sign, target_sha)\n        end",
    "comment": "Creates a commit  @param [User] user The committer of the commit. @param [String] branch_name: The name of the branch to be created/updated. @param [String] message: The commit message. @param [Array<Hash>] actions: An array of files to be added/updated/removed. @option actions: [Symbol] :action One of :create, :create_dir, :update, :move, :delete, :chmod @option actions: [String] :file_path The path of the file or directory being added/updated/removed. @option actions: [String] :previous_path The path of the file being moved. Only used for the :move action. @option actions: [String,IO] :content The file content for :create or :update @option actions: [String] :encoding One of text, base64 @option actions: [Boolean] :execute_filemode True sets the executable filemode on the file. @option actions: [Boolean] :infer_content True uses the existing file contents instead of using content on move. @param [String] author_email: The authors email, if unspecified the committers email is used. @param [String] author_name: The authors name, if unspecified the committers name is used. @param [String] start_branch_name: The name of the branch to be used as the parent of the commit. Only used if start_sha: is unspecified. @param [String] start_sha: The sha to be used as the parent of the commit. @param [Gitlab::Git::Repository] start_repository: The repository that contains the start branch or sha. Defaults to use this repository. @param [Boolean] force: Force update the branch. @param [String] target_sha: The latest sha of the target branch (optional). Used to prevent races in updates between different clients. @return [Gitlab::Git::OperationService::BranchUpdate]  rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "1840"
  },
  {
    "raw_code": "def disconnect_alternates\n        wrapped_gitaly_errors do\n          gitaly_repository_client.disconnect_alternates\n        end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "1841"
  },
  {
    "raw_code": "def empty_tree_id\n        container.repository.empty_tree_id\n      end",
    "comment": "Note: this problem should be addressed in https://gitlab.com/gitlab-org/gitlab/-/issues/441770 Gitlab::Git::Repository shouldn't call Repository directly Instead empty_tree_id value should be passed to Gitaly client via method arguments",
    "label": "",
    "id": "1842"
  },
  {
    "raw_code": "def detect_generated_files(base, head, changed_paths)\n        return Set.new if changed_paths.blank?\n\n        # We only display diffs upto the diff_max_files size so we can avoid\n        # checking the rest if it exceeds the limit.\n        changed_paths = changed_paths.take(Gitlab::CurrentSettings.diff_max_files)\n\n        # Check .gitattributes overrides first\n        checked_files = get_file_attributes(\n          base,\n          changed_paths.map(&:path),\n          Gitlab::Git::ATTRIBUTE_OVERRIDES[:generated]\n        ).map { |attrs| { path: attrs[:path], generated: attrs[:value] == \"set\" } }\n\n        # Check automatic generated file detection for the remaining paths\n        overridden_paths = checked_files.pluck(:path)\n        remainder = changed_paths.reject { |changed_path| overridden_paths.include?(changed_path.path) }\n        checked_files += check_blobs_generated(base, head, remainder) if remainder.present?\n\n        checked_files\n          .select { |attrs| attrs[:generated] }\n          .pluck(:path)\n          .to_set\n\n      rescue Gitlab::Git::CommandError, Gitlab::Git::ResourceExhaustedError => e\n        # An exception can be raised due to an unknown revision or paths.\n        # Gitlab::Git::ResourceExhaustedError could be raised if the request payload is too large.\n        Gitlab::ErrorTracking.track_exception(\n          e,\n          gl_project_path: @gl_project_path,\n          base: base,\n          head: head,\n          paths_count: changed_paths.count,\n          paths_bytesize: changed_paths.map(&:path).join.bytesize\n        )\n\n        Set.new\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- not an active record operation",
    "label": "",
    "id": "1843"
  },
  {
    "raw_code": "def gitaly_ref_exists?(ref_name)\n        gitaly_ref_client.ref_exists?(ref_name)\n      end",
    "comment": "Returns true if the given ref name exists  Ref names must start with `refs/`.",
    "label": "",
    "id": "1844"
  },
  {
    "raw_code": "def extract_commit_id_from_ref(ref)\n        return ref if Gitlab::Git.commit_id?(ref)\n\n        tag = find_tag(ref)\n        return tag.dereferenced_target.sha if tag\n\n        branch = find_branch(ref)\n        return branch.dereferenced_target.sha if branch\n\n        ref\n      end",
    "comment": "The order is based on git priority to resolve ambiguous references  `git show <ref>`  In case of name clashes, it uses this order: 1. Commit 2. Tag 3. Branch",
    "label": "",
    "id": "1845"
  },
  {
    "raw_code": "def filter_diff_options(options, default_options = {})\n          allowed_options = [:ignore_whitespace_change, :max_files, :max_lines,\n                             :limits, :expanded, :collect_all_paths, :generated_files, :offset_index]\n\n          if default_options\n            actual_defaults = default_options.dup\n            actual_defaults.keep_if do |key|\n              allowed_options.include?(key)\n            end",
    "comment": "Return a copy of the +options+ hash containing only recognized keys. Allowed options are:  :ignore_whitespace_change :: If true, changes in amount of whitespace will be ignored.  :max_files :: Limit how many files will patches be allowed for before collapsing  :max_lines :: Limit how many patch lines (across all files) will be allowed for before collapsing  :limits :: A hash with additional limits to check before collapsing patches. Allowed keys are: `max_bytes`, `safe_max_files`, `safe_max_lines` and `safe_max_bytes`  :expanded :: If false, patch raw data will not be included in the diff after `max_files`, `max_lines` or any of the limits in `limits` are exceeded :generated_files :: If the list of generated files is given, those files will be marked as generated.",
    "label": "",
    "id": "1846"
  },
  {
    "raw_code": "def binary_message(old_path, new_path)\n          \"Binary files #{old_path} and #{new_path} differ\\n\"\n        end",
    "comment": "Return a binary diff message like:  \"Binary files a/file/path and b/file/path differ\\n\" This is used when we detect that a diff is binary using CharlockHolmes.",
    "label": "",
    "id": "1847"
  },
  {
    "raw_code": "def patch_safe_limit_bytes(limit = patch_hard_limit_bytes)\n          limit / 10\n        end",
    "comment": "Returns the limit of bytes a single diff file can reach before it appears as 'collapsed' for end-users. By convention, it's 10% of the persisted `diff_max_patch_bytes`.  Example: If we have 100k for the `diff_max_patch_bytes`, it will be 10k by default.  Patches surpassing this limit should still be persisted in the database.",
    "label": "",
    "id": "1848"
  },
  {
    "raw_code": "def patch_hard_limit_bytes\n          Gitlab::CurrentSettings.diff_max_patch_bytes\n        end",
    "comment": "Returns the limit for a single diff file (patch).  Patches surpassing this limit shouldn't be persisted in the database and will be presented as 'too large' for end-users.",
    "label": "",
    "id": "1849"
  },
  {
    "raw_code": "def size\n          @size ||= @patches.sum(&:size)\n        end",
    "comment": "`@patches` is not an `ActiveRecord` relation, but an `Enumerable` We're using sum from `ActiveSupport`",
    "label": "",
    "id": "1850"
  },
  {
    "raw_code": "def initialize(group_path:, seed_count:, publish:)\n            @group = Group.find_by_full_path(group_path)\n            @seed_count = seed_count\n            @publish = publish\n            @current_user = @group&.first_owner\n          end",
    "comment": "Initializes the class  @param [String] Path of the group to find @param [Integer] Number of resources to create @param[Boolean] If the created resources should be published or not, defaults to false",
    "label": "",
    "id": "1851"
  },
  {
    "raw_code": "def initialize(logger = Gitlab::AppLogger, projects_to_runners:, job_count:, username:)\n            @logger = logger\n            @user = User.find_by_username!(username.presence || DEFAULT_USERNAME)\n            @projects_to_runners = projects_to_runners.map do |v|\n              { project_id: v[:project_id], runners: ::Ci::Runner.id_in(v[:runner_ids]).to_a }\n            end",
    "comment": "Initializes the class  @param [Gitlab::Logger] logger @param [Integer] job_count the number of jobs to create across the runners @param [Array<Hash>] projects_to_runners list of project IDs to respective runner IDs @param [String] username the user that will create the pipelines",
    "label": "",
    "id": "1852"
  },
  {
    "raw_code": "def initialize(logger = Gitlab::AppLogger, **options)\n            username = options[:username] || DEFAULT_USERNAME\n\n            @logger = logger\n            @user = User.find_by_username(username)\n            @registration_prefix = options[:registration_prefix] || DEFAULT_PREFIX\n            @runner_count = options[:runner_count] || DEFAULT_RUNNER_COUNT\n            @organization_id = options[:organization_id] || @user.organization.id\n            @groups = {}\n            @projects = {}\n          end",
    "comment": "Initializes the class  @param [Gitlab::Logger] logger @param [Hash] options @option options [String] :username username of the user that will create the fleet @option options [String] :registration_prefix string to use as prefix in group, project, and runner names @option options [Integer] :runner_count number of runners to create across the groups and projects @return [Array<Hash>] list of project IDs to respective runner IDs",
    "label": "",
    "id": "1853"
  },
  {
    "raw_code": "def seed\n            return unless within_plan_limits?\n\n            logger.info(\n              message: 'Starting seed of runner fleet',\n              user_id: @user.id,\n              registration_prefix: @registration_prefix,\n              runner_count: @runner_count\n            )\n\n            groups_and_projects = create_groups_and_projects\n            runner_ids = create_runners(groups_and_projects)\n\n            logger.info(\n              message: 'Completed seeding of runner fleet',\n              registration_prefix: @registration_prefix,\n              groups: @groups.count,\n              projects: @projects.count,\n              runner_count: @runner_count\n            )\n\n            %i[project_1_1_1_1 project_1_1_2_1 project_2_1_1].map do |project_key|\n              { project_id: groups_and_projects[project_key].id, runner_ids: runner_ids[project_key] }\n            end",
    "comment": "seed returns an array of hashes of projects to its assigned runners",
    "label": "",
    "id": "1854"
  },
  {
    "raw_code": "def self.ca_certs_paths\n        cert_paths = Dir[\"#{default_cert_dir}/*\"].select do |path|\n          !File.directory?(path) && File.readable?(path)\n        end",
    "comment": "Returns all top-level, readable files in the default CA cert directory",
    "label": "",
    "id": "1855"
  },
  {
    "raw_code": "def self.ca_certs_bundle\n        strong_memoize(:ca_certs_bundle) do\n          ca_certs_paths.flat_map do |cert_file|\n            load_ca_certs_bundle(File.read(cert_file))\n          rescue OpenSSL::OpenSSLError => e\n            Gitlab::ErrorTracking.track_and_raise_for_dev_exception(e, cert_file: cert_file)\n          end.uniq.join(\"\\n\")\n        end",
    "comment": "Returns a concatenated array of Strings, each being a PEM-coded CA certificate.",
    "label": "",
    "id": "1856"
  },
  {
    "raw_code": "def self.load_ca_certs_bundle(ca_certs_string)\n        return [] unless ca_certs_string\n\n        ca_certs_string.scan(CERT_REGEX).map do |ca_cert_string|\n          OpenSSL::X509::Certificate.new(ca_cert_string)\n        end",
    "comment": "Returns an array of OpenSSL::X509::Certificate objects, empty array if none found  Ruby OpenSSL::X509::Certificate.new will only load the first certificate if a bundle is presented, this allows to parse multiple certs in the same file",
    "label": "",
    "id": "1857"
  },
  {
    "raw_code": "def find_match_line_header(index)\n        return @match_line_headers[index] if @match_line_headers.key?(index)\n\n        @match_line_headers[index] = begin\n          if index >= 0\n            line = lines[index]\n\n            if line.type.nil? && line.text.match(/\\A[A-Za-z$_]/)\n              \" #{line.text}\"\n            else\n              find_match_line_header(index - 1)\n            end",
    "comment": "Any line beginning with a letter, an underscore, or a dollar can be used in a match line header. Only context sections can contain match lines, as match lines have to exist in both versions of the file.",
    "label": "",
    "id": "1858"
  },
  {
    "raw_code": "def update_match_line_text(match_line, line)\n        return unless match_line\n\n        header = find_match_line_header(match_line.index - 1)\n\n        match_line.text = \"@@ -#{match_line.old_pos},#{line.old_pos} +#{match_line.new_pos},#{line.new_pos} @@#{header}\"\n      end",
    "comment": "Set the match line's text for the current line. A match line takes its start position and context header (where present) from itself, and its end position from the line passed in.",
    "label": "",
    "id": "1859"
  },
  {
    "raw_code": "def with_skipped_callbacks_for(model)\n        raise StandardError, 'can only be used in a Rake environment' unless Gitlab::Runtime.rake?\n\n        skip_callbacks_for_model(model)\n\n        yield\n\n        skip_callbacks_for_model(model, reset: true)\n      end",
    "comment": "WARNING: using this logic in other places than a Rake task will need a different approach, as simply setting the callback again is not thread-safe",
    "label": "",
    "id": "1860"
  },
  {
    "raw_code": "def track_event(event_name, values:, property_name: nil, time: Time.current)\n          track(values, event_name, property_name: property_name, time: time)\n        end",
    "comment": "Track unique events  event_name - The event name. values - One or multiple values counted. property_name - Name of the values counted. time - Time of the action, set to Time.current.",
    "label": "",
    "id": "1861"
  },
  {
    "raw_code": "def unique_events(event_names:, start_date:, end_date:, property_name: nil)\n          used_in_aggregate_metric = event_names.is_a?(Array) && event_names.size > 1\n\n          count_unique_events(event_names: event_names, property_name: property_name, start_date: start_date, end_date: end_date, used_in_aggregate_metric: used_in_aggregate_metric)\n        end",
    "comment": "Count unique events for a given time range.  event_names - The list of the events to count. property_names - The list of the values for which the events are to be counted. start_date  - The start date of the time range. end_date  - The end date of the time range.",
    "label": "",
    "id": "1862"
  },
  {
    "raw_code": "def track_unique_action(name, args:, user:, project:)\n          return unless user\n\n          args ||= ''\n          name = prepare_name(name, args)\n\n          if INTERNAL_EVENTS.include?(name)\n            Gitlab::InternalEvents.track_event(\n              \"i_quickactions_#{name}\",\n              user: user,\n              project: project,\n              additional_properties: prepare_additional_properties(name, args)\n            )\n          else\n            # Legacy event implementation. Migrate existing events to internal events.\n            # See implementation of `convert_to_ticket` quickaction and\n            # https://docs.gitlab.com/ee/development/internal_analytics/internal_event_instrumentation/migration.html#backend-1\n            Gitlab::UsageDataCounters::HLLRedisCounter.track_event(:\"i_quickactions_#{name}\", values: user.id)\n          end",
    "comment": "Tracks the quick action with name `name`. `args` is expected to be a single string, will be split internally when necessary.",
    "label": "",
    "id": "1863"
  },
  {
    "raw_code": "def enqueue_stats_job(request_id)\n        return unless Feature.enabled?(:performance_bar_stats, type: :ops)\n\n        cache_existed = @client.exists?(GitlabPerformanceBarStatsWorker::STATS_KEY)\n        @client.sadd?(GitlabPerformanceBarStatsWorker::STATS_KEY, request_id)\n\n        return if cache_existed\n\n        # stats key should be periodically processed and deleted by\n        # GitlabPerformanceBarStatsWorker but if it doesn't happen for\n        # some reason, we set expiration for the stats key to avoid\n        # keeping millions of request ids which would be already expired\n        # anyway\n        @client.expire(\n          GitlabPerformanceBarStatsWorker::STATS_KEY,\n          GitlabPerformanceBarStatsWorker::STATS_KEY_EXPIRE\n        )\n      end",
    "comment": "schedules a job which parses peek profile data and adds them to a structured log rubocop:disable Gitlab/ModuleWithInstanceVariables rubocop:disable CodeReuse/ActiveRecord -- needed because of `.exists?` method usage (which is actually not AR method)",
    "label": "",
    "id": "1864"
  },
  {
    "raw_code": "def initialize(level)\n        @level = level\n      end",
    "comment": "@param [Integer] level The branch protection level as an Integer.",
    "label": "",
    "id": "1865"
  },
  {
    "raw_code": "def initialize(gist, user_id)\n          @gist = gist\n          @user = User.find(user_id)\n        end",
    "comment": "gist - An instance of `Gitlab::GithubGistsImport::Representation::Gist`.",
    "label": "",
    "id": "1866"
  },
  {
    "raw_code": "def execute\n          validate_gist!\n\n          @snippet = build_snippet\n          import_repository if snippet.save!\n          validate_repository!\n\n          ServiceResponse.success\n        rescue FileCountLimitError, RepoSizeLimitError, SnippetRepositoryError => exception\n          fail_and_track(snippet, exception)\n        end",
    "comment": "Marking for refactor in: https://gitlab.com/gitlab-org/gitlab/-/issues/469564 We'll want to use the Snippets::CreateService here.",
    "label": "",
    "id": "1867"
  },
  {
    "raw_code": "def organization_id\n          user.organization.id\n        end",
    "comment": "Duplicate logic in app/services/snippets/create_service.rb Remove as part of refactor in: https://gitlab.com/gitlab-org/gitlab/-/issues/469564",
    "label": "",
    "id": "1868"
  },
  {
    "raw_code": "def self.from_api_response(gist, additional_data = {})\n          hash = {\n            id: gist[:id],\n            description: gist[:description],\n            is_public: gist[:public],\n            files: gist[:files],\n            git_pull_url: gist[:git_pull_url],\n            created_at: gist[:created_at],\n            updated_at: gist[:updated_at]\n          }\n\n          new(hash)\n        end",
    "comment": "Builds a gist from a GitHub API response.  gist - An instance of `Hash` containing the gist details.",
    "label": "",
    "id": "1869"
  },
  {
    "raw_code": "def self.from_json_hash(raw_hash)\n          new(Gitlab::GithubImport::Representation.symbolize_hash(raw_hash))\n        end",
    "comment": "Builds a new gist using a Hash that was built from a JSON payload.",
    "label": "",
    "id": "1870"
  },
  {
    "raw_code": "def initialize(attributes)\n          @attributes = attributes\n        end",
    "comment": "attributes - A hash containing the raw gist details. The keys of this Hash (and any nested hashes) must be symbols.",
    "label": "",
    "id": "1871"
  },
  {
    "raw_code": "def truncated_title\n          title = description.presence || first_file[:file_name]\n\n          title.truncate(255)\n        end",
    "comment": "Gist description can be an empty string, so we returning nil to use first file name as a title in such case on snippet creation Gist description has a limit of 256, while the snippet's title can be up to 255",
    "label": "",
    "id": "1872"
  },
  {
    "raw_code": "def initialize(attrs = {})\n        @formatter = get_formatter_class(attrs[:position_type]).new(attrs)\n      end",
    "comment": "A position can belong to a text line or to an image coordinate it depends of the position_type argument. Text position will have: new_line and old_line Image position will have: width, height, x, y",
    "label": "",
    "id": "1873"
  },
  {
    "raw_code": "def init_with(coder)\n        initialize(coder['attributes'])\n\n        self\n      end",
    "comment": "`Gitlab::Diff::Position` objects are stored as serialized attributes in `DiffNote`, which use YAML to encode and decode objects. `#init_with` and `#encode_with` can be used to customize the en/decoding behavior. In this case, we override these to prevent memoized instance variables like `@diff_file` and `@diff_line` from being serialized.",
    "label": "",
    "id": "1874"
  },
  {
    "raw_code": "def each\n        # Prefixes of all diff lines, indicating their types\n        # For example: `\" - +  -+  ---+++ --+  -++\"`\n        line_prefixes = lines.each_with_object(+\"\") { |line, s| s << (line[0] || ' ') }.gsub(/[^ +-]/, ' ')\n\n        line_prefixes.scan(LINE_PAIRS_PATTERN) do\n          # For `\"---+++\"`, `begin_index == 0`, `end_index == 6`\n          begin_index, end_index = Regexp.last_match.offset(:del_ins)\n\n          # For `\"---+++\"`, `changed_line_count == 3`\n          changed_line_count = (end_index - begin_index) / 2\n\n          halfway_index = begin_index + changed_line_count\n          (begin_index...halfway_index).each do |i|\n            # For `\"---+++\"`, index 1 maps to 1 + 3 = 4\n            yield [i, i + changed_line_count]\n          end",
    "comment": "Finds pairs of old/new line pairs that represent the same line that changed rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1875"
  },
  {
    "raw_code": "def old_to_new(old_line)\n        map_line_number(old_line, from: :old_line, to: :new_line)\n      end",
    "comment": "Find new line number for old line number.",
    "label": "",
    "id": "1876"
  },
  {
    "raw_code": "def new_to_old(new_line)\n        map_line_number(new_line, from: :new_line, to: :old_line)\n      end",
    "comment": "Find old line number for new line number.",
    "label": "",
    "id": "1877"
  },
  {
    "raw_code": "def map_line_number(from_line, from:, to:)\n        # If no diff file could be found, the file wasn't changed, and the\n        # mapped line number is the same as the specified line number.\n        return from_line unless diff_file\n\n        # To find the mapped line number for the specified line number,\n        # we need to find:\n        # - The diff line with that exact line number, if it is in the diff context\n        # - The first diff line with a higher line number, if it falls between diff contexts\n        # - The last known diff line, if it falls after the last diff context\n        diff_line = diff_lines.find do |diff_line|\n          diff_from_line = diff_line.public_send(from) # rubocop:disable GitlabSecurity/PublicSend\n          diff_from_line && diff_from_line >= from_line\n        end",
    "comment": "Find old/new line number based on its old/new counterpart line number.",
    "label": "",
    "id": "1878"
  },
  {
    "raw_code": "def as_json(opts = nil)\n        DiffLineSerializer.new.represent(self)\n      end",
    "comment": "We have to keep this here since it is still used for conflict resolution Conflict::File#as_json renders json diff lines in sections",
    "label": "",
    "id": "1879"
  },
  {
    "raw_code": "def blob_highlighting(diff_line)\n        rich_line =\n          if diff_line.unchanged? || diff_line.added?\n            new_lines[diff_line.new_pos - 1]&.html_safe\n          elsif diff_line.removed?\n            old_lines[diff_line.old_pos - 1]&.html_safe\n          end",
    "comment": "Deprecated: https://gitlab.com/gitlab-org/gitlab/-/issues/324159 ------------------------------------------------------------------------",
    "label": "",
    "id": "1880"
  },
  {
    "raw_code": "def complete?\n        start_sha.present? && head_sha.present?\n      end",
    "comment": "There is only one case in which we will have `start_sha` and `head_sha`, but not `base_sha`, which is when a diff is generated between an orphaned branch and another branch, which means there _is_ no base, but we're still able to highlight it, and to create diff notes, which are the primary things `DiffRefs` are used for. `DiffRefs` are \"complete\" when they have `start_sha` and `head_sha`, because `base_sha` can always be derived from this, to return an actual sha, or `nil`. We have `base_sha` directly available on `DiffRefs` because it's faster# than having to look it up in the repo every time.",
    "label": "",
    "id": "1881"
  },
  {
    "raw_code": "def decorate(diff_file)\n        content = read_file(diff_file)\n\n        return [] unless content\n\n        # TODO: We could add some kind of flag to #initialize that would allow\n        #   us to force re-caching\n        #   https://gitlab.com/gitlab-org/gitlab/-/issues/263508\n        #\n        if content.empty? && recache_due_to_size?(diff_file)\n          # If the file is missing from the cache and there's reason to believe\n          #   it is uncached due to a size issue around changing the values for\n          #   max patch size, manually populate the hash and then set the value.\n          #\n          new_cache_content = {}\n          new_cache_content[diff_file.file_path] = diff_file.highlighted_diff_lines.map(&:to_hash)\n\n          write_to_redis_hash(new_cache_content)\n\n          set_highlighted_diff_lines(diff_file, read_file(diff_file))\n        else\n          set_highlighted_diff_lines(diff_file, content)\n        end",
    "comment": "- Reads from cache - Assigns DiffFile#highlighted_diff_lines for cached files ",
    "label": "",
    "id": "1882"
  },
  {
    "raw_code": "def write_if_empty\n        return if cacheable_files.empty?\n\n        new_cache_content = {}\n\n        cacheable_files.each do |diff_file|\n          new_cache_content[diff_file.file_path] = diff_file.highlighted_diff_lines.map(&:to_hash)\n        end",
    "comment": "For every file that isn't already contained in the redis hash, store the result of #highlighted_diff_lines, then submit the uncached content to #write_to_redis_hash to submit a single write. This avoids excessive IO generated by N+1's (1 writing for each highlighted line or file). ",
    "label": "",
    "id": "1883"
  },
  {
    "raw_code": "def write_to_redis_hash(hash)\n        with_redis do |redis|\n          redis.pipelined do |pipeline|\n            hash.each do |diff_file_id, highlighted_diff_lines_hash|\n              pipeline.hset(\n                key,\n                diff_file_id,\n                gzip_compress(highlighted_diff_lines_hash.to_json)\n              )\n            rescue Encoding::UndefinedConversionError, EncodingError, JSON::GeneratorError\n              nil\n            end",
    "comment": "Given a hash of: { \"file/to/cache\" => [ { line_code: \"a5cc2925ca8258af241be7e5b0381edf30266302_19_19\", rich_text: \" <span id=\\\"LC19\\\" class=\\\"line\\\" lang=\\\"plaintext\\\">config/initializers/secret_token.rb</span>\\n\", text: \" config/initializers/secret_token.rb\", type: nil, index: 3, old_pos: 19, new_pos: 19 } ] }  ...it will write/update a Gitlab::Redis hash (HSET) ",
    "label": "",
    "id": "1884"
  },
  {
    "raw_code": "def compare_path_parts(a_parts, b_parts)\n        a_part = a_parts.shift\n        b_part = b_parts.shift\n\n        return B_FOLLOWS_A if a_parts.size < b_parts.size && a_parts.empty?\n        return A_FOLLOWS_B if a_parts.size > b_parts.size && b_parts.empty?\n\n        comparison = a_part <=> b_part\n\n        return comparison unless comparison == EQUIVALENT\n        return compare_path_parts(a_parts, b_parts) if a_parts.any? && b_parts.any?\n\n        # If A and B have the same name (e.g. symlink change), they are identical so return 0\n        EQUIVALENT\n      end",
    "comment": "Used for sorting the file paths by: 1. Directory name 2. Depth 3. File name",
    "label": "",
    "id": "1885"
  },
  {
    "raw_code": "def parse(text, position:, project:, supports_suggestion: true)\n          return [] unless position.complete?\n\n          html = Banzai.render(text, project: nil,\n            no_original_data: true,\n            suggestions_filter_enabled: supports_suggestion)\n          doc = Nokogiri::HTML(html)\n          suggestion_nodes = doc.xpath(XPATH)\n\n          return [] if suggestion_nodes.empty?\n\n          diff_file = position.diff_file(project.repository)\n\n          suggestion_nodes.map do |node|\n            lang_param = node['data-lang-params']\n\n            lines_above, lines_below = nil\n\n            if lang_param && suggestion_params = fetch_suggestion_params(lang_param)\n              lines_above = suggestion_params[:above]\n              lines_below = suggestion_params[:below]\n            end",
    "comment": "Returns an array of Gitlab::Diff::Suggestion which represents each suggestion in the given text. ",
    "label": "",
    "id": "1886"
  },
  {
    "raw_code": "def unfolded_diff_lines\n        strong_memoize(:unfolded_diff_lines) do\n          next unless unfold_required?\n\n          merged_diff_with_blob_lines\n        end",
    "comment": "Returns merged diff lines with required blob lines with correct positions.",
    "label": "",
    "id": "1887"
  },
  {
    "raw_code": "def blob_lines\n        strong_memoize(:blob_lines) do\n          # Blob lines, unlike diffs, doesn't start with an empty space for\n          # unchanged line, so the parsing and highlighting step can get fuzzy\n          # without the following change.\n          line_prefix = ' '\n          blob_as_diff_lines = @blob.data.each_line.map { |line| \"#{line_prefix}#{line}\" }\n\n          lines = Gitlab::Diff::Parser.new.parse(blob_as_diff_lines, diff_file: @diff_file).to_a\n\n          from = from_blob_line - 1\n          to = to_blob_line - 1\n\n          lines[from..to]\n        end",
    "comment": "Returns the extracted lines from the old blob which should be merged with the current diff lines.",
    "label": "",
    "id": "1888"
  },
  {
    "raw_code": "def blob_lines_with_matches\n        old_pos = from_blob_line\n        new_pos = from_blob_line + offset\n\n        new_blob_lines = []\n\n        new_blob_lines.push(top_blob_match_line) if top_blob_match_line\n\n        blob_lines.each do |line|\n          new_blob_lines << Gitlab::Diff::Line.new(line.text, line.type, nil, old_pos, new_pos,\n            parent_file: @diff_file)\n\n          old_pos += 1\n          new_pos += 1\n        end",
    "comment": "Returns 'unchanged' blob lines with recalculated `old_pos` and `new_pos` and the recalculated new match line (needed if we for instance we unfolded once, but there are still folded lines).",
    "label": "",
    "id": "1889"
  },
  {
    "raw_code": "def calculate_from_blob_line!\n        return unless unfold_required?\n\n        from = comment_position - UNFOLD_CONTEXT_SIZE\n\n        prev_line_number =\n          if bottom?\n            last_line.old_pos\n          else\n            # There's no line before the match if it's in the top-most\n            # position.\n            line_before_unfold_position&.old_pos || 0\n          end",
    "comment": "Returns the first line position that should be extracted from `blob_lines`.",
    "label": "",
    "id": "1890"
  },
  {
    "raw_code": "def calculate_to_blob_line!\n        return unless unfold_required?\n\n        to = comment_position + UNFOLD_CONTEXT_SIZE\n\n        return to if bottom?\n\n        next_line_number = line_after_unfold_position.old_pos\n\n        if to >= next_line_number - 1\n          @generate_bottom_match_line = false\n          to = next_line_number - 1\n        end",
    "comment": "Returns the last line position that should be extracted from `blob_lines`.",
    "label": "",
    "id": "1891"
  },
  {
    "raw_code": "def unfold_line\n        strong_memoize(:unfold_line) do\n          next last_line if bottom?\n\n          @diff_file.diff_lines.find do |line|\n            line.old_pos > comment_position && line.type == 'match'\n          end",
    "comment": "Returns the line which needed to be expanded in order to send a comment in `@position`.",
    "label": "",
    "id": "1892"
  },
  {
    "raw_code": "def initialize(collection, diff_head_sha = nil)\n        @collection = collection\n        @diff_head_sha = diff_head_sha\n      end",
    "comment": "collection - An array of Gitlab::Diff::Position",
    "label": "",
    "id": "1893"
  },
  {
    "raw_code": "def unfoldable\n        select do |position|\n          position.unfoldable? && valid_head_sha?(position)\n        end",
    "comment": "Doing a lightweight filter in-memory given we're not prepared for querying positions (https://gitlab.com/gitlab-org/gitlab/issues/33271).",
    "label": "",
    "id": "1894"
  },
  {
    "raw_code": "def diff_hunk(diff_line)\n        diff_line_index = diff_line.index\n        # @@ (match) header is not kept if it's found in the top of the file,\n        # therefore we should keep an extra line on this scenario.\n        diff_line_index += 1 unless diff_lines.first.match?\n\n        diff_lines.select { |line| line.index <= diff_line_index }.map(&:text).join(\"\\n\")\n      end",
    "comment": "Returns the raw diff content up to the given line index",
    "label": "",
    "id": "1895"
  },
  {
    "raw_code": "def diff_lines\n        @diff_lines ||=\n          Gitlab::Diff::Parser.new.parse(raw_diff.each_line, diff_file: self).to_a\n      end",
    "comment": "Array of Gitlab::Diff::Line objects",
    "label": "",
    "id": "1896"
  },
  {
    "raw_code": "def unfold_diff_lines(position)\n        return unless position\n\n        unfolder = Gitlab::Diff::LinesUnfolder.new(self, position)\n\n        if unfolder.unfold_required?\n          @diff_lines = unfolder.unfolded_diff_lines\n          @unfolded = true\n        end",
    "comment": "Changes diff_lines according to the given position. That is, it checks whether the position requires blob lines into the diff in order to be presented.",
    "label": "",
    "id": "1897"
  },
  {
    "raw_code": "def parallel_diff_lines\n        @parallel_diff_lines ||= Gitlab::Diff::ParallelDiff.new(self).parallelize\n      end",
    "comment": "Array[<Hash>] with right/left keys that contains Gitlab::Diff::Line objects which text is highlighted",
    "label": "",
    "id": "1898"
  },
  {
    "raw_code": "def diff_lines_for_serializer\n        lines = diff_lines_with_match_tail\n        return if lines.empty?\n\n        lines\n      end",
    "comment": "This adds the bottom match line to the array if needed. It contains the data to load more context lines.",
    "label": "",
    "id": "1899"
  },
  {
    "raw_code": "def text_with_binary_notice?\n        text? && has_binary_notice?\n      end",
    "comment": "NOTE: Files with unsupported encodings (e.g. UTF-16) are treated as binary by git, but they are recognized as text files during encoding detection. These files have `Binary files a/filename and b/filename differ' as their raw diff content which cannot be used. We need to handle this special case and avoid displaying incorrect diff.",
    "label": "",
    "id": "1900"
  },
  {
    "raw_code": "def try_blobs(meth)\n        old_blob&.public_send(meth) || new_blob&.public_send(meth)\n      end",
    "comment": "We can't use Object#try because Blob doesn't inherit from Object, but from BasicObject (via SimpleDelegator).",
    "label": "",
    "id": "1901"
  },
  {
    "raw_code": "def line_positions_at_source_diff(lines, blocks)\n            last_mapped_old_pos = 0\n            last_mapped_new_pos = 0\n\n            lines.reverse_each.map do |line|\n              old_pos = source_line_from_block(line.old_pos, blocks[:from])\n              new_pos = source_line_from_block(line.new_pos, blocks[:to])\n\n              old_has_no_mapping = old_pos == 0\n              new_has_no_mapping = new_pos == 0\n\n              next [0, 0] if old_has_no_mapping && (new_has_no_mapping || line.type == 'old')\n              next [0, 0] if new_has_no_mapping && line.type == 'new'\n\n              new_pos = last_mapped_new_pos if new_has_no_mapping && line.type == 'old'\n              old_pos = last_mapped_old_pos if old_has_no_mapping && line.type == 'new'\n\n              last_mapped_old_pos = old_pos\n              last_mapped_new_pos = new_pos\n\n              [old_pos, new_pos]\n            end.reverse\n          end",
    "comment": "line_positions_at_source_diff: given the transformed lines, what are the correct values for old_pos and new_pos?  Example:  Original from | to A    | A B    | D C    | E F    | F  Original Diff A A - B - C +   D +   E F F  Transformed from | to A    | A C    | D B    | J L    | E K    | K F    | F  Transformed diff | transf old, new | OG old_pos, new_pos | A A             | 1, 1            | 1, 1                | -C               | 2, 2            | 3, 2                | -B               | 3, 2            | 2, 2                | -L               | 4, 2            | 0, 0                | +  D             | 5, 2            | 4, 2                | +  J             | 5, 3            | 0, 0                | +  E             | 5, 4            | 4, 3                | K K             | 5, 5            | 0, 0                | F F             | 6, 6            | 4, 4                |",
    "label": "",
    "id": "1902"
  },
  {
    "raw_code": "def load_paginated_collection(diff_options)\n          relation.offset(diff_options[:offset_index].to_i || 0)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- No need to abstract",
    "label": "",
    "id": "1903"
  },
  {
    "raw_code": "def load_paginated_collection(batch_page, batch_size, diff_options)\n          batch_page ||= DEFAULT_BATCH_PAGE\n          batch_size ||= DEFAULT_BATCH_SIZE\n\n          paths = diff_options&.fetch(:paths, nil)\n\n          paginated_collection = relation.offset(batch_page).limit([batch_size.to_i, DEFAULT_BATCH_SIZE].min)\n          paginated_collection = paginated_collection.by_paths(paths) if paths\n\n          paginated_collection\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1904"
  },
  {
    "raw_code": "def diff_file_paths\n          diffs.map do |diff|\n            diff.new_path.presence || diff.old_path\n          end",
    "comment": "This is either the new path, otherwise the old path for the diff_file",
    "label": "",
    "id": "1905"
  },
  {
    "raw_code": "def diff_paths\n          diff_files.flat_map(&:paths).uniq\n        end",
    "comment": "This is both the new and old paths for the diff_file",
    "label": "",
    "id": "1906"
  },
  {
    "raw_code": "def unfold_diff_files(positions)\n          positions_grouped_by_path = positions.group_by { |position| position.file_path }\n\n          diff_files.each do |diff_file|\n            positions = positions_grouped_by_path.fetch(diff_file.file_path, [])\n            positions.each { |position| diff_file.unfold_diff_lines(position) }\n          end",
    "comment": "This mutates `diff_files` lines.",
    "label": "",
    "id": "1907"
  },
  {
    "raw_code": "def with_highlights_preloaded\n          @with_highlights_preloaded ||= begin\n            start_time = Process.clock_gettime(Process::CLOCK_MONOTONIC, :second)\n\n            diff_files.each do |diff_file|\n              current_time = Process.clock_gettime(Process::CLOCK_MONOTONIC, :second)\n              use_plain_highlight = current_time - start_time >= DEFAULT_LIMIT_HIGHLIGHT_COLLECTION\n\n              diff_file.highlighted_diff_lines = Gitlab::Diff::Highlight.new(\n                diff_file,\n                repository: diff_file.repository,\n                plain: use_plain_highlight\n              ).highlight\n            end",
    "comment": "We need to preload the diffs highlighting to track every diff file and the time that they take to format. If the highlight rich collection limit is reached, then we render the rest of diff files as plain text to avoid saturating the resources.",
    "label": "",
    "id": "1908"
  },
  {
    "raw_code": "def find_correct_path(upload_path)\n        upload = Upload.find_by(uploader: 'FileUploader', path: upload_path)\n        return unless upload && upload.local? && upload.model\n\n        upload.absolute_path\n      rescue StandardError => e\n        logger.error e.message\n\n        # absolute_path depends on a lot of code. If it doesn't work, then it\n        # it doesn't matter if the upload file is in the right place. Treat it\n        # as uncorrectable.\n        # I.e. the project record might be missing, which raises an exception.\n        nil\n      end",
    "comment": "Accepts a path in the form of \"#{hex_secret}/#{filename}\" rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1909"
  },
  {
    "raw_code": "def move_to_lost_and_found(path, dry_run)\n        new_path = path.sub(/\\A#{ProjectUploadFileFinder::ABSOLUTE_UPLOAD_DIR}/o, LOST_AND_FOUND)\n\n        move(path, new_path, 'move to lost and found', dry_run)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1910"
  },
  {
    "raw_code": "def each_orphan_file\n        ProjectUploadFileFinder.new.each_file_batch do |file_paths|\n          logger.debug \"Processing batch of #{file_paths.size} project upload file paths, starting with #{file_paths.first}\"\n\n          file_paths.each do |path|\n            pup = ProjectUploadPath.from_path(path)\n\n            yield(path, pup.upload_path) if pup.orphan?\n          end",
    "comment": "Yields absolute paths of project upload files that are not in the uploads table",
    "label": "",
    "id": "1911"
  },
  {
    "raw_code": "def orphan?\n          return true if full_path.nil? || upload_path.nil?\n\n          # It's possible to reduce to one query, but `where_full_path_in` is complex\n          !Upload.exists?(path: upload_path, model_id: project_id, model_type: 'Project', uploader: 'FileUploader')\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1912"
  },
  {
    "raw_code": "def project_id\n          @project_id ||= Project.where_full_path_in([full_path], preload_routes: false).pluck(:id)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1913"
  },
  {
    "raw_code": "def each_orphan_file(batch_size = BATCH_SIZE)\n        # we want to skip files already moved to lost_and_found directory\n        lost_dir_match = \"^#{lost_and_found_dir}\\/\"\n\n        remote_directory.files.each_slice(batch_size) do |remote_files|\n          remote_files.reject! { |file| file.key.match(/#{lost_dir_match}/) }\n          file_paths = remote_files.map(&:key)\n          tracked_paths = find_tracked_paths(file_paths)\n\n          remote_files.reject! { |file| tracked_paths.include?(file.key) }\n          remote_files.each do |file|\n            yield file\n          end",
    "comment": "Default implementation, override in specific cleaner classes if needed",
    "label": "",
    "id": "1914"
  },
  {
    "raw_code": "def find_tracked_paths(file_paths)\n        file_paths.select do |file_path|\n          file_tracked_in_the_db?(file_path)\n        end",
    "comment": "@param file_paths [Array<String>] an array of remote file paths @return [Array<String>] a subset of the input paths that are tracked in the DB",
    "label": "",
    "id": "1915"
  },
  {
    "raw_code": "def file_tracked_in_the_db?(file_path)\n        # Default to \"file is tracked\"\n        return true unless valid_file_path_format?(file_path)\n\n        query = query_for_row_tracking_the_file(file_path)\n        is_tracked = query.exists?\n\n        log_file_tracked(file_path: file_path, is_tracked: is_tracked, query: query.select('1 as one').limit(1).to_sql)\n\n        is_tracked\n      end",
    "comment": "@param file_path [String] a remote file path @return [Boolean] whether or not the file is tracked in the DB. Defaults to \"file is tracked\" if there is any doubt, to AVOID DATA LOSS.",
    "label": "",
    "id": "1916"
  },
  {
    "raw_code": "def log_file_tracked(**args)\n        if args[:is_tracked]\n          message = \"Found DB record for remote stored file\"\n          logger.debug(args.merge(message: message))\n        else\n          message = \"Did not find DB record for remote stored file\"\n          logger.info(args.merge(message: message))\n        end",
    "comment": "@param args [Hash] log arguments, including at least :file_path and :is_tracked",
    "label": "",
    "id": "1917"
  },
  {
    "raw_code": "def valid_file_path_format?(file_path)\n        return true if file_path.match?(expected_file_path_format_regexp)\n\n        # This can happen if we need to implement support of path formats that we were not aware of. We should increase\n        # the severity of this log line after we are confident that we have accounted for all expected formats.\n        logger.info(message: \"Skipping because the file path doesn't match the expected format\", file_path: file_path,\n          expected_file_path_format_regexp: expected_file_path_format_regexp)\n\n        false\n      end",
    "comment": "@param file_path [String] a remote file path @return [Boolean] true if file_path matches the expected format, false otherwise.",
    "label": "",
    "id": "1918"
  },
  {
    "raw_code": "def query_for_row_tracking_the_file(file_path)\n        raise NotImplementedError\n      end",
    "comment": "@abstract @param file_path [String] a remote file path (the format is specific to each bucket, see the Uploader class for the model being cleaned up for the expected format). @return [ActiveRecord::Relation, nil] a relation that would match the corresponding row in the DB, if it exists, or nil if the file path doesn't match the expected format.",
    "label": "",
    "id": "1919"
  },
  {
    "raw_code": "def expected_file_path_format_regexp\n        raise NotImplementedError\n      end",
    "comment": "@abstract @return [Regexp] the expected file path format regexp specific to each cleaner class",
    "label": "",
    "id": "1920"
  },
  {
    "raw_code": "def handle_orphan_file(file, dry_run:, delete:)\n        msg = if dry_run\n                \"Would #{delete ? 'delete' : 'move to lost and found'}: #{file.key}\"\n              elsif delete\n                file.destroy\n                \"Deleted: #{file.key}\"\n              else\n                new_path = move_to_lost_and_found(file)\n                \"Moved to lost and found: #{file.key} -> #{new_path}\"\n              end",
    "comment": "@param file [Fog::Storage::File] the orphan file to handle. @param dry_run [Boolean] if true, only log what would be done. @param delete [Boolean] if true, delete the orphan file, otherwise move it to the lost and found directory. @return [void]",
    "label": "",
    "id": "1921"
  },
  {
    "raw_code": "def each_file_batch(batch_size: FIND_BATCH_SIZE, &block)\n        cmd = build_find_command(ABSOLUTE_UPLOAD_DIR)\n\n        Open3.popen2(*cmd) do |stdin, stdout, status_thread|\n          yield_paths_in_batches(stdout, batch_size, &block)\n\n          raise \"Find command failed\" unless status_thread.value.success?\n        end",
    "comment": "Paths are relative to the upload directory",
    "label": "",
    "id": "1922"
  },
  {
    "raw_code": "def self.log_requests!(headers = {})\n        @@inject_headers.replace(headers)\n        @@logged_requests.replace([])\n        @@log_requests.value = true\n      end",
    "comment": "Resets the current request log and starts logging requests",
    "label": "",
    "id": "1923"
  },
  {
    "raw_code": "def self.stop_logging!\n        @@log_requests.value = false\n      end",
    "comment": "Stops logging requests",
    "label": "",
    "id": "1924"
  },
  {
    "raw_code": "def self.block_requests!\n        @@block_requests.value = true\n      end",
    "comment": "Block requests according to robots.txt. Any new requests disallowed by robots.txt will return an HTTP 503 status.",
    "label": "",
    "id": "1925"
  },
  {
    "raw_code": "def self.allow_requests!\n        @@block_requests.value = false\n      end",
    "comment": "Allows the server to accept requests again.",
    "label": "",
    "id": "1926"
  },
  {
    "raw_code": "def self.num_active_requests\n        @@num_active_requests.value\n      end",
    "comment": "Returns the number of requests the server is currently processing.",
    "label": "",
    "id": "1927"
  },
  {
    "raw_code": "def self.block_requests!\n        @@block_requests.value = true\n      end",
    "comment": "Prevents the server from accepting new requests. Any new requests will return an HTTP 503 status.",
    "label": "",
    "id": "1928"
  },
  {
    "raw_code": "def self.slow_requests!\n        @@slow_requests.value = true\n      end",
    "comment": "Slows down incoming requests (useful for race conditions).",
    "label": "",
    "id": "1929"
  },
  {
    "raw_code": "def self.allow_requests!\n        @@block_requests.value = false\n        @@slow_requests.value = false\n      end",
    "comment": "Allows the server to accept requests again.",
    "label": "",
    "id": "1930"
  },
  {
    "raw_code": "def self.num_active_requests\n        @@num_active_requests.value\n      end",
    "comment": "Returns the number of requests the server is currently processing.",
    "label": "",
    "id": "1931"
  },
  {
    "raw_code": "def self.block_requests!\n        @@block_requests.value = true\n      end",
    "comment": "Prevents the server from accepting new requests. Any new requests will be skipped.",
    "label": "",
    "id": "1932"
  },
  {
    "raw_code": "def self.allow_requests!\n        @@block_requests.value = false\n      end",
    "comment": "Allows the server to accept requests again.",
    "label": "",
    "id": "1933"
  },
  {
    "raw_code": "def find_by_iid(iid)\n        collection.find_by(iid: iid)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1934"
  },
  {
    "raw_code": "def find_slack_integration\n        find_params = { team_id: params[:team_id], alias: project_alias }.compact\n        slack_app = SlackIntegration.find_by(find_params)\n\n        return unless slack_app\n\n        integration = slack_app.integration\n        integration if integration.active? && integration.project_level?\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1935"
  },
  {
    "raw_code": "def parse_command_text(params)\n        if params[:text] == 'incident declare'\n          [nil, params[:text]]\n        else\n          fragments = params[:text].split(/\\s/, 2)\n          fragments.size == 1 ? [nil, fragments.first] : fragments\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Splits the command '/gitlab help' => [nil, 'help'] '/gitlab group/project issue new some title' => ['group/project', 'issue new some title']",
    "label": "",
    "id": "1936"
  },
  {
    "raw_code": "def find_action(from, to)\n        environment = project.environments.find_by(name: from)\n        return unless environment\n\n        actions = environment.actions_for(to).select do |action|\n          action.deployment_job?\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1937"
  },
  {
    "raw_code": "def self.match(_text)\n        true\n      end",
    "comment": "This class has to be used last, as it always matches. It has to match because other commands were not triggered and we want to show the help command",
    "label": "",
    "id": "1938"
  },
  {
    "raw_code": "def format(string)\n          ::Slack::Messenger::Util::LinkFormatter.format(string)\n        end",
    "comment": "Convert Markdown to slacks format",
    "label": "",
    "id": "1939"
  },
  {
    "raw_code": "def present(pipeline)\n          build = pipeline.builds.take\n\n          if build && (responder = Chat::Responder.responder_for(build))\n            in_channel_response(responder.scheduled_output)\n          else\n            unsupported_chat_service\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1940"
  },
  {
    "raw_code": "def unsupported_chat_service\n          ephemeral_response(text: 'Sorry, this chat service is currently not supported by GitLab ChatOps.')\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1941"
  },
  {
    "raw_code": "def latest_source_head?(suggestion)\n        suggestion.position.head_sha == suggestion.noteable.source_branch_sha\n      end",
    "comment": "Checks whether the latest source branch HEAD matches with the position HEAD we're using to update the file content. Since the persisted HEAD is updated async (for MergeRequest), it's more consistent to fetch this data directly from the repository.",
    "label": "",
    "id": "1942"
  },
  {
    "raw_code": "def url(path)\n        base_url = Gitlab::Utils.append_path(integration.url, '/api/v2.0/')\n        Gitlab::Utils.append_path(base_url, path)\n      end",
    "comment": "url must be used within get method otherwise this would avoid validation by GitLab::HTTP",
    "label": "",
    "id": "1943"
  },
  {
    "raw_code": "def call(worker, job, _queue)\n        # ActiveJobs have wrapped class stored in 'wrapped' key\n        resolved_class = job['wrapped']&.safe_constantize || worker.class\n        if drop_job?(resolved_class)\n          # no-op, drop the job entirely\n          drop_job!(job, worker)\n          return\n        elsif !!defer_job?(resolved_class, job)\n          defer_job!(job, worker)\n          return\n        end",
    "comment": "This middleware decides whether a job is dropped, deferred or runs normally. In short: - `drop_sidekiq_jobs_#{worker_name}` FF enabled (disabled by default) --> drops the job - `run_sidekiq_jobs_#{worker_name}` FF disabled (enabled by default) --> defers the job  DROPPING JOBS A job is dropped when `drop_sidekiq_jobs_#{worker_name}` FF is enabled. This FF is disabled by default for all workers. Dropped jobs are completely ignored and not requeued for future processing.  DEFERRING JOBS Deferred jobs are rescheduled to perform in the future. There are 2 scenarios under which this middleware defers a job: 1. When run_sidekiq_jobs_#{worker_name} FF is disabled. This FF is enabled by default for all workers. 2. Gitlab::Database::HealthStatus, on evaluating the db health status if it returns any indicator with stop signal, the jobs will be delayed by 'x' seconds (set in worker).  Dropping jobs takes higher priority over deferring jobs. For example, when `drop_sidekiq_jobs` is enabled and `run_sidekiq_jobs` is disabled, it results to jobs being dropped.",
    "label": "",
    "id": "1944"
  },
  {
    "raw_code": "def schedule(&block)\n          PauseControl.for(strategy).new.schedule(job, &block)\n        end",
    "comment": "This will continue the middleware chain if the job should be scheduled It will return false if the job needs to be cancelled",
    "label": "",
    "id": "1945"
  },
  {
    "raw_code": "def perform(&block)\n          PauseControl.for(strategy).new.perform(job, &block)\n        end",
    "comment": "This will continue the server middleware chain if the job should be executed. It will return false if the job should not be executed.",
    "label": "",
    "id": "1946"
  },
  {
    "raw_code": "def schedule(&block)\n          Strategies.for(strategy).new(self).schedule(job, &block)\n        end",
    "comment": "This will continue the middleware chain if the job should be scheduled It will return false if the job needs to be cancelled",
    "label": "",
    "id": "1947"
  },
  {
    "raw_code": "def perform(&block)\n          Strategies.for(strategy).new(self).perform(job, &block)\n        end",
    "comment": "This will continue the server middleware chain if the job should be executed. It will return false if the job should not be executed.",
    "label": "",
    "id": "1948"
  },
  {
    "raw_code": "def check!(expiry = duplicate_key_ttl)\n          my_cookie = Cookie.new(\n            jid: jid,\n            existing_wal_locations: job_wal_locations\n          )\n\n          # Signal to any server middleware for the same idempotency_key that\n          # there is at least one client middleware performing deduplication checks.\n          #\n          # The server middleware can determine if there is a duplicate job using the signaling key\n          # instead of reading the cookie key. Using a single key, the server can atomically read and delete it.\n          # This prevents a data race between the server and client in trying to read/write the key.\n          #\n          # There are 2 cases for deleting this key:\n          # 1. Server middleware's until_executed strategy atomically reads and delete it. Reschedules if key exists.\n          # 2. Client deletes key if job is not deduplicated.\n          set_signaling_key(expiry)\n\n          # There are 3 possible scenarios. In order of decreasing likelihood:\n          # 1. SET NX succeeds.\n          # 2. SET NX fails, GET succeeds.\n          # 3. SET NX fails, the key expires and GET fails. In this case we must retry.\n          actual_cookie = {}\n          while actual_cookie.empty?\n            write_succeeded = my_cookie.write(cookie_key, expiry)\n            actual_cookie = write_succeeded ? my_cookie.cookie : get_cookie\n          end",
    "comment": "This method will return the jid that was set in redis",
    "label": "",
    "id": "1949"
  },
  {
    "raw_code": "def scheduled_deferred_job?\n            duplicate_job.scheduled? && duplicate_job.deferred?\n          end",
    "comment": "we do not deduplicate deferred perform_in/perform_at. note that the schedule enq will push the jobs out of the zset with `deferred: true`",
    "label": "",
    "id": "1950"
  },
  {
    "raw_code": "def schedule\n          if should_defer_schedule?\n            defer_job!\n            return\n          end",
    "comment": "This will continue the middleware chain if the job should be scheduled It will return false if the job needs to be cancelled",
    "label": "",
    "id": "1951"
  },
  {
    "raw_code": "def perform\n          if should_defer_perform?\n            defer_job!\n            return\n          end",
    "comment": "This will continue the server middleware chain if the job should be executed. It will return false if the job should not be executed.",
    "label": "",
    "id": "1952"
  },
  {
    "raw_code": "def limit_for(worker:)\n            return 0 if Feature.disabled?(:sidekiq_concurrency_limit_middleware, Feature.current_request, type: :ops)\n\n            worker_class = worker.is_a?(Class) ? worker : worker.class\n            limit = data[worker_class]&.call\n            return limit.to_i unless limit.nil?\n\n            unless Feature.enabled?(:use_max_concurrency_limit_percentage_as_default_limit, Feature.current_request)\n              return limit.to_i # limit must be nil now, so cast it to 0\n            end",
    "comment": "Returns an integer value where: - positive value is returned to enforce a valid concurrency limit - 0 value is returned for workers without concurrency limits - negative value is returned for paused workers",
    "label": "",
    "id": "1953"
  },
  {
    "raw_code": "def default_limit_from_max_percentage(worker)\n            return 0 unless worker.ancestors.include?(WorkerAttributes)\n\n            max_replicas = ENV.fetch('GITLAB_SIDEKIQ_MAX_REPLICAS', '0').to_i\n            concurrency = ENV.fetch('SIDEKIQ_CONCURRENCY', '0').to_i\n            max_total_threads = max_replicas * concurrency\n            percentage = worker.get_max_concurrency_limit_percentage\n\n            (percentage * max_total_threads).ceil\n          end",
    "comment": "Default limit based on urgency and number of total Sidekiq threads in the fleet. e.g. for low urgency worker class, maximum 20% of all Sidekiq workers can run concurrently.",
    "label": "",
    "id": "1954"
  },
  {
    "raw_code": "def current_limit\n          return 0 if worker_klass.nil?\n\n          value = with_redis do |r|\n            value = r.get(current_limit_key)\n            value.to_i if value\n          end",
    "comment": "Reads the current limit value in Redis. Falls back on the max limit value set by default in `worker.get_concurrency_limit` if no limit is set in Redis. @return [Integer] The current limit value, or 0 if no limit is set",
    "label": "",
    "id": "1955"
  },
  {
    "raw_code": "def set_current_limit!(value)\n          with_redis do |r|\n            r.set(current_limit_key, value.to_i, ex: TTL)\n          end",
    "comment": "Updates the current limit value in Redis @param value [Integer] The new limit value to set",
    "label": "",
    "id": "1956"
  },
  {
    "raw_code": "def call(worker_class_or_name, job, _queue, _redis_pool, &)\n          worker_name = worker_class_or_name.is_a?(Class) ? worker_class_or_name.name : worker_class_or_name\n          return yield unless worker_name\n\n          # metadata is written in QueueManager#bulk_send_to_processing_queue\n          metadata_queue = Gitlab::SafeRequestStore[metadata_key(worker_name)]\n          return yield if metadata_queue.nil?\n\n          if metadata_queue.empty?\n            Gitlab::ErrorTracking.track_exception(\n              EmptyJobMetadataError.new(\"Missing job metadata from ConcurrencyLimit::ResumeWorker\"))\n            return yield\n          end",
    "comment": "This middleware updates the job payload with stored context and additional information needed by the SidekiqMiddleware::ConcurrencyLimit::Middleware to resume the job.",
    "label": "",
    "id": "1957"
  },
  {
    "raw_code": "def schedule\n          if should_defer_schedule?\n            defer_job!\n            return\n          end",
    "comment": "This will continue the middleware chain if the job should be scheduled It will return false if the job needs to be cancelled",
    "label": "",
    "id": "1958"
  },
  {
    "raw_code": "def perform\n          if should_defer_perform?\n            defer_job!\n            return\n          end",
    "comment": "This will continue the server middleware chain if the job should be executed. It will return false if the job should not be executed.",
    "label": "",
    "id": "1959"
  },
  {
    "raw_code": "def execute\n          return Decision.new(false, Strategy::None) unless db_duration_exceeded_quota?\n\n          return Decision.new(true, Strategy::HardThrottle) if dominant_in_pg_stat_activity?\n\n          Decision.new(true, Strategy::SoftThrottle)\n        end",
    "comment": "Returns a Decision on how to throttle the worker  @return Decision",
    "label": "",
    "id": "1960"
  },
  {
    "raw_code": "def generate_log(exception, context_payload)\n        payload = {}\n\n        Gitlab::ExceptionLogFormatter.format!(exception, payload)\n        append_user_to_log!(payload, context_payload)\n        append_tags_to_log!(payload, context_payload)\n        append_extra_to_log!(payload, context_payload)\n\n        payload\n      end",
    "comment": "Note: all the accesses to Sentry's contexts here are to keep the backward-compatibility to Sentry's built-in integrations. In the future, they can be removed.",
    "label": "",
    "id": "1961"
  },
  {
    "raw_code": "def self.build(project)\n        strategy = OpenApiStrategy.new(project)\n\n        new(strategy)\n      end",
    "comment": "Builds an instance of error repository backed by a strategy.  @return [self]",
    "label": "",
    "id": "1962"
  },
  {
    "raw_code": "def initialize(strategy)\n        @strategy = strategy\n      end",
    "comment": "@private",
    "label": "",
    "id": "1963"
  },
  {
    "raw_code": "def report_error(\n        name:, description:, actor:, platform:,\n        environment:, level:, occurred_at: Time.zone.now, payload: {}\n      )\n        strategy.report_error(\n          name: name,\n          description: description,\n          actor: actor,\n          platform: platform,\n          environment: environment,\n          level: level,\n          occurred_at: occurred_at,\n          payload: payload\n        )\n\n        nil\n      end",
    "comment": "Stores an error and the related error event.  @param name [String] name of the error @param description [String] description of the error @param actor [String] culprit (class/method/function) which triggered this error @param platform [String] platform on which the error occurred @param environment [String] environment on which the error occurred @param level [String] severity of this error @param occurred_at [Time] timestamp when the error occurred @param payload [Hash] original error payload  @return [void] nothing  @raise [RecordInvalidError] if passed attributes were invalid to store an error or error event @raise [DatabaseError] if generic error occurred",
    "label": "",
    "id": "1964"
  },
  {
    "raw_code": "def find_error(id)\n        strategy.find_error(id)\n      end",
    "comment": "Finds an error by +id+.  @param id [Integer, String] unique error identifier  @return [Gitlab::ErrorTracking::DetailedError] a detail error",
    "label": "",
    "id": "1965"
  },
  {
    "raw_code": "def list_errors(sort: 'last_seen', filters: {}, query: nil, limit: 20, cursor: {})\n        limit = [limit.to_i, 100].min\n\n        strategy.list_errors(filters: filters, query: query, sort: sort, limit: limit, cursor: cursor)\n      end",
    "comment": "Lists errors.  @param sort [String] order list by 'first_seen', 'last_seen', or 'frequency' @param filters [Hash<Symbol, String>] filter list by @option filters [String] :status error status @params query [String, nil] free text search @param limit [Integer, String] limit result @param cursor [Hash] pagination information  @return [Array<Array<Gitlab::ErrorTracking::Error>, Pagination>]",
    "label": "",
    "id": "1966"
  },
  {
    "raw_code": "def last_event_for(id)\n        strategy.last_event_for(id)\n      end",
    "comment": "Fetches last event for error +id+.  @param id [Integer, String] unique error identifier  @return [Gitlab::ErrorTracking::ErrorEvent]  @raise [DatabaseError] if generic error occurred",
    "label": "",
    "id": "1967"
  },
  {
    "raw_code": "def update_error(id, status:)\n        strategy.update_error(id, status: status)\n      end",
    "comment": "Updates attributes of an error.  @param id [Integer, String] unique error identifier @param status [String] error status  @return [true, false] if update was successful  @raise [DatabaseError] if generic error occurred",
    "label": "",
    "id": "1968"
  },
  {
    "raw_code": "def extra_tags_from_env\n        Gitlab::Json.parse(ENV.fetch('GITLAB_SENTRY_EXTRA_TAGS', '{}')).to_hash\n      rescue StandardError => e\n        Gitlab::AppLogger.debug(\"GITLAB_SENTRY_EXTRA_TAGS could not be parsed as JSON: #{e.class.name}: #{e.message}\")\n\n        {}\n      end",
    "comment": "Static tags that are set on application start",
    "label": "",
    "id": "1969"
  },
  {
    "raw_code": "def external_url(id)\n          Gitlab::Routing.url_helpers.details_namespace_project_error_tracking_index_url(\n            namespace_id: @project.namespace,\n            project_id: @project,\n            issue_id: id)\n        end",
    "comment": "For compatibility with sentry integration",
    "label": "",
    "id": "1970"
  },
  {
    "raw_code": "def external_base_url\n          Gitlab::Routing.url_helpers.project_url(@project)\n        end",
    "comment": "For compatibility with sentry integration",
    "label": "",
    "id": "1971"
  },
  {
    "raw_code": "def self.call(event)\n          Gitlab::ErrorTracking::ContextPayloadGenerator.generate(nil, {}).each do |key, value|\n            event.public_send(key).deep_merge!(value) # rubocop:disable GitlabSecurity/PublicSend\n          end",
    "comment": "This processor is added to inject application context into Sentry events generated by Sentry built-in integrations. When the integrations are re-implemented and use Gitlab::ErrorTracking, this processor should be removed.",
    "label": "",
    "id": "1972"
  },
  {
    "raw_code": "def self.call(event)\n          if event.request.present?\n            event.request.cookies = {}\n            event.request.data = {}\n          end",
    "comment": "This processor removes sensitive fields or headers from the event before sending. Sentry versions above 4.0 don't support sanitized_fields and sanitized_http_headers anymore. The official document recommends using before_send instead.  For more information, please visit: https://docs.sentry.io/platforms/ruby/guides/rails/configuration/filtering/#using-beforesend",
    "label": "",
    "id": "1973"
  },
  {
    "raw_code": "def process_first_exception_value(event)\n            # Better in new version, will be event.exception.values\n            exceptions = extract_exceptions_from(event)\n            exception = exceptions.first\n\n            return unless valid_exception?(exception)\n\n            raw_message = exception.value\n\n            return unless exception.type&.start_with?('GRPC::')\n            return unless raw_message.present?\n\n            message, debug_str = split_debug_error_string(raw_message)\n\n            # Worse in new version, no setter! Have to poke at the\n            # instance variable\n            if message.present?\n              exceptions.each do |exception|\n                next unless valid_exception?(exception)\n\n                set_exception_message(exception, message)\n              end",
    "comment": "Sentry can report multiple exceptions in an event. Sanitize only the first one since that's what is used for grouping.",
    "label": "",
    "id": "1974"
  },
  {
    "raw_code": "def valid_signature_blob?\n        return false unless signature\n        return false unless signature.namespace == GIT_NAMESPACE\n\n        signature.verify(@signed_text)\n      end",
    "comment": "Verifies the signature using the public key embedded in the blob. This proves that the signed_text was signed by the private key of the public key identified by `key_fingerprint`. Afterwards, we still need to check that the key belongs to the committer.",
    "label": "",
    "id": "1975"
  },
  {
    "raw_code": "def verified_by_gitlab?\n        @signer == :SIGNER_SYSTEM\n      end",
    "comment": "If a commit is signed by Gitaly, the Gitaly returns `SIGNER_SYSTEM` as a signer In order to calculate it, the signature is Verified using the Gitaly's public key: https://gitlab.com/gitlab-org/gitaly/-/blob/v16.2.0-rc2/internal/gitaly/service/commit/commit_signatures.go#L63  It is safe to skip verification step if the commit has been signed by Gitaly",
    "label": "",
    "id": "1976"
  },
  {
    "raw_code": "def blocked_resource_event_attributes\n          %w[id issue_id merge_request_id]\n        end",
    "comment": "Overriden on EE::Gitlab::Issuable::Clone::CopyResourceEventsService These values should never be copied to the new entity. This service should always set a new appropriate value that references the new target.",
    "label": "",
    "id": "1977"
  },
  {
    "raw_code": "def user_info(person_id)\n        user_hash = user_map[person_id.to_s]\n\n        user_name = ''\n        gitlab_id = nil\n\n        unless user_hash.nil?\n          user_name = user_hash['name']\n          if user = User.find_by(id: user_hash['gitlab_user'])\n            user_name = \"@#{user.username}\"\n            gitlab_id = user.id\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1978"
  },
  {
    "raw_code": "def import_cases\n        return unless @cases\n\n        while bug = @cases.shift\n          author = user_info(bug['ixPersonOpenedBy'])[:name]\n          date = DateTime.parse(bug['dtOpened'])\n\n          comments = bug['events']['event']\n\n          content = format_content(opened_content(comments))\n          body = format_issue_body(author, date, content)\n\n          labels = []\n          [bug['sCategory'], bug['sPriority']].each do |label|\n            next if label.blank?\n\n            labels << label\n\n            unless @known_labels.include?(label)\n              create_label(label)\n              @known_labels << label\n            end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1979"
  },
  {
    "raw_code": "def opened_content(comments)\n        while comment = comments.shift\n          if comment['sVerb'] == 'Opened'\n            return comment['s']\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "1980"
  },
  {
    "raw_code": "def initialize(wiki_content)\n        @wiki_content = wiki_content\n      end",
    "comment": "@param [String] wiki_content",
    "label": "",
    "id": "1981"
  },
  {
    "raw_code": "def initialize(ip, port: nil)\n        @ip = ip\n        @port = port\n      end",
    "comment": "Argument ip should be an IPAddr object",
    "label": "",
    "id": "1982"
  },
  {
    "raw_code": "def ip_include?(requested_ip)\n        return true if ip.include?(requested_ip)\n        return ip.include?(requested_ip.ipv4_mapped) if requested_ip.ipv4? && ip.ipv6?\n        return ip.ipv4_mapped.include?(requested_ip) if requested_ip.ipv6? && ip.ipv4?\n\n        false\n      end",
    "comment": "Prior to ipaddr v1.2.3, if the allow list were the IPv4 to IPv6 mapped address ::ffff:169.254.168.100 and the requested IP were 169.254.168.100 or ::ffff:169.254.168.100, the IP would be considered in the allow list. However, with https://github.com/ruby/ipaddr/pull/31, IPAddr#include? will only match if the IP versions are the same. This method preserves backwards compatibility if the versions differ by checking inclusion by coercing an IPv4 address to its IPv6 mapped address.",
    "label": "",
    "id": "1983"
  },
  {
    "raw_code": "def rate_limit!\n        return false if no_limit?\n\n        ::Gitlab::ApplicationRateLimiter.throttled?(\n          limit_name,\n          scope: [root_namespace],\n          threshold: limit\n        )\n      end",
    "comment": "Increments the rate-limit counter. Returns true if the hook should be rate-limited.",
    "label": "",
    "id": "1984"
  },
  {
    "raw_code": "def rate_limited?\n        return false if no_limit?\n\n        Gitlab::ApplicationRateLimiter.peek(\n          limit_name,\n          scope: [root_namespace],\n          threshold: limit\n        )\n      end",
    "comment": "Returns true if the hook is currently over its rate-limit. It does not increment the rate-limit counter.",
    "label": "",
    "id": "1985"
  },
  {
    "raw_code": "def register!(hook)\n          cache_key = cache_key_for_hook(hook)\n\n          ::Gitlab::Redis::SharedState.with do |redis|\n            redis.multi do |multi|\n              multi.sadd?(cache_key, hook.id)\n              multi.expire(cache_key, TOUCH_CACHE_TTL)\n            end",
    "comment": "Before a webhook is executed, `.register!` should be called. Adds the webhook ID to a cache (see `#cache_key_for_hook` for details of the cache).",
    "label": "",
    "id": "1986"
  },
  {
    "raw_code": "def block?(hook)\n          # If a request UUID has not been set then we know the request was not\n          # made by a webhook, and no recursion is possible.\n          return false unless UUID.instance.request_uuid\n\n          cache_key = cache_key_for_hook(hook)\n\n          ::Gitlab::Redis::SharedState.with do |redis|\n            redis.sismember(cache_key, hook.id) ||\n              redis.scard(cache_key) >= COUNT_LIMIT\n          end",
    "comment": "Returns true if the webhook ID is present in the cache, or if the number of IDs in the cache exceeds the limit (see `#cache_key_for_hook` for details of the cache).",
    "label": "",
    "id": "1987"
  },
  {
    "raw_code": "def cache_key_for_hook(hook)\n          [:webhook_recursion_detection, UUID.instance.uuid_for_hook(hook)].join(':')\n        end",
    "comment": "Returns a cache key scoped to a UUID.  The particular UUID will be either:  - A UUID that was recycled from the request headers if the request was made by a webhook. - a new UUID initialized for the webhook.  This means that cycles of webhooks that are triggered from other webhooks will share the same cache, and other webhooks will use a new cache.",
    "label": "",
    "id": "1988"
  },
  {
    "raw_code": "def instance\n            Gitlab::SafeRequestStore[:web_hook_recursion_detection_uuid] ||= new\n          end",
    "comment": "Back the Singleton with RequestStore so it is isolated to this request.",
    "label": "",
    "id": "1989"
  },
  {
    "raw_code": "def uuid_for_hook(hook)\n          request_uuid || new_uuid_for_hook(hook)\n        end",
    "comment": "Returns a UUID, which will be either:  - The UUID that was recycled from the request headers if the request was made by a webhook. - A new UUID initialized for the webhook.",
    "label": "",
    "id": "1990"
  },
  {
    "raw_code": "def self.allows_restoring_secrets?\n        false\n      end",
    "comment": " Determines whether this strategy supports restoring secrets from the database. This allows detecting users trying to use a non-restorable strategy with +reuse_access_tokens+.",
    "label": "",
    "id": "1991"
  },
  {
    "raw_code": "def self.allows_restoring_secrets?\n        false\n      end",
    "comment": " Determines whether this strategy supports restoring secrets from the database. This allows detecting users trying to use a non-restorable strategy with +reuse_access_tokens+.",
    "label": "",
    "id": "1992"
  },
  {
    "raw_code": "def self.generate(*)\n          format(prefix_for_oauth_application_secret, token: SecureRandom.hex(32))\n        end",
    "comment": "Maintains compatibility with ::Doorkeeper::OAuth::Helpers::UniqueToken Returns a secure random token, prefixed with a GitLab identifier.",
    "label": "",
    "id": "1993"
  },
  {
    "raw_code": "def initialize(payload, console_messages)\n        @payload = payload\n        @console_messages = console_messages\n      end",
    "comment": "Example of payload:  { 'action' => 'geo_proxy_to_primary', 'data' => { 'api_endpoints' => %w{geo/proxy_git_ssh/info_refs_receive_pack geo/proxy_git_ssh/receive_pack}, 'gl_username' => user.username, 'primary_repo' => geo_primary_http_url_to_repo(container) } } ",
    "label": "",
    "id": "1994"
  },
  {
    "raw_code": "def export_path\n        @export_path ||= Gitlab::ImportExport.export_path(relative_path: relative_path)\n      end",
    "comment": "The path where the exportable metadata and repository bundle (in case of project) is saved",
    "label": "",
    "id": "1995"
  },
  {
    "raw_code": "def archive_path\n        @archive_path ||= Gitlab::ImportExport.export_path(relative_path: relative_archive_path)\n      end",
    "comment": "The path where the tarball is saved",
    "label": "",
    "id": "1996"
  },
  {
    "raw_code": "def project_tree\n        tree_by_key(:project)\n      end",
    "comment": "Outputs a hash in the format described here: http://api.rubyonrails.org/classes/ActiveModel/Serializers/JSON.html for outputting a project in JSON format, including its relations and sub relations.",
    "label": "",
    "id": "1997"
  },
  {
    "raw_code": "def wait_for_archived_file\n        MAX_RETRIES.times do |retry_number|\n          break if File.exist?(@archive_file)\n\n          sleep(2**retry_number)\n        end",
    "comment": "Exponentially sleep until I/O finishes copying the file",
    "label": "",
    "id": "1998"
  },
  {
    "raw_code": "def build_associations\n        stack = @attributes_finder.tree.deep_merge(@attributes_finder.import_only_tree).to_a\n\n        while stack.any?\n          model_name, relations = stack.pop\n\n          next unless relations.is_a?(Hash)\n\n          add_permitted_attributes(model_name, relations.keys)\n\n          stack.concat(relations.to_a)\n        end",
    "comment": "Deep traverse relations tree to build a list of allowed model relations",
    "label": "",
    "id": "1999"
  },
  {
    "raw_code": "def create_source_branch\n        if @merge_request.open?\n          @project.repository.create_branch(@merge_request.source_branch, @diff_head_sha)\n        end",
    "comment": "When the exported MR was in a fork, the source branch does not exist in the imported bundle - although the commits usually do - so it must be created manually. Ignore failures so we get the merge request itself if the commits are missing.",
    "label": "",
    "id": "2000"
  },
  {
    "raw_code": "def create_target_branch\n        @project.repository.create_branch(@merge_request.target_branch, @merge_request.target_branch_sha)\n      rescue StandardError => err\n        ::Import::Framework::Logger.warn(\n          message: 'Import warning: Failed to create target branch',\n          target_branch: @merge_request.target_branch,\n          diff_head_sha: @diff_head_sha,\n          merge_request_iid: @merge_request.iid,\n          error: err.message\n        )\n      end",
    "comment": "Ignore failures during target branch creation so we still create the merge request itself.",
    "label": "",
    "id": "2001"
  },
  {
    "raw_code": "def execute_cmd(cmd)\n        output, status = Gitlab::Popen.popen(cmd)\n\n        return true if status == 0\n\n        message = cmd_error_message(output, status)\n\n        if @shared.respond_to?(:error)\n          @shared.error(Gitlab::ImportExport::Error.new(message))\n\n          false\n        else\n          raise Gitlab::ImportExport::Error, message\n        end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "2002"
  },
  {
    "raw_code": "def copy_files(source, destination)\n        # if we are copying files, create the destination folder\n        destination_folder = File.file?(source) ? File.dirname(destination) : destination\n\n        mkdir_p(destination_folder)\n        FileUtils.copy_entry(source, destination)\n        true\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "2003"
  },
  {
    "raw_code": "def clean_extraction_dir!(dir)\n        # Using File::FNM_DOTMATCH to also delete symlinks starting with \".\"\n        Dir.glob(\"#{dir}/**/*\", File::FNM_DOTMATCH).each do |filepath|\n          next if CLEAN_DIR_IGNORE_FILE_NAMES.include?(File.basename(filepath))\n\n          raise HardLinkError, 'File shares hard link' if Gitlab::Utils::FileInfo.shares_hard_link?(filepath)\n\n          FileUtils.rm(filepath) if Gitlab::Utils::FileInfo.linked?(filepath) || File.pipe?(filepath)\n        end",
    "comment": "Scans and cleans the directory tree. Symlinks are considered legal but are removed. Files sharing hard links are considered illegal and the directory will be removed and a `HardLinkError` exception will be raised.  @raise [HardLinkError] if there multiple hard links to the same file detected. @return [Boolean] true",
    "label": "",
    "id": "2004"
  },
  {
    "raw_code": "def to_h\n        if merge_ee?\n          deep_merge(@hash, @ee_hash)\n        else\n          @hash\n        end",
    "comment": "Returns a Hash of the YAML file, including EE specific data if EE is used.",
    "label": "",
    "id": "2005"
  },
  {
    "raw_code": "def filename_with_object_id(upload_path)\n        upload_path.split('/').last(2).join('/')\n      end",
    "comment": "Returns filename and the dir name the upload is in e.g. 72a497a02fe3ee09edae2ed06d390038/image.png",
    "label": "",
    "id": "2006"
  },
  {
    "raw_code": "def existing_users_email_map\n        @existing_users_email_map ||= begin\n          emails = @exported_members.map { |member| get_email(member) }\n\n          User.by_user_email(emails).pluck(:email, :id).to_h\n        end",
    "comment": "Returns {email => user_id} hash where user_id is an ID at current instance",
    "label": "",
    "id": "2007"
  },
  {
    "raw_code": "def exported_members_email_map\n        @exported_members_email_map ||= begin\n          result = {}\n          @exported_members.each do |member|\n            email = get_email(member)\n\n            next unless email\n\n            result[member.dig('user', 'id')] = email\n          end",
    "comment": "Returns {user_id => email} hash where user_id is an ID at source \"old\" instance",
    "label": "",
    "id": "2008"
  },
  {
    "raw_code": "def with_isolated_sidekiq_job\n          Sidekiq::Testing.fake! do\n            ::Gitlab::SafeRequestStore.ensure_request_store do\n              # If you are attempting to import a large project into a development environment,\n              # you may see Gitaly throw an error about too many calls or invocations.\n              # This is due to a n+1 calls limit being set for development setups (not enforced in production)\n              # https://gitlab.com/gitlab-org/gitlab/-/merge_requests/24475#note_283090635\n              # For development setups, this code-path will be excluded from n+1 detection.\n              ::Gitlab::GitalyClient.allow_n_plus_1_calls do\n                yield\n              end",
    "comment": "We want to ensure that all Sidekiq jobs are executed synchronously as part of that process. This ensures that all expensive operations do not escape to general Sidekiq clusters/nodes.",
    "label": "",
    "id": "2009"
  },
  {
    "raw_code": "def where_clause_base\n          [].tap do |clauses|\n            clauses << table[:project_id].eq(project.id) if project\n            clauses << table[:group_id].in(group.self_and_ancestors_ids) if group\n          end.reduce(:or)\n        end",
    "comment": "Returns Arel clause `\"{table_name}\".\"project_id\" = {project.id}` if project is present For example: merge_request has :target_project_id, and we are searching by :iid or, if group is present: `\"{table_name}\".\"project_id\" = {project.id} OR \"{table_name}\".\"group_id\" = {group.id}`",
    "label": "",
    "id": "2010"
  },
  {
    "raw_code": "def where_clause_for_klass\n          return attrs_to_arel(attributes.slice('filename')).and(table[:issue_id].eq(nil)) if design?\n\n          attrs_to_arel(attributes.slice('iid', 'target_project_id')) if merge_request?\n        end",
    "comment": "Returns Arel clause for a particular model or `nil`.",
    "label": "",
    "id": "2011"
  },
  {
    "raw_code": "def claim_iid\n          # The milestone has to be a group milestone, as it's the only case where\n          # we set the IID as the maximum. The rest of them are fixed.\n          milestone = project.milestones.find_by(iid: attributes['iid'])\n\n          return unless milestone\n\n          milestone.iid = nil\n          milestone.ensure_project_iid!\n          milestone.save!\n        end",
    "comment": "If an existing group milestone used the IID claim the IID back and set the group milestone to use one available This is necessary to fix situations like the following: - Importing into a user namespace project with exported group milestones where the IID of the Group milestone could conflict with a project one.",
    "label": "",
    "id": "2012"
  },
  {
    "raw_code": "def batch_ordering(records, key, record_ids)\n          return if key == :merge_requests && record_ids\n\n          export_reorder = relations_schema[:export_reorder]&.dig(key)\n          return unless export_reorder\n\n          custom_reorder(records.klass, export_reorder)\n        end",
    "comment": "We can supply a custom `order_by` in `import_export.yml` if we need to sort by non-primary key or take advantage of advanced pagination techniques.  When using batched exports in direct transfer. A set of record_ids may already be provided. In this case, it makes sense to rely on the original `IN`-based query rather than applying our own custom sort.  @param records    The set of records currently being serialized. @param key        The relation key, e.g. :issues, :merge_requests @param record_ids An optional array of record IDs that may be provided during direct transfer batch export.",
    "label": "",
    "id": "2013"
  },
  {
    "raw_code": "def initialize(relation_sym:, relation_index:, relation_hash:, members_mapper:, object_builder:, user:, importable:, import_source:, excluded_keys: [], original_users_map: nil, rewrite_mentions: false)\n          @relation_sym = relation_sym\n          @relation_name = self.class.overrides[relation_sym]&.to_sym || relation_sym\n          @relation_index = relation_index\n          @relation_hash = relation_hash.except('noteable_id')\n          @members_mapper = members_mapper\n          @object_builder = object_builder\n          @user = user\n          @importable = importable\n          @import_source = import_source\n          @imported_object_retries = 0\n          @relation_hash[importable_column_name] = @importable.id\n          @original_user = {}\n          @original_users_map = original_users_map\n          @rewrite_mentions = rewrite_mentions\n\n          # Remove excluded keys from relation_hash\n          # We don't do this in the parsed_relation_hash because of the 'transformed attributes'\n          # For example, MergeRequestDiffFiles exports its diff attribute as utf8_diff. Then,\n          # in the create method that attribute is renamed to diff. And because diff is an excluded key,\n          # if we clean the excluded keys in the parsed_relation_hash, it will be removed\n          # from the object attributes and the export will fail.\n          @relation_hash.except!(*excluded_keys)\n        end",
    "comment": "rubocop:disable Metrics/ParameterLists -- Keyword arguments are not adding complexity to initializer",
    "label": "",
    "id": "2014"
  },
  {
    "raw_code": "def create\n          return @relation_hash if author_relation?\n          return if invalid_relation? || predefined_relation?\n\n          setup_base_models\n          setup_models\n\n          return if @relation_hash.empty?\n\n          generate_imported_object\n        end",
    "comment": "rubocop:enable Metrics/ParameterLists Creates an object from an actual model with name \"relation_sym\" with params from the relation_hash, updating references with new object IDs, mapping users using the \"members_mapper\" object, also updating notes if required.",
    "label": "",
    "id": "2015"
  },
  {
    "raw_code": "def remove_duplicate_assignees\n          associations = %w[issue_assignees merge_request_assignees merge_request_reviewers approvals]\n\n          associations.each do |association|\n            next unless @relation_hash.key?(association)\n            next unless @relation_hash[association].is_a?(Array)\n            next if @relation_hash[association].empty?\n\n            @relation_hash[association].select! { |record| record.respond_to?(:user_id) }\n            @relation_hash[association].uniq!(&:user_id)\n          end",
    "comment": "When an assignee (or any other listed association) did not exist in the members mapper, the importer is assigned. We only need to assign each user once.",
    "label": "",
    "id": "2016"
  },
  {
    "raw_code": "def set_note_author\n          old_author_id = @original_user['author_id']\n          author = @relation_hash.delete('author')\n\n          unless @members_mapper.include?(old_author_id)\n            @relation_hash['note'] = \"%{note}\\n\\n %{missing_author_note}\" % {\n              note: @relation_hash['note'].presence || '*Blank note*',\n              missing_author_note: missing_author_note(@relation_hash['updated_at'], author['name'])\n            }\n          end",
    "comment": "Sets the author for a note. If the user importing the project has admin access, an actual mapping with new project members will be used. Otherwise, a note stating the original author name is left.",
    "label": "",
    "id": "2017"
  },
  {
    "raw_code": "def cached_has_unique_index_on_importable_fk\n          Thread.current[:cached_has_unique_index_on_importable_fk] ||= {}\n        end",
    "comment": "Avoid unnecessary DB requests",
    "label": "",
    "id": "2018"
  },
  {
    "raw_code": "def prepare_attributes\n          attributes\n        end",
    "comment": "attributes wrapped in a method to be adjusted in sub-class if needed",
    "label": "",
    "id": "2019"
  },
  {
    "raw_code": "def attrs_to_arel(attrs)\n          attrs.map do |key, value|\n            table[key].eq(value)\n          end.reduce(:and)\n        end",
    "comment": "Returns Arel clause: `\"{table_name}\".\"{attrs.keys[0]}\" = '{attrs.values[0]} AND {table_name}\".\"{attrs.keys[1]}\" = '{attrs.values[1]}\"` from the given Hash of attributes.",
    "label": "",
    "id": "2020"
  },
  {
    "raw_code": "def where_clause_for_title\n          attrs_to_arel(attributes.slice('title'))\n        end",
    "comment": "Returns Arel clause `\"{table_name}\".\"title\" = '{attributes['title']}'` if attributes has 'title key, otherwise `nil`.",
    "label": "",
    "id": "2021"
  },
  {
    "raw_code": "def where_clause_for_description\n          attrs_to_arel(attributes.slice('description'))\n        end",
    "comment": "Returns Arel clause `\"{table_name}\".\"description\" = '{attributes['description']}'` if attributes has 'description key, otherwise `nil`.",
    "label": "",
    "id": "2022"
  },
  {
    "raw_code": "def where_clause_for_created_at\n          attrs_to_arel(attributes.slice('created_at'))\n        end",
    "comment": "Returns Arel clause `\"{table_name}\".\"created_at\" = '{attributes['created_at']}'` if attributes has 'created_at key, otherwise `nil`.",
    "label": "",
    "id": "2023"
  },
  {
    "raw_code": "def initialize(relation_object:, relation_key:, relation_definition:, importable:)\n          @relation_object = relation_object\n          @relation_key = relation_key\n          @relation_definition = relation_definition\n          @importable = importable\n          @invalid_subrelations = []\n          @failed_subrelations = []\n          @exceptions_rescued = 0\n        end",
    "comment": "@param relation_object [Object] Object of a project/group, e.g. an issue @param relation_key [String] Name of the object association to group/project, e.g. :issues @param relation_definition [Hash] Object subrelations as defined in import_export.yml @param importable [Project|Group] Project or group where relation object is getting saved to  @example Gitlab::ImportExport::Base::RelationObjectSaver.new( relation_key: 'merge_requests', relation_object: #<MergeRequest id: root/mrs!1, notes: [#<Note id: nil, note: 'test', ...>, #<Note id: nil, noteL 'another note'>]>, relation_definition: {\"metrics\"=>{}, \"award_emoji\"=>{}, \"notes\"=>{\"author\"=>{}, ... }} importable: @importable ).execute",
    "label": "",
    "id": "2024"
  },
  {
    "raw_code": "def save_subrelations\n          collection_subrelations.each_pair do |relation_name, records|\n            records.each_slice(BATCH_SIZE) do |batch|\n              save_batch_with_retry(relation_name, batch)\n            end",
    "comment": "Processes all collection subrelations in configurable batches.  Iterates through each collection subrelation that was extracted during the move_subrelations step and processes them in batches of BATCH_SIZE to prevent database timeouts.  @example Processing a collection of 300 notes # Initial state: { \"notes\" => [note1, note2, ..., note300] } # Will create 3 batches of 100 records each: # - Batch 1: notes[1-100] # - Batch 2: notes[101-200] # - Batch 3: notes[201-300]  rubocop:disable GitlabSecurity/PublicSend",
    "label": "",
    "id": "2025"
  },
  {
    "raw_code": "def save_batch_with_retry(relation_name, batch, retry_count = 0)\n          valid_records, invalid_records = batch.partition { |record| record.valid? }\n\n          invalid_records.map! { |record| ::Import::ImportRecordPreparer.recover_invalid_record(record) }\n\n          save_valid_records(relation_name, valid_records)\n\n          save_potentially_invalid_records(relation_name, invalid_records)\n\n          relation_object.save\n        rescue ActiveRecord::QueryCanceled => e # rubocop:disable Database/RescueQueryCanceled -- retry with smaller batches\n          # Feature flag is disabled, don't rescue, re-raise the exception\n          raise e unless Feature.enabled?(:import_rescue_query_canceled, importable)\n\n          # Check if we've exceeded the maximum number of exceptions to rescue for this relation_object (Ex. Issue)\n          raise e if @exceptions_rescued >= MAX_EXCEPTION_RESCUE_COUNT\n\n          @exceptions_rescued += 1\n\n          track_exception(batch, e, relation_name, retry_count)\n          process_with_smaller_batch_size(relation_name, batch, retry_count, e)\n        end",
    "comment": "Saves a batch of records for a specific relation with retry logic.  This method partitions the batch into valid and invalid records, processes them accordingly, and implements retry logic for database timeouts.  @example Processing a batch of records with potential timeout save_batch_with_retry(\"notes\", batch_of_notes, 0) # If successful, all notes will be saved # If timeout occurs, will call process_with_smaller_batch_size",
    "label": "",
    "id": "2026"
  },
  {
    "raw_code": "def process_with_smaller_batch_size(relation_name, batch, retry_count, exception)\n          unless retry_count < MAX_RETRY_COUNT\n            handle_max_retries_exceeded(batch, exception)\n            return\n          end",
    "comment": "Handles database timeouts by retrying with smaller batch sizes.  When a batch processing operation times out (ActiveRecord::QueryCanceled), this method reduces the batch size by BATCH_SIZE_REDUCTION_FACTOR and retries with smaller batches until reaching MAX_RETRY_COUNT.  @example Handling a timeout with a batch of 100 records # Initial batch of 100 causes timeout (retry_count = 0) # - New batch size = ceiling(100/4) = 25 # - Creates 4 smaller batches of 25 each # - Retry count incremented to 1",
    "label": "",
    "id": "2027"
  },
  {
    "raw_code": "def where_clause_base\n          table[:group_id].in(group_and_ancestor_ids)\n        end",
    "comment": "Returns Arel clause `\"{table_name}\".\"group_id\" = {group.id}`",
    "label": "",
    "id": "2028"
  },
  {
    "raw_code": "def create_relations!\n          relations.each do |relation_key, relation_definition|\n            process_relation!(relation_key, relation_definition)\n          end",
    "comment": "Loops through the tree of models defined in import_export.yml and finds them in the imported JSON so they can be instantiated and saved in the DB. The structure and relationships between models are guessed from the configuration yaml file too. Finally, it updates each attribute in the newly imported project/group.",
    "label": "",
    "id": "2029"
  },
  {
    "raw_code": "def existing_iids(relation_key)\n          strong_memoize_with(:existing_iids, relation_key) do\n            case relation_key\n            when 'issues' then @importable.issues.pluck(:iid)\n            when 'milestones' then @importable.milestones.pluck(:iid)\n            when 'ci_pipelines' then @importable.ci_pipelines.pluck(:iid)\n            when 'merge_requests' then @importable.merge_requests.pluck(:iid)\n            end.index_with(true)\n          end",
    "comment": "Generate the list of existing IIDs as a hash. { issues: { 1: true, 2: true, ... }} A hash is used rather than returning the array of IDs because lookup performance is greatly improved.",
    "label": "",
    "id": "2030"
  },
  {
    "raw_code": "def already_restored?(relation_item)\n          !relation_item.is_a?(Hash)\n        end",
    "comment": "Since we update the data hash in place as we restore relation items, and since we also de-duplicate items, we might encounter items that have already been restored in a previous iteration.",
    "label": "",
    "id": "2031"
  },
  {
    "raw_code": "def log_relation_creation(importable, relation_key, relation_object)\n          root_ancestor_group = importable.try(:root_ancestor)\n\n          return unless root_ancestor_group\n          return unless root_ancestor_group.instance_of?(::Group)\n\n          @shared.logger.info(\n            importable_type: importable.class.to_s,\n            importable_id: importable.id,\n            relation_key: relation_key,\n            relation_id: relation_object.id,\n            author_id: relation_object.try(:author_id),\n            message: '[Project/Group Import] Created new object relation'\n          )\n        end",
    "comment": "Enable logging of each top-level relation creation when Importing into a Group",
    "label": "",
    "id": "2032"
  },
  {
    "raw_code": "def initialize(name, query, materialized: true)\n        @table = Arel::Table.new(name)\n        @query = query\n        @materialized = materialized\n      end",
    "comment": "name - The name of the CTE as a String or Symbol.",
    "label": "",
    "id": "2033"
  },
  {
    "raw_code": "def to_arel\n        sql = Arel::Nodes::SqlLiteral.new(\"(#{query_as_sql})\")\n\n        Gitlab::Database::AsWithMaterialized.new(table, sql, materialized: @materialized)\n      end",
    "comment": "Returns the Arel relation for this CTE.",
    "label": "",
    "id": "2034"
  },
  {
    "raw_code": "def alias_to(alias_table)\n        Arel::Nodes::As.new(table, alias_table)\n      end",
    "comment": "Returns an \"AS\" statement that aliases the CTE name as the given table name. This allows one to trick ActiveRecord into thinking it's selecting from an actual table, when in reality it's selecting from a CTE.  alias_table - The Arel table to use as the alias.",
    "label": "",
    "id": "2035"
  },
  {
    "raw_code": "def apply_to(relation)\n        relation.except(:where)\n          .with(to_arel)\n          .from(alias_to(relation.model.arel_table))\n      end",
    "comment": "Applies the CTE to the given relation, returning a new one that will query from it.",
    "label": "",
    "id": "2036"
  },
  {
    "raw_code": "def fuzzy_arel_match(column, query, lower_exact_match: false, use_minimum_char_limit: true)\n          return unless query.is_a?(String)\n\n          query = query.squish\n          return unless query.present?\n\n          arel_column = column.is_a?(Arel::Attributes::Attribute) ? column : arel_table[column]\n\n          words = select_fuzzy_terms(query, use_minimum_char_limit: use_minimum_char_limit)\n\n          if words.any?\n            words.map { |word| arel_column.matches(to_pattern(word, use_minimum_char_limit: use_minimum_char_limit)) }.reduce(:and)\n          elsif lower_exact_match\n            # No words of at least 3 chars, but we can search for an exact\n            # case insensitive match with the query as a whole\n            Arel::Nodes::NamedFunction\n                .new('LOWER', [arel_column])\n                .eq(query)\n          else\n            arel_column.matches(sanitize_sql_like(query))\n          end",
    "comment": "column - The column name / Arel column to search in. query - The text to search for. lower_exact_match - When set to `true` we'll fall back to using `LOWER(column) = query` instead of using `ILIKE`.",
    "label": "",
    "id": "2037"
  },
  {
    "raw_code": "def to_like(pattern)\n        <<~SQL\n          REPLACE(REPLACE(REPLACE(#{pattern},\n                                  #{q('%')}, #{q('\\\\%')}),\n                          #{q('_')}, #{q('\\\\_')}),\n                  #{q('*')}, #{q('%')})\n        SQL\n      end",
    "comment": "Convert a simple glob pattern with wildcard (*) to SQL LIKE pattern with SQL expression",
    "label": "",
    "id": "2038"
  },
  {
    "raw_code": "def initialize(name, union_args: {})\n        @table = Arel::Table.new(name)\n        @queries = []\n        @union_args = union_args\n      end",
    "comment": "name - The name of the CTE as a String or Symbol. union_args - The arguments supplied to Gitlab::SQL::Union class when building inner recursive query",
    "label": "",
    "id": "2039"
  },
  {
    "raw_code": "def <<(relation)\n        @queries << relation\n      end",
    "comment": "Adds a query to the body of the CTE.  relation - The relation object to add to the body of the CTE.",
    "label": "",
    "id": "2040"
  },
  {
    "raw_code": "def to_arel\n        sql = Arel::Nodes::SqlLiteral.new(Union.new(@queries, **@union_args).to_sql)\n\n        Arel::Nodes::As.new(table, Arel::Nodes::Grouping.new(sql))\n      end",
    "comment": "Returns the Arel relation for this CTE.",
    "label": "",
    "id": "2041"
  },
  {
    "raw_code": "def alias_to(alias_table)\n        Arel::Nodes::As.new(table, Arel::Table.new(alias_table.name.tr('.', '_')))\n      end",
    "comment": "Returns an \"AS\" statement that aliases the CTE name as the given table name. This allows one to trick ActiveRecord into thinking it's selecting from an actual table, when in reality it's selecting from a CTE.  alias_table - The Arel table to use as the alias.",
    "label": "",
    "id": "2042"
  },
  {
    "raw_code": "def apply_to(relation)\n        relation.except(:where)\n          .with\n          .recursive(to_arel)\n          .from(alias_to(relation.model.arel_table))\n      end",
    "comment": "Applies the CTE to the given relation, returning a new one that will query from it.",
    "label": "",
    "id": "2043"
  },
  {
    "raw_code": "def operator_keyword_fragment\n        remove_duplicates ? self.class.operator_keyword : \"#{self.class.operator_keyword} ALL\"\n      end",
    "comment": "UNION [ALL] | INTERSECT [ALL] | EXCEPT [ALL]",
    "label": "",
    "id": "2044"
  },
  {
    "raw_code": "def self.reference_pattern\n        TIME_TRACKING_REGEX\n      end",
    "comment": "Expose the time tracking regex pattern to be used in cross-reference detection",
    "label": "",
    "id": "2045"
  },
  {
    "raw_code": "def resolve(raw)\n        case @token_type\n        when :personal_access_token\n          resolve_personal_access_token(raw)\n        when :job_token\n          resolve_job_token(raw)\n        when :deploy_token\n          resolve_deploy_token(raw)\n        when :personal_access_token_with_username\n          resolve_personal_access_token_with_username(raw)\n        when :job_token_with_username\n          resolve_job_token_with_username(raw)\n        when :deploy_token_with_username\n          resolve_deploy_token_with_username(raw)\n        when :personal_access_token_from_jwt\n          resolve_personal_access_token_from_jwt(raw)\n        when :deploy_token_from_jwt\n          resolve_deploy_token_from_jwt(raw)\n        when :job_token_from_jwt\n          resolve_job_token_from_jwt(raw)\n        when :oauth_token\n          resolve_oauth_token(raw)\n        end",
    "comment": "Existing behavior is known to be inconsistent across authentication methods with regards to whether to silently ignore present but invalid credentials or to raise an error/respond with 401.  If a token can be located from the provided credentials, but the token or credentials are in some way invalid, this implementation opts to raise an error.  For example, if the raw credentials include a username and password, and a token is resolved from the password, but the username does not match the token, an error will be raised.  See https://gitlab.com/gitlab-org/gitlab/-/issues/246569",
    "label": "",
    "id": "2046"
  },
  {
    "raw_code": "def uploads(store_type = [nil, ObjectStorage::Store::LOCAL])\n        Upload.class_eval { include EachBatch } unless Upload < EachBatch\n\n        uploads = Upload.where(store: store_type)\n        uploads = uploads.where(uploader: @uploader_class) if @uploader_class.present?\n        uploads = uploads.where(model_type: @model_class.base_class.sti_name) if @model_class.present?\n        uploads = uploads.where(mount_point: @mounted_as) if @mounted_as.present?\n\n        uploads\n      end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2047"
  },
  {
    "raw_code": "def read_blob(ref, filename)\n        blob = repository&.blob_at(ref, filename)\n\n        raise NoData, 'Blob not found' unless blob\n        raise NoData, 'File is not readable' unless blob.readable_text?\n\n        blob.data\n      end",
    "comment": "Gets a Blob at a path for a specific revision. This method will check that the Blob exists and contains readable text.  revision - The String SHA1. path     - The String file path.  Returns a string containing the blob content",
    "label": "",
    "id": "2048"
  },
  {
    "raw_code": "def resolve_relative_path(path, base_path)\n        p = Pathname(base_path)\n        p = p.dirname unless p.extname.empty?\n        p += path\n\n        p.cleanpath.to_s\n      end",
    "comment": "Resolves the given relative path of file in repository into canonical path based on the specified base_path.  Examples:  # File in the same directory as the current path resolve_relative_path(\"users.adoc\", \"doc/api/README.adoc\") # => \"doc/api/users.adoc\"  # File in the same directory, which is also the current path resolve_relative_path(\"users.adoc\", \"doc/api\") # => \"doc/api/users.adoc\"  # Going up one level to a different directory resolve_relative_path(\"../update/7.14-to-8.0.adoc\", \"doc/api/README.adoc\") # => \"doc/update/7.14-to-8.0.adoc\"  Returns a String",
    "label": "",
    "id": "2049"
  },
  {
    "raw_code": "def determine_start_and_finish(measurement_identifier, query_scope)\n          queries = custom_min_max_queries[measurement_identifier]\n\n          if queries\n            [queries[:minimum_query].call, queries[:maximum_query].call]\n          else\n            [query_scope.minimum(:id), query_scope.maximum(:id)]\n          end",
    "comment": "Determining the query range (id range) as early as possible in order to get more accurate counts.",
    "label": "",
    "id": "2050"
  },
  {
    "raw_code": "def use_aggregated_backend?\n          false\n        end",
    "comment": "FOSS version doesn't use the aggregated VSA backend",
    "label": "",
    "id": "2051"
  },
  {
    "raw_code": "def self.to_enum\n          enum_mapping.transform_keys(&:identifier)\n        end",
    "comment": "hash for defining ActiveRecord enum: identifier => number",
    "label": "",
    "id": "2052"
  },
  {
    "raw_code": "def self.internal_events\n          INTERNAL_EVENTS\n        end",
    "comment": "Events that are specific to the 7 default stages",
    "label": "",
    "id": "2053"
  },
  {
    "raw_code": "def order_by(query, sort, direction, extra_columns_to_select = [:id])\n          ordered_query = Gitlab::Analytics::CycleAnalytics::Sorting.new(stage: stage, query: query, params: params).apply(sort, direction)\n\n          # When filtering for more than one label, postgres requires the columns in ORDER BY to be present in the GROUP BY clause\n          if requires_grouping?\n            column_list = [].tap do |array|\n              array.concat(extra_columns_to_select)\n              array.concat(stage.end_event.column_list) unless in_progress?\n              array.concat(stage.start_event.column_list)\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2054"
  },
  {
    "raw_code": "def requires_grouping?\n          Array(params[:label_name]).size > 1\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2055"
  },
  {
    "raw_code": "def apply(sort, direction)\n          sorting_options = {\n            end_event: {\n              asc: -> { query.reorder(end_event_timestamp_projection.asc) },\n              desc: -> { query.reorder(end_event_timestamp_projection.desc) }\n            },\n            duration: {\n              asc: -> { query.reorder(duration.asc) },\n              desc: -> { query.reorder(duration.desc) }\n            }\n          }\n\n          sort_lambda = sorting_options.dig(sort, direction) || sorting_options.dig(:end_event, :desc)\n          sort_lambda.call\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2056"
  },
  {
    "raw_code": "def select_average\n          strong_memoize(:select_average) do\n            execute_query(@query.select(average_in_seconds.as('average')).reorder(nil)).first\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2057"
  },
  {
    "raw_code": "def average\n          Arel::Nodes::NamedFunction.new(\n            'AVG',\n            [duration]\n          )\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2058"
  },
  {
    "raw_code": "def limit_count\n          query.limit(MAX_COUNT).count\n        end",
    "comment": "Limiting the maximum number of records so the COUNT(*) query stays efficient for large groups. COUNT = 1001, show 1000+ on the UI COUNT < 1001, show the actual number on the UI",
    "label": "",
    "id": "2059"
  },
  {
    "raw_code": "def apply_end_event_query_customization(query)\n          if in_progress?\n            stage.end_event.apply_negated_query_customization(query)\n          else\n            query = stage.end_event.apply_query_customization(query)\n            query.where(duration_condition)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2060"
  },
  {
    "raw_code": "def seconds\n          @query = @query.select(median_duration_in_seconds.as('median')).reorder(nil)\n          result = execute_query(@query).first || {}\n\n          result['median'] || nil\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2061"
  },
  {
    "raw_code": "def days\n          seconds ? seconds.fdiv(1.day) : nil\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2062"
  },
  {
    "raw_code": "def records_for_graphql\n            # Convert duration milliseconds to seconds to be compatible with non-aggregated data format\n            extra_columns_to_select = ['duration_in_milliseconds / 1000 AS total_time']\n\n            preloads_for_issuable = MAPPINGS.fetch(subject_class).fetch(:includes_for_query)\n\n            query\n              .limit(MAX_RECORDS)\n              .select(stage_event_model.arel_table[Arel.star], extra_columns_to_select)\n              .preload(issuable: preloads_for_issuable)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2063"
  },
  {
    "raw_code": "def limited_query\n            query\n              .page(page)\n              .per(per_page)\n              .without_count\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2064"
  },
  {
    "raw_code": "def initialize(stage:, params: {})\n            @stage = stage\n            @params = params\n            @root_ancestor = stage.namespace.root_ancestor\n            @stage_event_model = MODEL_CLASSES.fetch(stage.subject_class.to_s)\n          end",
    "comment": "Allowed params: * from - stage end date filter start date * to - stage end date filter to date * author_username * milestone_title * label_name (array) * assignee_username (array) * project_ids (array)",
    "label": "",
    "id": "2065"
  },
  {
    "raw_code": "def seconds\n            @query = @query.select(duration_in_seconds(percentile_cont).as('median')).reorder(nil)\n            result = @query.take || {}\n\n            result['median'] || nil\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2066"
  },
  {
    "raw_code": "def days\n            seconds ? seconds.fdiv(1.day) : nil\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2067"
  },
  {
    "raw_code": "def apply_query_customization(query)\n            query\n              .joins(merge_requests_closing_issues_join)\n              .joins(issue_metrics_join)\n              .joins(mr_metrics_join)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2068"
  },
  {
    "raw_code": "def issue_metrics_join\n            mr_closing_issues_table\n              .join(issue_metrics_table, Arel::Nodes::OuterJoin)\n              .on(mr_closing_issues_table[:issue_id].eq(issue_metrics_table[:issue_id]))\n              .join_sources\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2069"
  },
  {
    "raw_code": "def apply_query_customization(query)\n            query.joins(:metrics)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2070"
  },
  {
    "raw_code": "def apply_negated_query_customization(query)\n            super.joins(:metrics)\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2071"
  },
  {
    "raw_code": "def column_list\n            [timestamp_projection]\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2072"
  },
  {
    "raw_code": "def apply_query_customization(query)\n            super.where(issue_metrics_table[:first_added_to_board_at].not_eq(nil).or(issue_metrics_table[:first_associated_with_milestone_at].not_eq(nil)))\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2073"
  },
  {
    "raw_code": "def apply_query_customization(query)\n            query.joins(merge_requests_closing_issues: { merge_request: [:metrics] }).where(mr_metrics_table[:first_deployed_to_production_at].gteq(mr_table[:created_at]))\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2074"
  },
  {
    "raw_code": "def include_in(query, **)\n            query.left_joins(merge_requests_closing_issues: { merge_request: [:metrics] })\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2075"
  },
  {
    "raw_code": "def apply_query_customization(query)\n            super.where(timestamp_projection.gteq(mr_table[:created_at]))\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2076"
  },
  {
    "raw_code": "def timestamp_projection\n            columns = column_list\n\n            columns.one? ? columns.first : Arel::Nodes::NamedFunction.new('COALESCE', columns)\n          end",
    "comment": "Each StageEvent must expose a timestamp or a timestamp like expression in order to build a range query. Example: get me all the Issue records between start event end end event",
    "label": "",
    "id": "2077"
  },
  {
    "raw_code": "def column_list\n            raise NotImplementedError\n          end",
    "comment": "List of columns that are referenced in the `timestamp_projection` expression Example timestamp projection: COALESCE(issue_metrics.created_at, issue_metrics.updated_at) Expected column list: issue_metrics.created_at, issue_metrics.updated_at",
    "label": "",
    "id": "2078"
  },
  {
    "raw_code": "def apply_query_customization(query)\n            query\n          end",
    "comment": "Optionally a StageEvent may apply additional filtering or join other tables on the base query.",
    "label": "",
    "id": "2079"
  },
  {
    "raw_code": "def apply_negated_query_customization(query)\n            query.where(timestamp_projection.eq(nil))\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2080"
  },
  {
    "raw_code": "def include_in(query, **)\n            query\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2081"
  },
  {
    "raw_code": "def apply_query_customization(query)\n            super\n              .where(issue_metrics_table[:first_added_to_board_at].not_eq(nil).or(issue_metrics_table[:first_associated_with_milestone_at].not_eq(nil)))\n              .where(issue_metrics_table[:first_mentioned_in_commit_at].not_eq(nil))\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2082"
  },
  {
    "raw_code": "def self.encode_project_name(project)\n        if project.namespace.has_parent?\n          encode_slash(project.full_path)\n        else\n          project.path\n        end",
    "comment": "To present two types of projects stored by Jira, Type 1 are projects imported prior to nested group support, those project names are not full_path, so they are presented differently to maintain backwards compatibility. Type 2 are projects imported after nested group support, those project names are encoded full path  @param [Project] project",
    "label": "",
    "id": "2083"
  },
  {
    "raw_code": "def self.restore_full_path(namespace:, project:)\n        if project.include?(ENCODED_SLASH)\n          # Replace multiple slashes with single ones to make sure the redirect stays on the same host\n          # full_path should not start with a `/`\n          project.gsub(ENCODED_SLASH, SLASH).gsub(%r{\\/{2,}}, '/').gsub(%r{^\\/}, '')\n        else\n          \"#{namespace}/#{project}\"\n        end",
    "comment": "To interpret two types of project names stored by Jira (see `encode_project_name`)  @param [String] project Either an encoded full path, or just project name @param [String] namespace",
    "label": "",
    "id": "2084"
  },
  {
    "raw_code": "def initialize(entity_type:, entity_path:, runner_authentication_token: nil, runner_registration_token: nil)\n        name =\n          if runner_authentication_token.present?\n            \"Authentication token: #{runner_authentication_token}\"\n          elsif runner_registration_token.present?\n            \"Registration token: #{runner_registration_token}\"\n          else\n            \"Token not available\"\n          end",
    "comment": "Represents a CI Runner token (registration or authentication)  @param [\"gitlab_instance\", \"Group\", \"Project\"] entity_type type of the scope that the token applies to @param [String] entity_path full path to the scope that the token applies to @param [String] runner_authentication_token authentication token used in a runner registration/un-registration operation @param [String] runner_registration_token authentication token used in a runner registration operation",
    "label": "",
    "id": "2085"
  },
  {
    "raw_code": "def name\n        @name || _('Deploy Token')\n      end",
    "comment": "Events that are authored by a deploy token, should be shown as authored by `Deploy Token` in the UI.",
    "label": "",
    "id": "2086"
  },
  {
    "raw_code": "def self.audit(context, &block)\n        auditor = new(context)\n\n        if block\n          return yield unless auditor.audit_enabled?\n\n          auditor.multiple_audit(&block)\n        else\n          return unless auditor.audit_enabled?\n\n          auditor.single_audit\n        end",
    "comment": "Record audit events  @param [Hash] context @option context [String] :name the operation name to be audited, used for error tracking @option context [User] :author the user who authors the change @option context [User, Project, Group] :scope the scope which audit event belongs to @option context [Object] :target the target object being audited @option context [String] :message the message describing the action @option context [Hash] :additional_details the additional details we want to merge into audit event details. @option context [Time] :created_at the time that the event occurred (defaults to the current time)  @example Using block (useful when events are emitted deep in the call stack) i.e. multiple audit events  audit_context = { name: 'merge_approval_rule_updated', author: current_user, scope: project_alpha, target: merge_approval_rule, message: 'a user has attempted to update an approval rule' }  # in the initiating service Gitlab::Audit::Auditor.audit(audit_context) do service.execute end  # in the model Auditable.push_audit_event('an approver has been added') Auditable.push_audit_event('an approval group has been removed')  @example Using standard method call i.e. single audit event  merge_approval_rule.save Gitlab::Audit::Auditor.audit(audit_context)  @return result of block execution",
    "label": "",
    "id": "2087"
  },
  {
    "raw_code": "def self.for(id, audit_event)\n        name = audit_event[:author_name] || audit_event.details[:author_name]\n\n        if audit_event.target_type == ::Ci::Runner.name\n          Gitlab::Audit::CiRunnerTokenAuthor.new(\n            entity_type: audit_event.entity_type, entity_path: audit_event.entity_path,\n            **audit_event.details.slice(:runner_authentication_token, :runner_registration_token).symbolize_keys\n          )\n        elsif id == -1\n          Gitlab::Audit::UnauthenticatedAuthor.new(name: name)\n        elsif id == -2\n          Gitlab::Audit::DeployTokenAuthor.new(name: name)\n        elsif id == -3\n          Gitlab::Audit::DeployKeyAuthor.new(name: name)\n        else\n          Gitlab::Audit::DeletedAuthor.new(id: id, name: name)\n        end",
    "comment": "Creates an Author  While tracking events that could take place even when a user is not logged in, (eg: downloading repo of a public project), we set the author_id of such events as -1  @param [Integer] id @param [String] name rubocop: disable Layout/LineLength @return [Gitlab::Audit::UnauthenticatedAuthor, Gitlab::Audit::DeletedAuthor, Gitlab::Audit::CiRunnerTokenAuthor, Gitlab::Audit::DeployTokenAuthor]",
    "label": "",
    "id": "2088"
  },
  {
    "raw_code": "def name\n        @name || _('An unauthenticated user')\n      end",
    "comment": "Events that are authored by unauthenticated users, should be shown as authored by `An unauthenticated user` in the UI.",
    "label": "",
    "id": "2089"
  },
  {
    "raw_code": "def job_version\n        @job_version ||= self.class.version\n      end",
    "comment": "Version is not set if `new.perform` is called directly, and in that case we fallback to latest version",
    "label": "",
    "id": "2090"
  },
  {
    "raw_code": "def parse_raw_content!\n        disallowed = []\n        allowed = []\n\n        @raw_content.each_line.each do |line|\n          if disallow_rule?(line)\n            disallowed << get_disallow_pattern(line)\n          elsif allow_rule?(line)\n            allowed << get_allow_pattern(line)\n          end",
    "comment": "This parser is very basic as it only knows about `Disallow:` and `Allow:` lines, and simply ignores all other lines.  Patterns ending in `$`, and `*` for 0 or more characters are recognized.  It is case insensitive and `Allow` rules takes precedence over `Disallow`.",
    "label": "",
    "id": "2091"
  },
  {
    "raw_code": "def run\n        [@gpg_key].concat(@gpg_key.subkeys).each do |key|\n          Gitlab::Gpg.using_tmp_keychain do\n            Gitlab::Gpg::CurrentKeyChain.add(key.key)\n            CommitSignatures::GpgSignature\n              .select(:id, :commit_sha, :project_id)\n              .where('gpg_key_id IS NULL OR verification_status <> ?', CommitSignatures::GpgSignature.verification_statuses[:verified])\n              .where(gpg_key_primary_keyid: [key.keyid, key.fingerprint])\n              .find_each do |sig|\n                sig.gpg_commit&.update_signature_with_keychain!(sig, key)\n              end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2092"
  },
  {
    "raw_code": "def verified_by_gitlab?\n        signer == :SIGNER_SYSTEM\n      end",
    "comment": "If a commit is signed by Gitaly, the Gitaly returns `SIGNER_SYSTEM` as a signer In order to calculate it, the signature is Verified using the Gitaly's public key: https://gitlab.com/gitlab-org/gitaly/-/blob/v16.2.0-rc2/internal/gitaly/service/commit/commit_signatures.go#L63  It is safe to skip verification step if the commit has been signed by Gitaly",
    "label": "",
    "id": "2093"
  },
  {
    "raw_code": "def verified_by_gitlab?\n        signer == :SIGNER_SYSTEM\n      end",
    "comment": "If a commit is signed by Gitaly, the Gitaly returns `SIGNER_SYSTEM` as a signer In order to calculate it, the signature is Verified using the Gitaly's public key: https://gitlab.com/gitlab-org/gitaly/-/blob/v16.2.0-rc2/internal/gitaly/service/commit/commit_signatures.go#L63  It is safe to skip verification step if the commit has been signed by Gitaly",
    "label": "",
    "id": "2094"
  },
  {
    "raw_code": "def ensure_connected(retryable: true)\n        super\n      ensure\n        Thread.current[:redis_client_error_count] = 0\n      end",
    "comment": "This patch resets the connection error tracker after each call to prevent state leak across calls and requests.  The purpose of the tracker is to silence RedisClient::ConnectionErrors during reconnection attempts. More details found in https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/2564#note_1665334335",
    "label": "",
    "id": "2095"
  },
  {
    "raw_code": "def initialize(*args, **kwargs)\n        super\n\n        @redis = self.class.build_redis(redis: kwargs[:redis])\n      end",
    "comment": "The initialize calls retrieve_pool_options method: https://github.com/rails/rails/blob/v7.1.5.1/activesupport/lib/active_support/cache/redis_cache_store.rb#L149 In Rails 7.1 the method changed and now it always returns something  - https://github.com/rails/rails/blob/v7.0.8.7/activesupport/lib/active_support/cache.rb#L183 - https://github.com/rails/rails/blob/v7.1.5.1/activesupport/lib/active_support/cache.rb#L206  As a result, an unexpected connection pool is initialized. This path always initializes redis without a connection pool, the pool is initialized in a wrapper.",
    "label": "",
    "id": "2096"
  },
  {
    "raw_code": "def read_multi_entries(names, **options)\n        return super unless enable_rails_cache_pipeline_patch?\n        return super unless use_patched_mget?\n\n        ::Gitlab::Redis::ClusterUtil.batch_entries(names) do |batched_names|\n          super(batched_names, **options)\n        end.reduce({}, &:merge)\n      end",
    "comment": "We will try keep patched code explicit and matching the original signature in https://github.com/rails/rails/blob/v7.1.3.4/activesupport/lib/active_support/cache/redis_cache_store.rb#L324",
    "label": "",
    "id": "2097"
  },
  {
    "raw_code": "def delete_multi_entries(entries, **options)\n        return super unless enable_rails_cache_pipeline_patch?\n\n        ::Gitlab::Redis::ClusterUtil.batch_entries(entries) do |batched_names|\n          super(batched_names)\n        end.sum\n      end",
    "comment": "`delete_multi_entries` in Rails runs a multi-key `del` command patch will run pipelined single-key `del` for Redis Cluster compatibility",
    "label": "",
    "id": "2098"
  },
  {
    "raw_code": "def pipeline_entries(entries, &block)\n        return super unless enable_rails_cache_pipeline_patch?\n\n        redis.with do |conn|\n          ::Gitlab::Redis::ClusterUtil.batch(entries, conn, &block)\n        end",
    "comment": "`pipeline_entries` is used by Rails for multi-key writes patch will run pipelined single-key for Redis Cluster compatibility",
    "label": "",
    "id": "2099"
  },
  {
    "raw_code": "def use_patched_mget?\n        redis.with do |conn|\n          next true unless conn.is_a?(Gitlab::Redis::MultiStore)\n\n          ::Gitlab::Redis::ClusterUtil.cluster?(conn.default_store)\n        end",
    "comment": "MultiStore reads ONLY from the default store (no fallback), hence we can use `mget` if the default store is not a Redis::Cluster. We should do that as pipelining gets on a single redis is slow",
    "label": "",
    "id": "2100"
  },
  {
    "raw_code": "def perform_async(*args)\n        # rubocop:disable Gitlab/ModuleWithInstanceVariables -- @klass is present in the class we are patching\n\n        route_with_klass = @klass\n\n        # If an ActiveJob JobWrapper is pushed, check the arg hash's job_class for routing decisions.\n        #\n        # See https://github.com/rails/rails/blob/v7.1.0/activejob/lib/active_job/queue_adapters/sidekiq_adapter.rb#L21\n        # `job.serialize` would return a hash containing `job_class` set in\n        # https://github.com/rails/rails/blob/v7.1.0/activejob/lib/active_job/core.rb#L110\n        #\n        # In the GitLab Rails application, this only applies to ActionMailer::MailDeliveryJob\n        # but routing using the `job_class` keeps the option of using ActiveJob available for us.\n        #\n        if @klass == ActiveJob::QueueAdapters::SidekiqAdapter::JobWrapper &&\n            args.first.is_a?(Hash) &&\n            args.first['job_class']\n          route_with_klass = args.first['job_class'].to_s.safe_constantize\n        end",
    "comment": "Sidekiq::Job::Setter's .perform_in and .perform_async indirectly calls perform_async so we only need to patch 1 method.",
    "label": "",
    "id": "2101"
  },
  {
    "raw_code": "def valid_username_segment\n        return import_type unless source_username\n\n        sanitized_source_username = source_username.gsub(/[^A-Za-z0-9]/, '')\n        return import_type if sanitized_source_username.empty?\n\n        sanitized_source_username.slice(0, User::MAX_USERNAME_LENGTH - 55)\n      end",
    "comment": "Some APIs don't expose users' usernames, so set a fallback if it's nil",
    "label": "",
    "id": "2102"
  },
  {
    "raw_code": "def insert_and_return_id(attributes, relation)\n        # We use bulk_insert here so we can bypass any queries executed by\n        # callbacks or validation rules, as doing this wouldn't scale when\n        # importing very large projects.\n        result = ApplicationRecord # rubocop:disable Gitlab/BulkInsert\n                 .legacy_bulk_insert(relation.table_name, [attributes], return_ids: true)\n\n        result.first\n      end",
    "comment": "Inserts a raw row and returns the ID of the inserted row.  attributes - The attributes/columns to set. relation - An ActiveRecord::Relation to use for finding the table name",
    "label": "",
    "id": "2103"
  },
  {
    "raw_code": "def create_merge_request_metrics(attributes)\n        metric = MergeRequest::Metrics.find_or_initialize_by(merge_request: merge_request) # rubocop: disable CodeReuse/ActiveRecord -- no need to move this to ActiveRecord model\n        metric.update(attributes)\n        metric\n      end",
    "comment": "@param attributes [Hash] @return MergeRequest::Metrics",
    "label": "",
    "id": "2104"
  },
  {
    "raw_code": "def create_merge_request_without_hooks(project, attributes, iid)\n        # This work must be wrapped in a transaction as otherwise we can leave\n        # behind incomplete data in the event of an error. This can then lead\n        # to duplicate key errors when jobs are retried.\n        MergeRequest.transaction do\n          # When creating merge requests there are a lot of hooks that may\n          # run, for many different reasons. Many of these hooks (e.g. the\n          # ones used for rendering Markdown) are completely unnecessary and\n          # may even lead to transaction timeouts.\n          #\n          # To ensure importing pull requests has a minimal impact and can\n          # complete in a reasonable time we bypass all the hooks by inserting\n          # the row and then retrieving it. We then only perform the\n          # additional work that is strictly necessary.\n          merge_request_id = insert_and_return_id(attributes, project.merge_requests)\n\n          merge_request = project.merge_requests.reset.find(merge_request_id)\n\n          [merge_request, false]\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2105"
  },
  {
    "raw_code": "def insert_or_replace_git_data(merge_request, source_branch_sha, target_branch_sha, already_exists = false)\n        # These fields are set so we can create the correct merge request\n        # diffs.\n        merge_request.source_branch_sha = source_branch_sha\n        merge_request.target_branch_sha = target_branch_sha\n\n        merge_request.keep_around_commit\n\n        # We force to recreate all diffs to replace all existing data\n        # We use `.all` as otherwise `dependent: :nullify` (the default)\n        # takes an effect\n        merge_request.merge_request_diffs.all.delete_all if already_exists\n\n        # MR diffs normally use an \"after_save\" hook to pull data from Git.\n        # All of this happens in the transaction started by calling\n        # create/save/etc. This in turn can lead to these transactions being\n        # held open for much longer than necessary. To work around this we\n        # first save the diff, then populate it.\n        diff = merge_request.merge_request_diffs.build\n        diff.importing = true\n        diff.save\n        diff.save_git_content\n        diff.set_as_latest_diff\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2106"
  },
  {
    "raw_code": "def self.merge_nested_errors(object)\n        object.errors.each do |error|\n          association = object.class.reflect_on_association(error.attribute)\n\n          next unless association&.collection?\n\n          records = object.public_send(error.attribute).select(&:invalid?) # rubocop: disable GitlabSecurity/PublicSend\n\n          records.each do |record|\n            merge_nested_errors(record)\n\n            object.errors.merge!(record.errors)\n          end",
    "comment": "Merges all nested subrelation errors into base errors object.  @example issue = Project.last.issues.new( title: 'test', author: User.first, notes: [Note.new( award_emoji: [AwardEmoji.new(name: 'test')] )])  issue.validate issue.errors.full_messages => [\"Notes is invalid\"]  Gitlab::Import::Errors.merge_nested_errors(issue) issue.errors.full_messages => [\"Notes is invalid\", \"Award emoji is invalid\", \"Awardable can't be blank\", \"Name is not a valid emoji name\", ... ]",
    "label": "",
    "id": "2107"
  },
  {
    "raw_code": "def set(value)\n        Gitlab::Cache::Import::Caching.write(cache_key, value)\n      end",
    "comment": "Set the key to the given value.  @param value [String] @return [String]",
    "label": "",
    "id": "2108"
  },
  {
    "raw_code": "def current\n        Gitlab::Cache::Import::Caching.read(cache_key)\n      end",
    "comment": "Get the current value from the cache  @return [String]",
    "label": "",
    "id": "2109"
  },
  {
    "raw_code": "def expire!\n        Gitlab::Cache::Import::Caching.expire(cache_key, 0)\n      end",
    "comment": "Expire the key  @return [Boolean]",
    "label": "",
    "id": "2110"
  },
  {
    "raw_code": "def set(page)\n        Gitlab::Cache::Import::Caching.write_if_greater(cache_key, page)\n      end",
    "comment": "Sets the page number to the given value.  Returns true if the page number was overwritten, false otherwise.",
    "label": "",
    "id": "2111"
  },
  {
    "raw_code": "def current\n        Gitlab::Cache::Import::Caching.read_integer(cache_key) || 1\n      end",
    "comment": "Returns the current value from the cache.",
    "label": "",
    "id": "2112"
  },
  {
    "raw_code": "def persist_failure\n        project.import_failures.create(\n          source: error_source,\n          exception_class: exception.class.to_s,\n          exception_message: exception.message.truncate(255),\n          correlation_id_value: Labkit::Correlation::CorrelationId.current_or_new_id,\n          retry_count: fail_import ? 0 : nil,\n          external_identifiers: external_identifiers\n        )\n      end",
    "comment": "Failures with `retry_count: 0` are considered \"hard_failures\" and those are exposed on the REST API projects/:id/import",
    "label": "",
    "id": "2113"
  },
  {
    "raw_code": "def find_source_user(source_user_identifier)\n        cache_from_request_store[source_user_identifier] ||= ::Import::SourceUser.uncached do\n          source_user = ::Import::SourceUser.find_source_user(\n            source_user_identifier: source_user_identifier,\n            namespace: namespace,\n            source_hostname: source_hostname,\n            import_type: import_type\n          )\n\n          # If the record has no `#mapped_user_id`, the record would be unusuable for import.\n          # It can be in this state if the reassigned_to_user, or placeholder_user were deleted\n          # unexpectedly. We intentionally do not have a cascade delete association with\n          # users on this record as we do not want to have unmapped contributions be lost.\n          # In this situation we reset the record.\n          source_user = reset_source_user!(source_user) if reset_source_user?(source_user)\n\n          source_user\n        end",
    "comment": "Finds a source user by the provided `source_user_identifier`.  This method first checks an in-memory LRU (Least Recently Used) cache, stored in `SafeRequestStore`, to avoid unnecessary database queries. If the source user is not present in the cache, it will query the database and store the result in the cache for future use.  Since jobs may create source users concurrently, the ActiveRecord query cache is explicitly disabled when querying the database to ensure that we always get the latest data.  @param [String] source_user_identifier The identifier for the source user to find. @return [Import::SourceUser, nil] The found source user object, or `nil` if no match is found.",
    "label": "",
    "id": "2114"
  },
  {
    "raw_code": "def find_or_create_source_user(source_name:, source_username:, source_user_identifier:, cache: true)\n        source_user = find_source_user(source_user_identifier)\n        return source_user if source_user\n\n        source_user = create_source_user(\n          source_name: source_name,\n          source_username: source_username,\n          source_user_identifier: source_user_identifier\n        )\n\n        cache_from_request_store[source_user_identifier] = source_user if cache\n\n        source_user\n      end",
    "comment": "Finds a source user by the provided `source_user_identifier` or creates a new one",
    "label": "",
    "id": "2115"
  },
  {
    "raw_code": "def duplicate_user_errors?(record)\n        record.errors.where(:email, :taken).any? || record.errors.where(:username, :taken).any?\n      end",
    "comment": "Check validation errors on User records (placeholder or import users) for non-uniqueness. rubocop: disable CodeReuse/ActiveRecord -- not ActiveRecord",
    "label": "",
    "id": "2116"
  },
  {
    "raw_code": "def place_at_position(position, lhs)\n        current_occupant = relative_siblings.find_by(relative_position: position)\n\n        if current_occupant.present?\n          Mover.new(position, range).move(object, lhs.object, current_occupant)\n        else\n          object.relative_position = position\n        end",
    "comment": "Handles the possibility that the position is already occupied by a sibling",
    "label": "",
    "id": "2117"
  },
  {
    "raw_code": "def move_sequence_before(include_self = false, next_gap: find_next_gap_before)\n        raise NoSpaceLeft unless next_gap.present?\n\n        delta = next_gap.delta\n\n        move_sequence(next_gap.start_pos, relative_position, -delta, include_self)\n      end",
    "comment": "Moves the sequence before the current item to the middle of the next gap For example, we have  5 . . . . . 11 12 13 14 [15] 16 . 17 -----------  This moves the sequence [11 12 13 14] to [8 9 10 11], so we have:  5 . . 8 9 10 11 . . . [15] 16 . 17 ---------  Creating a gap to the left of the current item. We can understand this as dividing the 5 spaces between 5 and 11 into two smaller gaps of 2 and 3.  If `include_self` is true, the current item will also be moved, creating a gap to the right of the current item:  5 . . 8 9 10 11 [14] . . . 16 . 17 --------------  As an optimization, the gap can be precalculated and passed to this method.  @api private @raises NoSpaceLeft if the sequence cannot be moved",
    "label": "",
    "id": "2118"
  },
  {
    "raw_code": "def move_sequence_after(include_self = false, next_gap: find_next_gap_after)\n        raise NoSpaceLeft unless next_gap.present?\n\n        delta = next_gap.delta\n\n        move_sequence(relative_position, next_gap.start_pos, delta, include_self)\n      end",
    "comment": "Moves the sequence after the current item to the middle of the next gap For example, we have:  8 . 10 [11] 12 13 14 15 . . . . . 21 -----------  This moves the sequence [12 13 14 15] to [15 16 17 18], so we have:  8 . 10 [11] . . . 15 16 17 18 . . 21 -----------  Creating a gap to the right of the current item. We can understand this as dividing the 5 spaces between 15 and 21 into two smaller gaps of 3 and 2.  If `include_self` is true, the current item will also be moved, creating a gap to the left of the current item:  8 . 10 . . . [14] 15 16 17 18 . . 21 ----------------  As an optimization, the gap can be precalculated and passed to this method.  @api private @raises NoSpaceLeft if the sequence cannot be moved",
    "label": "",
    "id": "2119"
  },
  {
    "raw_code": "def position_between(pos_before, pos_after)\n        pos_before ||= range.first\n        pos_after ||= range.last\n\n        pos_before, pos_after = [pos_before, pos_after].sort\n\n        gap_width = pos_after - pos_before\n\n        if gap_too_small?(pos_before, pos_after)\n          raise NoSpaceLeft\n        elsif gap_width > MAX_GAP\n          if pos_before <= range.first\n            pos_after - IDEAL_DISTANCE\n          elsif pos_after >= range.last\n            pos_before + IDEAL_DISTANCE\n          else\n            midpoint(pos_before, pos_after)\n          end",
    "comment": "This method takes two integer values (positions) and calculates the position between them. The range is huge as the maximum integer value is 2147483647.  We avoid open ranges by clamping the range to [MIN_POSITION, MAX_POSITION].  Then we handle one of three cases: - If the gap is too small, we raise NoSpaceLeft - If the gap is larger than MAX_GAP, we place the new position at most IDEAL_DISTANCE from the edge of the gap. - otherwise we place the new position at the midpoint.  The new position will always satisfy: pos_before <= midpoint <= pos_after  As a precondition, the gap between pos_before and pos_after MUST be >= 2. If the gap is too small, NoSpaceLeft is raised.  @raises NoSpaceLeft",
    "label": "",
    "id": "2120"
  },
  {
    "raw_code": "def noop?\n        true\n      end",
    "comment": "noop?=>true means these won't get extracted or removed by Gitlab::QuickActions::Extractor#extract_commands QuickActions::InterpretService#perform_substitutions handles them separately",
    "label": "",
    "id": "2121"
  },
  {
    "raw_code": "def apply_type_commands(new_type, command)\n        @updates[:issue_type] = new_type.base_type\n        @updates[:work_item_type] = new_type\n\n        success_msg[command]\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables -- @updates is already defined and part of Gitlab::QuickActions::Dsl implementation",
    "label": "",
    "id": "2122"
  },
  {
    "raw_code": "def handle_set_epic(_); end\n\n      # rubocop:disable Gitlab/ModuleWithInstanceVariables -- @updates is already defined and part of\n      # Gitlab::QuickActions::Dsl implementation\n      def handle_set_parent(parent_param)\n        parent = extract_work_items(parent_param).first\n        child = quick_action_target\n\n        error = set_parent_validation_message(parent, child)\n\n        @execution_message[:set_parent] = if error.nil?\n                                            @updates[:set_parent] = parent\n                                            success_msg[:set_parent]\n                                          else\n                                            error\n                                          end\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables overridden in EE",
    "label": "",
    "id": "2123"
  },
  {
    "raw_code": "def show_epic_alias?; end\n\n      def set_parent_validation_message(parent, child)\n        # child_work_item can be nil if this is a legacy issue and the quick action is used during creation,\n        # since the associated work_item (and child.id) won't exist for the issue until after save.\n        child_work_item = fetch_child_work_item(child)\n\n        # If the parent doesn't exist, or the current user can't access it\n        unless parent && current_user.can?(:read_work_item, parent)\n          return _(\"This parent item does not exist or you don't have sufficient permission.\")\n        end\n\n        # If the child has already been added to the parent\n        if child_work_item && child_work_item.work_item_parent == parent\n          return format(_('%{child_reference} has already been added to parent %{parent_reference}.'),\n            child_reference: child_work_item.to_reference,\n            parent_reference: parent.to_reference)\n        end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables overridden in EE",
    "label": "",
    "id": "2124"
  },
  {
    "raw_code": "def process_reviewer_users(users)\n        users\n      end",
    "comment": "Overriden in EE",
    "label": "",
    "id": "2125"
  },
  {
    "raw_code": "def process_reviewer_users_message\n        nil\n      end",
    "comment": "Overriden in EE",
    "label": "",
    "id": "2126"
  },
  {
    "raw_code": "def desc(text = '', &block)\n          @description = block || text\n        end",
    "comment": "Allows to give a description to the next quick action. This description is shown in the autocomplete menu. It accepts a block that will be evaluated with the context given to `CommandDefinition#to_h`.  Example:  desc do \"This is a dynamic description for #{quick_action_target.to_ability_name}\" end command :command_key do |arguments| # Awesome code block end",
    "label": "",
    "id": "2127"
  },
  {
    "raw_code": "def params(*params, &block)\n          @params = block || params\n        end",
    "comment": "Allows to define params for the next quick action. These params are shown in the autocomplete menu.  Example:  params \"~label ~label2\" command :command_key do |arguments| # Awesome code block end",
    "label": "",
    "id": "2128"
  },
  {
    "raw_code": "def explanation(text = '', &block)\n          @explanation = block || text\n        end",
    "comment": "Allows to give an explanation of what the command will do when executed. This explanation is shown when rendering the Markdown preview.  Example:  explanation do |arguments| \"Adds labels #{arguments.join(' ')}\" end command :command_key do |arguments| # Awesome code block end",
    "label": "",
    "id": "2129"
  },
  {
    "raw_code": "def execution_message(text = '', &block)\n          @execution_message = block || text\n        end",
    "comment": "Allows to provide a message about quick action execution result, success or failure. This message is shown after quick action execution and after saving the note.  Example:  execution_message do |arguments| \"Added labels #{arguments.join(' ')}\" end command :command_key do |arguments| # Awesome code block end  Note: The execution_message won't be executed unless the condition block returns true. execution_message block is executed always after the command block has run, for this reason if the condition block doesn't return true after the command block has run you need to set the @execution_message variable inside the command block instead as shown in the following example.  Example using instance variable:  command :command_key do |arguments| # Awesome code block @execution_message[:command_key] = 'command_key executed successfully' end ",
    "label": "",
    "id": "2130"
  },
  {
    "raw_code": "def types(*types_list)\n          @types = types_list\n        end",
    "comment": "Allows to define type(s) that must be met in order for the command to be returned by `.command_names` & `.command_definitions`.  It is being evaluated before the conditions block is being evaluated  If no types are passed then any type is allowed as the check is simply skipped.  Example:  types Commit, Issue, MergeRequest command :command_key do |arguments| # Awesome code block end",
    "label": "",
    "id": "2131"
  },
  {
    "raw_code": "def condition(&block)\n          @condition_block = block\n        end",
    "comment": "Allows to define conditions that must be met in order for the command to be returned by `.command_names` & `.command_definitions`. It accepts a block that will be evaluated with the context of a QuickActions::InterpretService instance Example:  condition do project.public? end command :command_key do |arguments| # Awesome code block end",
    "label": "",
    "id": "2132"
  },
  {
    "raw_code": "def parse_params(&block)\n          @parse_params_block = block\n        end",
    "comment": "Allows to perform initial parsing of parameters. The result is passed both to `command` and `explanation` blocks, instead of the raw parameters. It accepts a block that will be evaluated with the context given to `CommandDefinition#to_h`.  Example:  parse_params do |raw| raw.strip end command :command_key do |parsed| # Awesome code block end",
    "label": "",
    "id": "2133"
  },
  {
    "raw_code": "def command(*command_names, &block)\n          define_command(CommandDefinition, *command_names, &block)\n        end",
    "comment": "Registers a new command which is recognizable from body of email or comment. It accepts aliases and takes a block.  You can also set the @execution_message instance variable, on conflicts with execution_message method the instance variable has precedence.  Example:  command :my_command, :alias_for_my_command do |arguments| # Awesome code block @updates[:my_command] = 'foo'  @execution_message[:my_command] = 'my_command executed successfully' end",
    "label": "",
    "id": "2134"
  },
  {
    "raw_code": "def substitution(*substitution_names, &block)\n          define_command(SubstitutionDefinition, *substitution_names, &block)\n        end",
    "comment": "Registers a new substitution which is recognizable from body of email or comment. It accepts aliases and takes a block with the formatted content.  Example:  command :my_substitution, :alias_for_my_substitution do |text| \"#{text} MY AWESOME SUBSTITUTION\" end",
    "label": "",
    "id": "2135"
  },
  {
    "raw_code": "def extract_commands(content, only: nil)\n        return [content, []] unless content\n\n        perform_regex(content, only: only)\n      end",
    "comment": "Extracts commands from content and return an array of commands. The array looks like the following: [ ['command1'], ['command3', 'arg1 arg2'], ] The original command text and arguments are removed from the given `content`, unless `keep_actions` is true.  Usage: ``` extractor = Gitlab::QuickActions::Extractor.new([:open, :assign, :labels]) msg = %(hello\\n/labels ~foo ~\"bar baz\"\\nworld) commands = extractor.extract_commands(msg) #=> [['labels', '~foo ~\"bar baz\"']] msg #=> \"hello\\nworld\"  extractor = Gitlab::QuickActions::Extractor.new([:open, :assign, :labels], keep_actions: true) msg = %(hello\\n/labels ~foo ~\"bar baz\"\\nworld) commands = extractor.extract_commands(msg) #=> [['labels', '~foo ~\"bar baz\"']] msg #=> \"hello\\n/labels ~foo ~\"bar baz\"\\n\\nworld\" ```",
    "label": "",
    "id": "2136"
  },
  {
    "raw_code": "def redact_commands(content)\n        return \"\" unless content\n\n        content, _ = perform_regex(content, redact: true)\n\n        content\n      end",
    "comment": "Encloses quick action commands into code span markdown avoiding them being executed, for example, when sent via email to GitLab service desk. Example: /label ~label1 becomes `/label ~label1`",
    "label": "",
    "id": "2137"
  },
  {
    "raw_code": "def commands_regex(names:, sub_names:)\n        @commands_regex[names] ||= %r{\n            #{EXCLUSION_REGEX}\n          |\n            (?:\n              # Command such as:\n              # /close\n\n              ^\\/\n              (?<cmd>#{Regexp.new(Regexp.union(names).source, Regexp::IGNORECASE)})\n              (?:\n                [ ]\n                (?<arg>[^\\n]*)\n              )?\n              (?:\\s*\\n|$)\n            )\n          |\n            (?:\n              # Substitution such as:\n              # /shrug\n\n              ^\\/\n              (?<substitution>#{Regexp.new(Regexp.union(sub_names).source, Regexp::IGNORECASE)})\n              (?:\n                [ ]\n                (?<arg>[^\\n]*)\n              )?\n              (?:\\s*\\n|$)\n            )\n        }mix\n      end",
    "comment": "Builds a regular expression to match known commands. First match group captures the command name and second match group captures its arguments.  It looks something like:  /^\\/(?<cmd>close|reopen|...)(?:( |$))(?<arg>[^\\/\\n]*)(?:\\n|$)/",
    "label": "",
    "id": "2138"
  },
  {
    "raw_code": "def validate_single_and_plural_variables(errors, entry)\n        variables = entry.msgid.scan(VARIABLE_REGEX)\n        plural_variables = entry.plural_id.scan(VARIABLE_REGEX)\n\n        all_variables = (variables + plural_variables).uniq\n        validate_unnamed_variables(errors, all_variables)\n        validate_variables_in_message(errors, entry.plural_id, entry.plural_id)\n      end",
    "comment": "rubocop: disable Style/AsciiComments -- Need for clarity Don't allow mixing named and positional variables in singular and plural forms for languages such as Japanese. For example:  msgid \"GlobalSearch|Showing 1 code result for %{term}\" msgid_plural \"GlobalSearch|Showing %{resultsTotal} code results for %{term}\" msgstr[0] \"%{term}%{resultsTotal}\" variables  Here we see that both `term` and `resultsTotal` are needed in the final translation. If we mix named and positional variables in the singular and plural forms, it could be ambiguous as to which variables belong where. rubocop: enable Style/AsciiComments",
    "label": "",
    "id": "2139"
  },
  {
    "raw_code": "def pluralisation_rule\n          Gitlab::I18n::Pluralization\n        end",
    "comment": "FastGettext allows to set the rule via `FastGettext.pluralisation_rule=` which is on thread-level.  Because we are patching FastGettext at boot time per thread values won't work so we have to override the method implementation.  `FastGettext.pluralisation_rule=` has now no effect.",
    "label": "",
    "id": "2140"
  },
  {
    "raw_code": "def non_idle_connections_by_db(min_samples)\n        result = {}\n        samples_by_db.each do |db, samples|\n          result[db] = {}\n          parsed_samples = Gitlab::Json.parse(samples)\n\n          next if parsed_samples.length < min_samples\n\n          result[db] = parsed_samples.last(min_samples)\n                                     .map { |sample| non_idle_by_endpoints(sample) }\n                                     .reduce({}) do |res, hash|\n                                       res.merge(hash) { |_key, old_val, new_val| old_val + new_val }\n                                     end",
    "comment": "Returns the total of non-idle connections aggregated from the last `min_samples` for each database name, eg `gitlabhq_production_sidekiq`, `gitlabhq_production_sidekiq_urgent`. Hash for each database will be empty if there are not enough samples in Redis to meet `min_samples`.  Returns { \"gitlabhq_production_sidekiq\": { \"WorkerA\": 1, \"WorkerB\": 2 }, gitlabhq_production_sidekiq_urgent: { \"WorkerC\": 3 } }",
    "label": "",
    "id": "2141"
  },
  {
    "raw_code": "def lock_writes\n        Gitlab::Database::EachDatabase.each_connection(include_shared: false) do |connection, database_name|\n          next if skip_connection?(database_name)\n\n          schemas_for_connection = Gitlab::Database.gitlab_schemas_for_connection(connection)\n\n          tables_to_lock(connection) do |table_name, schema_name|\n            # TODO: https://gitlab.com/gitlab-org/gitlab/-/issues/366834\n            next if schema_name.in? GITLAB_SCHEMAS_TO_IGNORE\n\n            if schemas_for_connection.include?(schema_name)\n              unlock_writes_on_table(table_name, connection, database_name)\n            else\n              lock_writes_on_table(table_name, connection, database_name)\n            end",
    "comment": "It locks the tables on the database where they don't belong. Also it unlocks the tables on the database where they belong",
    "label": "",
    "id": "2142"
  },
  {
    "raw_code": "def unlock_writes_on_table(table_name, connection, database_name)\n        @result << lock_writes_manager(table_name, connection, database_name).unlock_writes\n        return unless @include_partitions\n\n        table_attached_partitions(table_name, connection) do |postgres_partition|\n          @result << lock_writes_manager(postgres_partition.identifier, connection, database_name).unlock_writes\n        end",
    "comment": "Unlocks the writes on the table and its partitions",
    "label": "",
    "id": "2143"
  },
  {
    "raw_code": "def lock_writes_on_table(table_name, connection, database_name)\n        @result << lock_writes_manager(table_name, connection, database_name).lock_writes\n        return unless @include_partitions\n\n        table_attached_partitions(table_name, connection) do |postgres_partition|\n          @result << lock_writes_manager(postgres_partition.identifier, connection, database_name).lock_writes\n        end",
    "comment": "It locks the writes on the table and its partitions",
    "label": "",
    "id": "2144"
  },
  {
    "raw_code": "def run(raise_on_exhaustion: false, &block)\n        raise 'no block given' unless block\n\n        @block = block\n\n        if lock_retries_disabled?\n          log(message: 'DISABLE_LOCK_RETRIES environment variable is true, executing the block without retry')\n\n          return run_block\n        end",
    "comment": "Executes a block of code, retrying it whenever a database lock can't be acquired in time  When a database lock can't be acquired, ActiveRecord throws ActiveRecord::LockWaitTimeout exception which we intercept to re-execute the block of code, until it finishes or we reach the max attempt limit. The default behavior when max attempts have been reached is to make a final attempt with the lock_timeout disabled, but this can be altered with the raise_on_exhaustion parameter.  @see DEFAULT_TIMING_CONFIGURATION for the timings used when attempting a retry @param [Boolean] raise_on_exhaustion whether to raise `AttemptsExhaustedError` when exhausting max attempts @param [Proc] block of code that will be executed",
    "label": "",
    "id": "2145"
  },
  {
    "raw_code": "def build_recursive_query(cursor, of, first_iteration)\n        ids = first_iteration ? cursor[:current_id] : ''\n\n        recursive_cte = Gitlab::SQL::RecursiveCTE.new(:result,\n          union_args: {\n            remove_order: false,\n            remove_duplicates: false\n          })\n\n        recursive_cte << base_namespace_class.select(\n          Arel.sql(\"#{cursor[:current_id]}::bigint\").as('current_id'),\n          Arel.sql(\"ARRAY[#{cursor[:depth].join(',')}]::bigint[]\").as('depth'),\n          Arel.sql(\"ARRAY[#{ids}]::bigint[]\").as('ids'),\n          Arel.sql('1::bigint AS count'),\n          Arel.sql('0::bigint AS index')\n        ).from('(VALUES (1)) AS initializer_row')\n          .where_exists(namespace_exists_query)\n\n        cte = Gitlab::SQL::CTE.new(:cte, base_namespace_class.select('result.*').from('result'))\n\n        union_query = base_namespace_class.with(cte.to_arel).from_union(\n          walk_down,\n          next_elements,\n          up_one_level,\n          remove_duplicates: false,\n          remove_order: false\n        ).select(*PROJECTIONS).order(base_namespace_class.arel_table[:index].asc).limit(1)\n\n        recursive_cte << union_query\n\n        base_namespace_class.with\n          .recursive(recursive_cte.to_arel)\n          .from(recursive_cte.alias_to(namespace_class.arel_table))\n          .select(*PROJECTIONS)\n          .limit(of + 1)\n      end",
    "comment": "rubocop: disable Style/AsciiComments -- Rendering a graph The depth-first algorithm is implemented here. Consider the following group hierarchy:   10                          41          72              32                                11   12    18          1. Start with node 10 and look up the left-hand child nodes until reaching the leaf. (walk_down) 2. While walking down, record the depth in an array and also store them in the ids array. 3. depth: 10, 41, 32, 11 | ids: 10, 41, 32, 11 4. Start collecting the ids by looking at the nodes on the deepest level. (next_elements) 5. This gives us the rest of the nodes on the same level (parent_id = 32 AND id > 11) 6. depth: 10, 41, 32, 11 | ids: 10, 41, 32, 11, 12, 18 7. When done, move one level up and pop the last value from the depth. (up_one_level) 8. depth: 10, 41, 32 | ids: 10, 41, 32, 11, 12, 18 9. Do the same, look at the nodes on the same level: no records, 32 was already collected 10. depth: 10, 41, 32 | ids: 10, 41, 32, 11, 12, 18 11. Move one level up again and look at the nodes on the same level. 12. depth: 10, 41 | ids: 10, 41, 32, 11, 12, 18, 72 13. Move one level up again, we reached the root node, iteration is done. 14. depth: 10 | ids: 10, 41, 32, 11, 12, 18, 72  By tracking the currently accessed node and the depth we can stop and restore the processing of the hierarchy at any point.  rubocop: enable Style/AsciiComments",
    "label": "",
    "id": "2146"
  },
  {
    "raw_code": "def lease_release?\n        false\n      end",
    "comment": "Overrides ExclusiveLeaseGuard to not release lease after the sample to ensure we do not oversample",
    "label": "",
    "id": "2147"
  },
  {
    "raw_code": "def initialize(gitlab_schema, increase_by, base_model = ApplicationRecord)\n        @base_model = base_model\n        @gitlab_schema = gitlab_schema\n        @increase_by = increase_by\n      end",
    "comment": "gitlab_schema: can be 'gitlab_main', 'gitlab_ci', 'gitlab_main_org', 'gitlab_shared' increase_by: positive number, to increase the sequence by base_model: is to choose which connection to use to query the tables",
    "label": "",
    "id": "2148"
  },
  {
    "raw_code": "def increment_sequence_by(connection, sequence_name, increment_by)\n        connection.transaction do\n          # The first call is to make sure that the sequence's is_called value is set to `true`\n          # This guarantees that the next call to `nextval` will increase the sequence by `increment_by`\n          connection.select_value(\"SELECT nextval($1)\", nil, [sequence_name])\n          connection.execute(\"ALTER SEQUENCE #{sequence_name} INCREMENT BY #{increment_by}\")\n          connection.select_value(\"select nextval($1)\", nil, [sequence_name])\n          connection.execute(\"ALTER SEQUENCE #{sequence_name} INCREMENT BY 1\")\n        end",
    "comment": "This method is going to increase the sequence next_value by: - increment_by + 1 if the sequence has the attribute is_called = True (which is the common case) - increment_by if the sequence has the attribute is_called = False (for example, a newly created sequence) It uses ALTER SEQUENCE as a safety mechanism to avoid that no concurrent insertions will cause conflicts on the sequence. This is because ALTER SEQUENCE blocks concurrent nextval, currval, lastval, and setval calls.",
    "label": "",
    "id": "2149"
  },
  {
    "raw_code": "def tables_dependencies\n        @tables.index_with do |table_name|\n          all_foreign_keys[table_name]\n        end",
    "comment": "it maps the tables to the tables that depend on it",
    "label": "",
    "id": "2150"
  },
  {
    "raw_code": "def deserialize(value)\n        value = super(value)\n        if value.present?\n          Base64.encode64(value).delete(\"=\").chomp(\"\\n\")\n        else\n          nil\n        end",
    "comment": "Casts binary data to a SHA256 and remove trailing = and newline from encode64",
    "label": "",
    "id": "2151"
  },
  {
    "raw_code": "def serialize(value)\n        arg = if value.present?\n                Base64.decode64(value)\n              else\n                nil\n              end",
    "comment": "Casts a SHA256 in a proper binary format. which is 32 bytes long",
    "label": "",
    "id": "2152"
  },
  {
    "raw_code": "def self.approximate_counts(models, strategies: [])\n        if strategies.empty?\n          # ExactCountStrategy is the only strategy working on read-only DBs, as others make\n          # use of tuple stats which use the primary DB to estimate tables size in a transaction.\n          strategies = if ::Gitlab::Database.read_write?\n                         [TablesampleCountStrategy, ReltuplesCountStrategy, ExactCountStrategy]\n                       else\n                         [ExactCountStrategy]\n                       end",
    "comment": "Takes in an array of models and returns a Hash for the approximate counts for them.  Various count strategies can be specified that are executed in sequence until all tables have an approximate count attached or we run out of strategies.  Note that not all strategies are available on all supported RDBMS.  @param [Array] @return [Hash] of Model -> count mapping",
    "label": "",
    "id": "2153"
  },
  {
    "raw_code": "def deserialize(value)\n        value = super(value)\n        value ? value.unpack1(PACK_FORMAT) : nil\n      end",
    "comment": "Casts binary data to a SHA1 in hexadecimal.",
    "label": "",
    "id": "2154"
  },
  {
    "raw_code": "def serialize(value)\n        arg = value ? [value].pack(PACK_FORMAT) : nil\n\n        BINARY_TYPE.new.serialize(arg)\n      end",
    "comment": "Casts a SHA1 in hexadecimal to the proper binary format.",
    "label": "",
    "id": "2155"
  },
  {
    "raw_code": "def self.serialize(value)\n        arg = value ? [value].pack(PACK_FORMAT) : nil\n\n        BINARY_TYPE.new.serialize(arg)\n      end",
    "comment": "Casts a SHA1 in hexadecimal to the proper binary format.",
    "label": "",
    "id": "2156"
  },
  {
    "raw_code": "def add_timestamps_with_timezone(table_name, options = {})\n        columns = options.fetch(:columns, DEFAULT_TIMESTAMP_COLUMNS)\n\n        columns.each do |column_name|\n          validate_timestamp_column_name!(column_name)\n\n          add_column(\n            table_name,\n            column_name,\n            :datetime_with_timezone,\n            default: options[:default],\n            null: options[:null] || false\n          )\n        end",
    "comment": "Adds `created_at` and `updated_at` columns with timezone information.  This method is an improved version of Rails' built-in method `add_timestamps`.  By default, adds `created_at` and `updated_at` columns, but these can be specified as:  add_timestamps_with_timezone(:my_table, columns: [:created_at, :deleted_at])  This allows you to create just the timestamps you need, saving space.  Available options are: :default - The default value for the column. :null - When set to `true` the column will allow NULL values. The default is to not allow NULL values. :columns - the column names to create. Must end with `_at`. Default value: `DEFAULT_TIMESTAMP_COLUMNS`  All options are optional.",
    "label": "",
    "id": "2157"
  },
  {
    "raw_code": "def remove_timestamps(table_name, options = {})\n        columns = options.fetch(:columns, DEFAULT_TIMESTAMP_COLUMNS)\n        columns.each do |column_name|\n          remove_column(table_name, column_name)\n        end",
    "comment": "To be used in the `#down` method of migrations that use `#add_timestamps_with_timezone`.  Available options are: :columns - the column names to remove. Must be one Default value: `DEFAULT_TIMESTAMP_COLUMNS`  All options are optional.",
    "label": "",
    "id": "2158"
  },
  {
    "raw_code": "def add_concurrent_index(table_name, column_name, options = {})\n        if transaction_open?\n          raise 'add_concurrent_index can not be run inside a transaction, ' \\\n            'you can disable transactions by calling disable_ddl_transaction! ' \\\n            'in the body of your migration class'\n        end",
    "comment": "Creates a new index, concurrently  Example:  add_concurrent_index :users, :some_column  See Rails' `add_index` for more info on the available arguments.",
    "label": "",
    "id": "2159"
  },
  {
    "raw_code": "def remove_concurrent_index(table_name, column_name, options = {})\n        if transaction_open?\n          raise 'remove_concurrent_index can not be run inside a transaction, ' \\\n            'you can disable transactions by calling disable_ddl_transaction! ' \\\n            'in the body of your migration class'\n        end",
    "comment": "Removes an existed index, concurrently  Example:  remove_concurrent_index :users, :some_column  See Rails' `remove_index` for more info on the available arguments.",
    "label": "",
    "id": "2160"
  },
  {
    "raw_code": "def remove_concurrent_index_by_name(table_name, index_name, options = {})\n        if transaction_open?\n          raise 'remove_concurrent_index_by_name can not be run inside a transaction, ' \\\n            'you can disable transactions by calling disable_ddl_transaction! ' \\\n            'in the body of your migration class'\n        end",
    "comment": "Removes an existing index, concurrently  Example:  remove_concurrent_index :users, \"index_X_by_Y\"  See Rails' `remove_index` for more info on the available arguments.",
    "label": "",
    "id": "2161"
  },
  {
    "raw_code": "def add_concurrent_foreign_key(source, target, column:, **options)\n        options.reverse_merge!({\n          on_delete: :cascade,\n          on_update: nil,\n          target_column: :id,\n          validate: true,\n          reverse_lock_order: false,\n          allow_partitioned: false,\n          column: column\n        })\n\n        # Transactions would result in ALTER TABLE locks being held for the\n        # duration of the transaction, defeating the purpose of this method.\n        if transaction_open?\n          raise 'add_concurrent_foreign_key can not be run inside a transaction'\n        end",
    "comment": "Adds a foreign key with only minimal locking on the tables involved.  This method only requires minimal locking  source - The source table containing the foreign key. target - The target table the key points to. column - The name of the column to create the foreign key on. target_column - The name of the referenced column, defaults to \"id\". on_delete - The action to perform when associated data is removed, defaults to \"CASCADE\". on_update - The action to perform when associated data is updated, defaults to nil. This is useful for multi column FKs if it's desirable to update one of the columns. name - The name of the foreign key. validate - Flag that controls whether the new foreign key will be validated after creation. If the flag is not set, the constraint will only be enforced for new data. reverse_lock_order - Flag that controls whether we should attempt to acquire locks in the reverse order of the ALTER TABLE. This can be useful in situations where the foreign key creation could deadlock with another process. ",
    "label": "",
    "id": "2162"
  },
  {
    "raw_code": "def concurrent_foreign_key_name(table, column, prefix: 'fk_')\n        identifier = \"#{table}_#{multiple_columns(column, separator: '_')}_fk\"\n        hashed_identifier = Digest::SHA256.hexdigest(identifier).first(10)\n\n        \"#{prefix}#{hashed_identifier}\"\n      end",
    "comment": "Returns the name for a concurrent foreign key.  PostgreSQL constraint names have a limit of 63 bytes. The logic used here is based on Rails' foreign_key_name() method, which unfortunately is private so we can't rely on it directly.  prefix: - The default prefix is `fk_` for backward compatibility with the existing concurrent foreign key helpers. - For standard rails foreign keys the prefix is `fk_rails_` ",
    "label": "",
    "id": "2163"
  },
  {
    "raw_code": "def update_column_in_batches(table_name, column, value, batch_size: nil, batch_column_name: :id, disable_lock_writes: false)\n        if transaction_open?\n          raise 'update_column_in_batches can not be run inside a transaction, ' \\\n            'you can disable transactions by calling disable_ddl_transaction! ' \\\n            'in the body of your migration class'\n        end",
    "comment": "Updates the value of a column in batches.  This method updates the table in batches of 5% of the total row count. A `batch_size` option can also be passed to set this to a fixed number. This method will continue updating rows until no rows remain.  When given a block this method will yield two values to the block:  1. An instance of `Arel::Table` for the table that is being updated. 2. The query to run as an Arel object.  By supplying a block one can add extra conditions to the queries being executed. Note that the same block is used for _all_ queries.  Example:  update_column_in_batches(:projects, :foo, 10) do |table, query| query.where(table[:some_column].eq('hello')) end  This would result in this method updating only rows where `projects.some_column` equals \"hello\".  table - The name of the table. column - The name of the column to update. value - The value for the column.  The `value` argument is typically a literal. To perform a computed update, an Arel literal can be used instead:  update_value = Arel.sql('bar * baz')  update_column_in_batches(:projects, :foo, update_value) do |table, query| query.where(table[:some_column].eq('hello')) end  Rubocop's Metrics/AbcSize metric is disabled for this method as Rubocop determines this method to be too complex while there's no way to make it less \"complex\" without introducing extra methods (which actually will make things _more_ complex).  `batch_column_name` option is for tables without primary key, in this case another unique integer column can be used. Example: :user_id  rubocop: disable Metrics/AbcSize",
    "label": "",
    "id": "2164"
  },
  {
    "raw_code": "def rename_column_concurrently(table, old, new, type: nil, type_cast_function: nil, batch_column_name: :id)\n        unless column_exists?(table, batch_column_name)\n          raise \"Column #{batch_column_name} does not exist on #{table}\"\n        end",
    "comment": "Renames a column without requiring downtime.  Concurrent renames work by using database triggers to ensure both the old and new column are in sync. However, this method will _not_ remove the triggers or the old column automatically; this needs to be done manually in a post-deployment migration. This can be done using the method `cleanup_concurrent_column_rename`.  table - The name of the database table containing the column. old - The old column name. new - The new column name. type - The type of the new column. If no type is given the old column's type is used. batch_column_name - option is for tables without primary key, in this case another unique integer column can be used. Example: :user_id",
    "label": "",
    "id": "2165"
  },
  {
    "raw_code": "def undo_rename_column_concurrently(table, old, new)\n        trigger_name = rename_trigger_name(table, old, new)\n\n        check_trigger_permissions!(table)\n\n        remove_rename_triggers(table, trigger_name)\n\n        remove_column(table, new)\n      end",
    "comment": "Reverses operations performed by rename_column_concurrently.  This method takes care of removing previously installed triggers as well as removing the new column.  table - The name of the database table. old - The name of the old column. new - The name of the new column.",
    "label": "",
    "id": "2166"
  },
  {
    "raw_code": "def install_rename_triggers(table, old, new, trigger_name: nil)\n        Gitlab::Database::UnidirectionalCopyTrigger.on_table(table, connection: connection).create(old, new, trigger_name: trigger_name)\n      end",
    "comment": "Installs triggers in a table that keep a new column in sync with an old one.  table - The name of the table to install the trigger in. old_column - The name of the old column. new_column - The name of the new column. trigger_name - The name of the trigger to use (optional).",
    "label": "",
    "id": "2167"
  },
  {
    "raw_code": "def remove_rename_triggers(table, trigger)\n        Gitlab::Database::UnidirectionalCopyTrigger.on_table(table, connection: connection).drop(trigger)\n      end",
    "comment": "Removes the triggers used for renaming a column concurrently.",
    "label": "",
    "id": "2168"
  },
  {
    "raw_code": "def rename_trigger_name(table, old, new)\n        Gitlab::Database::UnidirectionalCopyTrigger.on_table(table, connection: connection).name(old, new)\n      end",
    "comment": "Returns the (base) name to use for triggers when renaming columns.",
    "label": "",
    "id": "2169"
  },
  {
    "raw_code": "def install_sharding_key_assignment_trigger(**args)\n        Gitlab::Database::Triggers::AssignDesiredShardingKey.new(**args.merge(connection: connection)).create\n      end",
    "comment": "Installs a trigger in a table that assigns a sharding key from an associated table.  table: The table to install the trigger in. sharding_key: The column to be assigned on `table`. parent_table: The associated table with the sharding key to be copied. parent_sharding_key: The sharding key on the parent table that will be copied to `sharding_key` on `table`. foreign_key: The column used to fetch the relevant record from `parent_table`.",
    "label": "",
    "id": "2170"
  },
  {
    "raw_code": "def remove_sharding_key_assignment_trigger(**args)\n        Gitlab::Database::Triggers::AssignDesiredShardingKey.new(**args.merge(connection: connection)).drop\n      end",
    "comment": "Removes trigger used for assigning sharding keys.  table: The table to install the trigger in. sharding_key: The column to be assigned on `table`. parent_table: The associated table with the sharding key to be copied. parent_sharding_key: The sharding key on the parent table that will be copied to `sharding_key` on `table`. foreign_key: The column used to fetch the relevant record from `parent_table`.",
    "label": "",
    "id": "2171"
  },
  {
    "raw_code": "def change_column_type_concurrently(table, column, new_type, type_cast_function: nil, batch_column_name: :id)\n        temp_column = \"#{column}_for_type_change\"\n\n        rename_column_concurrently(table, column, temp_column, type: new_type, type_cast_function: type_cast_function, batch_column_name: batch_column_name)\n      end",
    "comment": "Changes the type of a column concurrently.  table - The table containing the column. column - The name of the column to change. new_type - The new column type.",
    "label": "",
    "id": "2172"
  },
  {
    "raw_code": "def undo_change_column_type_concurrently(table, column)\n        temp_column = \"#{column}_for_type_change\"\n\n        undo_rename_column_concurrently(table, column, temp_column)\n      end",
    "comment": "Reverses operations performed by change_column_type_concurrently.  table - The table containing the column. column - The name of the column to change.",
    "label": "",
    "id": "2173"
  },
  {
    "raw_code": "def cleanup_concurrent_column_type_change(table, column, temp_column: nil)\n        temp_column ||= \"#{column}_for_type_change\"\n\n        transaction do\n          # This has to be performed in a transaction as otherwise we might have\n          # inconsistent data.\n          cleanup_concurrent_column_rename(table, column, temp_column)\n          rename_column(table, temp_column, column)\n        end",
    "comment": "Performs cleanup of a concurrent type change.  table - The table containing the column. column - The name of the column to change. new_type - The new column type.",
    "label": "",
    "id": "2174"
  },
  {
    "raw_code": "def undo_cleanup_concurrent_column_type_change(table, column, old_type, type_cast_function: nil, batch_column_name: :id, limit: nil, temp_column: nil)\n        Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_ddl_mode!\n\n        temp_column ||= \"#{column}_for_type_change\"\n\n        # Using a descriptive name that includes orinal column's name risks\n        # taking us above the 63 character limit, so we use a hash\n        identifier = \"#{table}_#{column}_for_type_change\"\n        hashed_identifier = Digest::SHA256.hexdigest(identifier).first(10)\n        temp_undo_cleanup_column = \"tmp_undo_cleanup_column_#{hashed_identifier}\"\n\n        unless column_exists?(table, batch_column_name)\n          raise \"Column #{batch_column_name} does not exist on #{table}\"\n        end",
    "comment": "Reverses operations performed by cleanup_concurrent_column_type_change.  table - The table containing the column. column - The name of the column to change. old_type - The type of the original column used with change_column_type_concurrently. type_cast_function - Required if the conversion back to the original type is not automatic batch_column_name - option for tables without a primary key, in this case another unique integer column can be used. Example: :user_id",
    "label": "",
    "id": "2175"
  },
  {
    "raw_code": "def cleanup_concurrent_column_rename(table, old, new)\n        trigger_name = rename_trigger_name(table, old, new)\n\n        check_trigger_permissions!(table)\n\n        remove_rename_triggers(table, trigger_name)\n\n        remove_column(table, old)\n      end",
    "comment": "Cleans up a concurrent column name.  This method takes care of removing previously installed triggers as well as removing the old column.  table - The name of the database table. old - The name of the old column. new - The name of the new column.",
    "label": "",
    "id": "2176"
  },
  {
    "raw_code": "def undo_cleanup_concurrent_column_rename(table, old, new, type: nil, batch_column_name: :id)\n        unless column_exists?(table, batch_column_name)\n          raise \"Column #{batch_column_name} does not exist on #{table}\"\n        end",
    "comment": "Reverses the operations performed by cleanup_concurrent_column_rename.  This method adds back the old_column removed by cleanup_concurrent_column_rename. It also adds back the (old_column > new_column) trigger that is removed by cleanup_concurrent_column_rename.  table - The name of the database table containing the column. old - The old column name. new - The new column name. type - The type of the old column. If no type is given the new column's type is used. batch_column_name - option is for tables without primary key, in this case another unique integer column can be used. Example: :user_id",
    "label": "",
    "id": "2177"
  },
  {
    "raw_code": "def initialize_conversion_of_integer_to_bigint(table, columns, primary_key: :id) # rubocop:disable Lint/UnusedMethodArgument -- for backward compatibility, don't remove primary_key\n        Gitlab::Database::Migrations::Conversions::BigintConverter\n          .new(self, table, columns)\n          .init\n      end",
    "comment": "Initializes the conversion of a set of integer columns to bigint  It can be used for converting both a Primary Key and any Foreign Keys that may reference it or any other integer column that we may want to upgrade (e.g. columns that store IDs, but are not set as FKs).  - For primary keys and Foreign Keys (or other columns) defined as NOT NULL, the new bigint column is added with a hardcoded NOT NULL DEFAULT 0 which allows us to skip a very costly verification step once we are ready to switch it. This is crucial for Primary Key conversions, because setting a column as the PK converts even check constraints to NOT NULL constraints and forces an inline re-verification of the whole table. - It sets up a trigger to keep the two columns in sync.  Note: this helper is intended to be used in a regular (pre-deployment) migration.  This helper is part 1 of a multi-step migration process: 1. initialize_conversion_of_integer_to_bigint to create the new columns and database trigger 2. backfill_conversion_of_integer_to_bigint to copy historic data using background migrations 3. remaining steps TBD, see #288005  table - The name of the database table containing the column columns - The name, or array of names, of the column(s) that we want to convert to bigint. primary_key - The name of the primary key column (most often :id)",
    "label": "",
    "id": "2178"
  },
  {
    "raw_code": "def revert_initialize_conversion_of_integer_to_bigint(table, columns)\n        Gitlab::Database::Migrations::Conversions::BigintConverter\n          .new(self, table, columns)\n          .revert_init\n      end",
    "comment": "Reverts `initialize_conversion_of_integer_to_bigint`  table - The name of the database table containing the columns columns - The name, or array of names, of the column(s) that we're converting to bigint.",
    "label": "",
    "id": "2179"
  },
  {
    "raw_code": "def cleanup_conversion_of_integer_to_bigint(table, columns)\n        Gitlab::Database::Migrations::Conversions::BigintConverter\n          .new(self, table, columns)\n          .cleanup\n      end",
    "comment": "Similar to `revert_initialize_conversion_of_integer_to_bigint`, but `cleanup_conversion_of_integer_to_bigint` updates the yaml file",
    "label": "",
    "id": "2180"
  },
  {
    "raw_code": "def restore_conversion_of_integer_to_bigint(table, columns, primary_key: :id) # rubocop:disable Lint/UnusedMethodArgument -- for backward compatibility, don't remove primary_key\n        Gitlab::Database::Migrations::Conversions::BigintConverter\n          .new(self, table, columns)\n          .restore_cleanup\n      end",
    "comment": "Reverts `cleanup_conversion_of_integer_to_bigint`  table - The name of the database table containing the columns columns - The name, or array of names, of the column(s) that we have converted to bigint. primary_key - The name of the primary key column (most often :id)",
    "label": "",
    "id": "2181"
  },
  {
    "raw_code": "def backfill_conversion_of_integer_to_bigint(\n        table,\n        columns,\n        primary_key: :id,\n        batch_size: 20_000,\n        sub_batch_size: 1000,\n        pause_ms: 100,\n        interval: 2.minutes\n      )\n        Gitlab::Database::Migrations::Conversions::BigintConverter\n          .new(self, table, columns)\n          .backfill(\n            primary_key: primary_key,\n            batch_size: batch_size,\n            sub_batch_size: sub_batch_size,\n            pause_ms: pause_ms,\n            job_interval: interval\n          )\n      end",
    "comment": "Backfills the new columns used in an integer-to-bigint conversion using background migrations.  - This helper should be called from a post-deployment migration. - In order for this helper to work properly,  the new columns must be first initialized with the `initialize_conversion_of_integer_to_bigint` helper. - It tracks the scheduled background jobs through Gitlab::Database::BackgroundMigration::BatchedMigration, which allows a more thorough check that all jobs succeeded in the cleanup migration and is way faster for very large tables.  Note: this helper is intended to be used in a post-deployment migration, to ensure any new code is deployed (including background job changes) before we begin processing the background migration.  This helper is part 2 of a multi-step migration process: 1. initialize_conversion_of_integer_to_bigint to create the new columns and database trigger 2. backfill_conversion_of_integer_to_bigint to copy historic data using background migrations 3. remaining steps TBD, see #288005  table - The name of the database table containing the column columns - The name, or an array of names, of the column(s) we want to convert to bigint. primary_key - The name of the primary key column (most often :id) batch_size - The number of rows to schedule in a single background migration sub_batch_size - The smaller batches that will be used by each scheduled job to update the table. Useful to keep each update at ~100ms while executing more updates per interval (2.minutes) Note that each execution of a sub-batch adds a constant 100ms sleep time in between the updates, which must be taken into account while calculating the batch, sub_batch and interval values. interval - The time interval between every background migration  example: Assume that we have figured out that updating 200 records of the events table takes ~100ms on average. We can set the sub_batch_size to 200, leave the interval to the default and set the batch_size to 50_000 which will require ~50s = (50000 / 200) * (0.1 + 0.1) to complete and leaves breathing space between the scheduled jobs",
    "label": "",
    "id": "2182"
  },
  {
    "raw_code": "def ensure_backfill_conversion_of_integer_to_bigint_is_finished(table, columns, primary_key: :id)\n        Gitlab::Database::Migrations::Conversions::BigintConverter\n          .new(self, table, columns)\n          .ensure_backfill(primary_key: primary_key)\n      end",
    "comment": "Handy helper to ensure data finalization for bigint conversion process",
    "label": "",
    "id": "2183"
  },
  {
    "raw_code": "def revert_backfill_conversion_of_integer_to_bigint(table, columns, primary_key: :id)\n        Gitlab::Database::Migrations::Conversions::BigintConverter\n          .new(self, table, columns)\n          .revert_backfill(primary_key: primary_key)\n      end",
    "comment": "Reverts `backfill_conversion_of_integer_to_bigint`  table - The name of the database table containing the column columns - The name, or an array of names, of the column(s) we want to convert to bigint. primary_key - The name of the primary key column (most often :id)",
    "label": "",
    "id": "2184"
  },
  {
    "raw_code": "def indexes_for(table, column)\n        column = column.to_s\n\n        indexes(table).select { |index| index.columns.include?(column) }\n      end",
    "comment": "Returns an Array containing the indexes for the given column",
    "label": "",
    "id": "2185"
  },
  {
    "raw_code": "def foreign_keys_for(table, column)\n        column = column.to_s\n\n        foreign_keys(table).select { |fk| fk.column == column }\n      end",
    "comment": "Returns an Array containing the foreign keys for the given column.",
    "label": "",
    "id": "2186"
  },
  {
    "raw_code": "def copy_indexes(table, old, new)\n        old = old.to_s\n        new = new.to_s\n\n        indexes_for(table, old).each do |index|\n          new_columns = index.columns.map do |column|\n            column == old ? new : column\n          end",
    "comment": "Copies all indexes for the old column to a new column.  table - The table containing the columns and indexes. old - The old column. new - The new column.",
    "label": "",
    "id": "2187"
  },
  {
    "raw_code": "def copy_foreign_keys(table, old, new)\n        foreign_keys_for(table, old).each do |fk|\n          add_concurrent_foreign_key(fk.from_table,\n            fk.to_table,\n            column: new,\n            on_delete: fk.on_delete)\n        end",
    "comment": "Copies all foreign keys for the old column to the new column.  table - The table containing the columns and indexes. old - The old column. new - The new column.",
    "label": "",
    "id": "2188"
  },
  {
    "raw_code": "def column_for(table, name)\n        name = name.to_s\n\n        column = columns(table).find { |column| column.name == name }\n        raise(missing_schema_object_message(table, \"column\", name)) if column.nil?\n\n        column\n      end",
    "comment": "Returns the column for the given table and column name.",
    "label": "",
    "id": "2189"
  },
  {
    "raw_code": "def replace_sql(column, pattern, replacement)\n        quoted_pattern = Arel::Nodes::Quoted.new(pattern.to_s)\n        quoted_replacement = Arel::Nodes::Quoted.new(replacement.to_s)\n\n        replace = Arel::Nodes::NamedFunction.new(\n          \"regexp_replace\", [column, quoted_pattern, quoted_replacement]\n        )\n\n        Arel::Nodes::SqlLiteral.new(replace.to_sql)\n      end",
    "comment": "This will replace the first occurrence of a string in a column with the replacement using `regexp_replace`",
    "label": "",
    "id": "2190"
  },
  {
    "raw_code": "def index_exists_by_name?(table, index)\n        # We can't fall back to the normal `index_exists?` method because that\n        # does not find indexes without passing a column name.\n        if indexes(table).map(&:name).include?(index.to_s)\n          true\n        else\n          postgres_exists_by_name?(table, index)\n        end",
    "comment": "Fetches indexes on a column by name for postgres.  This will include indexes using an expression on the column, for example: `CREATE INDEX CONCURRENTLY index_name ON table (LOWER(column));`  We can remove this when upgrading to Rails 5 with an updated `index_exists?`: - https://github.com/rails/rails/commit/edc2b7718725016e988089b5fb6d6fb9d6e16882  Or this can be removed when we no longer support postgres < 9.5, so we can use `CREATE INDEX IF NOT EXISTS`.",
    "label": "",
    "id": "2191"
  },
  {
    "raw_code": "def backfill_iids(table)\n        sql = <<-END\n          UPDATE #{table}\n          SET iid = #{table}_with_calculated_iid.iid_num\n          FROM (\n            SELECT id, ROW_NUMBER() OVER (PARTITION BY project_id ORDER BY id ASC) AS iid_num FROM #{table}\n          ) AS #{table}_with_calculated_iid\n          WHERE #{table}.id = #{table}_with_calculated_iid.id\n        END\n\n        execute(sql)\n      end",
    "comment": "Note this should only be used with very small tables",
    "label": "",
    "id": "2192"
  },
  {
    "raw_code": "def remove_column_default(table_name, column_name)\n        column = connection.columns(table_name).find { |col| col.name == column_name.to_s }\n\n        if column.default || column.default_function\n          change_column_default(table_name, column_name, to: nil)\n        end",
    "comment": "While it is safe to call `change_column_default` on a column without default it would still require access exclusive lock on the table and for tables with high autovacuum(wraparound prevention) it will fail if their executions overlap. ",
    "label": "",
    "id": "2193"
  },
  {
    "raw_code": "def mapping_has_required_columns?(mapping)\n        %i[from_type to_type].map do |required_key|\n          mapping.has_key?(required_key)\n        end.all?\n      end",
    "comment": "mappings => {} where keys are column names and values are hashes with the following keys: from_type - from which type we're migrating to_type - to which type we're migrating default_value - custom default value, if not provided will be taken from neutral_values_for_type",
    "label": "",
    "id": "2194"
  },
  {
    "raw_code": "def convert_array_to_hash(subject)\n        result = {}\n\n        subject&.each do |item|\n          if item.is_a?(Hash)\n            item.each do |key, value|\n              result[key.to_sym] = { specific_tables: value[:specific_tables].to_set }\n            end",
    "comment": "Convert from: - schema_a - schema_b: specific_tables: - table_b_of_schema_b - table_c_of_schema_b  To: { :schema_a => nil, :schema_b => { specific_tables : ['table_b_of_schema_b', 'table_c_of_schema_b'] } } ",
    "label": "",
    "id": "2195"
  },
  {
    "raw_code": "def add_table_specific_allows(type, schema_allows)\n        result = schema_allows\n        all_table_allows(type).each do |schema_from, tables|\n          # Preserve the meaning of `nil` as defined in convert_array_to_hash as a nil value means that we allow all\n          # tables\n          next if result.key?(schema_from) && result[schema_from].nil?\n\n          # Now we add the table to the specific_tables list because this table specifies it is allowed in this schema\n          result[schema_from] ||= { specific_tables: Set.new }\n          result[schema_from][:specific_tables] += tables\n        end",
    "comment": "This method loops over all the `db/docs` files for every table and injects any allow_cross_joins/allow_cross_transactions/allow_cross_foreign_keys into the specific_tables lists for the current schema.",
    "label": "",
    "id": "2196"
  },
  {
    "raw_code": "def all_table_allows(type)\n        @all_table_allows ||= {}\n        @all_table_allows[type] ||= begin\n          result = {}\n          ::Gitlab::Database::Dictionary.entries.each do |entry|\n            allowed_schemas = entry.allow_cross_to_schemas(type)\n            allowed_schemas.each do |schema|\n              # In the context of this GitlabSchemaInfo we only need the tables that have allowed this schema\n              next unless schema == name\n\n              result[entry.gitlab_schema.to_sym] ||= []\n              result[entry.gitlab_schema.to_sym] << entry.key_name\n            end",
    "comment": "For the given type we iterate over all db/docs files build a Hash like:  { gitlab_main_org: ['table_a', 'table_b'] }  This specifies that in the `gitlab_main_org` schema the 'table_a` and `table_b` tables are allowing cross queries with the current schema",
    "label": "",
    "id": "2197"
  },
  {
    "raw_code": "def self.automatic_reindexing(maximum_records: DEFAULT_INDEXES_PER_INVOCATION)\n        # Cleanup leftover temporary indexes from previous, possibly aborted runs (if any)\n        cleanup_leftovers!\n\n        # Consume from the explicit reindexing queue first\n        done_counter = perform_from_queue(maximum_records: maximum_records)\n\n        return if done_counter >= maximum_records\n\n        # Execute reindexing based on bloat heuristic\n        perform_with_heuristic(maximum_records: maximum_records - done_counter)\n      end",
    "comment": "Performs automatic reindexing for a limited number of indexes per call 1. Consume from the explicit reindexing queue 2. Apply bloat heuristic to find most bloated indexes and reindex those",
    "label": "",
    "id": "2198"
  },
  {
    "raw_code": "def self.perform_with_heuristic(candidate_indexes = Gitlab::Database::PostgresIndex.reindexing_support, maximum_records: DEFAULT_INDEXES_PER_INVOCATION)\n        IndexSelection.new(candidate_indexes).take(maximum_records).each do |index|\n          Coordinator.new(index).perform\n        end",
    "comment": "Reindex based on bloat heuristic for a limited number of indexes per call  We use a bloat heuristic to estimate the index bloat and pick the most bloated indexes for reindexing.",
    "label": "",
    "id": "2199"
  },
  {
    "raw_code": "def self.perform_from_queue(maximum_records: DEFAULT_INDEXES_PER_INVOCATION)\n        QueuedAction.in_queue_order.limit(maximum_records).each do |queued_entry|\n          Coordinator.new(queued_entry.index).perform\n\n          queued_entry.done!\n        rescue StandardError => e\n          queued_entry.failed!\n\n          Gitlab::AppLogger.error(\"Failed to perform reindexing action on queued entry #{queued_entry}: #{e}\")\n        end.size\n      end",
    "comment": "Reindex indexes that have been explicitly enqueued (for a limited number of indexes per call)",
    "label": "",
    "id": "2200"
  },
  {
    "raw_code": "def self.build_expression(search:, rules:)\n        return EXPRESSION_ON_INVALID_INPUT if search.blank? || rules.empty?\n\n        quoted_search = ApplicationRecord.connection.quote(search.to_s)\n\n        first_expression, *expressions = rules.map do |rule|\n          rule_to_arel(quoted_search, rule)\n        end",
    "comment": "This method returns an Arel expression that can be used in an ActiveRecord query to order the resultset by similarity.  Note: Calculating similarity score for large volume of records is inefficient. use SimilarityScore only for smaller resultset which is already filtered by other conditions (< 10_000 records).  ==== Parameters * +search+ - [String] the user provided search string * +rules+ - [{ column: COLUMN, multiplier: 1 }, { column: COLUMN_2, multiplier: 0.5 }] rules for the scoring. * +column+ - Arel column expression, example: Project.arel_table[\"name\"] * +multiplier+ - Integer or Float to increase or decrease the score (optional, defaults to 1)  ==== Use case  We'd like to search for projects by path, name and description. We want to rank higher the path and name matches, since it's more likely that the user was remembering the path or the name of the project.  Rules: [ { column: Project.arel_table['path'], multiplier: 1 }, { column: Project.arel_table['name'], multiplier: 1 }, { column: Project.arel_table['description'], multiplier: 0.5 } ]  ==== Examples  Similarity calculation based on one column:  Gitlab::Database::SimilarityScore.build_expession(search: 'my input', rules: [{ column: Project.arel_table['name'] }])  Similarity calculation based on two column, where the second column has lower priority:  Gitlab::Database::SimilarityScore.build_expession(search: 'my input', rules: [ { column: Project.arel_table['name'], multiplier: 1 }, { column: Project.arel_table['description'], multiplier: 0.5 } ])  Integration with an ActiveRecord query:  table = Project.arel_table  order_expression = Gitlab::Database::SimilarityScore.build_expession(search: 'input', rules: [ { column: table['name'], multiplier: 1 }, { column: table['description'], multiplier: 0.5 } ])  Project.where(\"name LIKE ?\", '%' + 'input' + '%').order(order_expression.desc)  The expression can be also used in SELECT:  results = Project.select(order_expression.as('similarity')).where(\"name LIKE ?\", '%' + 'input' + '%').order(similarity: :desc) puts results.map(&:similarity) ",
    "label": "",
    "id": "2201"
  },
  {
    "raw_code": "def self.rule_to_arel(search, rule)\n        Arel::Nodes::Grouping.new(\n          Arel::Nodes::Multiplication.new(\n            similarity_function_call(search, column_expression(rule)),\n            multiplier_expression(rule)\n          )\n        )\n      end",
    "comment": "(SIMILARITY(COALESCE(column, ''), 'search_string') * CAST(multiplier AS numeric))",
    "label": "",
    "id": "2202"
  },
  {
    "raw_code": "def self.column_expression(rule)\n        Arel::Nodes::NamedFunction.new('COALESCE', [rule.fetch(:column), EMPTY_STRING])\n      end",
    "comment": "COALESCE(column, '')",
    "label": "",
    "id": "2203"
  },
  {
    "raw_code": "def self.similarity_function_call(search, column)\n        Arel::Nodes::NamedFunction.new(SIMILARITY_FUNCTION_CALL_WITH_ANNOTATION, [column, Arel.sql(search)])\n      end",
    "comment": "SIMILARITY(COALESCE(column, ''), 'search_string')",
    "label": "",
    "id": "2204"
  },
  {
    "raw_code": "def self.multiplier_expression(rule)\n        quoted_multiplier = ApplicationRecord.connection.quote(rule.fetch(:multiplier, DEFAULT_MULTIPLIER).to_s)\n\n        Arel::Nodes::NamedFunction.new('CAST', [Arel.sql(quoted_multiplier).as('numeric')])\n      end",
    "comment": "CAST(multiplier AS numeric)",
    "label": "",
    "id": "2205"
  },
  {
    "raw_code": "def self.with_read_consistency(&block)\n        ::Gitlab::Database::LoadBalancing::SessionMap\n          .with_sessions(Gitlab::Database::LoadBalancing.base_models)\n          .use_primary(&block)\n      end",
    "comment": " Within the block, disable the database load balancing for calls that require read consistency after recent writes. ",
    "label": "",
    "id": "2206"
  },
  {
    "raw_code": "def hook!\n        @subscriber = ActiveSupport::Notifications.subscribe('sql.active_record') do |event|\n          # In some cases analyzer code might trigger another SQL call\n          # to avoid stack too deep this detects recursive call of subscriber\n          with_ignored_recursive_calls do\n            process_sql(event.payload[:sql], event.payload[:connection], event.payload[:name].to_s)\n          end",
    "comment": "@info most common event names are: Model Load, Model Create, Model Update, Model Pluck, Model Destroy, Model Insert, Model Delete All Model Exists?, nil, TRANSACTION, SCHEMA",
    "label": "",
    "id": "2207"
  },
  {
    "raw_code": "def begin!(analyzers)\n        analyzers.select do |analyzer|\n          next if enabled_analyzers.include?(analyzer)\n\n          if analyzer.enabled?\n            analyzer.begin!\n            enabled_analyzers.append(analyzer)\n\n            true\n          end",
    "comment": "Enable query analyzers (only the ones that were not yet enabled) Returns a list of newly enabled analyzers",
    "label": "",
    "id": "2208"
  },
  {
    "raw_code": "def end!(analyzers)\n        analyzers.each do |analyzer|\n          next unless enabled_analyzers.delete(analyzer)\n\n          analyzer.end!\n        rescue StandardError, ::Gitlab::Database::QueryAnalyzers::Base::QueryAnalyzerError => e\n          Gitlab::ErrorTracking.track_and_raise_for_dev_exception(e)\n        end",
    "comment": "Disable enabled query analyzers (only the ones that were enabled previously)",
    "label": "",
    "id": "2209"
  },
  {
    "raw_code": "def initialize(\n        table_name:, connection:, database_name:,\n        with_retries: true, logger: nil, dry_run: false, force: true\n      )\n        @table_name = table_name.to_s\n        @connection = connection\n        @database_name = database_name\n        @logger = logger\n        @dry_run = dry_run\n        @force = force\n        @with_retries = with_retries\n\n        @table_name_without_schema = ActiveRecord::ConnectionAdapters::PostgreSQL::Utils\n          .extract_schema_qualified_name(table_name.to_s)\n          .identifier\n      end",
    "comment": "table_name can include schema name as a prefix. For example: 'gitlab_partitions_static.events_03', otherwise, it will default to current used schema, for example 'public'.",
    "label": "",
    "id": "2210"
  },
  {
    "raw_code": "def subtract_datetimes(query_so_far, start_time_attrs, end_time_attrs, as)\n        diff_fn = subtract_datetimes_diff(query_so_far, start_time_attrs, end_time_attrs)\n\n        query_so_far.project(diff_fn.as(as))\n      end",
    "comment": "Find the first of the `end_time_attrs` that isn't `NULL`. Subtract from it the first of the `start_time_attrs` that isn't NULL. `SELECT` the resulting interval along with an alias specified by the `as` parameter.  Note: the interval is returned as an INTERVAL type.",
    "label": "",
    "id": "2211"
  },
  {
    "raw_code": "def clear!\n        super\n\n        clear_renamed_tables_cache!\n      end",
    "comment": "Override methods in ActiveRecord::ConnectionAdapters::SchemaCache https://github.com/rails/rails/blob/v7.2.2.2/activerecord/lib/active_record/connection_adapters/schema_cache.rb",
    "label": "",
    "id": "2212"
  },
  {
    "raw_code": "def self.create_and_execute_trigger?(table)\n        # We _must not_ use quote_table_name as this will produce double\n        # quotes on PostgreSQL and for \"has_table_privilege\" we need single\n        # quotes.\n        connection = ApplicationRecord.connection\n        quoted_table = connection.quote(table)\n\n        begin\n          connection.select_one(\"SELECT has_table_privilege(#{quoted_table}, 'TRIGGER')\").present?\n        rescue ActiveRecord::StatementInvalid\n          # This error is raised when using a non-existing table name. In this\n          # case we just want to return false as a user technically can't\n          # create triggers for such a table.\n          false\n        end",
    "comment": "Returns true if the current user can create and execute triggers on the given table.",
    "label": "",
    "id": "2213"
  },
  {
    "raw_code": "def recovery?\n        recovery = connection.select_value('SELECT pg_is_in_recovery()')\n\n        Gitlab::Utils.to_boolean(recovery)\n      end",
    "comment": "Check whether the server is in recovery mode (recovering from a backup or archive)",
    "label": "",
    "id": "2214"
  },
  {
    "raw_code": "def self.current_version\n        2.3\n      end",
    "comment": "The current version to be used in new migrations",
    "label": "",
    "id": "2215"
  },
  {
    "raw_code": "def append_mismatches_details(source_data, target_data)\n        # Mapping difference the sort key to the item values\n        # source - target\n        source_diff_hash = (source_data - target_data).index_by { |item| item.shift }\n        # target - source\n        target_diff_hash = (target_data - source_data).index_by { |item| item.shift }\n\n        matches = source_data.length - source_diff_hash.length\n\n        # Items that exist in the first table + Different items\n        source_diff_hash.each do |id, values|\n          result[:mismatches_details] << {\n            id: id,\n            source_table: values,\n            target_table: target_diff_hash[id]\n          }\n        end",
    "comment": "This where comparing the items happen, and building the diff log It returns the number of matching elements",
    "label": "",
    "id": "2216"
  },
  {
    "raw_code": "def self.db_role_for_connection(connection)\n        return ROLE_UNKNOWN if connection.is_a?(::Gitlab::Database::LoadBalancing::ConnectionProxy)\n\n        db_config = Database.db_config_for_connection(connection)\n        return ROLE_UNKNOWN unless db_config\n\n        if db_config.name.ends_with?(LoadBalancer::REPLICA_SUFFIX)\n          ROLE_REPLICA\n        else\n          ROLE_PRIMARY\n        end",
    "comment": "Returns the role (primary/replica) of the database the connection is connecting to.",
    "label": "",
    "id": "2217"
  },
  {
    "raw_code": "def clear!\n        super\n\n        clear_renamed_tables_cache!\n      end",
    "comment": "Override methods in ActiveRecord::ConnectionAdapters::SchemaCache for Rails 7.1 https://github.com/rails/rails/blob/v7.1.5.2/activerecord/lib/active_record/connection_adapters/schema_cache.rb",
    "label": "",
    "id": "2218"
  },
  {
    "raw_code": "def connection_pool\n          connection.pool\n        end",
    "comment": "in case the connection has been switched with using_connection",
    "label": "",
    "id": "2219"
  },
  {
    "raw_code": "def add_concurrent_partitioned_index(table_name, column_names, options = {})\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          raise ArgumentError, 'A name is required for indexes added to partitioned tables' unless options[:name]\n\n          partitioned_table = find_partitioned_table(table_name)\n\n          if index_name_exists?(table_name, options[:name])\n            Gitlab::AppLogger.warn \"Index not created because it already exists (this may be due to an aborted \" \\\n              \"migration or similar): table_name: #{table_name}, index_name: #{options[:name]}\"\n\n            return\n          end",
    "comment": "Concurrently creates a new index on a partitioned table. In concept this works similarly to `add_concurrent_index`, and won't block reads or writes on the table while the index is being built.  A special helper is required for partitioning because Postgres does not support concurrently building indexes on partitioned tables. This helper concurrently adds the same index to each partition, and creates the final index on the parent table once all of the partitions are indexed. This is the recommended safe way to add indexes to partitioned tables.  Example:  add_concurrent_partitioned_index :users, :some_column  See Rails' `add_index` for more info on the available arguments.",
    "label": "",
    "id": "2220"
  },
  {
    "raw_code": "def remove_concurrent_partitioned_index_by_name(table_name, index_name)\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          find_partitioned_table(table_name)\n\n          unless index_name_exists?(table_name, index_name)\n            Gitlab::AppLogger.warn \"Index not removed because it does not exist (this may be due to an aborted \" \\\n              \"migration or similar): table_name: #{table_name}, index_name: #{index_name}\"\n\n            return\n          end",
    "comment": "Safely removes an existing index from a partitioned table. The method name is a bit inaccurate as it does not drop the index concurrently, but it's named as such to maintain consistency with other similar helpers, and indicate that this should be safe to use in a production environment.  In current versions of Postgres it's impossible to drop an index concurrently, or drop an index from an individual partition that exists across the entire partitioned table. As a result this helper drops the index from the parent table, which automatically cascades to all partitions. While this does require an exclusive lock, dropping an index is a fast operation that won't block the table for a significant period of time.  Example:  remove_concurrent_partitioned_index_by_name :users, 'index_name_goes_here'",
    "label": "",
    "id": "2221"
  },
  {
    "raw_code": "def rename_partitioned_index(table_name, old_index_name, new_index_name)\n          partitioned_table = find_partitioned_table(table_name)\n\n          old_index_json = index_json(find_index(partitioned_table.name) { |i| i.name == old_index_name.to_s })\n\n          partitioned_table.postgres_partitions.order(:name).each do |partition|\n            old_partition_index_name = find_index(partition.identifier) { |i| index_json(i) == old_index_json }.name\n            new_partition_index_name = generated_index_name(partition.identifier, new_index_name)\n            rename_index_with_schema(\n              partition.identifier, old_partition_index_name, new_partition_index_name, schema: partition.schema\n            )\n          end",
    "comment": "Rename the index for partitioned table and its partitions. The `new_index_name` will be the new name of the partitioned index. The new name of the partition indexes will be generated by the partition table name and `new_index_name` and look like e.g. `index_000925dbd7`.  Example:  rename_partitioned_index :users, 'existing_partitioned_index_name', 'new_index_name'",
    "label": "",
    "id": "2222"
  },
  {
    "raw_code": "def swap_partitioned_indexes(table_name, old_index_name, new_index_name)\n          partitioned_table = find_partitioned_table(table_name)\n\n          old_index_json = index_json(find_index(partitioned_table.name) { |i| i.name == old_index_name.to_s })\n          new_index_json = index_json(find_index(partitioned_table.name) { |i| i.name == new_index_name.to_s })\n\n          partitioned_table.postgres_partitions.order(:name).each do |partition|\n            old_partition_index_name = find_index(partition.identifier) { |i| index_json(i) == old_index_json }.name\n            new_partition_index_name = find_index(partition.identifier) { |i| index_json(i) == new_index_json }.name\n            swap_indexes(\n              partition.identifier, old_partition_index_name, new_partition_index_name, schema: partition.schema\n            )\n          end",
    "comment": "Swap the index names for partitioned table and its partitions.  Example:  swap_partitioned_indexes :users, 'existing_partitioned_index_name_1', 'existing_partitioned_index_name_2'",
    "label": "",
    "id": "2223"
  },
  {
    "raw_code": "def find_duplicate_indexes(table_name, schema_name: connection.current_schema)\n          find_indexes(table_name, schema_name: schema_name)\n            .group_by { |r| r['index_id'] }\n            .select { |_, v| v.size > 1 }\n            .map { |_, indexes| indexes.map { |index| index['index_name'] } }\n        end",
    "comment": "Finds duplicate indexes for a given schema and table. This finds indexes where the index definition is identical but the names are different. Returns an array of arrays containing duplicate index name pairs.  Example:  find_duplicate_indexes('table_name_goes_here')",
    "label": "",
    "id": "2224"
  },
  {
    "raw_code": "def indexes_by_definition_for_table(table_name, schema_name: connection.current_schema)\n          duplicate_indexes = find_duplicate_indexes(table_name, schema_name: schema_name)\n\n          unless duplicate_indexes.empty?\n            raise DuplicatedIndexesError, \"#{table_name} has duplicate indexes: #{duplicate_indexes}\"\n          end",
    "comment": "Retrieves a hash of index names for a given table and schema, by index definition.  Example:  indexes_by_definition_for_table('table_name_goes_here')  Returns:  { \"CREATE _ btree (created_at)\" => \"index_on_created_at\" }",
    "label": "",
    "id": "2225"
  },
  {
    "raw_code": "def rename_indexes_for_table(table_name, new_index_names, schema_name: connection.current_schema)\n          current_index_names = indexes_by_definition_for_table(table_name, schema_name: schema_name)\n          rename_indexes(current_index_names, new_index_names, schema_name: schema_name)\n        end",
    "comment": "Renames indexes for a given table and schema, mapping by index definition, to a hash of new index names.  Example:  index_names = indexes_by_definition_for_table('source_table_name_goes_here') drop_table('source_table_name_goes_here') rename_indexes_for_table('destination_table_name_goes_here', index_names)",
    "label": "",
    "id": "2226"
  },
  {
    "raw_code": "def prepare_partitioned_async_index(table_name, column_name, **options)\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_ddl_mode!\n          return unless async_index_creation_available?\n\n          partitioned_table = find_partitioned_table(table_name)\n\n          if index_exists?(table_name, column_name, **options)\n            return Gitlab::AppLogger.warn(\n              message: 'Partitioned index not prepared because it already exists',\n              table_name: table_name,\n              column_name: column_name,\n              index_name: options[:name]\n            )\n          end",
    "comment": "Prepare async index creation for partitions Once the partition indexes are created, we can then use `#add_concurrent_partitioned_index` below to synchronously create the partitioned index",
    "label": "",
    "id": "2227"
  },
  {
    "raw_code": "def add_concurrent_partitioned_foreign_key(source, target, column:, **options)\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          options.reverse_merge!({\n            target_column: :id,\n            on_delete: :cascade,\n            on_update: nil,\n            name: nil,\n            validate: true,\n            reverse_lock_order: false,\n            column: column\n          })\n\n          # We'll use the same FK name for all partitions and match it to\n          # the name used for the partitioned table to follow the convention\n          # used by PostgreSQL when adding FKs to new partitions\n          options[:name] ||= concurrent_partitioned_foreign_key_name(source, column)\n          check_options = options.slice(:column, :on_delete, :on_update, :name)\n          check_options[:primary_key] = options[:target_column]\n\n          if foreign_key_exists?(source, target, **check_options)\n            warning_message = \"Foreign key not created because it exists already \" \\\n              \"(this may be due to an aborted migration or similar): \" \\\n              \"source: #{source}, target: #{target}, column: #{options[:column]}, \"\\\n              \"name: #{options[:name]}, on_delete: #{options[:on_delete]}, \"\\\n              \"on_update: #{options[:on_update]}\"\n\n            Gitlab::AppLogger.warn warning_message\n\n            return\n          end",
    "comment": "Adds a foreign key with only minimal locking on the tables involved.  In concept it works similarly to add_concurrent_foreign_key, but we have to add a special helper for partitioned tables for the following reasons: - add_concurrent_foreign_key sets the constraint to `NOT VALID` before validating it - Setting an FK to NOT VALID is not supported currently in Postgres (up to PG13) - Also, PostgreSQL will currently ignore NOT VALID constraints on partitions when adding a valid FK to the partitioned table, so they have to also be validated before we can add the final FK. Solution: - Add the foreign key first to each partition by using add_concurrent_foreign_key and validating it - Once all partitions have a foreign key, add it also to the partitioned table (there will be no need for a validation at that level) For those reasons, this method does not include an option to delay the validation, we have to force validate: true.  source - The source (partitioned) table containing the foreign key. target - The target table the key points to. column - The name of the column to create the foreign key on. on_delete - The action to perform when associated data is removed, defaults to \"CASCADE\". on_update - The action to perform when associated data is updated, no default value is set. name - The name of the foreign key. validate - Flag that controls whether the new foreign key will be validated after creation and if it will be added on the parent table. If the flag is not set, the constraint will only be enforced for new data in the existing partitions. The helper will need to be called again with the flag set to `true` to add the foreign key on the parent table after validating it on all partitions. `validate: false` should be paired with `prepare_partitioned_async_foreign_key_validation` reverse_lock_order - Flag that controls whether we should attempt to acquire locks in the reverse order of the ALTER TABLE. This can be useful in situations where the foreign key creation could deadlock with another process. ",
    "label": "",
    "id": "2228"
  },
  {
    "raw_code": "def rename_partitioned_foreign_key(table_name, old_foreign_key, new_foreign_key)\n          partitioned_table = find_partitioned_table(table_name)\n          partitioned_table.postgres_partitions.order(:name).each do |partition|\n            rename_constraint(partition.identifier, old_foreign_key, new_foreign_key)\n          end",
    "comment": "Rename the foreign key for partitioned table and its partitions.  Example:  rename_partitioned_foreign_key :users, 'existing_partitioned_fk_name', 'new_fk_name'",
    "label": "",
    "id": "2229"
  },
  {
    "raw_code": "def swap_partitioned_foreign_keys(table_name, old_foreign_key, new_foreign_key)\n          partitioned_table = find_partitioned_table(table_name)\n          partitioned_table.postgres_partitions.order(:name).each do |partition|\n            swap_foreign_keys(partition.identifier, old_foreign_key, new_foreign_key)\n          end",
    "comment": "Swap the foreign key names for partitioned table and its partitions.  Example:  swap_partitioned_foreign_keys :users, 'existing_partitioned_fk_name_1', 'existing_partitioned_fk_name_2'",
    "label": "",
    "id": "2230"
  },
  {
    "raw_code": "def remove_partitioned_foreign_key(source, target = nil, column: nil, name: nil, reverse_lock_order: false)\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          # Determine the foreign key name if not provided\n          name = concurrent_partitioned_foreign_key_name(source, column) if name.nil? && column.present?\n\n          raise ArgumentError, 'Either name or column must be specified' if name.nil?\n\n          # Remove foreign key from the partitioned table first\n          # This is important because PostgreSQL doesn't allow dropping inherited constraints\n          # from partitions while they still exist on the parent table\n          with_lock_retries do\n            remove_foreign_key_if_exists(source, target, name: name, reverse_lock_order: reverse_lock_order)\n          end",
    "comment": "Removes a foreign key from a partitioned table and all its partitions.  This method handles the removal of foreign keys from partitioned tables properly. Unlike the regular remove_foreign_key method, this method removes the foreign key from the partitioned table first, then from each partition individually. This is necessary because PostgreSQL doesn't allow dropping inherited constraints from partitions while they still exist on the parent table.  source - The source (partitioned) table containing the foreign key. target - The target table the key points to (optional if using name parameter). column - The name of the column with the foreign key (optional if using name parameter). name - The name of the foreign key constraint (optional if using column parameter). reverse_lock_order - Flag that controls whether we should attempt to acquire locks in the reverse order of the ALTER TABLE. This can be useful in situations where the foreign key removal could deadlock with another process.  Example:  remove_partitioned_foreign_key :users, :projects, column: :project_id remove_partitioned_foreign_key :users, name: 'fk_rails_123456' ",
    "label": "",
    "id": "2231"
  },
  {
    "raw_code": "def concurrent_partitioned_foreign_key_name(table, column, prefix: 'fk_rails_')\n          identifier = \"#{table}_#{column}_fk\"\n          hashed_identifier = Digest::SHA256.hexdigest(identifier).first(10)\n\n          \"#{prefix}#{hashed_identifier}\"\n        end",
    "comment": "Returns the name for a concurrent partitioned foreign key.  Similar to concurrent_foreign_key_name (Gitlab::Database::MigrationHelpers) we just keep a separate method in case we want a different behavior for partitioned tables ",
    "label": "",
    "id": "2232"
  },
  {
    "raw_code": "def partition_table_by_int_range(table_name, column_name, partition_size:, primary_key:)\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_ddl_mode!\n\n          assert_table_is_allowed(table_name)\n\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          current_primary_key = Array.wrap(connection.primary_key(table_name))\n          raise \"primary key not defined for #{table_name}\" if current_primary_key.blank?\n\n          partition_column = find_column_definition(table_name, column_name)\n          raise \"partition column #{column_name} does not exist on #{table_name}\" if partition_column.nil?\n\n          primary_key = Array.wrap(primary_key).map(&:to_s)\n          raise \"the partition column must be part of the primary key\" unless primary_key.include?(column_name.to_s)\n\n          primary_key_objects = connection.columns(table_name).select { |column| primary_key.include?(column.name) }\n\n          raise 'partition_size must be greater than 1' unless partition_size > 1\n\n          max_id = Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.with_suppressed do\n            Gitlab::Database::QueryAnalyzers::GitlabSchemasValidateConnection.with_suppressed do\n              define_batchable_model(table_name, connection: connection).maximum(column_name) || (partition_size * PARTITION_BUFFER)\n            end",
    "comment": "Creates a partitioned copy of an existing table, using a RANGE partitioning strategy on a int/bigint column. One partition is created per partition_size between 1 and MAX(column_name). Also installs a trigger on the original table to copy writes into the partitioned table. To copy over historic data from before creation of the partitioned table, use the `enqueue_partitioning_data_migration` helper in a post-deploy migration. Note: If the original table is empty the system creates 6 partitions in the new table.  A copy of the original table is required as PG currently does not support partitioning existing tables.  Example:  partition_table_by_int_range :merge_request_diff_commits, :merge_request_diff_id, partition_size: 500, primary_key: ['merge_request_diff_id', 'relative_order']  Options are: :partition_size - a int specifying the partition size :primary_key - a array specifying the primary query of the new table  Note: The system always adds a buffer of 6 partitions.",
    "label": "",
    "id": "2233"
  },
  {
    "raw_code": "def partition_table_by_date(table_name, column_name, min_date: nil, max_date: nil)\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_ddl_mode!\n\n          assert_table_is_allowed(table_name)\n\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          max_date ||= Date.today + 1.month\n\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.with_suppressed do\n            min_date ||= connection.select_one(<<~SQL)['minimum'] || (max_date - 1.month)\n              SELECT date_trunc('MONTH', MIN(#{column_name})) AS minimum\n              FROM #{table_name}\n            SQL\n          end",
    "comment": "Creates a partitioned copy of an existing table, using a RANGE partitioning strategy on a timestamp column. One partition is created per month between the given `min_date` and `max_date`. Also installs a trigger on the original table to copy writes into the partitioned table. To copy over historic data from before creation of the partitioned table, use the `enqueue_partitioning_data_migration` helper in a post-deploy migration.  A copy of the original table is required as PG currently does not support partitioning existing tables.  Example:  partition_table_by_date :audit_events, :created_at, min_date: Date.new(2020, 1), max_date: Date.new(2020, 6)  Options are: :min_date - a date specifying the lower bounds of the partition range :max_date - a date specifying the upper bounds of the partitioning range, defaults to today + 1 month  Unless min_date is specified explicitly, we default to 1. The minimum value for the partitioning column in the table 2. If no data is present yet, the current month",
    "label": "",
    "id": "2234"
  },
  {
    "raw_code": "def partition_table_by_list(\n          table_name, column_name,\n          primary_key:, partition_mappings: nil, partition_name_format: nil,\n          create_partitioned_table_fn: nil, sync_trigger: true\n        )\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_ddl_mode!\n\n          assert_table_is_allowed(table_name)\n\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          current_primary_key = Array.wrap(connection.primary_key(table_name))\n          raise \"primary key not defined for #{table_name}\" if current_primary_key.blank?\n\n          partition_column = find_column_definition(table_name, column_name)\n          raise \"partition column #{column_name} does not exist on #{table_name}\" if partition_column.nil?\n\n          primary_key = Array.wrap(primary_key).map(&:to_s)\n          raise \"the partition column must be part of the primary key\" unless primary_key.include?(column_name.to_s)\n\n          primary_key_objects = connection.columns(table_name).select { |column| primary_key.include?(column.name) }\n\n          if partition_mappings.nil?\n            distinct_partitions = Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.with_suppressed do\n              Gitlab::Database::QueryAnalyzers::GitlabSchemasValidateConnection.with_suppressed do\n                define_batchable_model(table_name, connection: connection).distinct(column_name).pluck(column_name)\n              end",
    "comment": "Creates a partitioned copy of an existing table, using a LIST partitioning strategy on a int/bigint column. One partition is created per column_name value. Also installs a trigger on the original table to copy writes into the partitioned table. To copy over historic data from before creation of the partitioned table, use the `enqueue_partitioning_data_migration` helper in a post-deploy migration.  A copy of the original table is required as PG currently does not support partitioning existing tables.  Example:  partition_table_by_list :ci_runners, :runner_type, primary_key: ['id', 'runner_type'], partition_mappings: { instance_type: 1, group_type: 2, project_type: 3 }, partition_name_format: \"%{partition_name}_%{table_name}\", create_partitioned_table_fn: ->(name) { create_custom_partitioned_table(name) }  Options are: :primary_key - a array specifying the primary query of the new table :partition_name_format - the format to be used when naming partitions. The %{table_name} and %{partition_name} variables are made available. If not specified, a default is generated :partition_mappings - a hash specifying the mappings between partition name and respective column value(s) :create_partitioned_table_fn - a lambda allowing a custom function to create the partitioned table If not specified, the partitioned table will be created with the same schema as the non-partitioned table ",
    "label": "",
    "id": "2235"
  },
  {
    "raw_code": "def drop_partitioned_table_for(table_name)\n          assert_table_is_allowed(table_name)\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          with_lock_retries do\n            drop_sync_trigger(table_name)\n          end",
    "comment": "Clean up a partitioned copy of an existing table. First, deletes the database function and trigger that were used to copy writes to the partitioned table, then removes the partitioned table (also removing partitions).  Example:  drop_partitioned_table_for :audit_events ",
    "label": "",
    "id": "2236"
  },
  {
    "raw_code": "def enqueue_partitioning_data_migration(table_name, migration = MIGRATION)\n          assert_table_is_allowed(table_name)\n\n          assert_not_in_transaction_block(scope: ERROR_SCOPE)\n\n          partitioned_table_name = make_partitioned_table_name(table_name)\n          primary_key = connection.primary_key(table_name)\n\n          queue_batched_background_migration(\n            migration,\n            table_name,\n            primary_key,\n            partitioned_table_name,\n            batch_size: BATCH_SIZE,\n            sub_batch_size: SUB_BATCH_SIZE,\n            job_interval: BATCH_INTERVAL\n          )\n        end",
    "comment": "Enqueue the background jobs that will backfill data in the partitioned table, by batch-copying records from original table. This helper should be called from a post-deploy migration.  Example:  enqueue_partitioning_data_migration :audit_events ",
    "label": "",
    "id": "2237"
  },
  {
    "raw_code": "def cleanup_partitioning_data_migration(table_name, migration = MIGRATION)\n          assert_table_is_allowed(table_name)\n\n          partitioned_table_name = make_partitioned_table_name(table_name)\n          primary_key = connection.primary_key(table_name)\n\n          delete_batched_background_migration(migration, table_name, primary_key, [partitioned_table_name])\n        end",
    "comment": "Cleanup a previously enqueued background migration to copy data into a partitioned table. This will not prevent the enqueued jobs from executing, but instead cleans up information in the database used to track the state of the batched background migration. It should be safe to also remove the partitioned table even if the background jobs are still in-progress, as the absence of the table will cause them to safely exit.  Example:  cleanup_partitioning_data_migration :audit_events ",
    "label": "",
    "id": "2238"
  },
  {
    "raw_code": "def finalize_backfilling_partitioned_table(table_name)\n          assert_table_is_allowed(table_name)\n\n          partitioned_table_name = make_partitioned_table_name(table_name)\n\n          unless table_exists?(partitioned_table_name)\n            raise \"could not find partitioned table for #{table_name}, \" \\\n              \"this could indicate the previous partitioning migration has been rolled back.\"\n          end",
    "comment": "Executes jobs from previous BatchedBackgroundMigration to backfill the partitioned table by finishing pending jobs.  **NOTE** Migrations using this method cannot be scheduled in the same release as the migration that schedules the background migration using the `enqueue_partitioning_data_migration` helper, or else the background migration jobs will be force-executed.  Example:  finalize_backfilling_partitioned_table :audit_events ",
    "label": "",
    "id": "2239"
  },
  {
    "raw_code": "def replace_with_partitioned_table(table_name)\n          assert_table_is_allowed(table_name)\n\n          partitioned_table_name = make_partitioned_table_name(table_name)\n          archived_table_name = make_archived_table_name(table_name)\n          primary_key_name = connection.primary_key(table_name)\n\n          replace_table(table_name, partitioned_table_name, archived_table_name, primary_key_name)\n        end",
    "comment": "Replaces a non-partitioned table with its partitioned copy. This is the final step in a partitioning migration, which makes the partitioned table ready for use by the application. The partitioned copy should be replaced with the original table in such a way that it appears seamless to any database clients. The replaced table will be renamed to \"#{replaced_table}_archived\". Partitions and primary key constraints will also be renamed to match the naming scheme of the parent table.  **NOTE** This method should only be used after all other migration steps have completed successfully. There are several limitations to this method that MUST be handled before, or during, the swap migration:  - Secondary indexes and foreign keys are not automatically recreated on the partitioned table. - Some types of constraints (UNIQUE and EXCLUDE) which rely on indexes, will not automatically be recreated on the partitioned table, since the underlying index will not be present. - Foreign keys referencing the original non-partitioned table, would also need to be updated to reference the partitioned table, but unfortunately this is not supported in PG11. - Views referencing the original table will not be automatically updated to reference the partitioned table.  Example:  replace_with_partitioned_table :audit_events ",
    "label": "",
    "id": "2240"
  },
  {
    "raw_code": "def rollback_replace_with_partitioned_table(table_name)\n          assert_table_is_allowed(table_name)\n\n          partitioned_table_name = make_partitioned_table_name(table_name)\n          archived_table_name = make_archived_table_name(table_name)\n          primary_key_name = connection.primary_key(archived_table_name)\n\n          replace_table(table_name, archived_table_name, partitioned_table_name, primary_key_name)\n        end",
    "comment": "Rolls back a migration that replaced a non-partitioned table with its partitioned copy. This can be used to restore the original non-partitioned table in the event of an unexpected issue.  Example:  rollback_replace_with_partitioned_table :audit_events ",
    "label": "",
    "id": "2241"
  },
  {
    "raw_code": "def prepare_async_index(table_name, column_name, **options)\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_ddl_mode!\n\n          if table_partitioned?(table_name)\n            raise ArgumentError, 'prepare_async_index can not be used on a partitioned ' \\\n              'table. Please use prepare_partitioned_async_index on the partitioned table.'\n          end",
    "comment": "Prepares an index for asynchronous creation.  Stores the index information in the postgres_async_indexes table to be created later. The index will be always be created CONCURRENTLY, so that option does not need to be given. If an existing asynchronous definition exists with the same name, the existing entry will be updated with the new definition.  If the requested index has already been created, it is not stored in the table for asynchronous creation.  Note: The `add_index_options` is the same method Rails uses to generate the index creation statements. As such, we can pass index creation options to the method the same as we would standard index creation.  Example usage:  INITIAL_PIPELINE_INDEX = 'tmp_index_vulnerability_occurrences_id_and_initial_pipline_id' INITIAL_PIPELINE_COLUMNS = [:id, :initial_pipeline_id]  prepare_async_index TABLE_NAME, INITIAL_PIPELINE_COLUMNS, name: INITIAL_PIPELINE_INDEX, where: 'initial_pipeline_id IS NULL'",
    "label": "",
    "id": "2242"
  },
  {
    "raw_code": "def prepare_async_index_removal(table_name, column_name, options = {})\n          index_name = options.fetch(:name)\n          raise 'prepare_async_index_removal must get an index name defined' if index_name.blank?\n\n          unless index_exists?(table_name, column_name, **options)\n            Gitlab::AppLogger.warn \"Index not removed because it does not exist (this may be due to an aborted migration or similar): table_name: #{table_name}, index_name: #{index_name}\"\n            return\n          end",
    "comment": "Prepares an index for asynchronous destruction.  Stores the index information in the postgres_async_indexes table to be removed later. The index will be always be removed CONCURRENTLY, so that option does not need to be given. Except for partitioned tables where indexes cannot be dropped using this option. https://www.postgresql.org/docs/current/sql-dropindex.html  If the requested index has already been removed, it is not stored in the table for asynchronous destruction.",
    "label": "",
    "id": "2243"
  },
  {
    "raw_code": "def index_statement_from!(definition)\n          parsed_query = PgQuery.parse(definition)\n\n          parsed_query.tree.stmts[0].stmt.index_stmt\n        end",
    "comment": "This raises `PgQuery::ParseError` if the given statement is syntactically incorrect, therefore, validates that the index definition is correct.",
    "label": "",
    "id": "2244"
  },
  {
    "raw_code": "def prepare_async_foreign_key_validation(table_name, column_name = nil, name: nil)\n          ensure_async_constraint_validation_is_enabled do\n            fk_name = name || concurrent_foreign_key_name(table_name, column_name)\n\n            unless foreign_key_exists?(table_name, name: fk_name)\n              raise missing_schema_object_message(table_name, \"foreign key\", fk_name)\n            end",
    "comment": "Prepares a foreign key for asynchronous validation.  Stores the FK information in the postgres_async_foreign_key_validations table to be executed later. ",
    "label": "",
    "id": "2245"
  },
  {
    "raw_code": "def prepare_async_check_constraint_validation(table_name, name:)\n          ensure_async_constraint_validation_is_enabled do\n            unless check_constraint_exists?(table_name, name)\n              raise missing_schema_object_message(table_name, \"check constraint\", name)\n            end",
    "comment": "Prepares a check constraint for asynchronous validation.  Stores the constraint information in the postgres_async_foreign_key_validations table to be executed later. ",
    "label": "",
    "id": "2246"
  },
  {
    "raw_code": "def connector\n          CONNECTORS.fetch(connector_provider, StorageConnectors::Local).new(connector_settings)\n        end",
    "comment": "Fetches the configured provider or uses +StorageConnectors::Local+ as fallback connector.",
    "label": "",
    "id": "2247"
  },
  {
    "raw_code": "def upload(filename, data)\n            bucket.create_file(\n              StringIO.new(data),\n              CGI.escape(filename),\n              metadata: {\n                original_filename: filename,\n                encoded: true\n              }\n            )\n          end",
    "comment": "We have to escape \"/\" from the filename to avoid gcp to interpret as a subfolder. This can be a problem if we use the primary write location compose the filename, which can include an address like +\"1F/4BE69098\"+",
    "label": "",
    "id": "2248"
  },
  {
    "raw_code": "def bucket\n            @bucket ||= client.bucket(settings.bucket)\n          end",
    "comment": "Permission 'storage.buckets.get' must be granted to access to the Google Cloud Storage bucket",
    "label": "",
    "id": "2249"
  },
  {
    "raw_code": "def self.current(load_balancer)\n          cached_instance.lookup(load_balancer)\n        end",
    "comment": "lb - Gitlab::Database::LoadBalancing::LoadBalancer instance",
    "label": "",
    "id": "2250"
  },
  {
    "raw_code": "def self.with_sessions(models = Gitlab::Database::LoadBalancing.base_models)\n          dbs = models.map { |m| m.load_balancer.name }.uniq\n          dbs.each { |db| cached_instance.validate_db_name(db) }\n          ScopedSessions.new(dbs, cached_instance.session_map)\n        end",
    "comment": "models - Array<ActiveRecord::Base>",
    "label": "",
    "id": "2251"
  },
  {
    "raw_code": "def initialize(host, load_balancer, port: nil)\n          @host = host\n          @port = port\n          @load_balancer = load_balancer\n          @pool = load_balancer.create_replica_connection_pool(\n            load_balancer.configuration.pool_size,\n            host,\n            port\n          )\n          @online = true\n          @last_checked_at = Time.zone.now\n          @lag_time = nil\n          @lag_size = nil\n\n          # Randomly somewhere in between interval and 2*interval we'll refresh the status of the host\n          interval = load_balancer.configuration.replica_check_interval\n          @intervals = (interval..(interval * 2)).step(0.5).to_a\n        end",
    "comment": "host - The address of the database. load_balancer - The LoadBalancer that manages this Host.",
    "label": "",
    "id": "2252"
  },
  {
    "raw_code": "def disconnect!(timeout: 120)\n          start_time = ::Gitlab::Metrics::System.monotonic_time\n\n          while (::Gitlab::Metrics::System.monotonic_time - start_time) <= timeout\n            return if try_disconnect\n\n            sleep(2)\n          end",
    "comment": "Disconnects the pool, once all connections are no longer in use.  timeout - The time after which the pool should be forcefully disconnected.",
    "label": "",
    "id": "2253"
  },
  {
    "raw_code": "def try_disconnect\n          if pool.connections.none?(&:in_use?)\n            pool_disconnect!\n            return true\n          end",
    "comment": "Attempt to disconnect the pool if all connections are no longer in use. Returns true if the pool was disconnected, false if not.",
    "label": "",
    "id": "2254"
  },
  {
    "raw_code": "def online?\n          # Avoid using a discarded connection pool because attempting\n          # to use it will fail. After the main process forks, all of\n          # its connection pools are discarded from Rails' ForkTracker.\n          return false if discarded?\n          return @online unless check_replica_status?\n\n          was_online = @online\n          refresh_status\n\n          # Log that the host came back online if it was previously offline\n          if @online && !was_online\n            ::Gitlab::Database::LoadBalancing::Logger.info(\n              event: :host_online,\n              message: 'Host is online after replica status check',\n              db_host: @host,\n              db_port: @port,\n              lag_time: @lag_time,\n              lag_size: @lag_size\n            )\n          # Always log if the host goes offline\n          elsif !@online\n            ::Gitlab::Database::LoadBalancing::Logger.warn(\n              event: :host_offline,\n              message: 'Host is offline after replica status check',\n              db_host: @host,\n              db_port: @port,\n              lag_time: @lag_time,\n              lag_size: @lag_size\n            )\n          end",
    "comment": "Returns true if the host is online.",
    "label": "",
    "id": "2255"
  },
  {
    "raw_code": "def data_is_recent_enough?\n          # It's possible for a replica to not replay WAL data for a while,\n          # despite being up to date. This can happen when a primary does not\n          # receive any writes for a while.\n          #\n          # To prevent this from happening we check if the lag size (in bytes)\n          # of the replica is small enough for the replica to be useful. We\n          # only do this if we haven't replicated in a while so we only need\n          # to connect to the primary when truly necessary.\n          if (@lag_size = replication_lag_size)\n            @lag_size <= load_balancer.configuration.max_replication_difference\n          else\n            false\n          end",
    "comment": "Returns true if the replica has replicated enough data to be useful.",
    "label": "",
    "id": "2256"
  },
  {
    "raw_code": "def replication_lag_time\n          row = query_and_release(REPLICATION_LAG_QUERY)\n\n          row['lag'].to_f if row.any?\n        end",
    "comment": "Returns the replication lag time of this secondary in seconds as a float.  This method will return nil if no lag time could be calculated.",
    "label": "",
    "id": "2257"
  },
  {
    "raw_code": "def replication_lag_size(location = primary_write_location)\n          location = connection.quote(location)\n\n          row = query_and_release(<<-SQL.squish)\n            SELECT pg_wal_lsn_diff(#{location}, (#{latest_lsn_query}))::float AS diff\n          SQL\n\n          row['diff'].to_i if row.any?\n        rescue *CONNECTION_ERRORS\n          nil\n        end",
    "comment": "Returns the number of bytes this secondary is lagging behind the primary.  This method will return nil if no lag size could be calculated.",
    "label": "",
    "id": "2258"
  },
  {
    "raw_code": "def caught_up?(location)\n          lag = replication_lag_size(location)\n          lag.present? && lag.to_i <= 0\n        end",
    "comment": "Returns true if this host has caught up to the given transaction write location.  location - The transaction write location as reported by a primary.",
    "label": "",
    "id": "2259"
  },
  {
    "raw_code": "def latest_lsn_query\n          @latest_lsn_query ||= can_track_logical_lsn? ? LATEST_LSN_WITH_LOGICAL_QUERY : LATEST_LSN_WITHOUT_LOGICAL_QUERY\n        end",
    "comment": "The LATEST_LSN_WITH_LOGICAL query requires permissions that may not be present in self-managed configurations. We fallback gracefully to the query that does not correctly handle logical replicas for such configurations.",
    "label": "",
    "id": "2260"
  },
  {
    "raw_code": "def initialize(\n          load_balancer,\n          nameserver:,\n          port:,\n          record:,\n          record_type: 'A',\n          interval: 60,\n          disconnect_timeout: 120,\n          use_tcp: false,\n          max_replica_pools: nil\n        )\n          @nameserver = nameserver\n          @port = port\n          @record = record\n          @record_type = record_type_for(record_type)\n          @interval = interval\n          @disconnect_timeout = disconnect_timeout\n          @use_tcp = use_tcp\n          @load_balancer = load_balancer\n          @max_replica_pools = max_replica_pools\n          @nameserver_ttl = 1.second.ago # Begin with an expired ttl to trigger a nameserver dns lookup\n        end",
    "comment": "nameserver - The nameserver to use for DNS lookups. port - The port of the nameserver. record - The DNS record to look up for retrieving the secondaries. record_type - The type of DNS record to look up interval - The time to wait between lookups. disconnect_timeout - The time after which an old host should be forcefully disconnected. use_tcp - Use TCP instaed of UDP to look up resources load_balancer - The load balancer instance to use rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "2261"
  },
  {
    "raw_code": "def start\n          self.refresh_thread = Thread.new do\n            loop do\n              self.refresh_thread_last_run = Time.current\n\n              next_sleep_duration = perform_service_discovery\n\n              # We slightly randomize the sleep() interval. This should reduce\n              # the likelihood of _all_ processes refreshing at the same time,\n              # possibly putting unnecessary pressure on the DNS server.\n              sleep(next_sleep_duration + rand(MAX_SLEEP_ADJUSTMENT))\n            end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "2262"
  },
  {
    "raw_code": "def refresh_if_necessary\n          wait_time, from_dns = addresses_from_dns\n\n          current = addresses_from_load_balancer\n\n          if from_dns != current\n            ::Gitlab::Database::LoadBalancing::Logger.info(\n              event: :host_list_update,\n              message: \"Updating the host list for service discovery\",\n              host_list_length: from_dns.length,\n              old_host_list_length: current.length\n            )\n            replace_hosts(from_dns)\n          end",
    "comment": "Refreshes the hosts, but only if the DNS record returned a new list of addresses.  The return value is the amount of time (in seconds) to wait before checking the DNS record for any changes.",
    "label": "",
    "id": "2263"
  },
  {
    "raw_code": "def replace_hosts(addresses)\n          old_hosts = load_balancer.host_list.hosts\n\n          load_balancer.host_list.hosts = addresses.map do |addr|\n            Host.new(addr.hostname, load_balancer, port: addr.port)\n          end",
    "comment": "Replaces all the hosts in the load balancer with the new ones, disconnecting the old connections.  addresses - An Array of Address structs to use for the new hosts.",
    "label": "",
    "id": "2264"
  },
  {
    "raw_code": "def addresses_from_dns\n          response = resolver.search(record, record_type)\n          resources = response.answer\n\n          addresses =\n            case record_type\n            when Net::DNS::A\n              addresses_from_a_record(resources)\n            when Net::DNS::SRV\n              addresses_from_srv_record(response)\n            end",
    "comment": "Returns an Array containing:  1. The time to wait for the next check. 2. An array containing the hostnames of the DNS record.",
    "label": "",
    "id": "2265"
  },
  {
    "raw_code": "def self.for_model(model)\n          cfg = model.connection_db_config.configuration_hash.deep_symbolize_keys\n          lb_cfg = cfg[:load_balancing] || {}\n          config = new(model)\n\n          if (diff = lb_cfg[:max_replication_difference])\n            config.max_replication_difference = diff\n          end",
    "comment": "Creates a configuration object for the given ActiveRecord model.",
    "label": "",
    "id": "2266"
  },
  {
    "raw_code": "def load_balancing_enabled?\n          return false if Gitlab::Runtime.rake?\n\n          hosts.any? || service_discovery_enabled?\n        end",
    "comment": "Returns `true` if the use of load balancing replicas should be enabled.  This is disabled for Rake tasks to ensure e.g. database migrations always produce consistent results.",
    "label": "",
    "id": "2267"
  },
  {
    "raw_code": "def service_discovery_enabled?\n          return false if Gitlab::Runtime.rake?\n\n          service_discovery[:record].present?\n        end",
    "comment": "This is disabled for Rake tasks to ensure e.g. database migrations always produce consistent results.",
    "label": "",
    "id": "2268"
  },
  {
    "raw_code": "def use_replicas_for_read_queries\n          previous_flag = @use_replicas_for_read_queries\n          @use_replicas_for_read_queries = true\n          yield\n        ensure\n          @use_replicas_for_read_queries = previous_flag\n        end",
    "comment": "Indicates that the read SQL statements from anywhere inside this blocks should use a replica, regardless of the current primary stickiness or whether a write query is already performed in the current session. This interface is reserved mostly for performance purpose. This is a good tool to push expensive queries, which can tolerate the replica lags, to the replicas.  Write and ambiguous queries inside this block are still handled by the primary.",
    "label": "",
    "id": "2269"
  },
  {
    "raw_code": "def fallback_to_replicas_for_ambiguous_queries\n          previous_flag = @fallback_to_replicas_for_ambiguous_queries\n          @fallback_to_replicas_for_ambiguous_queries = true\n          yield\n        ensure\n          @fallback_to_replicas_for_ambiguous_queries = previous_flag\n        end",
    "comment": "Indicate that the ambiguous SQL statements from anywhere inside this block should use a replica. The ambiguous statements include: - Transactions. - Custom queries (via exec_query, execute, etc.) - In-flight connection configuration change (SET LOCAL statement_timeout = 5000)  This is a weak enforcement. This helper incorporates well with primary stickiness: - If the queries are about to write - The current session already performed writes - It prefers to use primary, aka, use_primary or use_primary! were called",
    "label": "",
    "id": "2270"
  },
  {
    "raw_code": "def databases_in_sync?(wal_locations)\n          return true unless wal_locations.present?\n\n          ::Gitlab::Database::LoadBalancing.each_load_balancer.all? do |lb|\n            if (location = wal_locations.with_indifferent_access[lb.name])\n              lb.select_up_to_date_host(location) != LoadBalancer::NONE_CAUGHT_UP\n            else\n              true\n            end",
    "comment": "NOTE: If there's no entry for a load balancer or no WAL locations were passed we assume the sender does not care about LB and we assume nodes are in-sync.",
    "label": "",
    "id": "2271"
  },
  {
    "raw_code": "def initialize(load_balancer)\n          @load_balancer = load_balancer\n        end",
    "comment": "hosts - The hosts to use for load balancing.",
    "label": "",
    "id": "2272"
  },
  {
    "raw_code": "def method_missing(...)\n          if current_session.fallback_to_replicas_for_ambiguous_queries?\n            read_using_load_balancer(...)\n          else\n            write_using_load_balancer(...)\n          end",
    "comment": "Delegates all unknown messages to a read-write connection.",
    "label": "",
    "id": "2273"
  },
  {
    "raw_code": "def read_using_load_balancer(...)\n          if current_session.use_primary? &&\n              !current_session.use_replicas_for_read_queries?\n            @load_balancer.read_write do |connection|\n              connection.public_send(...)\n            end",
    "comment": "Performs a read using the load balancer.  name - The name of the method to call on a connection object.",
    "label": "",
    "id": "2274"
  },
  {
    "raw_code": "def write_using_load_balancer(...)\n          if read_only_transaction?\n            raise WriteInsideReadOnlyTransactionError, 'A write query is performed inside a read-only transaction'\n          end",
    "comment": "Performs a write using the load balancer.  name - The name of the method to call on a connection object. sticky - If set to true the session will stick to the master after the write.",
    "label": "",
    "id": "2275"
  },
  {
    "raw_code": "def initialize(configuration)\n          @configuration = configuration\n          @primary_only = !configuration.load_balancing_enabled?\n          @host_list =\n            if @primary_only\n              HostList.new([PrimaryHost.new(self)])\n            else\n              HostList.new(configuration.hosts.map { |addr| Host.new(addr, self) })\n            end",
    "comment": "configuration - An instance of `LoadBalancing::Configuration` that contains the configuration details (such as the hosts) for this load balancer.",
    "label": "",
    "id": "2276"
  },
  {
    "raw_code": "def read(&block)\n          raise_if_concurrent_ruby!\n\n          service_discovery&.log_refresh_thread_interruption\n\n          conflict_retried = 0\n\n          while host\n            ensure_caching!\n\n            begin\n              connection = host.connection\n              Thread.current[READ_HOST_CONN_CHECKOUT_KEY] = true\n\n              return yield connection\n            rescue StandardError => error\n              if primary_only?\n                # If we only have primary configured, retrying is pointless\n                raise error\n              elsif serialization_failure?(error)\n                # This error can occur when a query conflicts. See\n                # https://www.postgresql.org/docs/current/static/hot-standby.html#HOT-STANDBY-CONFLICT\n                # for more information.\n                #\n                # In this event we'll cycle through the secondaries at most 3\n                # times before using the primary instead.\n                will_retry = conflict_retried < @host_list.length * 3\n\n                ::Gitlab::Database::LoadBalancing::Logger.warn(\n                  event: :host_query_conflict,\n                  message: 'Query conflict on host',\n                  conflict_retried: conflict_retried,\n                  will_retry: will_retry,\n                  db_host: host.host,\n                  db_port: host.port,\n                  host_list_length: @host_list.length\n                )\n\n                if will_retry\n                  conflict_retried += 1\n                  release_host\n                else\n                  break\n                end",
    "comment": "Yields a connection that can be used for reads.  If no secondaries were available this method will use the primary instead.",
    "label": "",
    "id": "2277"
  },
  {
    "raw_code": "def read_write\n          raise_if_concurrent_ruby!\n\n          service_discovery&.log_refresh_thread_interruption\n          transaction_open = nil\n\n          # Retry only once when in a transaction (see https://gitlab.com/gitlab-org/gitlab/-/issues/220242)\n          connection =\n            if Gitlab.next_rails?\n              pool.lease_connection\n            else\n              pool.connection\n            end",
    "comment": "Yields a connection that can be used for both reads and writes.",
    "label": "",
    "id": "2278"
  },
  {
    "raw_code": "def host\n          request_cache[CACHE_KEY] ||= @host_list.next\n        end",
    "comment": "Returns a host to use for queries.  Hosts are scoped per thread so that multiple threads don't accidentally re-use the same host + connection.",
    "label": "",
    "id": "2279"
  },
  {
    "raw_code": "def release_host\n          if host = request_cache[CACHE_KEY]\n            host.disable_query_cache!\n            host.release_connection\n          end",
    "comment": "Releases the host and connection for the current thread.",
    "label": "",
    "id": "2280"
  },
  {
    "raw_code": "def primary_write_location\n          location = read_write do |connection|\n            get_write_location(connection)\n          end",
    "comment": "Returns the transaction write location of the primary.",
    "label": "",
    "id": "2281"
  },
  {
    "raw_code": "def select_up_to_date_host(location)\n          all_hosts = @host_list.hosts.shuffle\n          first_caught_up_host = nil\n\n          # We must loop through all of them so that we know if all are caught up. Some callers only care about finding\n          # one caught up host and storing it in request_cache. But Sticking needs to know if ALL_CAUGHT_UP so that it\n          # can clear the LSN position from Redis and not ask again in future.\n          results = all_hosts.map do |host|\n            caught_up = host.caught_up?(location)\n            first_caught_up_host ||= host if caught_up\n            caught_up\n          end",
    "comment": "Finds any up to date replica for the given LSN location and stores an up to date replica in the SafeRequestStore to be used later for read-only queries. It returns a symbol to indicate if :any, :all or :none were found to be caught up.",
    "label": "",
    "id": "2282"
  },
  {
    "raw_code": "def retry_with_backoff(attempts: 3, time: 2)\n          # In CI we only use the primary, but databases may not always be\n          # available (or take a few seconds to become available). Retrying in\n          # this case can slow down CI jobs. In addition, retrying with _only_\n          # a primary being present isn't all that helpful.\n          #\n          # To prevent this from happening, we don't make any attempt at\n          # retrying unless one or more replicas are used. This matches the\n          # behaviour from before we enabled load balancing code even if no\n          # replicas were configured.\n          return yield if primary_only?\n\n          attempt = 1\n          last_error = nil\n\n          while attempt <= attempts\n            begin\n              return yield attempt # Yield the current attempt count\n            rescue StandardError => error\n              raise error unless connection_error?(error)\n\n              # We need to release the primary connection as otherwise Rails\n              # will keep raising errors when using the connection.\n              release_primary_connection\n\n              last_error = error\n              sleep(time)\n              attempt += 1\n              time **= 2\n            end",
    "comment": "Yields a block, retrying it upon error using an exponential backoff.",
    "label": "",
    "id": "2283"
  },
  {
    "raw_code": "def create_replica_connection_pool(pool_size, host = nil, port = nil)\n          db_config = @configuration.db_config\n\n          env_config = db_config.configuration_hash.dup\n          env_config[:pool] = pool_size\n          env_config[:host] = host if host\n          env_config[:port] = port if port\n\n          db_config = ActiveRecord::DatabaseConfigurations::HashConfig.new(\n            db_config.env_name,\n            db_config.name + REPLICA_SUFFIX,\n            env_config\n          )\n\n          # We cannot use ActiveRecord::Base.connection_handler.establish_connection\n          # as it will rewrite ActiveRecord::Base.connection\n          ActiveRecord::ConnectionAdapters::ConnectionHandler\n            .new\n            .establish_connection(db_config)\n        end",
    "comment": "pool_size - The size of the DB pool. host - An optional host name to use instead of the default one. port - An optional port to connect to.",
    "label": "",
    "id": "2284"
  },
  {
    "raw_code": "def pool\n          ActiveRecord::Base.connection_handler.retrieve_connection_pool(\n            @configuration.connection_specification_name,\n            role: ActiveRecord.writing_role,\n            shard: ActiveRecord::Base.default_shard\n          ) || raise(::ActiveRecord::ConnectionNotEstablished)\n        end",
    "comment": "ActiveRecord::ConnectionAdapters::ConnectionHandler handles fetching, and caching for connections pools for each \"connection\", so we leverage that. rubocop:disable Database/MultipleDatabases",
    "label": "",
    "id": "2285"
  },
  {
    "raw_code": "def wal_diff(location1, location2)\n          read_write do |connection|\n            lsn1 = connection.quote(location1)\n            lsn2 = connection.quote(location2)\n\n            query = <<-SQL.squish\n            SELECT pg_wal_lsn_diff(#{lsn1}, #{lsn2})\n              AS result\n            SQL\n\n            row = connection.select_all(query).first\n            row['result'] if row\n          end",
    "comment": "rubocop:enable Database/MultipleDatabases",
    "label": "",
    "id": "2286"
  },
  {
    "raw_code": "def get_write_location(ar_connection)\n          use_new_load_balancer_query = Gitlab::Utils\n            .to_boolean(ENV['USE_NEW_LOAD_BALANCER_QUERY'], default: true)\n\n          sql =\n            if use_new_load_balancer_query\n              <<~NEWSQL\n                SELECT CASE\n                    WHEN pg_is_in_recovery() = true AND EXISTS (SELECT 1 FROM pg_stat_get_wal_senders())\n                      THEN pg_last_wal_replay_lsn()::text\n                    WHEN pg_is_in_recovery() = false\n                      THEN pg_current_wal_insert_lsn()::text\n                      ELSE NULL\n                    END AS location;\n              NEWSQL\n            else\n              <<~SQL\n                SELECT pg_current_wal_insert_lsn()::text AS location\n              SQL\n            end",
    "comment": "@param [ActiveRecord::Connection] ar_connection @return [String]",
    "label": "",
    "id": "2287"
  },
  {
    "raw_code": "def find_caught_up_replica(env)\n          namespaces_and_ids = sticking_namespaces(env)\n\n          namespaces_and_ids.each do |(sticking, namespace, id)|\n            sticking.find_caught_up_replica(namespace, id)\n          end",
    "comment": "Determine if we need to stick based on currently available user data.  Typically this code will only be reachable for Rails requests as Grape data is not yet available at this point.",
    "label": "",
    "id": "2288"
  },
  {
    "raw_code": "def stick_if_necessary(env)\n          namespaces_and_ids = sticking_namespaces(env)\n\n          namespaces_and_ids.each do |sticking, namespace, id|\n            lb = sticking.load_balancer\n            sticking.stick(namespace, id) if ::Gitlab::Database::LoadBalancing::SessionMap.current(lb).performed_write?\n          end",
    "comment": "Determine if we need to stick after handling a request.",
    "label": "",
    "id": "2289"
  },
  {
    "raw_code": "def sticking_namespaces(env)\n          warden = env['warden']\n\n          if warden && warden.user\n            # When sticking per user, _only_ sticking the main connection could\n            # result in the application trying to read data from a different\n            # connection, while that data isn't available yet.\n            #\n            # To prevent this from happening, we scope sticking to all the\n            # models that support load balancing. In the future (if we\n            # determined this to be OK) we may be able to relax this.\n            ::Gitlab::Database::LoadBalancing.base_models.map do |model|\n              [model.sticking, :user, warden.user.id]\n            end",
    "comment": "Determines the sticking namespace and identifier based on the Rack environment.  For Rails requests this uses warden, but Grape and others have to manually set the right environment variable.",
    "label": "",
    "id": "2290"
  },
  {
    "raw_code": "def find_caught_up_replica(namespace, id, use_primary_on_failure: true, use_primary_on_empty_location: false)\n          location = last_write_location_for(namespace, id)\n\n          result = if location\n                     up_to_date_result = @load_balancer.select_up_to_date_host(location)\n\n                     unstick(namespace, id) if up_to_date_result == LoadBalancer::ALL_CAUGHT_UP\n\n                     up_to_date_result != LoadBalancer::NONE_CAUGHT_UP\n                   else\n                     # Some callers want to err on the side of caution and be really sure that a caught up replica was\n                     # found. If we did not have any location to check then we must force `use_primary!` if they they\n                     # use_primary_on_empty_location\n                     !use_primary_on_empty_location\n                   end",
    "comment": "Returns true if any caught up replica is found. This does not mean all replicas are caught up but the found caught up replica will be stored in the SafeRequestStore available as LoadBalancer#host for future queries. With use_primary_on_empty_location: true we will assume you need the primary if we can't find a matching location for the namespace, id pair. You should only use use_primary_on_empty_location in rare cases because we unstick once we find all replicas are caught up one time so it can be wasteful on the primary.",
    "label": "",
    "id": "2291"
  },
  {
    "raw_code": "def stick(namespace, id)\n          with_primary_write_location do |location|\n            set_write_location_for(namespace, id, location)\n          end",
    "comment": "Starts sticking to the primary for the given namespace and id, using the latest WAL pointer from the primary.",
    "label": "",
    "id": "2292"
  },
  {
    "raw_code": "def initialize(hosts = [])\n          @hosts = hosts.shuffle\n          @index = 0\n          @mutex = Mutex.new\n          @hosts_gauge = Gitlab::Metrics.gauge(:db_load_balancing_hosts, 'Current number of load balancing hosts')\n\n          set_metrics!\n        end",
    "comment": "hosts - The list of secondary hosts to add.",
    "label": "",
    "id": "2293"
  },
  {
    "raw_code": "def next\n          next_host.tap do |_|\n            set_metrics!\n          end",
    "comment": "Sets metrics before returning next host",
    "label": "",
    "id": "2294"
  },
  {
    "raw_code": "def next_host\n          @mutex.synchronize do\n            break if @hosts.empty?\n\n            started_at = @index\n\n            loop do\n              host = @hosts[@index]\n              @index = (@index + 1) % @hosts.length\n\n              break host if host.online?\n\n              # Return nil once we have cycled through all hosts and none were\n              # available.\n              break if @index == started_at\n            end",
    "comment": "Returns the next available host.  Returns a Gitlab::Database::LoadBalancing::Host instance, or nil if no hosts were available.",
    "label": "",
    "id": "2295"
  },
  {
    "raw_code": "def estimated_distinct_count\n          @estimated_distinct_count ||= estimate_cardinality\n        end",
    "comment": "Based on HyperLogLog structure estimates number of unique elements in analysed set.  @return [Float] Estimate number of unique elements",
    "label": "",
    "id": "2296"
  },
  {
    "raw_code": "def merge_hash!(other_buckets_hash)\n          buckets.merge!(other_buckets_hash) { |_key, old, new| new > old ? new : old }\n        end",
    "comment": "Updates instance underlying HyperLogLog structure by merging it with other HyperLogLog structure  @param other_buckets_hash hash with HyperLogLog structure representation",
    "label": "",
    "id": "2297"
  },
  {
    "raw_code": "def to_json(_ = nil)\n          buckets.to_json\n        end",
    "comment": "Serialize instance underlying HyperLogLog structure to JSON format, that can be stored in various persistence layers  @return [String] HyperLogLog data structure serialized to JSON",
    "label": "",
    "id": "2298"
  },
  {
    "raw_code": "def estimate_cardinality\n          num_zero_buckets = TOTAL_BUCKETS - buckets.size\n\n          num_uniques = (\n            ((TOTAL_BUCKETS**2) * (0.7213 / (1 + (1.079 / TOTAL_BUCKETS)))) /\n            (num_zero_buckets + buckets.values.sum { |bucket_hash| 2**(-1 * bucket_hash) })\n          ).to_i\n\n          if num_zero_buckets > 0 && num_uniques < 2.5 * TOTAL_BUCKETS\n            TOTAL_BUCKETS * Math.log(TOTAL_BUCKETS.to_f / num_zero_buckets)\n          else\n            num_uniques\n          end",
    "comment": "arbitrary values that are present in #estimate_cardinality are sourced from https://www.sisense.com/blog/hyperloglog-in-pure-sql/ article, they are not representing any entity and serves as tune value for the whole equation",
    "label": "",
    "id": "2299"
  },
  {
    "raw_code": "def execute(batch_size: nil, start: nil, finish: nil)\n          raise 'BatchCount can not be run inside a transaction' if transaction_open?\n\n          batch_size ||= DEFAULT_BATCH_SIZE\n          start = actual_start(start)\n          finish = actual_finish(finish)\n\n          raise WRONG_CONFIGURATION_ERROR if unwanted_configuration?(start, finish, batch_size)\n\n          batch_start = start\n          hll_buckets = Buckets.new\n\n          while batch_start <= finish\n            hll_buckets.merge_hash!(hll_buckets_for_batch(batch_start, batch_start + batch_size))\n            batch_start += batch_size\n            sleep(SLEEP_TIME_IN_SECONDS)\n          end",
    "comment": "Executes counter that iterates over database source and return Gitlab::Database::PostgresHll::Buckets that can be used to estimation of number of uniq elements in analysed set  @param batch_size maximal number of rows that will be analysed by single database query @param start initial pkey range @param finish final pkey range @return [Gitlab::Database::PostgresHll::Buckets] HyperLogLog data structure instance that can estimate number of unique elements",
    "label": "",
    "id": "2300"
  },
  {
    "raw_code": "def source_query(start, finish)\n          col_as_arel = @column.is_a?(Arel::Attributes::Attribute) ? @column : Arel.sql(@column.to_s)\n          col_as_text = Arel::Nodes::NamedFunction.new('CAST', [col_as_arel.as('text')])\n          md5_of_col = Arel::Nodes::NamedFunction.new('md5', [col_as_text])\n          md5_as_hex = Arel::Nodes::Concat.new(Arel.sql(\"'X'\"), md5_of_col)\n          bits = Arel::Nodes::NamedFunction.new('CAST', [md5_as_hex.as('bit(32)')])\n\n          @relation\n            .where(@relation.primary_key => (start...finish))\n            .where(col_as_arel.not_eq(nil))\n            .select(bits.as('attr_hash_32_bits')).to_sql\n        end",
    "comment": "Generate the source query SQL snippet for the provided id range  @example SQL query template SELECT CAST(('X' || md5(CAST(%{column} as text))) as bit(32)) attr_hash_32_bits FROM %{relation} WHERE %{pkey} >= %{batch_start} AND %{pkey} < %{batch_end} AND %{column} IS NOT NULL  @param start initial id range @param finish final id range @return [String] SQL query fragment",
    "label": "",
    "id": "2301"
  },
  {
    "raw_code": "def bucketed_data_sql\n          <<~SQL\n            WITH hashed_attributes AS MATERIALIZED (%{source_query})\n            SELECT (attr_hash_32_bits & #{BIT_32_NORMALIZED_BUCKET_ID_MASK})::int AS bucket_num,\n              (31 - floor(log(2, min((attr_hash_32_bits & #{BIT_31_MASK})::int))))::int as bucket_hash\n            FROM hashed_attributes\n            GROUP BY 1\n          SQL\n        end",
    "comment": "@example source_query SELECT CAST(('X' || md5(CAST(%{column} as text))) as bit(32)) attr_hash_32_bits FROM %{relation} WHERE %{pkey} >= %{batch_start} AND %{pkey} < %{batch_end} AND %{column} IS NOT NULL",
    "label": "",
    "id": "2302"
  },
  {
    "raw_code": "def initialize(status_checker, connection, tables)\n          @status_checker = status_checker\n          @connection = connection\n          @tables = tables\n        end",
    "comment": "status_checker: the caller object which checks for database health status eg: BackgroundMigration::BatchedMigration or DeferJobs::DatabaseHealthStatusChecker",
    "label": "",
    "id": "2303"
  },
  {
    "raw_code": "def alert_condition\n            ALERT_CONDITIONS[:above]\n          end",
    "comment": "By default SLIs are expected to be above SLOs, but there can be cases where we want it to be below SLO (eg: WAL rate). For such indicators the sub-class should override this default alert_condition.",
    "label": "",
    "id": "2304"
  },
  {
    "raw_code": "def pending_wal_count\n            Gitlab::Database::LoadBalancing::SessionMap.current(connection.load_balancer).use_primary do\n              connection.execute(PENDING_WAL_COUNT_SQL).to_a.first&.fetch('pending_wal_count')\n            end",
    "comment": "Returns number of WAL segments pending archival",
    "label": "",
    "id": "2305"
  },
  {
    "raw_code": "def too_late_for_reindexing?\n          return false unless Gitlab.com_except_jh? # rubocop:disable Gitlab/AvoidGitlabInstanceChecks -- Not related to SaaS offerings\n\n          !Time.current.on_weekend?\n        end",
    "comment": "We need to check the time explicitly because we execute 4 reindexing action per rake invocation and one action can take up to 24 hours. This means that it can span for more than the weekend.",
    "label": "",
    "id": "2306"
  },
  {
    "raw_code": "def too_late_for_very_large_table?\n          return false unless Gitlab.com_except_jh? # rubocop:disable Gitlab/AvoidGitlabInstanceChecks -- Not related to SaaS offerings\n\n          !Date.today.saturday?\n        end",
    "comment": "The reindexing process takes place during the weekends and starting a reindexing action on a large table late on Sunday could span during Monday. We don't want this because it prevents vacuum from running.",
    "label": "",
    "id": "2307"
  },
  {
    "raw_code": "def count\n          size_estimates\n        rescue *CONNECTION_ERRORS\n          {}\n        end",
    "comment": "Returns a hash of the table names that have recently updated tuples.  @returns [Hash] Table name to count mapping (e.g. { 'projects' => 5, 'users' => 100 })",
    "label": "",
    "id": "2308"
  },
  {
    "raw_code": "def non_sti_models(models)\n          models.reject { |model| sti_model?(model) }\n        end",
    "comment": "Models using single-type inheritance (STI) don't work with reltuple count estimates. We just have to ignore them and use another strategy to compute them.",
    "label": "",
    "id": "2309"
  },
  {
    "raw_code": "def get_statistics(table_names, check_statistics: true)\n          time = 6.hours.ago\n\n          query = ::Gitlab::Database::PgClass.joins(\"LEFT JOIN pg_stat_user_tables ON pg_stat_user_tables.relid = pg_class.oid\")\n            .where(relname: table_names)\n            .where('schemaname = current_schema()')\n            .select('pg_class.relname AS table_name, reltuples::bigint AS estimate')\n\n          if check_statistics\n            query = query.where('last_vacuum > ? OR last_autovacuum > ? OR last_analyze > ? OR last_autoanalyze > ?',\n              time, time, time, time)\n          end",
    "comment": "Generates the PostgreSQL query to return the tuples for tables that have been vacuumed or analyzed in the last hour.  @param [Array] table names @returns [Hash] Table name to count mapping (e.g. { 'projects' => 5, 'users' => 100 })",
    "label": "",
    "id": "2310"
  },
  {
    "raw_code": "def can_execute_on?(*tables)\n          return true unless Gitlab.com_except_jh?\n          return true unless wraparound_prevention_on_tables?(tables)\n\n          Gitlab::AppLogger.info(message: \"Wraparound prevention vacuum detected\", class: self.class)\n          say \"Wraparound prevention vacuum detected, skipping migration\"\n          false\n        end",
    "comment": "This is used for partitioning CI tables because the autovacuum for wraparound prevention can take many hours to complete on some of the tables and this in turn blocks the post deployment migrations pipeline. Intended workflow for this helper: 1. Introduce a migration that is guarded with this helper for self-managed so that the tests and everything else depending on it can reflect the changes 2. Introduce the migration again for .com so that we can keep trying until it succeeds on .com",
    "label": "",
    "id": "2311"
  },
  {
    "raw_code": "def skip_require_disable_ddl_transactions!\n            @skip_require_disable_ddl_transactions = true\n          end",
    "comment": "If you are aware of the potential consequences and still want to disable this cop in a particular scenario, you can disable it by calling skip_require_disable_ddl_transactions!",
    "label": "",
    "id": "2312"
  },
  {
    "raw_code": "def add_cascading_namespace_setting(setting_name, type, **options)\n          lock_column_name = \"lock_#{setting_name}\".to_sym\n\n          check_cascading_namespace_setting_consistency(setting_name, lock_column_name)\n\n          namespace_options = options.merge(null: true, default: nil)\n\n          add_column(:namespace_settings, setting_name, type, **namespace_options)\n          add_column(:namespace_settings, lock_column_name, :boolean, default: false, null: false)\n\n          add_column(:application_settings, setting_name, type, **options)\n          add_column(:application_settings, lock_column_name, :boolean, default: false, null: false)\n        end",
    "comment": "Creates the four required columns that constitutes a single cascading namespace settings attribute. This helper is only appropriate if the setting is not already present as a non-cascading attribute.  Creates the `setting_name` column along with the `lock_setting_name` column in both `namespace_settings` and `application_settings`.  This helper is not reversible and must be defined in conjunction with `remove_cascading_namespace_setting` in separate up and down directions.  setting_name - The name of the cascading attribute - same as defined in `NamespaceSetting` with the `cascading_attr` method. type - The column type for the setting itself (:boolean, :integer, etc.) options - Standard Rails column options hash. Accepts keys such as `null` and `default`.  `null` and `default` options will only be applied to the `application_settings` column. In most cases, a non-null default value should be specified.",
    "label": "",
    "id": "2313"
  },
  {
    "raw_code": "def track_record_deletions(table_name)\n          trigger_name = record_deletion_trigger_name(table_name)\n\n          execute(<<~SQL.squish)\n            CREATE TRIGGER #{trigger_name}\n            AFTER DELETE ON #{table_name} REFERENCING OLD TABLE AS old_table\n            FOR EACH STATEMENT\n            EXECUTE FUNCTION #{INSERT_FUNCTION_NAME}();\n          SQL\n        end",
    "comment": "This adds a LFK standard trigger to tables, where the loose_foreign_keys_deleted_records record is referencing the table. This should be used for non-partitioned tables.",
    "label": "",
    "id": "2314"
  },
  {
    "raw_code": "def track_record_deletions_override_table_name(table_identifier, parent_table = nil)\n          table_name = table_identifier.to_s.split('.').last\n          parent_table ||= table_name\n\n          execute(<<~SQL.squish)\n            CREATE TRIGGER #{record_deletion_trigger_name(table_name)}\n            AFTER DELETE ON #{table_identifier} REFERENCING OLD TABLE AS old_table\n            FOR EACH STATEMENT\n            EXECUTE FUNCTION\n            #{INSERT_FUNCTION_NAME_OVERRIDE_TABLE}(#{connection.quote(parent_table)});\n          SQL\n        end",
    "comment": "This is used to track deletions on partitioned tables and their partitions. parent_table is the table name that is insert into loose_foreign_keys_deleted_records table it defaults to the table_name, and that's for when we track deletions on partitioned (parent) tables.",
    "label": "",
    "id": "2315"
  },
  {
    "raw_code": "def untrack_record_deletions(table)\n          trigger_name = record_deletion_trigger_name(table)\n          drop_trigger(table, trigger_name)\n        end",
    "comment": "This method also works on tables that are not in the default schema, but the full table identifier has to be passed in this case.",
    "label": "",
    "id": "2316"
  },
  {
    "raw_code": "def up_migrate_to_setting(feature_flag_name:, setting_name:, default_enabled:)\n          if feature_flag_name.blank? || setting_name.blank? || default_enabled.nil?\n            raise ArgumentError, 'feature_flag_name, setting_name, and default_enabled are required'\n          end",
    "comment": "Migrates a feature flag to an application setting.  @param feature_flag_name [Symbol, String] The name of the feature flag to migrate @param setting_name [Symbol, String] The name of the application setting column to update @param default_enabled [Boolean] The default value to use if the feature flag is not set @return [Integer] The number of affected rows for UPDATE statement",
    "label": "",
    "id": "2317"
  },
  {
    "raw_code": "def up_migrate_to_jsonb_setting(feature_flag_name:, setting_name:, jsonb_column_name:, default_enabled:)\n          if feature_flag_name.blank? || setting_name.blank? || jsonb_column_name.blank? || default_enabled.nil?\n            raise ArgumentError, 'feature_flag_name, jsonb_column_name, setting_name, and default_enabled are required'\n          end",
    "comment": "Migrates a feature flag to a JSONB application setting.  @param feature_flag_name[Symbol, String] The name of the feature flag to migrate @param setting_name [Symbol, String] The name of the application setting to update @param jsonb_column_name [Symbol, String] The name of the application setting JSONB column to update @param default_enabled [Boolean] The default value to use if the feature flag is not set @return [Integer] The number of affected rows for UPDATE statement",
    "label": "",
    "id": "2318"
  },
  {
    "raw_code": "def down_migrate_to_setting(setting_name:, default_enabled:)\n          if setting_name.blank? || default_enabled.nil?\n            raise ArgumentError, 'setting_name and default_enabled are required'\n          end",
    "comment": "Reverts an application setting to its default value during a migration rollback.  @param setting_name [Symbol, String] The name of the application setting column to revert @param default_enabled [Boolean] The default value to set for the application setting @return [Integer] The number of affected rows for UPDATE statement",
    "label": "",
    "id": "2319"
  },
  {
    "raw_code": "def down_migrate_to_jsonb_setting(setting_name:, jsonb_column_name:)\n          if setting_name.blank? || jsonb_column_name.nil?\n            raise ArgumentError, 'setting_name and jsonb_column_name are required'\n          end",
    "comment": "Reverts a JSONB application setting to its default state during a migration rollback. This method removes the specified setting from the JSONB column.  @param setting_name [Symbol, String] The name of the application setting to remove from the JSONB column @param jsonb_column_name [Symbol, String] The name of the application setting JSONB column to update @return [Integer] The number of affected rows for UPDATE statement",
    "label": "",
    "id": "2320"
  },
  {
    "raw_code": "def bigint_index_name(int_column_index_name)\n          # First 20 digits of the hash is chosen to make sure it fits the 63 chars limit\n          digest = Digest::SHA256.hexdigest(int_column_index_name).first(20)\n          \"bigint_idx_#{digest}\"\n        end",
    "comment": "default 'index_name' method is not used because this method can be reused while swapping/dropping the indexes",
    "label": "",
    "id": "2321"
  },
  {
    "raw_code": "def lock_writes_on_table(connection, table_name)\n          database_name = Gitlab::Database.db_config_name(connection)\n          LockWritesManager.new(\n            table_name: table_name,\n            connection: connection,\n            database_name: database_name,\n            with_retries: !connection.transaction_open?,\n            logger: Logger.new($stdout)\n          ).lock_writes\n        end",
    "comment": "with_retries creates new a transaction. So we set it to false if the connection is already has an open transaction, to avoid sub-transactions.",
    "label": "",
    "id": "2322"
  },
  {
    "raw_code": "def create_table(table_name, *args, **kwargs, &block)\n          helper_context = self\n\n          super do |t|\n            t.define_singleton_method(:text) do |column_name, **kwargs|\n              limit = kwargs.delete(:limit)\n\n              super(column_name, **kwargs)\n\n              if limit\n                # rubocop:disable GitlabSecurity/PublicSend\n                name = helper_context.send(:text_limit_name, table_name, column_name)\n                # rubocop:enable GitlabSecurity/PublicSend\n\n                column_name = helper_context.quote_column_name(column_name)\n                definition = \"char_length(#{column_name}) <= #{limit}\"\n\n                t.check_constraint(definition, name: name)\n              end",
    "comment": "Creates a new table, optionally allowing the caller to add text limit constraints to the table. This method only extends Rails' `create_table` method  Example:  create_table :db_guides do |t| t.bigint :stars, default: 0, null: false t.text :title, limit: 128 t.text :notes, limit: 1024  t.check_constraint 'stars > 1000', name: 'so_many_stars' end  See Rails' `create_table` for more info on the available arguments.  When adding foreign keys to other tables, consider wrapping the call into a with_lock_retries block to avoid traffic stalls.",
    "label": "",
    "id": "2323"
  },
  {
    "raw_code": "def with_lock_retries(*args, **kwargs, &block)\n          if transaction_open?\n            if with_lock_retries_used?\n              Gitlab::AppLogger.warn 'WithLockRetries used already, executing the block directly'\n              yield\n            else\n              raise <<~EOF\n              #{__callee__} can not be run inside an already open transaction.\n\n              Lock retries are enabled by default for transactional migrations, so this can be run without `#{__callee__}`.\n              For more details, see: https://docs.gitlab.com/ee/development/migration_style_guide.html#transactional-migrations\n              EOF\n            end",
    "comment": "Executes the block with a retry mechanism that alters the +lock_timeout+ and +sleep_time+ between attempts. The timings can be controlled via the +timing_configuration+ parameter. If the lock was not acquired within the retry period, a last attempt is made without using +lock_timeout+.  In order to retry the block, the method wraps the block into a transaction.  When called inside an open transaction it will execute the block directly.  ==== Examples # Invoking without parameters with_lock_retries do drop_table :my_table end  # Invoking with custom +timing_configuration+ t = [ [1.second, 1.second], [2.seconds, 2.seconds] ]  with_lock_retries(timing_configuration: t) do drop_table :my_table # this will be retried twice end  # Disabling the retries using an environment variable > export DISABLE_LOCK_RETRIES=true  with_lock_retries do drop_table :my_table # one invocation, it will not retry at all end  ==== Parameters * +timing_configuration+ - [[ActiveSupport::Duration, ActiveSupport::Duration], ...] lock timeout for the block, sleep time before the next iteration, defaults to `Gitlab::Database::WithLockRetries::DEFAULT_TIMING_CONFIGURATION` * +logger+ - [Gitlab::JsonLogger] * +env+ - [Hash] custom environment hash, see the example with `DISABLE_LOCK_RETRIES`",
    "label": "",
    "id": "2324"
  },
  {
    "raw_code": "def rename_column_concurrently(table, old_column, new_column, type: nil, batch_column_name: :id, type_cast_function: nil)\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_ddl_mode!\n\n          setup_renamed_column(\n            __callee__, table, old_column, new_column,\n            type: type, batch_column_name: batch_column_name, type_cast_function: type_cast_function\n          )\n\n          with_lock_retries do\n            install_bidirectional_triggers(table, old_column, new_column)\n          end",
    "comment": "Renames a column without requiring downtime.  Concurrent renames work by using database triggers to ensure both the old and new column are in sync. However, this method will _not_ remove the triggers or the old column automatically; this needs to be done manually in a post-deployment migration. This can be done using the method `cleanup_concurrent_column_rename`.  table - The name of the database table containing the column. old_column - The old column name. new_column - The new column name. type - The type of the new column. If no type is given the old column's type is used. batch_column_name - option is for tables without primary key, in this case another unique integer column can be used. Example: :user_id",
    "label": "",
    "id": "2325"
  },
  {
    "raw_code": "def undo_rename_column_concurrently(table, old_column, new_column)\n          teardown_rename_mechanism(table, old_column, new_column, column_to_remove: new_column)\n        end",
    "comment": "Reverses operations performed by rename_column_concurrently.  This method takes care of removing previously installed triggers as well as removing the new column.  table - The name of the database table. old_column - The name of the old column. new_column - The name of the new column.",
    "label": "",
    "id": "2326"
  },
  {
    "raw_code": "def cleanup_concurrent_column_rename(table, old_column, new_column)\n          teardown_rename_mechanism(table, old_column, new_column, column_to_remove: old_column)\n        end",
    "comment": "Cleans up a concurrent column name.  This method takes care of removing previously installed triggers as well as removing the old column.  table - The name of the database table. old_column - The name of the old column. new_column - The name of the new column.",
    "label": "",
    "id": "2327"
  },
  {
    "raw_code": "def undo_cleanup_concurrent_column_rename(table, old_column, new_column, type: nil, batch_column_name: :id)\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_ddl_mode!\n\n          setup_renamed_column(\n            __callee__, table, new_column, old_column,\n            type: type, batch_column_name: batch_column_name\n          )\n\n          with_lock_retries do\n            install_bidirectional_triggers(table, old_column, new_column)\n          end",
    "comment": "Reverses the operations performed by cleanup_concurrent_column_rename.  This method adds back the old_column removed by cleanup_concurrent_column_rename. It also adds back the triggers that are removed by cleanup_concurrent_column_rename.  table - The name of the database table containing the column. old_column - The old column name. new_column - The new column name. type - The type of the old column. If no type is given the new column's type is used. batch_column_name - option is for tables without primary key, in this case another unique integer column can be used. Example: :user_id ",
    "label": "",
    "id": "2328"
  },
  {
    "raw_code": "def truncate_tables!(*table_names, connection: self.connection)\n          table_schemas = Gitlab::Database::GitlabSchema.table_schemas!(table_names)\n\n          raise ArgumentError, \"`table_names` must resolve to only one `gitlab_schema`\" if table_schemas.size != 1\n\n          return unless Gitlab::Database.gitlab_schemas_for_connection(connection).include?(table_schemas.first)\n\n          quoted_tables = table_names.map { |table_name| quote_table_name(table_name) }.join(', ')\n\n          execute(\"TRUNCATE TABLE #{quoted_tables}\")\n        end",
    "comment": "TRUNCATE is a DDL statement (it drops the table and re-creates it), so we want to run the migration in DDL mode, but we also don't want to execute it against all schemas because it will be prevented by the lock_writes trigger.  For example, a `gitlab_main` table on `:gitlab_main` database will be truncated, and a `gitlab_main` table on `:gitlab_ci` database will be skipped.  Note Rails already has a truncate_tables, see https://github.com/rails/rails/blob/6-1-stable/activerecord/lib/active_record/connection_adapters/abstract/database_statements.rb#L193",
    "label": "",
    "id": "2329"
  },
  {
    "raw_code": "def rename_index_with_schema(table_name, old_index_name, new_index_name, schema: nil)\n          if schema.blank?\n            schema, table_name_without_schema = table_name.to_s.scan(/[^\".]+|\"[^\"]*\"/)\n            schema = nil if table_name_without_schema.nil?\n          end",
    "comment": "Rename an index that exists in a different schema other than current_schema() `public`, for example, an index under schema `gitlab_partitions_dynamic`  table_name - The table name that old_index_name is under, e.g. `gitlab_partitions_dynamic.ci_builds_101` or `ci_builds` schema name in the table name will be used unless the `schema` argument is given schema - The schema name that old_index_name is under",
    "label": "",
    "id": "2330"
  },
  {
    "raw_code": "def add_widget_definitions(widgets:, type_enum_value: nil, type_enum_values: [])\n            enum_values = Array(type_enum_values) + [type_enum_value].compact\n\n            work_item_types = migration_work_item_type.where(base_type: enum_values)\n\n            # Work item types should exist in production applications, checking here to avoid failures\n            # if inconsistent data is present.\n            validate_work_item_types(enum_values, work_item_types)\n\n            widget_definitions = work_item_types.flat_map do |work_item_type|\n              widgets.map do |w|\n                { work_item_type_id: work_item_type.id, widget_options: nil }.merge(w)\n              end",
    "comment": "Adds a list of widget definitions to a given work item type Include module into your migration file with: include Gitlab::Database::MigrationHelpers::WorkItems::Widgets  Define the following constants in the migration class  Use [8] for a single type WORK_ITEM_TYPE_ENUM_VALUES = [8,9]  Use only one array item to add a single widget WIDGETS = [ { name: 'Designs', widget_type: 22 }, { name: 'Weight', widget_type: 8, # widget_options is optional widget_options: { editable: true, rollup: false } }, ]  Then define the #up and down methods like this: def up add_widget_definitions(type_enum_values: WORK_ITEM_TYPE_ENUM_VALUES, widgets: WIDGETS) end  def down remove_widget_definitions(type_enum_values: WORK_ITEM_TYPE_ENUM_VALUES, widgets: WIDGETS) end  Run a migration test for migrations that use this helper with: require 'spec_helper' require_migration!  RSpec.describe AddDesignsAndDevelopmentWidgetsToTicketWorkItemType, :migration do it_behaves_like 'migration that adds widgets to a work item type' end",
    "label": "",
    "id": "2331"
  },
  {
    "raw_code": "def calculate_multiplier\n          [TARGET_EFFICIENCY.max / time_efficiency, MAX_MULTIPLIER].min\n        end",
    "comment": "Assumption: time efficiency is linear in the batch size",
    "label": "",
    "id": "2332"
  },
  {
    "raw_code": "def disable_statement_timeout\n          if block_given?\n            if statement_timeout_disabled?\n              # Don't do anything if the statement_timeout is already disabled\n              # Allows for nested calls of disable_statement_timeout without\n              # resetting the timeout too early (before the outer call ends)\n              yield\n            else\n              begin\n                execute('SET statement_timeout TO 0')\n\n                yield\n              ensure\n                execute('RESET statement_timeout')\n              end",
    "comment": "Long-running migrations may take more than the timeout allowed by the database. Disable the session's statement timeout to ensure migrations don't get killed prematurely.  There are two possible ways to disable the statement timeout:  - Per transaction (this is the preferred and default mode) - Per connection (requires a cleanup after the execution)  When using a per connection disable statement, code must be inside a block so we can automatically execute `RESET statement_timeout` after block finishes otherwise the statement will still be disabled until connection is dropped or `RESET statement_timeout` is executed",
    "label": "",
    "id": "2333"
  },
  {
    "raw_code": "def within_context_for_database(database)\n            original_db_config = ActiveRecord::Base.connection_db_config\n            # The config only works if passed a string\n            db_config = ActiveRecord::Base.configurations.configs_for(env_name: Rails.env, name: database.to_s)\n            raise ArgumentError, \"Cannot find a database configuration for #{database}\" unless db_config\n\n            ActiveRecord::Base.establish_connection(db_config) # rubocop:disable Database/EstablishConnection\n\n            yield\n          ensure\n            ActiveRecord::Base.establish_connection(original_db_config) # rubocop:disable Database/EstablishConnection\n          end",
    "comment": "rubocop:disable Database/MultipleDatabases",
    "label": "",
    "id": "2334"
  },
  {
    "raw_code": "def batched_migrations_last_id(for_database)\n            runner = nil\n            base_dir = background_migrations_dir(for_database)\n\n            Gitlab::Database::EachDatabase.each_connection(only: for_database) do |connection|\n              runner = Gitlab::Database::Migrations::BatchedMigrationLastId\n                         .new(connection, base_dir)\n            end",
    "comment": "rubocop:enable Database/MultipleDatabases",
    "label": "",
    "id": "2335"
  },
  {
    "raw_code": "def with_restored_connection_stack(&block)\n          original_handler = ActiveRecord::Base.connection_handler\n\n          original_db_config = ActiveRecord::Base.connection_db_config\n          if ActiveRecord::Base.configurations.primary?(original_db_config.name)\n            return yield(ActiveRecord::Base.connection)\n          end",
    "comment": "This is workaround for `db:migrate` that switches `ActiveRecord::Base.connection` depending on execution. This is subject to be removed once proper fix is implemented: https://gitlab.com/gitlab-org/gitlab/-/issues/362341  In some cases when we run application code we need to restore application connection stack: - ApplicationRecord (in fact ActiveRecord::Base): points to main - Ci::ApplicationRecord: points to ci  rubocop:disable Database/MultipleDatabases",
    "label": "",
    "id": "2336"
  },
  {
    "raw_code": "def ddl_transaction(migration, &block)\n            if use_transaction?(migration)\n              migration.with_lock_retries_used!\n\n              Gitlab::Database::WithLockRetries.new(\n                connection: migration.migration_connection,\n                klass: migration.migration_class,\n                logger: Gitlab::BackgroundMigration::Logger\n              ).run(raise_on_exhaustion: true, &block)\n            else\n              super\n            end",
    "comment": "We patch the original method to start a transaction using the WithLockRetries methodology for the whole migration.",
    "label": "",
    "id": "2337"
  },
  {
    "raw_code": "def with_lock_retries(*args, **kwargs, &block)\n          raise_on_exhaustion = !!kwargs.fetch(:raise_on_exhaustion, true)\n          merged_args = {\n            connection: connection,\n            klass: self.class,\n            logger: Gitlab::BackgroundMigration::Logger,\n            allow_savepoints: true\n          }.merge(kwargs.except(:raise_on_exhaustion))\n\n          Gitlab::Database::WithLockRetries.new(**merged_args)\n            .run(raise_on_exhaustion: raise_on_exhaustion, &block)\n        end",
    "comment": "Executes the block with a retry mechanism that alters the +lock_timeout+ and +sleep_time+ between attempts. The timings can be controlled via the +timing_configuration+ parameter. If the lock was not acquired within the retry period, a last attempt is made without using +lock_timeout+.  Note this helper uses subtransactions when run inside an already open transaction.  ==== Examples # Invoking without parameters with_lock_retries do drop_table :my_table end  # Invoking with custom +timing_configuration+ t = [ [1.second, 1.second], [2.seconds, 2.seconds] ]  with_lock_retries(timing_configuration: t) do drop_table :my_table # this will be retried twice end  # Disabling the retries using an environment variable > export DISABLE_LOCK_RETRIES=true  with_lock_retries do drop_table :my_table # one invocation, it will not retry at all end  ==== Parameters * +timing_configuration+ - [[ActiveSupport::Duration, ActiveSupport::Duration], ...] lock timeout for the block, sleep time before the next iteration, defaults to `Gitlab::Database::WithLockRetries::DEFAULT_TIMING_CONFIGURATION` * +logger+ - [Gitlab::JsonLogger] * +env+ - [Hash] custom environment hash, see the example with `DISABLE_LOCK_RETRIES`",
    "label": "",
    "id": "2338"
  },
  {
    "raw_code": "def read\n          return unless File.exist?(file_path)\n\n          Integer(File.read(file_path).presence, exception: false)\n        end",
    "comment": "Reads the last id from the file  @info casts the file content into an +Integer+. Casts any unexpected content to +nil+  @example Integer('4', exception: false) # => 4 Integer('', exception: false) # => nil  @return [Integer, nil]",
    "label": "",
    "id": "2339"
  },
  {
    "raw_code": "def check_constraint_name(table, column, type)\n          identifier = \"#{table}_#{column}_check_#{type}\"\n          # Check concurrent_foreign_key_name() for info on why we use a hash\n          hashed_identifier = Digest::SHA256.hexdigest(identifier).first(10)\n\n          \"check_#{hashed_identifier}\"\n        end",
    "comment": "Returns the name for a check constraint  type: - Any value, as long as it is unique - Constraint names are unique per table in Postgres, and, additionally, we can have multiple check constraints over a column So we use the (table, column, type) triplet as a unique name - e.g. we use 'max_length' when adding checks for text limits or 'not_null' when adding a NOT NULL constraint ",
    "label": "",
    "id": "2340"
  },
  {
    "raw_code": "def add_check_constraint(table, check, constraint_name, validate: true)\n          # Transactions would result in ALTER TABLE locks being held for the\n          # duration of the transaction, defeating the purpose of this method.\n          validate_not_in_transaction!(:add_check_constraint)\n\n          validate_check_constraint_name!(constraint_name)\n\n          if check_constraint_exists?(table, constraint_name)\n            warning_message = <<~MESSAGE\n              Check constraint was not created because it exists already\n              (this may be due to an aborted migration or similar)\n              table: #{table}, check: #{check}, constraint name: #{constraint_name}\n            MESSAGE\n\n            Gitlab::AppLogger.warn warning_message\n          else\n            # Only add the constraint without validating it\n            # Even though it is fast, ADD CONSTRAINT requires an EXCLUSIVE lock\n            # Use with_lock_retries to make sure that this operation\n            # will not timeout on tables accessed by many processes\n            with_lock_retries do\n              execute <<~SQL\n              ALTER TABLE #{table}\n              ADD CONSTRAINT #{constraint_name}\n              CHECK ( #{check} )\n              NOT VALID;\n              SQL\n            end",
    "comment": "Adds a check constraint to a table  This method is the generic helper for adding any check constraint More specialized helpers may use it (e.g. add_text_limit or add_not_null)  This method only requires minimal locking: - The constraint is added using NOT VALID This allows us to add the check constraint without validating it - The check will be enforced for new data (inserts) coming in - If `validate: true` the constraint is also validated Otherwise, validate_check_constraint() can be used at a later stage - Check comments on add_concurrent_foreign_key for more info  table  - The table the constraint will be added to check  - The check clause to add e.g. 'char_length(name) <= 5' or 'store IS NOT NULL' constraint_name - The name of the check constraint (otherwise auto-generated) Should be unique per table (not per column) validate - Whether to validate the constraint in this call ",
    "label": "",
    "id": "2341"
  },
  {
    "raw_code": "def copy_check_constraints(table, old, new, schema: nil)\n          raise 'copy_check_constraints can not be run inside a transaction' if transaction_open?\n\n          raise \"Column #{old} does not exist on #{table}\" unless column_exists?(table, old)\n\n          raise \"Column #{new} does not exist on #{table}\" unless column_exists?(table, new)\n\n          table_with_schema = schema.present? ? \"#{schema}.#{table}\" : table\n\n          check_constraints_for(table, old, schema: schema).each do |check_c|\n            validate = !(check_c[\"constraint_def\"].end_with? \"NOT VALID\")\n\n            # Normalize:\n            # - Old constraint definitions:\n            #    '(char_length(entity_path) <= 5500)'\n            # - Definitionss from pg_get_constraintdef(oid):\n            #    'CHECK ((char_length(entity_path) <= 5500))'\n            # - Definitions from pg_get_constraintdef(oid, pretty_bool):\n            #    'CHECK (char_length(entity_path) <= 5500)'\n            # - Not valid constraints: 'CHECK (...) NOT VALID'\n            # to a single format that we can use:\n            #    '(char_length(entity_path) <= 5500)'\n            check_definition = check_c[\"constraint_def\"]\n                                .sub(/^\\s*(CHECK)?\\s*\\({0,2}/, '(')\n                                .sub(/\\){0,2}\\s*(NOT VALID)?\\s*$/, ')')\n\n            constraint_name = if check_definition == \"(#{old} IS NOT NULL)\"\n                                not_null_constraint_name(table_with_schema, new)\n                              elsif check_definition.start_with? \"(char_length(#{old}) <=\"\n                                text_limit_name(table_with_schema, new)\n                              else\n                                check_constraint_name(table_with_schema, new, 'copy_check_constraint')\n                              end",
    "comment": "Copies all check constraints for the old column to the new column.  table - The table containing the columns. old - The old column. new - The new column. schema - The schema the table is defined for If it is not provided, then the current_schema is used",
    "label": "",
    "id": "2342"
  },
  {
    "raw_code": "def add_text_limit(table, column, limit, constraint_name: nil, validate: true)\n          add_check_constraint(\n            table,\n            \"char_length(#{column}) <= #{limit}\",\n            text_limit_name(table, column, name: constraint_name),\n            validate: validate\n          )\n        end",
    "comment": "Migration Helpers for adding limit to text columns",
    "label": "",
    "id": "2343"
  },
  {
    "raw_code": "def add_not_null_constraint(table, column, constraint_name: nil, validate: true)\n          if column_is_nullable?(table, column)\n            add_check_constraint(\n              table,\n              \"#{column} IS NOT NULL\",\n              not_null_constraint_name(table, column, name: constraint_name),\n              validate: validate\n            )\n          else\n            warning_message = <<~MESSAGE\n              NOT NULL check constraint was not created:\n              column #{table}.#{column} is already defined as `NOT NULL`\n            MESSAGE\n\n            Gitlab::AppLogger.warn warning_message\n          end",
    "comment": "Migration Helpers for managing not null constraints",
    "label": "",
    "id": "2344"
  },
  {
    "raw_code": "def check_constraints_for(table, column, schema: nil)\n          check_sql = <<~SQL\n            SELECT\n              ccu.table_schema as schema_name,\n              ccu.table_name as table_name,\n              ccu.column_name as column_name,\n              con.conname as constraint_name,\n              pg_get_constraintdef(con.oid) as constraint_def\n            FROM pg_catalog.pg_constraint con\n              INNER JOIN pg_catalog.pg_class rel\n                ON rel.oid = con.conrelid\n              INNER JOIN pg_catalog.pg_namespace nsp\n                ON nsp.oid = con.connamespace\n              INNER JOIN information_schema.constraint_column_usage ccu\n                ON con.conname = ccu.constraint_name\n                      AND nsp.nspname = ccu.constraint_schema\n                      AND rel.relname = ccu.table_name\n            WHERE  nsp.nspname = #{connection.quote(schema.presence || current_schema)}\n              AND rel.relname = #{connection.quote(table)}\n              AND ccu.column_name = #{connection.quote(column)}\n              AND con.contype = 'c'\n            ORDER BY constraint_name\n          SQL\n\n          connection.exec_query(check_sql)\n        end",
    "comment": "Returns an ActiveRecord::Result containing the check constraints defined for the given column.  If the schema is not provided, then the current_schema is used",
    "label": "",
    "id": "2345"
  },
  {
    "raw_code": "def jobs_by_migration_name\n          set_shared_model_connection do\n            Gitlab::Database::BackgroundMigration::BatchedMigration\n              .executable\n              .where('id > ?', from_id)\n              .to_h do |migration|\n              batching_strategy = migration.batch_class.new(connection: connection)\n\n              is_cursor = migration.cursor?\n\n              # Pretend every migration is a cursor migration. When actually running the job,\n              # we can unwrap the cursor if it is not.\n              cursor_columns = is_cursor ? migration.job_class.cursor_columns : [migration.column_name]\n\n              # Wrap the single result into an array (that we pretend is a cursor) if this\n              # is not a cursor migration. (next_min_value has an if check on cursor? and returns either array or int)\n              table_min_cursor = Array.wrap(migration.next_min_value)\n\n              ordering = cursor_columns.map { |c| { c => :desc } }\n\n              rows_ordered_backwards = define_batchable_model(migration.table_name, connection: connection)\n                                        .order(*ordering)\n              # If only one column, pluck.first returns a single value for that column instead of an array of\n              # all (1) column(s)\n              # So wrap the result for consistency between 1 and many columns\n              table_max_cursor = Array.wrap(rows_ordered_backwards.pick(*cursor_columns))\n\n              # variance is the portion of the batch range that we shrink between variance * 0 and variance * 1\n              # to pick actual batches to sample.\n\n              # Here we're going to do something that is explicitly WRONG, but good enough - we assume that we can\n              # just scale the first element of the cursor to get a reasonable percentage of the way through the table.\n              # This is really not true at all, but it's close enough for testing.\n              # For the rest of the components of our example cursors, we'll reuse parts of the end cursors for each\n              # batch for the start cursors of the next batch\n              variance = table_max_cursor[0] - table_min_cursor[0]\n\n              batch_first_elems = uniform_fractions.lazy.map { |frac| (variance * frac).to_i }\n\n              jobs_to_sample = Enumerator.new do |y|\n                completed_batches = []\n                # We construct the starting cursor from the end of the prev loop,\n                # or just the beginning of the table on the first loop\n                # This way, cursors for our batches start at interesting places in all of their positions\n                prev_end_cursor = table_min_cursor\n\n                loop do\n                  first_elem = batch_first_elems.next\n                  batch_start = [first_elem] + prev_end_cursor[1..]\n                  break if completed_batches.any? { |batch| batch.cover?(batch_start) }\n\n                  # The current block is lazily evaluated as part of the jobs_to_sample enumerable\n                  # so it executes after the enclosing using_connection block has already executed\n                  # Therefore we need to re-associate with the explicit connection again\n                  Gitlab::Database::SharedModel.using_connection(connection) do\n                    next_bounds = batching_strategy.next_batch(\n                      migration.table_name,\n                      migration.column_name,\n                      batch_min_value: is_cursor ? batch_start : batch_start[0],\n                      batch_size: migration.batch_size,\n                      job_class: migration.job_class,\n                      job_arguments: migration.job_arguments\n                    )\n\n                    # If no rows match, the next_bounds are nil.\n                    # This will only happen if there are zero rows to match from the current sampling point to the end\n                    # of the table\n                    # Simulate the approach in the actual background migration worker by not sampling a batch\n                    # from this range.\n                    # (The actual worker would finish the migration, but we may find batches that can be sampled\n                    # elsewhere in the table)\n                    if next_bounds.nil?\n                      # If the migration has no work to do across the entire table, sampling can get stuck\n                      # in a loop if we don't mark the attempted batches as completed\n                      # We need to guess a size for this. The batch size of the migration is way too big in all\n                      # cases with a 2-element or more cursor, but it doesn't really matter so we just guess that.\n                      synthetic_cursor_offset = migration.batch_size\n                      batch_end = batch_start.dup\n                      batch_end[0] += synthetic_cursor_offset\n                      completed_batches << (batch_start..batch_end)\n                      next\n                    end",
    "comment": "rubocop:disable Metrics/AbcSize -- This method is temporarily more complex while it deals with both cursor and non-cursor migrations. The complexity will significantly decrease when non-cursor migration support is removed.",
    "label": "",
    "id": "2346"
  },
  {
    "raw_code": "def run_job(job)\n          set_shared_model_connection do\n            Gitlab::Database::BackgroundMigration::BatchedMigrationWrapper.new(connection: connection).perform(job)\n          end",
    "comment": "rubocop:enable Metrics/AbcSize",
    "label": "",
    "id": "2347"
  },
  {
    "raw_code": "def queue_batched_background_migration( # rubocop:disable Metrics/ParameterLists\n          job_class_name,\n          batch_table_name,\n          batch_column_name,\n          *job_arguments,\n          job_interval: BATCH_MIN_DELAY,\n          batch_min_value: BATCH_MIN_VALUE,\n          batch_max_value: nil,\n          batch_class_name: BATCH_CLASS_NAME,\n          batch_size: BATCH_SIZE,\n          pause_ms: MINIMUM_PAUSE_MS,\n          max_batch_size: nil,\n          sub_batch_size: SUB_BATCH_SIZE,\n          gitlab_schema: nil\n        )\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_dml_mode!\n\n          gitlab_schema ||= gitlab_schema_from_context\n          # Version of the migration that queued the BBM, this is used to establish dependencies\n          queued_migration_version = version\n\n          Gitlab::Database::BackgroundMigration::BatchedMigration.reset_column_information\n\n          if Gitlab::Database::BackgroundMigration::BatchedMigration.for_configuration(gitlab_schema, job_class_name, batch_table_name, batch_column_name, job_arguments, include_compatible: true).exists?\n            Gitlab::AppLogger.warn \"Batched background migration not enqueued because it already exists: \" \\\n              \"job_class_name: #{job_class_name}, table_name: #{batch_table_name}, column_name: #{batch_column_name}, \" \\\n              \"job_arguments: #{job_arguments.inspect}\"\n            return\n          end",
    "comment": "Creates a batched background migration for the given table. A batched migration runs one job at a time, computing the bounds of the next batch based on the current migration settings and the previous batch bounds. Each job's execution status is tracked in the database as the migration runs. The given job class must be present in the Gitlab::BackgroundMigration module, and the batch class (if specified) must be present in the Gitlab::BackgroundMigration::BatchingStrategies module.  If a migration with same job_class_name, table_name, column_name, and job_arguments already exists, this helper will log a warning and not create a new one.  job_class_name - The background migration job class as a string batch_table_name - The name of the table the migration will batch over batch_column_name - The name of the column the migration will batch over job_arguments - Extra arguments to pass to the job instance when the migration runs job_interval - The pause interval between each job's execution, minimum of 2 minutes, defaults to BATCH_MIN_DELAY batch_min_value - The value in the column the batching will begin at batch_max_value - The value in the column the batching will end at, defaults to `SELECT MAX(batch_column)` batch_class_name - The name of the class that will be called to find the range of each next batch batch_size - The maximum number of rows per job sub_batch_size - The maximum number of rows processed per \"iteration\" within the job  *Returns the created BatchedMigration record*  Example:  queue_batched_background_migration( 'CopyColumnUsingBackgroundMigrationJob', :events, :id, other_job_arguments: ['column1', 'column2'])  Where the background migration exists:  class Gitlab::BackgroundMigration::CopyColumnUsingBackgroundMigrationJob def perform(start_id, end_id, batch_table, batch_column, sub_batch_size, *other_args) # do something end end",
    "label": "",
    "id": "2348"
  },
  {
    "raw_code": "def delete_batched_background_migration(job_class_name, table_name, column_name, job_arguments)\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_dml_mode!\n\n          Gitlab::Database::BackgroundMigration::BatchedMigration.reset_column_information\n\n          Gitlab::Database::BackgroundMigration::BatchedMigration\n            .for_configuration(\n              gitlab_schema_from_context, job_class_name, table_name, column_name, job_arguments,\n              include_compatible: true\n            ).delete_all\n        end",
    "comment": "Deletes batched background migration for the given configuration.  job_class_name - The background migration job class as a string table_name - The name of the table the migration iterates over column_name - The name of the column the migration will batch over job_arguments - Migration arguments  Example:  delete_batched_background_migration( 'CopyColumnUsingBackgroundMigrationJob', :events, :id, ['column1', 'column2'])",
    "label": "",
    "id": "2349"
  },
  {
    "raw_code": "def assign_attributes_safely(migration, max_batch_size, batch_table_name, gitlab_schema, queued_migration_version)\n          # We keep track of the estimated number of tuples in 'total_tuple_count' to reason later\n          # about the overall progress of a migration.\n          safe_attributes_value = {\n            max_batch_size: max_batch_size,\n            total_tuple_count: Gitlab::Database::SharedModel.using_connection(connection) do\n              Gitlab::Database::PgClass.for_table(batch_table_name)&.cardinality_estimate\n            end,\n            gitlab_schema: gitlab_schema,\n            queued_migration_version: queued_migration_version\n          }\n\n          # rubocop:disable GitlabSecurity/PublicSend\n          safe_attributes_value.each do |safe_attribute, value|\n            migration.public_send(\"#{safe_attribute}=\", value) if migration.respond_to?(safe_attribute)\n          end",
    "comment": "Below `BatchedMigration` attributes were introduced after the initial `batched_background_migrations` table was created, so any migrations that ran relying on initial table schema would not know about columns introduced later on because this model is not isolated in migrations, which is why we need to check for existence of these columns first.",
    "label": "",
    "id": "2350"
  },
  {
    "raw_code": "def sidekiq_remove_jobs(\n          job_klasses:,\n          times_in_a_row: DEFAULT_TIMES_IN_A_ROW,\n          max_attempts: DEFAULT_MAX_ATTEMPTS\n        )\n          kwargs = { times_in_a_row: times_in_a_row, max_attempts: max_attempts }\n\n          if transaction_open?\n            raise 'sidekiq_remove_jobs can not be run inside a transaction, ' \\\n                  'you can disable transactions by calling disable_ddl_transaction! ' \\\n                  'in the body of your migration class'\n          end",
    "comment": "Probabilistically removes job_klasses from their specific queues, the retry set and the scheduled set.  If jobs are still being processed at the same time, then there is a small chance it will not remove all instances of job_klass. To minimize this risk, it repeatedly removes matching jobs from each until nothing is removed twice in a row.  Before calling this method, you should make sure that job_klass is no longer being scheduled within the running application.",
    "label": "",
    "id": "2351"
  },
  {
    "raw_code": "def migrate_across_instance(queue_from, to, src_store, dst_stores)\n          _, src_pool = Gitlab::SidekiqSharding::Router.get_shard_instance(src_store)\n          buffer_queue_name = \"migration_buffer:queue:#{queue_from}\"\n\n          while Sidekiq::Client.via(src_pool) { sidekiq_queue_length(queue_from) } > 0\n            job = Sidekiq::Client.via(src_pool) do\n              Sidekiq.redis do |c|\n                c.rpoplpush(\"queue:#{queue_from}\", buffer_queue_name)\n              end",
    "comment": "cross instance transfers are not atomic and data loss is possible",
    "label": "",
    "id": "2352"
  },
  {
    "raw_code": "def delete_jobs_for(set:, kwargs:, job_klasses:)\n          until_equal_to(0, **kwargs) do\n            set.count do |job|\n              job_klasses.include?(job.klass) && job.delete\n            end",
    "comment": "Handle the \"jobs deleted\" tracking that is needed in order to track whether a job was deleted or not.",
    "label": "",
    "id": "2353"
  },
  {
    "raw_code": "def until_equal_to(target, times_in_a_row:, max_attempts:)\n          streak = 0\n\n          result = { attempts: 0, success: false }\n\n          1.upto(max_attempts) do |current_attempt|\n            # yield's return value is a count of \"jobs_deleted\"\n            if yield == target\n              streak += 1\n            elsif streak > 0\n              streak = 0\n            end",
    "comment": "Control how many times in a row you want to see a job deleted 0 times. The idea is that if you see 0 jobs deleted x number of times in a row you've *likely* covered the case in which the queue was mutating while this was running.",
    "label": "",
    "id": "2354"
  },
  {
    "raw_code": "def rename_column(table_name, column_name, column2_name)\n          clear_cache!\n          @migration_context.execute <<~SQL\n            ALTER TABLE #{quote_table_name(table_name)}\n              RENAME COLUMN #{quote_column_name(column_name)} TO #{quote_column_name(column2_name)}\n          SQL\n        end",
    "comment": "Rails' `rename_column` will rename related indexes using a format e.g. `index_{TABLE_NAME}_on_{KEY1}_and_{KEY2}` This will break the migration if the formated index name is longer than 63 chars, e.g. `index_ci_pipeline_variables_on_pipeline_id_convert_to_bigint_and_key` Therefore, we need to duplicate what Rails has done here without the part renaming related indexes",
    "label": "",
    "id": "2355"
  },
  {
    "raw_code": "def queue_redis_migration_job(job_name)\n          RedisMigrationWorker.fetch_migrator!(job_name)\n          RedisMigrationWorker.perform_async(job_name, SCAN_START_CURSOR)\n        end",
    "comment": "Check if the migration exists before enqueueing the worker",
    "label": "",
    "id": "2356"
  },
  {
    "raw_code": "def enable_runner_backoff?\n              !!migration.try(:enable_runner_backoff?)\n            end",
    "comment": "Regular AR migrations don't have this, only ones inheriting from Gitlab::Database::Migration have",
    "label": "",
    "id": "2357"
  },
  {
    "raw_code": "def backfill(primary_key:, batch_size:, sub_batch_size:, pause_ms:, job_interval:)\n            verify_table_and_columns_exist!\n            migration.queue_batched_background_migration(\n              'CopyColumnUsingBackgroundMigrationJob',\n              table,\n              primary_key,\n              *column_pair,\n              job_interval: job_interval,\n              pause_ms: pause_ms,\n              batch_size: batch_size,\n              sub_batch_size: sub_batch_size\n            )\n          end",
    "comment": "We will queue the job and let the job to check if columns exist and continue with the copy",
    "label": "",
    "id": "2358"
  },
  {
    "raw_code": "def initialization_for_all_integer_ids_not_required?\n            !migration.respond_to?(:milestone) ||\n              Gitlab::VersionInfo.parse_from_milestone(migration.milestone) <\n                Gitlab::VersionInfo.parse_from_milestone(MIN_MILESTONE)\n          end",
    "comment": "This check can be removed once we convert all integer IDs to bigint in https://gitlab.com/gitlab-org/gitlab/-/issues/465805",
    "label": "",
    "id": "2359"
  },
  {
    "raw_code": "def create_trigger\n            migration.install_rename_triggers(table, *column_pair)\n          end",
    "comment": "Since trigger checks if bigint column for conversion exists or not, the full list of columns can be used to ensure same trigger name can be used across all different instances.",
    "label": "",
    "id": "2360"
  },
  {
    "raw_code": "def requires_tracking?(parsed)\n            return false if parsed.raw.size < MIN_QUERY_SIZE\n\n            EVENT_NAMES.include?(parsed.event_name)\n          end",
    "comment": "Skips queries containing less than 10000 chars or any other events than +load+ and +pluck+",
    "label": "",
    "id": "2361"
  },
  {
    "raw_code": "def self.suppress_schema_issues_for_decomposed_tables\n          Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.with_suppressed do\n            Gitlab::Database::QueryAnalyzers::GitlabSchemasValidateConnection.with_suppressed do\n              yield\n            end",
    "comment": "During database decomposition, db migrations using tables that will be decomposed will begin to contravene their configuration for intended gitlab_schema and database connection. As these migrations already exist, ideally they should be finalized and removed prior to decomposition. In this situations, it's necessary to suppress warnings related to their incorrect connection and schema to progress our CI pipelines.",
    "label": "",
    "id": "2362"
  },
  {
    "raw_code": "def dml_tables(parsed)\n            select_tables = self.dml_from_create_view?(parsed) ? [] : parsed.pg.select_tables\n\n            select_tables + parsed.pg.dml_tables\n          end",
    "comment": "There is a special case where CREATE VIEW DDL statement can include DML statements. For this case, +select_tables+ should be empty, to avoid false positives.  @example CREATE VIEW issues AS SELECT * FROM tickets",
    "label": "",
    "id": "2363"
  },
  {
    "raw_code": "def analyze(parsed)\n            # This analyzer requires the PgQuery parsed query to be present\n            return unless parsed.pg\n\n            select_tables = QueryAnalyzerHelpers.dml_from_create_view?(parsed) ? [] : parsed.pg.select_tables\n            tables = select_tables + parsed.pg.dml_tables\n            table_schemas = ::Gitlab::Database::GitlabSchema.table_schemas!(tables)\n            return if table_schemas.empty?\n\n            allowed_schemas = ::Gitlab::Database.gitlab_schemas_for_connection(parsed.connection)\n            return unless allowed_schemas\n\n            invalid_schemas = table_schemas - allowed_schemas\n\n            return if invalid_schemas.empty?\n\n            schema_list = table_schemas.sort.join(',')\n\n            message = \"The query tried to access #{tables} (of #{schema_list}) \"\n            message += \"which is outside of allowed schemas (#{allowed_schemas}) \"\n            message += \"for the current connection '#{Gitlab::Database.db_config_name(parsed.connection)}'\"\n\n            raise CrossSchemaAccessError, message\n          end",
    "comment": "There is a special case where CREATE VIEW DDL statement can include DML statements. For this case, +select_tables+ should be empty, to keep the schema consistent between +main+ and +ci+.  @example CREATE VIEW issues AS SELECT * FROM tickets",
    "label": "",
    "id": "2364"
  },
  {
    "raw_code": "def self.allow_cross_database_modification_within_transaction(url:, &blk)\n          self.with_suppressed(true, &blk)\n        end",
    "comment": "This method will allow cross database modifications within the block Example:  allow_cross_database_modification_within_transaction(url: 'url-to-an-issue') do create(:build) # inserts ci_build and project record in one transaction end",
    "label": "",
    "id": "2365"
  },
  {
    "raw_code": "def self.with_cross_database_modification_prevented(&blk)\n          self.with_suppressed(false, &blk)\n        end",
    "comment": "This method will prevent cross database modifications within the block if it was allowed previously",
    "label": "",
    "id": "2366"
  },
  {
    "raw_code": "def self.temporary_ignore_tables_in_transaction(tables, url:, &blk)\n          return yield unless context&.dig(:ignored_tables)\n\n          begin\n            prev_ignored_tables = context[:ignored_tables]\n            context[:ignored_tables] = prev_ignored_tables + tables.map(&:to_s)\n            yield\n          ensure\n            context[:ignored_tables] = prev_ignored_tables\n          end",
    "comment": "This method will temporary ignore the given tables in a current transaction This is meant to disable `PreventCrossDB` check for some well known failures",
    "label": "",
    "id": "2367"
  },
  {
    "raw_code": "def self.analyze(parsed)\n          # This analyzer requires the PgQuery parsed query to be present\n          return unless parsed.pg\n\n          database = ::Gitlab::Database.db_config_name(parsed.connection)\n          sql = parsed.sql\n\n          # We ignore BEGIN in tests as this is the outer transaction for\n          # DatabaseCleaner\n          if self.transaction_begin?(parsed)\n            context[:transaction_depth_by_db][database] += 1\n\n            return\n          elsif self.transaction_end?(parsed)\n            context[:transaction_depth_by_db][database] -= 1\n            if context[:transaction_depth_by_db][database] == 0\n              context[:modified_tables_by_db][database].clear\n              clear_queries\n\n              # Attempt to troubleshoot https://gitlab.com/gitlab-org/gitlab/-/issues/351531\n              ::CrossDatabaseModification::TransactionStackTrackRecord.log_gitlab_transactions_stack(action: :end_of_transaction)\n            elsif context[:transaction_depth_by_db][database] < 0\n              context[:transaction_depth_by_db][database] = 0\n              raise CrossDatabaseModificationAcrossUnsupportedTablesError, \"Misaligned cross-DB transactions discovered at query #{sql}. This could be a bug in #{self.class} or a valid issue to investigate. Read more at https://docs.gitlab.com/ee/development/database/multiple_databases.html#removing-cross-database-transactions .\"\n            end",
    "comment": "rubocop:disable Metrics/AbcSize",
    "label": "",
    "id": "2368"
  },
  {
    "raw_code": "def self.transaction_begin?(parsed)\n          # We ignore BEGIN or START in tests\n          unless Rails.env.test?\n            return true if transaction_stmt?(parsed, :TRANS_STMT_BEGIN)\n            return true if transaction_stmt?(parsed, :TRANS_STMT_START)\n          end",
    "comment": "rubocop:enable Metrics/AbcSize",
    "label": "",
    "id": "2369"
  },
  {
    "raw_code": "def self.transaction_stmt?(parsed, kind)\n          parsed.pg.tree.stmts.map(&:stmt).any? do |stmt|\n            stmt.node == :transaction_stmt && stmt.transaction_stmt.kind == kind\n          end",
    "comment": "Known kinds: https://github.com/pganalyze/pg_query/blob/f6588703deb9d7a94b87b34b7c3bab240087fbc4/ext/pg_query/include/nodes/parsenodes.h#L3050",
    "label": "",
    "id": "2370"
  },
  {
    "raw_code": "def initialize(node, inherited_cte_references = {})\n            @node = node\n            @cte_references = CommonTableExpressions.references(node, inherited_cte_references)\n            from_references = Froms.references(node, cte_references)\n            @all_references = from_references.merge(cte_references)\n          end",
    "comment": "@param [PgQuery::SelectStmt] node The PgQuery node of the select statement. @param [Hash] inherited_cte_references CTE References available to the select statement.",
    "label": "",
    "id": "2371"
  },
  {
    "raw_code": "def types\n            if set_operator?\n              resolve_set_operator_select_types\n            else\n              resolve_normal_select_types\n            end",
    "comment": "returns Set of Types.  STATIC - queries that don't require a database schema lookup. E.g. `SELECT users.id FROM users` DYNAMIC - queries that require a database schema lookup. E.g. `SELECT users.* FROM users` INVALID - set operator queries that mix static and dynamic queries.",
    "label": "",
    "id": "2372"
  },
  {
    "raw_code": "def resolve_normal_select_types\n            # Cross reference resolved sources with what is requested by the SELECT.\n            types = Columns.types(self)\n\n            # Mixed dynamic and static queries can be normalized to simply dynamic queries for the purposes of\n            # detecting mismatched set operator parts.\n            types.delete(Type::STATIC) if types.include?(Type::DYNAMIC)\n\n            types\n          end",
    "comment": "Standard SELECT, not a set operator (UNION/INTERSECT/EXCEPT)",
    "label": "",
    "id": "2373"
  },
  {
    "raw_code": "def resolve_set_operator_select_types\n            types = Set.new\n\n            # Recurse each set operator part as a SELECT statement.\n            # select statement part => type\n            set_operator_parts do |part|\n              types += SelectStmt.new(part, cte_references).types\n            end",
    "comment": "Set operator (UNION/INTERSECT/EXCEPT)",
    "label": "",
    "id": "2374"
  },
  {
    "raw_code": "def types(select_stmt)\n              # Forward through any errors when the column refers to a part of the SQL query that is known to include\n              # errors. For example, the column may refer to a column from a CTE that was invalid.\n              return Set.new([Type::INVALID]) if References.errors?(select_stmt.all_references)\n\n              types = Set.new\n\n              # Resolve the type of reference for each target in the select statement.\n              target_list = select_stmt.node.target_list\n              targets = target_list.map(&:res_target)\n              targets.each do |target|\n                target_type = get_target_type(target, select_stmt)\n\n                # A NULL target is of the form:\n                # SELECT NULL::namespaces FROM namespaces\n                types += if Targets.null?(target)\n                           # Maintain any errors but otherwise ignore this target.\n                           target_type & [Type::INVALID]\n                         else\n                           target_type\n                         end",
    "comment": "Determine the type of each column in the select statement. Returns a Set object containing a Types enum. When an error is found parsing will return immediately.",
    "label": "",
    "id": "2375"
  },
  {
    "raw_code": "def reference_names(target, select_stmt)\n              # Parse all targets to determine what is referenced.\n              fields = fields(target)\n              case fields.count\n              when 0\n                literal_ref_names(target, select_stmt)\n              when 1\n                unqualified_ref_names(fields, select_stmt)\n              else\n                # The target is qualified such as SELECT reference.id\n                field_ref = fields[fields.count - 2]\n                [field_ref.string.sval]\n              end",
    "comment": "Return the reference names used by the given target.  For example: `SELECT users.id` would return ['users'] `SELECT * FROM users, namespaces` would return ['users', 'namespaces']",
    "label": "",
    "id": "2376"
  },
  {
    "raw_code": "def a_star?(target)\n              Node.locate_descendant(target, :a_star)\n            end",
    "comment": "True when `SELECT *`",
    "label": "",
    "id": "2377"
  },
  {
    "raw_code": "def null?(target)\n              target&.val&.type_cast&.arg&.a_const&.isnull\n            end",
    "comment": "Null targets are used to produce \"polymorphic\" query result sets that can be aggregated through a UNION without having to worry about mismatched columns.  A null target would be something like: SELECT NULL::namespaces FROM namespaces",
    "label": "",
    "id": "2378"
  },
  {
    "raw_code": "def references(node, cte_refs)\n              refs = {}\n\n              return refs unless node\n\n              node.from_clause.each do |from|\n                range_var = Node.dig(from, :range_var)\n                range_sq = Node.dig(from, :range_subselect)\n\n                if range_var\n                  # FROM some_table\n                  # FROM some_table some_alias\n                  refs.merge!(range_var_reference(range_var, cte_refs))\n                elsif Node.dig(from, :join_expr)\n                  # FROM some_table INNER JOIN other_table\n                  range_vars = Node.locate_descendants(from, :range_var)\n                  range_vars.each do |range_var|\n                    refs.merge!(range_var_reference(range_var, cte_refs))\n                  end",
    "comment": "Parse the FROM part of the SELECT. Construct a mapping of FROM names to their PgQuery node. Recurse any sub-queries and resolve to a Set of dynamic/static/error.  Whenever a node is aliased, use the alias name as it's reference and ignore it's original name.  For example, given:  SELECT id FROM namespaces ns  Return a Hash of { 'ns' => NodeObject }  @param [PgQuery::Node] node The PgQuery SELECT statement node containing the CTEs. @param [References] cte_refs Inherited CTEs from scopes that wrap this SELECT statement.  @return [Hash] name of from references mapped to the node that defines their value, or Set if already resolved.",
    "label": "",
    "id": "2379"
  },
  {
    "raw_code": "def descendants(node, filter: DEFAULT_FIELD_FILTER, &blk)\n              if blk\n                children(node, filter: filter) do |child_node, child_field|\n                  yield(child_node, child_field)\n\n                  descendants(child_node, filter: filter, &blk)\n                end",
    "comment": "Recurse through children. The block will yield the child node and the name of that node. Calling without a block will return an Enumerator.",
    "label": "",
    "id": "2380"
  },
  {
    "raw_code": "def locate_descendant(node, field, filter: DEFAULT_FIELD_FILTER)\n              descendants(node, filter: filter).find { |_, child_field| child_field == field }&.first\n            end",
    "comment": "Return the first node that matches the field.",
    "label": "",
    "id": "2381"
  },
  {
    "raw_code": "def locate_descendants(node, field, filter: DEFAULT_FIELD_FILTER)\n              descendants(node, filter: filter).select { |_, child_field| child_field == field }.map(&:first)\n            end",
    "comment": "Return all nodes that match the field.",
    "label": "",
    "id": "2382"
  },
  {
    "raw_code": "def dig(node, *attrs)\n              obj = node\n              attrs.each do |attr|\n                if obj.respond_to?(attr)\n                  obj = obj.public_send(attr) # rubocop:disable GitlabSecurity/PublicSend\n                else\n                  obj = nil\n                  break\n                end",
    "comment": "Like Hash#dig, traverse attributes in sequential order and return the final value. Return nil if any of the fields are not available.",
    "label": "",
    "id": "2383"
  },
  {
    "raw_code": "def children(node, filter: DEFAULT_FIELD_FILTER, &_blk)\n              attributes = case node\n                           when Google::Protobuf::MessageExts\n                             descriptor_fields(node.class.descriptor)\n                           when Google::Protobuf::RepeatedField\n                             node.count.times.to_a\n                           end",
    "comment": "Interface with a PgQuery result as though it was a tree node. All elements in a PgQuery result are ancestors of Google::Protobuf::AbstractMessage  Based off PgQuery's treewalker https://github.com/pganalyze/pg_query/blob/main/lib/pg_query/treewalker.rb",
    "label": "",
    "id": "2384"
  },
  {
    "raw_code": "def resolved(refs)\n              refs.select { |_name, ref| ref.is_a?(Set) }\n            end",
    "comment": "All references that have already been parsed to determine static/dynamic/error state. @param [Hash] refs A Hash of reference names mapped to the parse tree node or resolved Set of Types.",
    "label": "",
    "id": "2385"
  },
  {
    "raw_code": "def unresolved(refs)\n              refs.select { |_name, ref| unresolved?(ref) }\n            end",
    "comment": "All references that have not been parsed to determine static/dynamic/error state. @param [Hash] refs A Hash of reference names mapped to the parse tree node or resolved Set of Types.",
    "label": "",
    "id": "2386"
  },
  {
    "raw_code": "def errors?(refs)\n              resolved(refs).any? { |_, values| values.include?(Type::INVALID) }\n            end",
    "comment": "Whether any currently resolved references have resulted in an error state. @param [Hash] refs A Hash of reference names mapped to the parse tree node or resolved Set of Types.",
    "label": "",
    "id": "2387"
  },
  {
    "raw_code": "def references(node, cte_refs)\n              return cte_refs if node&.with_clause.nil?\n\n              refs = cte_refs.dup\n\n              node.with_clause.ctes.each do |cte|\n                cte_name = name(cte)\n                cte_select_stmt = select_stmt(cte)\n\n                # Resolve the CTE type to dynamic/static/error.\n                refs[cte_name] = if node.with_clause.recursive\n                                   # Recursive CTEs need special handling to avoid infinite loops.\n                                   recursive_refs(cte_refs, cte_name, cte_select_stmt)\n                                 else\n                                   SelectStmt.new(cte_select_stmt, cte_refs).types\n                                 end",
    "comment": "Convert CTEs available within this SELECT statement into a set of References.  @param [PgQuery::Node] node The PgQuery SELECT statement node containing the CTEs. @param [References] cte_refs Inherited CTEs from scopes that wrap this SELECT statement.",
    "label": "",
    "id": "2388"
  },
  {
    "raw_code": "def recursive_refs(cte_refs, cte_name, select_stmt)\n              # Resolve the non-recursive term before the recursive term.\n              larg_select_stmt = SelectStmt.new(select_stmt.larg, cte_refs)\n              larg_type = larg_select_stmt.types\n              new_cte_refs = cte_refs.merge({ cte_name => larg_type })\n\n              # Now we can resolve the recursive side.\n              rarg_type = SelectStmt.new(select_stmt.rarg, new_cte_refs).types\n\n              final_type = larg_type | rarg_type\n              if final_type.count > 1\n                final_type | [Type::INVALID]\n              else\n                final_type\n              end",
    "comment": "Return whether the recursive CTE is dynamic/static/error.",
    "label": "",
    "id": "2389"
  },
  {
    "raw_code": "def after_adding_partitions\n          if different_connection_names?\n            Gitlab::AppLogger.warn(\n              message: 'Skipping changing column default because connections mismatch',\n              event: :partition_manager_after_adding_partitions_connection_mismatch,\n              model_connection_name: Gitlab::Database.db_config_name(model.connection),\n              shared_connection_name: Gitlab::Database.db_config_name(Gitlab::Database::SharedModel.connection),\n              table_name: model.table_name\n            )\n\n            return\n          end",
    "comment": "The partition manager is initialized with both connections and creates partitions in both databases, but here we change the default on the model's connection, meaning that it will not respect the manager's connection config so we need to check that it's changing the default only when called with the model's connection. Also since we prevent writes in the other database, we should not change the default there. ",
    "label": "",
    "id": "2390"
  },
  {
    "raw_code": "def create_partition_tables(partitions)\n          partitions.each do |partition|\n            connection.execute(partition.to_create_sql)\n          end",
    "comment": "Create all partition tables (doesn't take any lock on parent)",
    "label": "",
    "id": "2391"
  },
  {
    "raw_code": "def attach_partition_tables(partitions)\n          partitions.each do |partition|\n            connection.execute(partition.to_attach_sql)\n            process_created_partition(partition)\n          end",
    "comment": "Attach all partitions (takes SHARE UPDATE EXCLUSIVE lock)",
    "label": "",
    "id": "2392"
  },
  {
    "raw_code": "def drop_foreign_key_if_present(detached_partition, foreign_key)\n          # It is important to only drop one foreign key per transaction.\n          # Dropping a foreign key takes an ACCESS EXCLUSIVE lock on both tables participating in the foreign key.\n\n          partition_identifier = detached_partition.fully_qualified_table_name\n          with_lock_retries do\n            connection.transaction(requires_new: false) do\n              next unless try_lock_detached_partition(detached_partition.id)\n\n              # Another process may have already dropped this foreign key\n              next unless PostgresForeignKey.by_constrained_table_identifier(partition_identifier).where(name: foreign_key.name).exists?\n\n              connection.execute(\"ALTER TABLE #{connection.quote_table_name(partition_identifier)} DROP CONSTRAINT #{connection.quote_table_name(foreign_key.name)}\")\n\n              Gitlab::AppLogger.info(message: \"Dropped foreign key for previously detached partition\",\n                partition_name: detached_partition.table_name,\n                referenced_table_name: foreign_key.referenced_table_identifier,\n                foreign_key_name: foreign_key.name)\n            end",
    "comment": "Drops the given foreign key for the given detached partition, but only if another process has not already detached the partition first. This method must be safe to call even if the associated partition table has already been detached, as it could be called by multiple processes at once.",
    "label": "",
    "id": "2393"
  },
  {
    "raw_code": "def missing_partitions\n          desired_partitions - current_partitions\n        end",
    "comment": "Check the currently existing partitions and determine which ones are missing",
    "label": "",
    "id": "2394"
  },
  {
    "raw_code": "def sequences_owned_by(table_name)\n            sequence_data = connection.exec_query(<<~SQL, nil, [table_name])\n              SELECT seq_pg_class.relname AS seq_name,\n                     dep_pg_class.relname AS table_name,\n                     pg_attribute.attname AS col_name\n              FROM pg_class seq_pg_class\n                   INNER JOIN pg_depend ON seq_pg_class.oid = pg_depend.objid\n                   INNER JOIN pg_class dep_pg_class ON pg_depend.refobjid = dep_pg_class.oid\n                   INNER JOIN pg_attribute ON dep_pg_class.oid = pg_attribute.attrelid\n                                           AND pg_depend.refobjsubid = pg_attribute.attnum\n              WHERE pg_depend.classid = 'pg_class'::regclass\n                AND pg_depend.refclassid = 'pg_class'::regclass\n                AND seq_pg_class.relkind = 'S'\n                AND dep_pg_class.relname = $1\n            SQL\n\n            sequence_data.map do |seq_info|\n              name, column_name = seq_info.values_at('seq_name', 'col_name')\n              { name: name, column_name: column_name }\n            end",
    "comment": "TODO: https://gitlab.com/gitlab-org/gitlab/-/issues/373887",
    "label": "",
    "id": "2395"
  },
  {
    "raw_code": "def locking_order_for(tables)\n            tables = Array.wrap(tables)\n            assert_table_names_unqualified!(tables)\n\n            @table_locking_order.intersection(tables.map(&:to_s))\n          end",
    "comment": "Sorts and subsets `tables` to the tables that were explicitly requested for locking in the order that that locking was requested.",
    "label": "",
    "id": "2396"
  },
  {
    "raw_code": "def missing_partitions\n            desired_partitions - current_partitions\n          end",
    "comment": "Check the currently existing partitions and determine which ones are missing",
    "label": "",
    "id": "2397"
  },
  {
    "raw_code": "def missing_partitions\n            raise NotImplementedError\n          end",
    "comment": "Check the currently existing partitions and determine which ones are missing",
    "label": "",
    "id": "2398"
  },
  {
    "raw_code": "def missing_partitions\n            desired_partitions - current_partitions\n          end",
    "comment": "Check the currently existing partitions and determine which ones are missing",
    "label": "",
    "id": "2399"
  },
  {
    "raw_code": "def relevant_range\n            first_partition = current_partitions.min\n\n            if first_partition\n              # Case 1: First partition starts with MINVALUE, i.e. from is nil -> start with first real partition\n              # Case 2: Rather unexpectedly, first partition does not start with MINVALUE, i.e. from is not nil\n              #         In this case, use first partition beginning as a start\n              min_date = first_partition.from || first_partition.to\n            end",
    "comment": "This determines the relevant time range for which we expect to have data (and therefore need to create partitions for).  Note: We typically expect the first partition to be half-unbounded, i.e. to start from MINVALUE to a specific date `x`. The range returned does not include the range of the first, half-unbounded partition.",
    "label": "",
    "id": "2400"
  },
  {
    "raw_code": "def print_error(msg)\n          puts Rainbow(msg).red\n          exit 1 # rubocop:disable Rails/Exit -- used only in rake tasks\n        end",
    "comment": "rubocop:disable Rails/Output -- We do want to write to stdout",
    "label": "",
    "id": "2401"
  },
  {
    "raw_code": "def perform(batch_tracking_record)\n          start_tracking_execution(batch_tracking_record)\n\n          execute_batch(batch_tracking_record)\n\n          batch_tracking_record.succeed!\n        rescue SubBatchTimeoutError => exception\n          caused_by = exception.caused_by\n          batch_tracking_record.failure!(error: caused_by, from_sub_batch: true)\n\n          raise caused_by\n        rescue Exception => error # rubocop:disable Lint/RescueException\n          batch_tracking_record.failure!(error: error)\n\n          raise\n        ensure\n          metrics.track(batch_tracking_record)\n        end",
    "comment": "Wraps the execution of a batched_background_migration.  Updates the job's tracking records with the status of the migration when starting and finishing execution, and optionally saves batch_metrics the migration provides, if any are given.  @info The job's batch_metrics are serialized to JSON for storage.  @info Track exceptions that could happen when processing sub-batches through +Gitlab::BackgroundMigration::SubBatchTimeoutException+",
    "label": "",
    "id": "2402"
  },
  {
    "raw_code": "def run_migration_job(active_migration)\n          if next_batched_job = find_or_create_next_batched_job(active_migration)\n            migration_wrapper.perform(next_batched_job)\n\n            adjust_migration(active_migration)\n\n            active_migration.failure! if next_batched_job.failed? && active_migration.should_stop?\n          else\n            finish_active_migration(active_migration)\n          end",
    "comment": "Runs the next batched_job for a batched_background_migration.  The batch bounds of the next job are calculated at runtime, based on the migration configuration and the bounds of the most recently created batched_job. Updating the migration configuration will cause future jobs to use the updated batch sizes.  The job instance will automatically receive a set of arguments based on the migration configuration. For more details, see the BatchedMigrationWrapper class.  Note that this method is primarily intended to called by a scheduled worker.",
    "label": "",
    "id": "2403"
  },
  {
    "raw_code": "def run_entire_migration(migration)\n          unless Rails.env.development? || Rails.env.test?\n            raise 'this method is not intended for use in real environments'\n          end",
    "comment": "Runs all remaining batched_jobs for a batched_background_migration.  This method is intended to be used in a test/dev environment to execute the background migration inline. It should NOT be used in a real environment for any non-trivial migrations.",
    "label": "",
    "id": "2404"
  },
  {
    "raw_code": "def finalize(job_class_name, table_name, column_name, job_arguments)\n          migration = BatchedMigration.find_for_configuration(\n            Gitlab::Database.gitlab_schemas_for_connection(connection),\n            job_class_name, table_name, column_name, job_arguments\n          )\n\n          configuration = {\n            job_class_name: job_class_name,\n            table_name: table_name,\n            column_name: column_name,\n            job_arguments: job_arguments\n          }\n\n          if migration.nil?\n            Gitlab::AppLogger.warn \"Could not find batched background migration for the given configuration: #{configuration}\"\n          elsif migration.finished?\n            Gitlab::AppLogger.warn \"Batched background migration for the given configuration is already finished: #{configuration}\"\n          else\n            migration.reset_attempts_of_blocked_jobs!\n\n            migration.finalize!\n            migration.batched_jobs.with_status(:pending).each { |job| migration_wrapper.perform(job) }\n\n            run_migration_while(migration, :finalizing)\n\n            error_message = \"Batched migration #{migration.job_class_name} could not be completed and a manual action is required.\"\\\n                            \"Check the admin panel at (`/admin/background_migrations`) for more details.\"\n\n            raise FailedToFinalize, error_message unless migration.finished?\n          end",
    "comment": "Finalize migration for given configuration.  If the migration is already finished, do nothing. Otherwise change its status to `finalizing` in order to prevent it being picked up by the background worker. Perform all pending jobs, then keep running until migration is finished.",
    "label": "",
    "id": "2405"
  },
  {
    "raw_code": "def progress\n          return FINISHED_PROGRESS_VALUE if finished? || finalized?\n\n          return unless total_tuple_count.to_i > 0\n\n          100 * migrated_tuple_count / total_tuple_count\n        end",
    "comment": "Computes an estimation of the progress of the migration in percents.  Because `total_tuple_count` is an estimation of the tuples based on DB statistics when the migration is complete there can actually be more or less tuples that initially estimated as `total_tuple_count` so the progress may not show 100%. For that reason when we know migration completed successfully, we just return the 100 value",
    "label": "",
    "id": "2406"
  },
  {
    "raw_code": "def reduce_sub_batch_size!\n          raise ReduceSubBatchSizeError, 'Only sub_batch_size of failed jobs can be reduced' unless failed?\n\n          return if sub_batch_exceeds_threshold?\n\n          with_lock do\n            actual_sub_batch_size = sub_batch_size\n            reduced_sub_batch_size = (sub_batch_size * SUB_BATCH_SIZE_REDUCE_FACTOR).to_i.clamp(1, batch_size)\n\n            update!(sub_batch_size: reduced_sub_batch_size)\n\n            Gitlab::AppLogger.warn(\n              message: 'Sub batch size reduced due to timeout',\n              batched_job_id: id,\n              sub_batch_size: actual_sub_batch_size,\n              reduced_sub_batch_size: reduced_sub_batch_size,\n              attempts: attempts,\n              batched_migration_id: batched_migration.id,\n              job_class_name: migration_job_class_name,\n              job_arguments: migration_job_arguments\n            )\n          end",
    "comment": "It reduces the size of +sub_batch_size+ by 25%",
    "label": "",
    "id": "2407"
  },
  {
    "raw_code": "def sub_batch_exceeds_threshold?\n          initial_sub_batch_size = batched_migration.sub_batch_size\n          reduced_sub_batch_size = (sub_batch_size * SUB_BATCH_SIZE_REDUCE_FACTOR).to_i\n          diff = initial_sub_batch_size - reduced_sub_batch_size\n\n          (1.0 * diff / initial_sub_batch_size * 100).round(2) > SUB_BATCH_SIZE_THRESHOLD\n        end",
    "comment": "It doesn't allow sub-batch size to be reduced lower than the threshold  @info It will prevent the next iteration to reduce the +sub_batch_size+ lower than the +SUB_BATCH_SIZE_THRESHOLD+ or 65% of its original size.",
    "label": "",
    "id": "2408"
  },
  {
    "raw_code": "def _semver_major_minor_patch_regex\n        @_semver_major_minor_patch_regex ||= /\n          #{_semver_major_regex}\\.#{_semver_minor_regex}\\.#{_semver_patch_regex}\n        /x\n      end",
    "comment": "These partial semver regexes are intended for use in composing other regexes rather than being used alone.",
    "label": "",
    "id": "2409"
  },
  {
    "raw_code": "def is_namespace_homepage? # rubocop:disable Naming/PredicateName -- namespace_homepage is not an\n        # adjective, so adding \"is_\" improves understandability\n        project_path.downcase == \"#{project_namespace}.#{instance_pages_domain}\"\n      end",
    "comment": "If the project path is the same as host, we serve it as group/user page.  e.g. For Pages external url `example.io`, `acmecorp/acmecorp.example.io` project will publish to `http(s)://acmecorp.example.io` See https://docs.gitlab.com/ee/user/project/pages/getting_started_part_one.html#user-and-group-website-examples.",
    "label": "",
    "id": "2410"
  },
  {
    "raw_code": "def hostname\n        return instance_pages_domain if namespace_in_path?\n        return project_path if is_namespace_homepage?\n        return instance_pages_domain.prepend(\"#{subdomain}.\") if subdomain.present?\n\n        instance_pages_domain\n      end",
    "comment": "Defines the full hostname, eg. group.gitlab.io",
    "label": "",
    "id": "2411"
  },
  {
    "raw_code": "def unique_domain_enabled?\n        project.project_setting.pages_unique_domain_enabled?\n      end",
    "comment": "Defines whether the Pages site is published on a unique subdomain instead of a single subdomain for the entire namespace and a path segment for each project.  Examples: unique_domain_enabled? is true: \"https://my-project-12345.example.com unique_domain_enabled? is false: \"https://my-group.example.com/my-project",
    "label": "",
    "id": "2412"
  },
  {
    "raw_code": "def namespace_in_path?\n        config.namespace_in_path\n      end",
    "comment": "Defines whether the namespace should be included as part of the path instead of using a subdomain. Useful for self-hosted instances where auto-generated subdomains don't work.  Examples: namespace_in_path? is false: \"https://my-group.example.com/my-project namespace_in_path? is true: \"https://example.com/my-group/my-project",
    "label": "",
    "id": "2413"
  },
  {
    "raw_code": "def instance_pages_domain\n        config_url.host\n      end",
    "comment": "Defines the top-most domain that is not autogenerated. For gitlab.com this is `gitlab.io`. If you need the fully qualified domain for the project, use `hostname` instead.",
    "label": "",
    "id": "2414"
  },
  {
    "raw_code": "def total_size\n        build.artifacts_metadata_entry(\"#{publish_path.delete_suffix('/')}/\", recursive: true).total_size\n      end",
    "comment": "Calculate page size after extract",
    "label": "",
    "id": "2415"
  },
  {
    "raw_code": "def validate_outdated_sha\n        return if latest_build?\n        return if latest_pipeline_id.blank?\n        return if latest_pipeline_id <= build.pipeline_id\n\n        errors.add(:base, 'build SHA is outdated for this ref')\n      end",
    "comment": "If a newer pipeline already build a PagesDeployment",
    "label": "",
    "id": "2416"
  },
  {
    "raw_code": "def max_size_from_settings = Gitlab::CurrentSettings.max_pages_size.megabytes\n\n      def path_prefix = build.pages&.fetch(:path_prefix, '')\n      strong_memoize_attr :path_prefix\n\n      def sha = build.sha\n      strong_memoize_attr :sha\n    end\n  end\nend",
    "comment": "overridden in EE",
    "label": "",
    "id": "2417"
  },
  {
    "raw_code": "def generate\n        domain = project_path.byteslice(0, PROJECT_PATH_LIMIT)\n\n        # PS.: SecureRandom.hex return an string twice the size passed as argument.\n        domain.concat('-', SecureRandom.hex(3))\n\n        # Slugify ensures the format and size (63 chars) of the given string\n        Gitlab::Utils.slugify(domain)\n      end",
    "comment": "Subdomains have a limit of 63 bytes (https://www.freesoft.org/CIE/RFC/1035/9.htm) For this reason we're limiting each part of the unique subdomain  The domain is made up of 2 parts, like: projectpath-randomstring - project path: between 1 and 56 chars - random hexadecimal: to ensure a random value of length 6",
    "label": "",
    "id": "2418"
  },
  {
    "raw_code": "def initialize(input = '')\n        @lines = input.lines\n        @locations = {}\n\n        @lines.each_with_index do |line, index|\n          matches = line.match(RELEASE_REGEX)\n\n          next if !matches || !matches[:version]\n\n          @locations[matches[:version]] = index\n        end",
    "comment": "The `input` argument must be a `String` containing the existing changelog Markdown. If no changelog exists, this should be an empty `String`.",
    "label": "",
    "id": "2419"
  },
  {
    "raw_code": "def add(release)\n        versions = [release.version, *@locations.keys]\n\n        VersionSorter.rsort!(versions)\n\n        new_index = versions.index(release.version)\n        new_lines = @lines.dup\n        markdown = release.to_markdown\n\n        if (insert_after = versions[new_index + 1])\n          line_index = @locations[insert_after]\n\n          new_lines.insert(line_index, markdown)\n        else\n          # When adding to the end of the changelog, the previous section only\n          # has a single newline, resulting in the release section title\n          # following it immediately. When this is the case, we insert an extra\n          # empty line to keep the changelog readable in its raw form.\n          new_lines.push(\"\\n\") if versions.length > 1\n          new_lines.push(markdown.rstrip)\n          new_lines.push(\"\\n\")\n        end",
    "comment": "Generates the Markdown for the given release and returns the new changelog Markdown content.  The `release` argument must be an instance of `Gitlab::Changelog::Release`.",
    "label": "",
    "id": "2420"
  },
  {
    "raw_code": "def commit(release:, file:, branch:, message:)\n        # When retrying, we need to reprocess the existing changelog from\n        # scratch, otherwise we may end up throwing away changes. As such, all\n        # the logic is contained within the retry block.\n        Retriable.retriable(on: Error) do\n          commit = Gitlab::Git::Commit.last_for_path(\n            @project.repository,\n            branch,\n            file,\n            literal_pathspec: true\n          )\n\n          content = blob_content(file, commit)\n\n          # If the release has already been added (e.g. concurrently by another\n          # API call), we don't want to add it again.\n          break if content&.match?(release.header_start_pattern)\n\n          service = Files::MultiService.new(\n            @project,\n            @user,\n            commit_message: message,\n            branch_name: branch,\n            start_branch: branch,\n            actions: [\n              {\n                action: content ? 'update' : 'create',\n                content: Generator.new(content.to_s).add(release),\n                file_path: file,\n                last_commit_id: commit&.sha\n              }\n            ]\n          )\n\n          result = service.execute\n\n          raise Error, result[:message] if result[:status] != :success\n        end",
    "comment": "Commits a release's changelog to a file on a branch.  The `release` argument is a `Gitlab::Changelog::Release` for which to update the changelog.  The `file` argument specifies the path to commit the changes to.  The `branch` argument specifies the branch to commit the changes on.  The `message` argument specifies the commit message to use.",
    "label": "",
    "id": "2421"
  },
  {
    "raw_code": "def config\n        raise NotImplementedError\n      end",
    "comment": "This can be overridden for a custom config",
    "label": "",
    "id": "2422"
  },
  {
    "raw_code": "def unsubscribe_address(key)\n        incoming_email_config.address.sub(WILDCARD_PLACEHOLDER, \"#{key}#{UNSUBSCRIBE_SUFFIX}\")\n      end",
    "comment": "example: incoming+1234567890abcdef1234567890abcdef-unsubscribe@incoming.gitlab.com",
    "label": "",
    "id": "2423"
  },
  {
    "raw_code": "def validate_single_recipient_in_opts!(opts)\n        return unless opts\n\n        symbolized_opts = opts.symbolize_keys\n\n        if opts.keys.length != symbolized_opts.keys.length\n          raise Gitlab::Email::MultipleRecipientsError, \"opts has colliding key names\"\n        end",
    "comment": "It is possible to send email to one or more recipients in one email by setting the list of emails to the :to key, or by :cc or :bcc-ing recipients. https://guides.rubyonrails.org/action_mailer_basics.html#sending-email-to-multiple-recipients  This method ensures an email will only go to zero or one recipients.",
    "label": "",
    "id": "2424"
  },
  {
    "raw_code": "def validate_single_recipient_in_email(email)\n        email.is_a?(String) && !email.match(/[;,]/)\n      end",
    "comment": "This method validates that only a single recipient is present. It must be a String; e.g. an Array of Strings is not valid. Email separators are commas in RFC5322, but semicolons are also permitted in RFC1485.  It does not validate that the recipient is a valid email address.",
    "label": "",
    "id": "2425"
  },
  {
    "raw_code": "def fix_charset(object)\n        return if object.nil?\n\n        if object.charset\n          # A part of a multi-part may have a different encoding. Its encoding\n          # is denoted in its header. For example:\n          #\n          # ```\n          # ------=_Part_2192_32400445.1115745999735\n          # Content-Type: text/plain; charset=ISO-8859-1\n          # Content-Transfer-Encoding: 7bit\n          #\n          # Plain email.\n          # ```\n          object.body.decoded.force_encoding(object.charset.gsub(/utf8/i, \"UTF-8\")).encode(\"UTF-8\").to_s\n        else\n          object.body.to_s\n        end",
    "comment": "Force encoding to UTF-8 on a Mail::Message or Mail::Part",
    "label": "",
    "id": "2426"
  },
  {
    "raw_code": "def filter_signature_attachments(message)\n        attachments = message.attachments\n        content_type = normalize_mime(message.content_type)\n        protocol = normalize_mime(message.content_type_parameters&.fetch(:protocol, nil))\n\n        if content_type == 'multipart/signed' && protocol\n          attachments.delete_if { |attachment| protocol == normalize_mime(attachment.content_type) }\n        end",
    "comment": "If this is a signed message (e.g. S/MIME or PGP), remove the signature from the uploaded attachments",
    "label": "",
    "id": "2427"
  },
  {
    "raw_code": "def normalize_mime(content_type)\n        MIME::Type.simplified(content_type, remove_x_prefix: true)\n      end",
    "comment": "normalizes mime-type ignoring case and removing extra data also removes potential \"x-\" prefix from subtype, since some MUAs mix them e.g. \"application/x-pkcs7-signature\" with \"application/pkcs7-signature\"",
    "label": "",
    "id": "2428"
  },
  {
    "raw_code": "def sanitize_exif_if_needed(content, path)\n        exif_sanitizer = Gitlab::Sanitizers::Exif.new\n        exif_sanitizer.clean_existing_path(path, content: content, skip_unallowed_types: true)\n      end",
    "comment": "https://gitlab.com/gitlab-org/gitlab/-/issues/239343",
    "label": "",
    "id": "2429"
  },
  {
    "raw_code": "def self.convert(html)\n        html = fix_newlines(replace_entities(html))\n        doc = Nokogiri::HTML(html)\n\n        new(doc).convert\n      end",
    "comment": "This redefinition can be removed once https://github.com/soundasleep/html2text_ruby/pull/30 is merged and released.",
    "label": "",
    "id": "2430"
  },
  {
    "raw_code": "def key_from_settings(email)\n            return unless email.present?\n\n            # Normalize custom email to also include verification emails.\n            potential_custom_email = email.sub(ServiceDeskSetting::CUSTOM_EMAIL_VERIFICATION_SUBADDRESS, '')\n\n            settings = ServiceDeskSetting.find_by_custom_email(potential_custom_email)\n            return unless settings.present?\n\n            ::ServiceDesk::Emails.new(settings.project).default_subaddress_part\n          end",
    "comment": "Checks whether the given email is a custom email and returns the project's mail key.",
    "label": "",
    "id": "2431"
  },
  {
    "raw_code": "def author\n          @author ||= User.find_by(incoming_email_token: incoming_email_token)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2432"
  },
  {
    "raw_code": "def metrics_event\n          :receive_email_create_issue\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2433"
  },
  {
    "raw_code": "def project\n          return @project if instance_variable_defined?(:@project)\n\n          if project_id\n            @project = Project.find_by_id(project_id)\n            @project = nil unless valid_project_slug?(@project)\n          else\n            @project = Project.find_by_full_path(project_path)\n          end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "2434"
  },
  {
    "raw_code": "def author\n          @author ||= User.find_by(incoming_email_token: incoming_email_token)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2435"
  },
  {
    "raw_code": "def create_note\n          Notes::CreateService.new(project, author, note_params).execute\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2436"
  },
  {
    "raw_code": "def author\n          @author ||= User.find_by(incoming_email_token: incoming_email_token)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2437"
  },
  {
    "raw_code": "def metrics_params\n          super.merge(includes_patches: patch_attachments.any?)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2438"
  },
  {
    "raw_code": "def metrics_event\n          raise NotImplementedError\n        end",
    "comment": "Each handler should use it's own metric event.  Otherwise there is a possibility that within the same Sidekiq process, that same event with different metrics_params will cause Prometheus to throw an error",
    "label": "",
    "id": "2439"
  },
  {
    "raw_code": "def self.sign(cert:, key:, data:, ca_certs: nil)\n          signed_data = OpenSSL::PKCS7.sign(cert, key, data, Array.wrap(ca_certs), OpenSSL::PKCS7::DETACHED)\n          OpenSSL::PKCS7.write_smime(signed_data)\n        end",
    "comment": "The `ca_certs` parameter, if provided, is an array of CA certificates that will be attached in the signature together with the main `cert`. This will be typically intermediate CAs",
    "label": "",
    "id": "2440"
  },
  {
    "raw_code": "def self.verify_signature(signed_data:, ca_certs: nil)\n          store = OpenSSL::X509::Store.new\n          store.set_default_paths\n          Array.wrap(ca_certs).compact.each { |ca_cert| store.add_cert(ca_cert) }\n\n          signed_smime = OpenSSL::PKCS7.read_smime(signed_data)\n\n          # The S/MIME certificate(s) are included in the message and the trusted\n          # CAs are in the store parameter, so we pass no certs as parameters\n          # to `PKCS7.verify`\n          # See https://www.openssl.org/docs/manmaster/man3/PKCS7_verify.html\n          signed_smime if signed_smime.verify(nil, store)\n        end",
    "comment": "Return nil if data cannot be verified, otherwise the signed content data  Be careful with the `ca_certs` parameter, it will implicitly trust all the CAs in the array by creating a trusted store, stopping validation at the first match This is relevant when using intermediate CAs, `ca_certs` should only include the trusted, root CA",
    "label": "",
    "id": "2441"
  },
  {
    "raw_code": "def self.delivering_email(message)\n          unless Gitlab::CurrentSettings.html_emails_enabled\n            message.parts.delete_if do |part|\n              part.content_type.start_with?('text/html')\n            end",
    "comment": " Remove HTML part if HTML emails are disabled. ",
    "label": "",
    "id": "2442"
  },
  {
    "raw_code": "def find_next_heading_level(content, position)\n        content_before_position = content[0...position]\n        previous_headings = content_before_position.scan(HEADING_PATTERN)\n\n        return DEFAULT_HEADING_LEVEL unless previous_headings.any?\n\n        nearest_heading_level = previous_headings.last[0].length\n        subheading_level = nearest_heading_level + 1\n        [subheading_level, MAX_HEADING_LEVEL].min\n      end",
    "comment": "Determines the appropriate heading level for a new section, based on preceding Markdown content. Returns a value between 2-6, representing h2-h6 in Markdown.",
    "label": "",
    "id": "2443"
  },
  {
    "raw_code": "def normalize_email(email)\n        return email unless email.is_a?(String)\n        return email unless EMAIL_REGEXP.match?(email.strip)\n\n        portions = email.downcase.strip.split('@')\n        mailbox = portions.shift\n        domain = portions.join\n\n        mailbox_root = mailbox.split('+')[0]\n\n        # Gmail addresses strip the \".\" from their emails.\n        # For example, user.name@gmail.com is the same as username@gmail.com\n        mailbox_root = mailbox_root.tr('.', '') if domain == 'gmail.com'\n\n        [mailbox_root, domain].join('@')\n      end",
    "comment": "Method copied from lib/gitlab/utils/email.rb",
    "label": "",
    "id": "2444"
  },
  {
    "raw_code": "def create_sql(from_id, to_id)\n        <<~SQL\n          WITH created_records AS MATERIALIZED (\n            INSERT INTO services (project_id, #{DEFAULTS.keys.map { |key| %(\"#{key}\") }.join(',')}, created_at, updated_at)\n            #{select_insert_values_sql(from_id, to_id)}\n            RETURNING *\n          )\n          SELECT COUNT(*) as number_of_created_records\n          FROM created_records\n        SQL\n      end",
    "comment": "there is no uniq constraint on project_id and type pair, which prevents us from using ON CONFLICT",
    "label": "",
    "id": "2445"
  },
  {
    "raw_code": "def update_sql(from_id, to_id)\n        <<~SQL\n          WITH updated_records AS MATERIALIZED (\n            UPDATE services SET active = TRUE\n            WHERE services.project_id BETWEEN #{Integer(from_id)} AND #{Integer(to_id)} AND services.properties = '{}' AND services.type = '#{Migratable::PrometheusService.type}'\n            AND #{group_cluster_condition(from_id, to_id)} AND services.active = FALSE\n            RETURNING *\n          )\n          SELECT COUNT(*) as number_of_updated_records\n          FROM updated_records\n        SQL\n      end",
    "comment": "there is no uniq constraint on project_id and type pair, which prevents us from using ON CONFLICT",
    "label": "",
    "id": "2446"
  },
  {
    "raw_code": "def perform; end\n    end",
    "comment": "@return [Void]",
    "label": "",
    "id": "2447"
  },
  {
    "raw_code": "def skip_epic_color_sync?(epic, work_item_color)\n        (epic.color.to_s == DEFAULT_EPIC_COLOR && work_item_color.nil?) || epic.color == work_item_color&.color\n      end",
    "comment": "If epic color is default and there is no record for the work item color then do not sync, we do not have to sync default color as that is assumed by default on the epic work item. Also if the epic color is the same with the existing work item color, skip synching as well.",
    "label": "",
    "id": "2448"
  },
  {
    "raw_code": "def batch_column\n        group_id.present? ? :iid : :id\n      end",
    "comment": "when we need to filter epics tobe back-filled by group_id we do not have a good index coverage on (id, group_id) pair, so instead because we have (group_id, iid) pair covered by an unique index we can use that to iterate in batches",
    "label": "",
    "id": "2449"
  },
  {
    "raw_code": "def perform\n        each_sub_batch do |sub_batch|\n          # rubocop:disable Rails/FindEach -- This already operates on a sub_batch\n          sub_batch.where(\"name LIKE '%/%'\").each do |src_component|\n            dst_component = Component.find_by(\n              name: component_name_without_os_prefix(src_component.name),\n              purl_type: src_component.purl_type,\n              component_type: src_component.component_type,\n              organization_id: src_component.organization_id\n            )\n\n            # This uses loop based batching to efficiently iterate over\n            # all occurrences. Since we update the component_id column,\n            # this will eventually return no results and break out of the\n            # loop.\n            loop do\n              occurrences = Occurrence.includes(:component_version)\n                .where(component_id: src_component.id)\n                .limit(OCCURRENCE_BATCH_SIZE)\n\n              break unless occurrences.present?\n\n              component_version_attributes = build_component_version_attributes(dst_component, occurrences)\n\n              component_versions = bulk_upsert_component_versions(component_version_attributes).to_h do |row|\n                [row['version'], row['id']]\n              end",
    "comment": "rubocop:disable Metrics/AbcSize -- It went above limit when adding support for organization_id sharding key.",
    "label": "",
    "id": "2450"
  },
  {
    "raw_code": "def update_design_management_repositories_namespace_id!(sub_batch_ids)\n        sql = <<~SQL\n          UPDATE design_management_repositories\n          SET namespace_id = p.project_namespace_id\n          FROM projects p\n          WHERE design_management_repositories.project_id = p.id\n          AND design_management_repositories.id IN (#{sub_batch_ids.to_sql})\n        SQL\n\n        ApplicationRecord.connection.execute(sql)\n      end",
    "comment": "Sets the namespace_id on design_management_repositories to the corresponding project's project_namespace_id",
    "label": "",
    "id": "2451"
  },
  {
    "raw_code": "def update_design_management_repository_states_namespace_id!(sub_batch_ids)\n        sql = <<~SQL\n          UPDATE design_management_repository_states\n          SET namespace_id = design_management_repositories.namespace_id\n          FROM design_management_repositories\n          WHERE design_management_repositories.id = design_management_repository_states.design_management_repository_id\n          AND design_management_repository_states.design_management_repository_id IN (#{sub_batch_ids.to_sql})\n        SQL\n\n        ApplicationRecord.connection.execute(sql)\n      end",
    "comment": "Sets the namespace_id on design_management_repository_states to the corresponding design_management_repositories's namespace_id",
    "label": "",
    "id": "2452"
  },
  {
    "raw_code": "def missing_work_item_parent_links(sub_batch)\n        missing_parent_links = EpicIssues\n            .select('issues.id as issue_id', 'epics.issue_id as work_item_parent_id, epic_issues.relative_position')\n            .joins(\"JOIN issues ON (issues.id = epic_issues.issue_id)\")\n            .joins('JOIN epics ON (epic_issues.epic_id = epics.id)')\n            .joins(\"LEFT JOIN work_item_parent_links ON (epic_issues.issue_id = work_item_parent_links.work_item_id)\")\n            .where(epic_id: sub_batch.select(:id))\n            .where(work_item_parent_links: { id: nil })\n\n        missing_parent_links.map do |record|\n          {\n            work_item_parent_id: record.work_item_parent_id,\n            work_item_id: record.issue_id,\n            relative_position: record.relative_position\n          }\n        end",
    "comment": "WorkItemParentLinks that do not exist because we did not update the reference from the old issue_id to the new one.",
    "label": "",
    "id": "2453"
  },
  {
    "raw_code": "def orphaned_work_item_parent_links(sub_batch)\n        WorkItemParentLinks.where(work_item_parent_id: sub_batch.select(:issue_id))\n          .joins(\"LEFT JOIN epic_issues ON (work_item_parent_links.work_item_id = epic_issues.issue_id)\")\n          .joins('JOIN issues ON (work_item_parent_links.work_item_id = issues.id)')\n          .where(epic_issues: { id: nil })\n          .where(issues: { work_item_type_id: issue_work_item_type_id })\n      end",
    "comment": "WorkItemParentLinks where the the epic is still the correct one, but the work_item_id is still the issue of the old project. Because epic_issues is the SSoT, we expect that these records would have a matching epic_issue record. We need to join issues as only epic <> issue relationships are affected.",
    "label": "",
    "id": "2454"
  },
  {
    "raw_code": "def original_filename\n          @original_filename ||= self.class.original_filename(checksum)\n        end",
    "comment": "This method is used by the `carrierwave` gem",
    "label": "",
    "id": "2455"
  },
  {
    "raw_code": "def update_design_management_designs_namespace_id!(sub_batch_ids)\n        sql = <<~SQL\n          UPDATE design_management_designs\n          SET namespace_id = p.project_namespace_id\n          FROM projects p\n          WHERE design_management_designs.project_id = p.id\n          AND design_management_designs.id IN (#{sub_batch_ids.to_sql})\n        SQL\n\n        ApplicationRecord.connection.execute(sql)\n      end",
    "comment": "Sets the namespace_id on design_management_designs to the corresponding project's project_namespace_id",
    "label": "",
    "id": "2456"
  },
  {
    "raw_code": "def update_design_management_designs_versions_namespace_id!(sub_batch_ids)\n        sql = <<~SQL\n          UPDATE design_management_designs_versions\n          SET namespace_id = design_management_designs.namespace_id\n          FROM design_management_designs\n          WHERE design_management_designs.id = design_management_designs_versions.design_id\n          AND design_management_designs_versions.design_id IN (#{sub_batch_ids.to_sql})\n        SQL\n\n        ApplicationRecord.connection.execute(sql)\n      end",
    "comment": "Sets the namespace_id on design_management_designs_versions to the corresponding design_management_designs's namespace_id",
    "label": "",
    "id": "2457"
  },
  {
    "raw_code": "def update_design_user_mentions_namespace_id!(sub_batch_ids)\n        sql = <<~SQL\n          UPDATE design_user_mentions\n          SET namespace_id = design_management_designs.namespace_id\n          FROM design_management_designs\n          WHERE design_management_designs.id = design_user_mentions.design_id\n          AND design_user_mentions.design_id IN (#{sub_batch_ids.to_sql})\n        SQL\n\n        ApplicationRecord.connection.execute(sql)\n      end",
    "comment": "Sets the namespace_id on design_user_mentions to the corresponding design_management_designs's namespace_id",
    "label": "",
    "id": "2458"
  },
  {
    "raw_code": "def perform\n        each_sub_batch do |sub_batch|\n          sub_batch\n            .where.not(auto_canceled_by_id: nil)\n            .where('ci_pipelines.auto_canceled_by_id = canceling_pipelines.id')\n            .update_all('auto_canceled_by_partition_id = canceling_pipelines.partition_id FROM ci_pipelines as canceling_pipelines')\n        end",
    "comment": "rubocop:disable Layout/LineLength -- Improve readability",
    "label": "",
    "id": "2459"
  },
  {
    "raw_code": "def generate_discussion_id\n          Digest::SHA1.hexdigest(\n            [:discussion, noteable_type.try(:underscore), noteable_id || commit_id, SecureRandom.hex].join('-')\n          )\n        end",
    "comment": "Based on https://gitlab.com/gitlab-org/gitlab/blob/117c14d0c79403e169cf52922b48f69d1dcf6a85/app/models/discussion.rb#L62-74",
    "label": "",
    "id": "2460"
  },
  {
    "raw_code": "def trackers_data\n        @trackers_data ||= JiraTrackerData\n          .where(deployment_type: 'unknown')\n          .where(batch_column => start_id..end_id)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2461"
  },
  {
    "raw_code": "def backfill_via_table_primary_key\n        'group_id'\n      end",
    "comment": "override as parent table primary key is group_id",
    "label": "",
    "id": "2462"
  },
  {
    "raw_code": "def perform; end\n    end",
    "comment": "This batched background migration is EE-only, Migration file - ee/lib/gitlab/background_migration/backfill_user_group_member_roles.rb",
    "label": "",
    "id": "2463"
  },
  {
    "raw_code": "def process_upload_type(sub_batch, model_table, model_name, sources, targets, join_key, db_name)\n        relation = sub_batch.select(:id, :model_type).limit(sub_batch_size)\n        targets ||= sources\n        join_key ||= 'id'\n        # Columns that will be reset (nullified) as they are not used for sharding keys\n        reset_columns = sharding_key_columns - targets\n        # All columns to back-fill\n        target_columns = (columns + targets + reset_columns).join(', ')\n        # All columns to source from\n        source_columns = source_columns_sql(sources, reset_columns)\n        # For existing records update only sharding key columns (if any)\n        on_conflict = if targets.any?\n                        \"UPDATE SET #{sharding_key_columns.map { |c| \"#{c} = EXCLUDED.#{c}\" }.join(', ')}\"\n                      else\n                        \"NOTHING\"\n                      end",
    "comment": "Back-fill partitioned table `uploads_9ba88c4165`. For each sub-batch execute an upsert query per model_type, joining with the respective model_table. This join will exclude uploads belonging to records that no longer exist.  Arguments are: sub_batch -  batch to operate on. model_table - table storing the parent model model_name - model class name source - columns to source the sharding key values from targets - sharding key columns to back-fill join_key - column to join with the model table, defaults to id db_name - database the model table belongs to",
    "label": "",
    "id": "2464"
  },
  {
    "raw_code": "def perform\n        each_sub_batch do |sub_batch|\n          sub_batch.connection.execute(construct_query(sub_batch: sub_batch.where(backfill_column => nil)))\n        end",
    "comment": "NOTE: Moving scope to sub-batch scope since we're missing an index on project_id, build_id",
    "label": "",
    "id": "2465"
  },
  {
    "raw_code": "def destroy_snippet_repository(snippet)\n        snippet.snippet_repository&.delete\n      rescue StandardError => e\n        logger.error(message: \"Snippet Migration: error destroying snippet repository. Reason: #{e.message}\", snippet: snippet.id)\n      end",
    "comment": "Removing the db record",
    "label": "",
    "id": "2466"
  },
  {
    "raw_code": "def delete_repository(snippet)\n        return unless snippet.repository_exists?\n\n        snippet.repository.remove\n        snippet.repository.expire_exists_cache\n      rescue StandardError => e\n        logger.error(message: \"Snippet Migration: error deleting repository. Reason: #{e.message}\", snippet: snippet.id)\n      end",
    "comment": "Removing the repository in disk",
    "label": "",
    "id": "2467"
  },
  {
    "raw_code": "def commit_author(snippet)\n        return migration_bot_user if snippet_content_size_over_limit?(snippet)\n        return migration_bot_user if @invalid_signature_error\n\n        if Gitlab::UserAccessSnippet.new(snippet.author, snippet: snippet).can_do_action?(:update_snippet)\n          snippet.author\n        else\n          migration_bot_user\n        end",
    "comment": "If the user is not allowed to access git or update the snippet because it is blocked, internal, ghost, ... we cannot commit files because these users are not allowed to, but we need to migrate their snippets as well. In this scenario the migration bot user will be the one that will commit the files.",
    "label": "",
    "id": "2468"
  },
  {
    "raw_code": "def set_file_path_error(error)\n        @invalid_path_error ||= error.is_a?(SnippetRepository::InvalidPathError)\n      end",
    "comment": "We sometimes receive invalid path errors from Gitaly if the Snippet filename cannot be parsed into a valid git path. In this situation, we need to parameterize the file name of the Snippet so that the migration can succeed, to achieve that, we'll identify in migration retries that the path is invalid",
    "label": "",
    "id": "2469"
  },
  {
    "raw_code": "def set_signature_error(error)\n        @invalid_signature_error ||= error.is_a?(SnippetRepository::InvalidSignatureError)\n      end",
    "comment": "We sometimes receive invalid signature from Gitaly if the commit author name or email is invalid to create the commit signature. In this situation, we set the error and use the migration_bot since the information used to build it is valid",
    "label": "",
    "id": "2470"
  },
  {
    "raw_code": "def max_retries\n        MAX_RETRIES + (@invalid_signature_error && @invalid_path_error ? 1 : 0)\n      end",
    "comment": "In the case where the snippet file_name is invalid and also the snippet author has invalid commit info, we need to increase the number of retries by 1, because we will receive two errors from Gitaly and, in the third one, we will commit successfully.",
    "label": "",
    "id": "2471"
  },
  {
    "raw_code": "def normalized_version_name(name)\n        name = name.capitalize.delete_prefix('V')\n\n        first_part, *other_parts = name.split(/([-,+])/)\n        dot_count = first_part.count('.')\n\n        return name unless dot_count != 2 && /^[\\d.]+$/.match?(first_part)\n\n        if dot_count < 2\n          add_zeroes = '.0' * (2 - dot_count)\n\n          [first_part, add_zeroes, *other_parts].join\n        else\n          # Assuming the format is 1.2.3.4, we only want to keep the first 3 digits.\n          truncated_first_part = first_part.split(/([.])/)[0..4]\n\n          [truncated_first_part, *other_parts].join\n        end",
    "comment": "Removes `v` prefix and converts partial or extended semver numbers into a normalized format. Examples:  1         => 1.0.0 v1.2      => 1.2.0 1.2-alpha => 1.2.0-alpha 1.0+123   => 1.0.0+123 1.2.3.4   => 1.2.3 ",
    "label": "",
    "id": "2472"
  },
  {
    "raw_code": "def perform\n        each_sub_batch do |sub_batch|\n          sub_batch\n            .joins('LEFT OUTER JOIN projects ON redirect_routes.source_id = projects.id')\n            .where(projects: { id: nil })\n            .delete_all\n        end",
    "comment": "rubocop:enable Database/AvoidScopeTo",
    "label": "",
    "id": "2473"
  },
  {
    "raw_code": "def process_wiki_notes_with_ctes(ctes)\n        # Check if there are project wikis to update\n        check_project_wikis = <<~SQL\n          WITH\n            #{ctes},\n            wiki_notes AS (\n              SELECT * FROM filtered_relation\n              WHERE noteable_type = 'WikiPage::Meta'\n            )\n          SELECT EXISTS(\n            SELECT 1\n            FROM wiki_notes\n            JOIN wiki_page_meta ON wiki_notes.noteable_id = wiki_page_meta.id\n            WHERE wiki_page_meta.project_id IS NOT NULL\n            LIMIT 1\n          )\n        SQL\n\n        if connection.select_value(check_project_wikis)\n          # Handle project wikis\n          update_sql = <<~SQL\n            WITH\n              #{ctes},\n              relation_by_type AS (\n                SELECT * FROM filtered_relation\n                WHERE noteable_type = 'WikiPage::Meta'\n              )\n            UPDATE notes\n            SET namespace_id = projects.project_namespace_id\n            FROM relation_by_type\n            JOIN wiki_page_meta ON relation_by_type.noteable_id = wiki_page_meta.id\n            JOIN projects ON wiki_page_meta.project_id = projects.id\n            WHERE notes.id = relation_by_type.id\n              AND wiki_page_meta.project_id IS NOT NULL\n          SQL\n          connection.execute(update_sql)\n        end",
    "comment": "rubocop:disable Metrics/MethodLength -- Complex SQL queries are more readable as a single method",
    "label": "",
    "id": "2474"
  },
  {
    "raw_code": "def process_snippet_notes_with_ctes(ctes)\n        update_sql = <<~SQL\n          WITH\n            #{ctes},\n            relation_by_type AS (\n              SELECT * FROM filtered_relation\n              WHERE noteable_type = 'Snippet'\n            )\n          UPDATE notes\n          SET namespace_id = NULL,\n              organization_id = snippets.organization_id\n          FROM relation_by_type\n          JOIN snippets ON relation_by_type.noteable_id = snippets.id\n          WHERE notes.id = relation_by_type.id\n            AND snippets.project_id IS NULL\n        SQL\n\n        connection.execute(update_sql)\n      end",
    "comment": "rubocop:enable Metrics/MethodLength",
    "label": "",
    "id": "2475"
  },
  {
    "raw_code": "def archive_orphaned_notes_with_ctes(ctes)\n        # After all updates, check if there are any notes that STILL have NULL namespace_id\n        # These are true orphans that couldn't be matched to any valid noteable\n        # EXCEPT for personal snippet notes which legitimately have NULL namespace_id\n        check_sql = <<~SQL\n          WITH\n            #{ctes}\n          SELECT EXISTS(\n            SELECT 1\n            FROM notes\n            WHERE id IN (SELECT id FROM filtered_relation)\n              AND namespace_id IS NULL\n              AND NOT (noteable_type = 'Snippet' AND organization_id IS NOT NULL)\n            LIMIT 1\n          )\n        SQL\n\n        return unless connection.select_value(check_sql)\n\n        # Log orphaned notes before archiving\n        log_orphaned_notes_with_ctes(ctes)\n\n        # Archive and delete only notes that still have NULL namespace_id after all updates\n        # BUT exclude personal snippet notes which legitimately have NULL namespace_id\n        delete_sql = <<~SQL\n          WITH\n            #{ctes},\n            remaining_orphaned AS (\n              SELECT notes.*\n              FROM notes\n              WHERE id IN (SELECT id FROM filtered_relation)\n                AND namespace_id IS NULL\n                AND NOT (noteable_type = 'Snippet' AND organization_id IS NOT NULL)\n            ),\n            deleted_notes AS (\n              DELETE FROM notes\n              WHERE id IN (SELECT id FROM remaining_orphaned)\n              RETURNING #{notes_columns_for_archive}\n            )\n          INSERT INTO notes_archived (#{notes_columns_for_archive}, archived_at)\n          SELECT #{notes_columns_for_archive}, CURRENT_TIMESTAMP\n          FROM deleted_notes\n        SQL\n\n        result = connection.execute(delete_sql)\n\n        # Log the result\n        if result.cmd_tuples > 0\n          Gitlab::BackgroundMigration::Logger.info(\n            message: 'Archived and deleted orphaned notes',\n            count: result.cmd_tuples\n          )\n        end",
    "comment": "rubocop:disable Metrics/MethodLength -- Complex SQL queries with CTEs are more readable as a single method",
    "label": "",
    "id": "2476"
  },
  {
    "raw_code": "def log_orphaned_notes_with_ctes(ctes)\n        orphaned_sql = <<~SQL\n          WITH\n            #{ctes}\n          SELECT notes.id, notes.noteable_type, notes.noteable_id, notes.author_id, notes.created_at\n          FROM notes\n          WHERE id IN (SELECT id FROM filtered_relation)\n            AND namespace_id IS NULL\n            AND NOT (noteable_type = 'Snippet' AND organization_id IS NOT NULL)\n        SQL\n\n        orphaned_details = connection.select_all(orphaned_sql)\n\n        orphaned_details.each do |note|\n          Gitlab::BackgroundMigration::Logger.warn(\n            message: 'Orphaned note to be archived',\n            note_id: note['id'],\n            noteable_type: note['noteable_type'],\n            noteable_id: note['noteable_id'],\n            author_id: note['author_id'],\n            created_at: note['created_at']\n          )\n        end",
    "comment": "rubocop:enable Metrics/MethodLength",
    "label": "",
    "id": "2477"
  },
  {
    "raw_code": "def perform\n        each_sub_batch do |sub_batch|\n          sub_batch.update_all(<<~SQL)\n          start_date = LEAST(start_date, '9999-12-31'::date),\n          start_date_fixed = LEAST(start_date_fixed, '9999-12-31'::date),\n          end_date = LEAST(end_date, '9999-12-31'::date),\n          due_date_fixed = LEAST(due_date_fixed, '9999-12-31'::date)\n          SQL\n        end",
    "comment": "rubocop:enable Database/AvoidScopeTo",
    "label": "",
    "id": "2478"
  },
  {
    "raw_code": "def update_with_retry(sub_batch)\n        update_attempt = 1\n\n        begin\n          update_batch(sub_batch)\n        rescue ActiveRecord::StatementTimeout, ActiveRecord::QueryCanceled => e\n          update_attempt += 1\n\n          if update_attempt <= MAX_UPDATE_RETRIES\n            sleep(5)\n            retry\n          end",
    "comment": "rubocop:disable Database/RescueQueryCanceled rubocop:disable Database/RescueStatementTimeout",
    "label": "",
    "id": "2479"
  },
  {
    "raw_code": "def update_batch(sub_batch)\n        connection.execute <<~SQL\n            UPDATE issues\n            SET namespace_id = projects.project_namespace_id\n            FROM (#{sub_batch.to_sql}) AS projects(issue_id, project_namespace_id)\n            WHERE issues.id = issue_id\n        SQL\n      end",
    "comment": "rubocop:enable Database/RescueQueryCanceled rubocop:enable Database/RescueStatementTimeout",
    "label": "",
    "id": "2480"
  },
  {
    "raw_code": "def next_batch(table_name, column_name, batch_min_value:, batch_size:, job_arguments:, job_class: nil)\n          model_class = define_batchable_model(table_name, connection: connection)\n\n          quoted_column_name = model_class.connection.quote_column_name(column_name)\n          relation = model_class.where(\"#{quoted_column_name} >= ?\", batch_min_value)\n          next_batch_bounds = nil\n\n          relation.distinct_each_batch(of: batch_size, column: column_name) do |batch|\n            next_batch_bounds = batch.pick(Arel.sql(\"MIN(#{quoted_column_name}), MAX(#{quoted_column_name})\"))\n\n            break\n          end",
    "comment": "Finds and returns the next batch in the table.  table_name - The table to batch over column_name - The column to batch over batch_min_value - The minimum value which the next batch will start at batch_size - The size of the next batch job_arguments - The migration job arguments job_class - The migration job class",
    "label": "",
    "id": "2481"
  },
  {
    "raw_code": "def next_batch(table_name, column_name, batch_min_value:, batch_size:, job_arguments:, job_class: nil)\n          base_class = Gitlab::Database.application_record_for_connection(connection)\n          model_class = define_batchable_model(table_name, connection: connection, base_class: base_class)\n          next_batch_bounds = nil\n\n          # rubocop:disable Lint/UnreachableLoop -- we need to use each_batch to pull one batch out\n          if job_class.cursor?\n            cursor_columns = job_class.cursor_columns\n\n            Gitlab::Pagination::Keyset::Iterator.new(\n              scope: model_class.order(cursor_columns),\n              cursor: cursor_columns.zip(batch_min_value).to_h\n            ).each_batch(of: batch_size, load_batch: false) do |batch|\n              break unless batch.first && batch.last # skip if the batch is empty for some reason\n\n              next_batch_bounds = [batch.first.values_at(cursor_columns), batch.last.values_at(cursor_columns)]\n              break\n            end",
    "comment": "Finds and returns the next batch in the table.  table_name - The table to batch over column_name - The column to batch over batch_min_value - The minimum value which the next batch will start at batch_size - The size of the next batch job_arguments - The migration job arguments job_class - The migration job class rubocop:disable Metrics/AbcSize -- temporarily contains two branches for cursor and non-cursor batching",
    "label": "",
    "id": "2482"
  },
  {
    "raw_code": "def validate_each(record, attribute, value)\n          unless value.is_a?(Array)\n            record.errors.add(attribute, \"must be an array\")\n            return\n          end",
    "comment": "@param [RemoteDevelopment::DesiredConfig] record @param [Symbol] attribute @param [Array] value @return [void] The result is stored in the errors param",
    "label": "",
    "id": "2483"
  },
  {
    "raw_code": "def validate_order(normalized_config_array, normalized_expected_array, errors, attribute_symbol)\n          normalized_config_array.each_with_index do |item, index|\n            expected_index = normalized_expected_array.index(item)\n\n            if expected_index.nil?\n              errors.add(attribute_symbol, \"item #{item} at index #{index} is unexpected\")\n              next\n            end",
    "comment": "@param [Array] normalized_config_array @param [Array] normalized_expected_array @param [ActiveModel::Errors] errors - stores the validation results @param [Symbol] attribute_symbol - symbol of the attribute associated with the error @return [Void]",
    "label": "",
    "id": "2484"
  },
  {
    "raw_code": "def normalize_expected_order(config_array)\n          expected_positions = []\n\n          EXPECTED_SEQUENCE.each_with_index do |expected, _|\n            config_array.each do |item|\n              item => {\n                kind: String => item_kind,\n                metadata: {\n                  name: String => item_name\n                }\n              }\n\n              expected_positions << \"#{item_kind}/#{item_name}\" if matches?(item_kind, item_name, expected)\n            end",
    "comment": "This method normalizes the expected order of items based on the config array It creates a mapping of expected positions for each kind/name combination  @param [Array] config_array The array of configuration items to normalize @return [Array] An array with expected order of items",
    "label": "",
    "id": "2485"
  },
  {
    "raw_code": "def matches?(kind, name, expected)\n          kind == expected[:kind] && expected[:name_pattern].match?(name)\n        end",
    "comment": "Validates if the given configuration matches the expected value  @param [String] kind The type of configuration to validate @param [String] name The name of the configuration item @param [Object] expected The expected value to match against. See {#EXPECTED_SEQUENCE} above. @return [Boolean] Returns true if the configuration matches the expected value, false otherwise",
    "label": "",
    "id": "2486"
  },
  {
    "raw_code": "def self.create_and_save(workspace_id:, dry_run: false)\n          workspace = RemoteDevelopment::Models::BmWorkspace.find(workspace_id)\n\n          result = BackgroundMigration::RemoteDevelopment::WorkspaceOperations::Create::DesiredConfig::BmMain.main(\n            {\n              params: {\n                agent: workspace.agent\n              },\n              workspace: workspace,\n              logger: logger\n            }\n          )\n\n          validate_and_create_workspace_agentk_state(\n            workspace: workspace,\n            desired_config: result[:desired_config],\n            logger: logger,\n            dry_run: dry_run\n          )\n        end",
    "comment": "@param [Integer] workspace_id @param [Boolean] dry_run @return [Void]",
    "label": "",
    "id": "2487"
  },
  {
    "raw_code": "def self.validate_and_create_workspace_agentk_state(workspace:, desired_config:, logger:, dry_run:)\n          if dry_run\n            puts \"For workspace_id #{workspace.id}\"\n            puts \"Valid desired_config? #{desired_config.valid?}\"\n            desired_config.errors.full_messages.each do |message|\n              puts message\n            end",
    "comment": "rubocop:disable Metrics/MethodLength -- need it big @param [BackgroundMigration::RemoteDevelopment::Models::BMWorkspace] workspace @param [BackgroundMigration::RemoteDevelopment::WorkspaceOperations::BMDesiredConfig] desired_config @param [Gitlab::BackgroundMigration::Logger] logger @param [Boolean] dry_run @return [Void]",
    "label": "",
    "id": "2488"
  },
  {
    "raw_code": "def self.logger\n          @logger ||= ::Gitlab::BackgroundMigration::Logger.build\n        end",
    "comment": "rubocop:enable Metrics/MethodLength @return [Gitlab::BackgroundMigration::Logger]",
    "label": "",
    "id": "2489"
  },
  {
    "raw_code": "def self.read_file(path)\n          File.read(File.join(__dir__, path))\n        end",
    "comment": "@param [String] path - file path relative to domain logic root (this directory, `ee/lib/remote_development`) @return [String] content of the file",
    "label": "",
    "id": "2490"
  },
  {
    "raw_code": "def self.default_devfile_yaml\n          # When updating DEFAULT_DEVFILE_YAML contents in `bm_default_devfile.yaml`, update the user facing doc as well\n          # https://docs.gitlab.com/ee/user/workspace/#gitlab-default-devfile\n          #\n          # The container image is pinned to linux/amd64 digest, instead of the tag digest.\n          # This is to prevent Rancher Desktop from pulling the linux/arm64 architecture of the image\n          # which will disrupt local development since vscode fork and workspace tools image does not support\n          # that architecture yet and thus the workspace won't start.\n          # This will be fixed in https://gitlab.com/gitlab-org/gitlab/-/issues/550128\n          read_file(\"settings/bm_default_devfile.yaml\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2491"
  },
  {
    "raw_code": "def self.git_credential_store_script\n          read_file(\"workspace_operations/create/bm_workspace_variables_git_credential_store.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2492"
  },
  {
    "raw_code": "def self.kubernetes_legacy_poststart_hook_command\n          read_file(\"workspace_operations/create/desired_config/bm_kubernetes_legacy_poststart_hook_command.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2493"
  },
  {
    "raw_code": "def self.kubernetes_poststart_hook_command\n          read_file(\"workspace_operations/create/desired_config/bm_kubernetes_poststart_hook_command.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2494"
  },
  {
    "raw_code": "def self.container_keepalive_command_args\n          read_file(\"workspace_operations/create/bm_container_keepalive_command_args.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2495"
  },
  {
    "raw_code": "def self.internal_poststart_command_start_vscode_script\n          read_file(\"workspace_operations/create/bm_internal_poststart_command_start_vscode.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2496"
  },
  {
    "raw_code": "def self.internal_poststart_command_sleep_until_workspace_is_running_script\n          read_file(\"workspace_operations/create/bm_internal_poststart_command_sleep_until_workspace_is_running.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2497"
  },
  {
    "raw_code": "def self.internal_poststart_command_start_sshd_script\n          read_file(\"workspace_operations/create/bm_internal_poststart_command_start_sshd.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2498"
  },
  {
    "raw_code": "def self.internal_poststart_command_clone_project_script\n          read_file(\"workspace_operations/create/bm_internal_poststart_command_clone_project.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2499"
  },
  {
    "raw_code": "def self.internal_poststart_command_clone_unshallow_script\n          read_file(\"workspace_operations/create/bm_internal_poststart_command_clone_unshallow.sh\")\n        end",
    "comment": "@return [String] content of the file",
    "label": "",
    "id": "2500"
  },
  {
    "raw_code": "def self.all_expected_file_constants\n          # NOTE: We explicitly keep a duplicate list of the defined constants, to ensure that we keep both the explicit\n          #       declarations above and the dynamically defined ones in reload_constants! in sync.\n          [\n            :DEFAULT_DEVFILE_YAML,\n            :GIT_CREDENTIAL_STORE_SCRIPT,\n            :INTERNAL_POSTSTART_COMMAND_CLONE_PROJECT_SCRIPT,\n            :INTERNAL_POSTSTART_COMMAND_CLONE_UNSHALLOW_SCRIPT,\n            :INTERNAL_POSTSTART_COMMAND_START_VSCODE_SCRIPT,\n            :INTERNAL_POSTSTART_COMMAND_SLEEP_UNTIL_WORKSPACE_IS_RUNNING_SCRIPT,\n            :INTERNAL_POSTSTART_COMMAND_START_SSHD_SCRIPT,\n            :KUBERNETES_LEGACY_POSTSTART_HOOK_COMMAND,\n            :KUBERNETES_POSTSTART_HOOK_COMMAND,\n            :CONTAINER_KEEPALIVE_COMMAND_ARGS\n          ]\n        end",
    "comment": "@return [Array]",
    "label": "",
    "id": "2501"
  },
  {
    "raw_code": "def self.reload_constants!\n          expected_count = 10 # Update this count if you add/remove constants\n          raise \"File constants count mismatch!\" unless all_expected_file_constants.count == expected_count\n\n          all_expected_file_constants.each do |const_name|\n            # If you get an exception on this line, update the `all_file_constants` method above\n            remove_const(const_name)\n            method_name = const_name.to_s.downcase\n            const_set(const_name, public_method(method_name).call)\n          end",
    "comment": "@return [void]",
    "label": "",
    "id": "2502"
  },
  {
    "raw_code": "def ==(other)\n            return false unless other.is_a?(self.class)\n            return true if equal?(other)\n\n            desired_config_array == other.desired_config_array\n          end",
    "comment": "@param [BmDesiredConfig] other @return [Boolean]",
    "label": "",
    "id": "2503"
  },
  {
    "raw_code": "def diff(other)\n            raise ArgumentError, \"Expected #{self.class}, got #{other.class}\" unless other.is_a?(self.class)\n\n            # we do not want to calculate diff using the longest common subsequence\n            # because we want to catch changes at the index of self rather than find\n            # the common elements between the two arrays. This example should help explain\n            # the difference https://github.com/liufengyun/hashdiff/issues/43#issuecomment-485497196\n            # noinspection RubyMismatchedArgumentType -- hashdiff also supports arrays\n            Hashdiff.diff(desired_config_array, other.desired_config_array, use_lcs: false)\n          end",
    "comment": "@param [BmDesiredConfig] other @return [Array]",
    "label": "",
    "id": "2504"
  },
  {
    "raw_code": "def desired_config_validator\n            validator = BmDesiredConfigArrayValidator.new(attributes: [:desired_config_array])\n            validator.validate_each(self, :desired_config_array, desired_config_array)\n          end",
    "comment": "@return [Object]",
    "label": "",
    "id": "2505"
  },
  {
    "raw_code": "def symbolized_desired_config_array\n            as_json.fetch(\"desired_config_array\").map(&:deep_symbolize_keys)\n          end",
    "comment": "@return [Array]",
    "label": "",
    "id": "2506"
  },
  {
    "raw_code": "def valid_desired_state?(state)\n            VALID_DESIRED_STATES.include?(state)\n          end",
    "comment": "@param [String] state @return [TrueClass, FalseClass]",
    "label": "",
    "id": "2507"
  },
  {
    "raw_code": "def valid_actual_state?(state)\n            VALID_ACTUAL_STATES.include?(state)\n          end",
    "comment": "@param [String] state @return [TrueClass, FalseClass]",
    "label": "",
    "id": "2508"
  },
  {
    "raw_code": "def self.insert(configmap_name:, containers:, volumes:)\n                volume =\n                  {\n                    name: WORKSPACE_SCRIPTS_VOLUME_NAME,\n                    projected: {\n                      defaultMode: WORKSPACE_SCRIPTS_VOLUME_DEFAULT_MODE,\n                      sources: [\n                        {\n                          configMap: {\n                            name: configmap_name\n                          }\n                        }\n                      ]\n                    }\n                  }\n                volume_mount =\n                  {\n                    name: WORKSPACE_SCRIPTS_VOLUME_NAME,\n                    mountPath: WORKSPACE_SCRIPTS_VOLUME_PATH\n                  }\n\n                volumes << volume\n                containers.each do |container|\n                  container.fetch(:volumeMounts) << volume_mount\n                end",
    "comment": "@param [String] configmap_name @param [Array<Hash>] containers @param [Array<Hash>] volumes @return [void]",
    "label": "",
    "id": "2509"
  },
  {
    "raw_code": "def self.insert(containers:, devfile_commands:, devfile_events:) # rubocop:disable Metrics/MethodLength -- copied from ee/lib/remote_development\n                internal_blocking_command_label_present = devfile_commands.any? do |command|\n                  command.dig(:exec, :label) == INTERNAL_BLOCKING_COMMAND_LABEL\n                end",
    "comment": "@param [Array] containers @param [Array<Hash>] devfile_commands @param [Hash] devfile_events @return [void]",
    "label": "",
    "id": "2510"
  },
  {
    "raw_code": "def self.append(\n                desired_config_array:,\n                name:,\n                namespace:,\n                labels:,\n                annotations:,\n                devfile_commands:,\n                devfile_events:\n              )\n                configmap_data = {}\n\n                configmap =\n                  {\n                    kind: \"ConfigMap\",\n                    apiVersion: \"v1\",\n                    metadata: {\n                      name: name,\n                      namespace: namespace,\n                      labels: labels,\n                      annotations: annotations\n                    },\n                    data: configmap_data\n                  }\n\n                add_devfile_command_scripts_to_configmap_data(\n                  configmap_data: configmap_data,\n                  devfile_commands: devfile_commands,\n                  devfile_events: devfile_events\n                )\n\n                add_run_poststart_commands_script_to_configmap_data(\n                  configmap_data: configmap_data,\n                  devfile_commands: devfile_commands,\n                  devfile_events: devfile_events\n                )\n\n                # noinspection RubyMismatchedArgumentType - RubyMine is misinterpreting types for Hash values\n                configmap[:data] = Gitlab::Utils.deep_sort_hashes(configmap_data).to_h\n\n                desired_config_array.append(configmap)\n\n                nil\n              end",
    "comment": "@param [Array] desired_config_array @param [String] name @param [String] namespace @param [Hash] labels @param [Hash] annotations @param [Array<Hash>] devfile_commands @param [Hash] devfile_events @return [void]",
    "label": "",
    "id": "2511"
  },
  {
    "raw_code": "def self.add_devfile_command_scripts_to_configmap_data(\n                configmap_data:,\n                devfile_commands:,\n                devfile_events:\n              )\n                devfile_events => { postStart: Array => poststart_command_ids }\n\n                poststart_command_ids.each do |poststart_command_id|\n                  command = devfile_commands.find { |command| command.fetch(:id) == poststart_command_id }\n                  command => {\n                    exec: {\n                      commandLine: String => command_line\n                    }\n                  }\n\n                  configmap_data[poststart_command_id.to_sym] = command_line\n                end",
    "comment": "@param [Hash] configmap_data @param [Array<Hash>] devfile_commands @param [Hash] devfile_events @return [void]",
    "label": "",
    "id": "2512"
  },
  {
    "raw_code": "def self.add_run_poststart_commands_script_to_configmap_data(\n                configmap_data:,\n                devfile_commands:,\n                devfile_events:\n              )\n                devfile_events => { postStart: Array => poststart_command_ids }\n\n                internal_blocking_command_label_present = devfile_commands.find do |command|\n                  command.dig(:exec, :label) == INTERNAL_BLOCKING_COMMAND_LABEL\n                end",
    "comment": "@param [Hash] configmap_data @param [Array<Hash>] devfile_commands @param [Hash] devfile_events @return [void]",
    "label": "",
    "id": "2513"
  },
  {
    "raw_code": "def self.get_poststart_command_script_content(poststart_command_ids:)\n                poststart_command_ids.map do |poststart_command_id|\n                  # NOTE: We force all the poststart scripts to exit successfully with `|| true`, to\n                  #       prevent the Kubernetes poststart hook from failing, and thus prevent the\n                  #       container from exiting. Then users can view logs to debug failures.\n                  #       See https://github.com/eclipse-che/che/issues/23404#issuecomment-2787779571\n                  #       for more context.\n                  <<~SH\n                    echo \"$(date -Iseconds): ----------------------------------------\"\n                    echo \"$(date -Iseconds): Running #{WORKSPACE_SCRIPTS_VOLUME_PATH}/#{poststart_command_id}...\"\n                    #{WORKSPACE_SCRIPTS_VOLUME_PATH}/#{poststart_command_id} || true\n                    echo \"$(date -Iseconds): Finished running #{WORKSPACE_SCRIPTS_VOLUME_PATH}/#{poststart_command_id}.\"\n                  SH\n                end.join\n              end",
    "comment": "@param [Array] poststart_command_ids @return [String]",
    "label": "",
    "id": "2514"
  },
  {
    "raw_code": "def self.parse(context)\n                context => {\n                  desired_config_yaml: desired_config_yaml\n                }\n\n                desired_config_array = YAML.load_stream(desired_config_yaml).map(&:deep_symbolize_keys)\n\n                context.merge({\n                  desired_config_array: desired_config_array\n                })\n              end",
    "comment": "@param [Hash] context @return [Hash]",
    "label": "",
    "id": "2515"
  },
  {
    "raw_code": "def self.main(parent_context) # rubocop:disable Metrics/MethodLength -- this is a won't fix\n                parent_context => {\n                  params: params,\n                  workspace: workspace,\n                  logger: logger\n                }\n\n                context = {\n                  workspace_id: workspace.id,\n                  workspace_name: workspace.name,\n                  workspace_namespace: workspace.namespace,\n                  workspace_desired_state_is_running: workspace.desired_state_running?,\n                  workspaces_agent_id: params[:agent].id,\n                  workspaces_agent_config: workspace.workspaces_agent_config,\n                  processed_devfile_yaml: workspace.processed_devfile,\n                  logger: logger,\n                  desired_config_array: []\n                }\n\n                initial_result = Gitlab::Fp::Result.ok(context)\n\n                result =\n                  initial_result\n                    .map(BmConfigValuesExtractor.method(:extract))\n                    .map(BmDevfileParserGetter.method(:get))\n                    .map(BmDesiredConfigYamlParser.method(:parse))\n                    .map(BmDevfileResourceModifier.method(:modify))\n                    .map(BmDevfileResourceAppender.method(:append))\n                    .map(\n                      ->(context) do\n                        context.merge(\n                          desired_config:\n                            RemoteDevelopment::WorkspaceOperations::BmDesiredConfig.new(\n                              desired_config_array: context.fetch(:desired_config_array)\n                            )\n                        )\n                      end",
    "comment": "@param [Hash] parent_context @return [Hash]",
    "label": "",
    "id": "2516"
  },
  {
    "raw_code": "def self.modify(context)\n                context => {\n                  workspace_name: String => workspace_name,\n                  desired_config_array: Array => desired_config_array,\n                  use_kubernetes_user_namespaces: TrueClass | FalseClass => use_kubernetes_user_namespaces,\n                  default_runtime_class: String => default_runtime_class,\n                  allow_privilege_escalation: TrueClass | FalseClass => allow_privilege_escalation,\n                  default_resources_per_workspace_container: Hash => default_resources_per_workspace_container,\n                  env_secret_name: String => env_secret_name,\n                  file_secret_name: String => file_secret_name,\n                }\n\n                set_host_users(\n                  desired_config_array: desired_config_array,\n                  use_kubernetes_user_namespaces: use_kubernetes_user_namespaces\n                )\n\n                set_runtime_class(\n                  desired_config_array: desired_config_array,\n                  runtime_class_name: default_runtime_class\n                )\n\n                set_security_context(\n                  desired_config_array: desired_config_array,\n                  allow_privilege_escalation: allow_privilege_escalation\n                )\n\n                patch_default_resources(\n                  desired_config_array: desired_config_array,\n                  default_resources_per_workspace_container:\n                    default_resources_per_workspace_container\n                )\n\n                inject_secrets(\n                  desired_config_array: desired_config_array,\n                  env_secret_name: env_secret_name,\n                  file_secret_name: file_secret_name\n                )\n\n                set_service_account(\n                  desired_config_array: desired_config_array,\n                  service_account_name: workspace_name\n                )\n\n                context.merge({ desired_config_array: desired_config_array })\n              end",
    "comment": "@param [Hash] context @return [Hash]",
    "label": "",
    "id": "2517"
  },
  {
    "raw_code": "def self.set_host_users(desired_config_array:, use_kubernetes_user_namespaces:)\n                # NOTE: Not setting the use_kubernetes_user_namespaces always since setting it now would require\n                # migration\n                # from old config version to a new one. Set this field always\n                # when a new devfile parser is created for some other reason.\n                return desired_config_array unless use_kubernetes_user_namespaces\n\n                find_pod_spec(desired_config_array)[:hostUsers] = use_kubernetes_user_namespaces\n\n                nil\n              end",
    "comment": "@param [Array<Hash>] desired_config_array @param [Boolean] use_kubernetes_user_namespaces @return [void]",
    "label": "",
    "id": "2518"
  },
  {
    "raw_code": "def self.set_runtime_class(desired_config_array:, runtime_class_name:)\n                # NOTE: Not setting the runtime_class_name always since changing it now would require migration\n                # from old config version to a new one. Update this field to `runtime_class_name.presence`\n                # when a new devfile parser is created for some other reason.\n                return desired_config_array if runtime_class_name.empty?\n\n                find_pod_spec(desired_config_array)[:runtimeClassName] = runtime_class_name\n\n                nil\n              end",
    "comment": "@param [Array<Hash>] desired_config_array @param [String] runtime_class_name @return [void]",
    "label": "",
    "id": "2519"
  },
  {
    "raw_code": "def self.set_security_context(\n                desired_config_array:,\n                allow_privilege_escalation:\n              )\n                pod_security_context = {\n                  runAsNonRoot: true,\n                  runAsUser: RUN_AS_USER,\n                  fsGroup: 0,\n                  fsGroupChangePolicy: 'OnRootMismatch'\n                }\n                container_security_context = {\n                  allowPrivilegeEscalation: allow_privilege_escalation,\n                  privileged: false,\n                  runAsNonRoot: true,\n                  runAsUser: RUN_AS_USER\n                }\n\n                pod_spec = find_pod_spec(desired_config_array)\n                # Explicitly set security context for the pod\n                pod_spec[:securityContext] = pod_security_context\n                # Explicitly set security context for all containers\n                pod_spec[:containers].each do |container|\n                  container[:securityContext] = container_security_context\n                end",
    "comment": "Devfile library allows specifying the security context of pods/containers as mentioned in https://github.com/devfile/api/issues/920 through `pod-overrides` and `container-overrides` attributes. However, https://github.com/devfile/library/pull/158 which is implementing this feature, is not part of v2.2.0 which is the latest release of the devfile which is being used in the devfile-gem. TODO: https://gitlab.com/gitlab-org/gitlab/-/issues/409189 Once devfile library releases a new version, update the devfile-gem and move the logic of setting the security context as part of workspace creation. @param [Array<Hash>] desired_config_array @param [Boolean] allow_privilege_escalation @param [Boolean] use_kubernetes_user_namespaces @return [void]",
    "label": "",
    "id": "2520"
  },
  {
    "raw_code": "def self.patch_default_resources(desired_config_array:, default_resources_per_workspace_container:)\n                pod_spec = find_pod_spec(desired_config_array)\n\n                container_types = [:initContainers, :containers]\n                container_types.each do |container_type|\n                  # the purpose of this deep_merge is to ensure\n                  # the values from the devfile override any defaults defined at the agent\n                  pod_spec.fetch(container_type).each do |container|\n                    container\n                      .fetch(:resources, {})\n                      .deep_merge!(default_resources_per_workspace_container) { |_, val, _| val }\n                  end",
    "comment": "@param [Array<Hash>] desired_config_array @param [Hash] default_resources_per_workspace_container @return [void]",
    "label": "",
    "id": "2521"
  },
  {
    "raw_code": "def self.inject_secrets(desired_config_array:, env_secret_name:, file_secret_name:)\n                volume = {\n                  name: VARIABLES_VOLUME_NAME,\n                  projected: {\n                    defaultMode: VARIABLES_VOLUME_DEFAULT_MODE,\n                    sources: [{ secret: { name: file_secret_name } }]\n                  }\n                }\n\n                volume_mount = {\n                  name: VARIABLES_VOLUME_NAME,\n                  mountPath: VARIABLES_VOLUME_PATH\n                }\n\n                env_from = [{ secretRef: { name: env_secret_name } }]\n\n                pod_spec = find_pod_spec(desired_config_array)\n                pod_spec.fetch(:volumes) << volume unless file_secret_name.empty?\n\n                pod_spec.fetch(:initContainers).each do |init_container|\n                  init_container.fetch(:volumeMounts) << volume_mount unless file_secret_name.empty?\n                  init_container[:envFrom] = env_from unless env_secret_name.empty?\n                end",
    "comment": "@param [Array<Hash>] desired_config_array @param [String] env_secret_name @param [String] file_secret_name @return [void]",
    "label": "",
    "id": "2522"
  },
  {
    "raw_code": "def self.set_service_account(desired_config_array:, service_account_name:)\n                find_pod_spec(desired_config_array)[:serviceAccountName] = service_account_name\n\n                nil\n              end",
    "comment": "@param [Array<Hash>] desired_config_array @param [String] service_account_name @return [void]",
    "label": "",
    "id": "2523"
  },
  {
    "raw_code": "def self.find_pod_spec(desired_config_array)\n                desired_config_array => [\n                  *_,\n                  {\n                    kind: \"Deployment\",\n                    spec: {\n                      template: {\n                        spec: pod_spec\n                      }\n                    }\n                  },\n                  *_\n                ]\n\n                pod_spec\n              end",
    "comment": "@param [Array<Hash>] desired_config_array @return [Hash]",
    "label": "",
    "id": "2524"
  },
  {
    "raw_code": "def self.append(context)\n                context => {\n                  common_annotations: common_annotations,\n                  common_annotations_for_partial_reconciliation: common_annotations_for_partial_reconciliation,\n                  desired_config_array: desired_config_array,\n                  env_secret_name: env_secret_name,\n                  file_secret_name: file_secret_name,\n                  gitlab_workspaces_proxy_namespace: gitlab_workspaces_proxy_namespace,\n                  image_pull_secrets: image_pull_secrets,\n                  labels: labels,\n                  max_resources_per_workspace: max_resources_per_workspace,\n                  network_policy_egress: network_policy_egress,\n                  network_policy_enabled: network_policy_enabled,\n                  processed_devfile_yaml: processed_devfile_yaml,\n                  scripts_configmap_name: scripts_configmap_name,\n                  secrets_inventory_annotations: secrets_inventory_annotations,\n                  secrets_inventory_name: secrets_inventory_name,\n                  shared_namespace: shared_namespace,\n                  workspace_inventory_annotations: workspace_inventory_annotations,\n                  workspace_inventory_annotations_for_partial_reconciliation:\n                  workspace_inventory_annotations_for_partial_reconciliation,\n                  workspace_inventory_name: workspace_inventory_name,\n                  workspace_name: workspace_name,\n                  workspace_namespace: workspace_namespace\n                }\n\n                append_inventory_configmap(\n                  desired_config_array: desired_config_array,\n                  name: workspace_inventory_name,\n                  namespace: workspace_namespace,\n                  labels: labels,\n                  annotations: common_annotations_for_partial_reconciliation,\n                  prepend: true\n                )\n\n                append_image_pull_secrets_service_account(\n                  desired_config_array: desired_config_array,\n                  name: workspace_name,\n                  namespace: workspace_namespace,\n                  image_pull_secrets: image_pull_secrets,\n                  labels: labels,\n                  annotations: workspace_inventory_annotations_for_partial_reconciliation\n                )\n\n                append_network_policy(\n                  desired_config_array: desired_config_array,\n                  name: workspace_name,\n                  namespace: workspace_namespace,\n                  gitlab_workspaces_proxy_namespace: gitlab_workspaces_proxy_namespace,\n                  network_policy_enabled: network_policy_enabled,\n                  network_policy_egress: network_policy_egress,\n                  labels: labels,\n                  annotations: workspace_inventory_annotations_for_partial_reconciliation\n                )\n\n                append_scripts_resources(\n                  desired_config_array: desired_config_array,\n                  processed_devfile_yaml: processed_devfile_yaml,\n                  name: scripts_configmap_name,\n                  namespace: workspace_namespace,\n                  labels: labels,\n                  annotations: workspace_inventory_annotations_for_partial_reconciliation\n                )\n\n                append_inventory_configmap(\n                  desired_config_array: desired_config_array,\n                  name: secrets_inventory_name,\n                  namespace: workspace_namespace,\n                  labels: labels,\n                  annotations: common_annotations\n                )\n\n                append_resource_quota(\n                  desired_config_array: desired_config_array,\n                  name: workspace_name,\n                  namespace: workspace_namespace,\n                  labels: labels,\n                  annotations: workspace_inventory_annotations,\n                  max_resources_per_workspace: max_resources_per_workspace,\n                  shared_namespace: shared_namespace\n                )\n\n                append_secret(\n                  desired_config_array: desired_config_array,\n                  name: env_secret_name,\n                  namespace: workspace_namespace,\n                  labels: labels,\n                  annotations: secrets_inventory_annotations\n                )\n\n                append_secret(\n                  desired_config_array: desired_config_array,\n                  name: file_secret_name,\n                  namespace: workspace_namespace,\n                  labels: labels,\n                  annotations: secrets_inventory_annotations\n                )\n\n                context.merge({ desired_config_array: desired_config_array })\n              end",
    "comment": "@param [Hash] context @return [Hash]",
    "label": "",
    "id": "2525"
  },
  {
    "raw_code": "def self.append_inventory_configmap(\n                desired_config_array:,\n                name:,\n                namespace:,\n                labels:,\n                annotations:,\n                prepend: false\n              )\n                extra_labels = { \"cli-utils.sigs.k8s.io/inventory-id\": name }\n\n                configmap = {\n                  kind: \"ConfigMap\",\n                  apiVersion: \"v1\",\n                  metadata: {\n                    name: name,\n                    namespace: namespace,\n                    labels: labels.merge(extra_labels),\n                    annotations: annotations\n                  }\n                }\n\n                if prepend\n                  desired_config_array.prepend(configmap)\n                else\n                  desired_config_array.append(configmap)\n                end",
    "comment": "@param [Array] desired_config_array @param [String] name @param [String] namespace @param [Hash<String, String>] labels @param [Hash<String, String>] annotations @param [Boolean] prepend -- If true, prepend the configmap to the desired_config_array @return [void]",
    "label": "",
    "id": "2526"
  },
  {
    "raw_code": "def self.append_secret(desired_config_array:, name:, namespace:, labels:, annotations:)\n                secret = {\n                  kind: \"Secret\",\n                  apiVersion: \"v1\",\n                  metadata: {\n                    name: name,\n                    namespace: namespace,\n                    labels: labels,\n                    annotations: annotations\n                  },\n                  data: {}\n                }\n\n                desired_config_array.append(secret)\n\n                nil\n              end",
    "comment": "@param [Array] desired_config_array @param [String] name @param [String] namespace @param [Hash] labels @param [Hash] annotations @return [void]",
    "label": "",
    "id": "2527"
  },
  {
    "raw_code": "def self.append_network_policy(\n                desired_config_array:,\n                name:,\n                namespace:,\n                gitlab_workspaces_proxy_namespace:,\n                network_policy_enabled:,\n                network_policy_egress:,\n                labels:,\n                annotations:\n              )\n                return unless network_policy_enabled\n\n                egress_ip_rules = network_policy_egress\n\n                policy_types = %w[Ingress Egress]\n\n                proxy_namespace_selector = {\n                  matchLabels: {\n                    \"kubernetes.io/metadata.name\": gitlab_workspaces_proxy_namespace\n                  }\n                }\n                proxy_pod_selector = {\n                  matchLabels: {\n                    \"app.kubernetes.io/name\": \"gitlab-workspaces-proxy\"\n                  }\n                }\n                ingress = [{ from: [{ namespaceSelector: proxy_namespace_selector, podSelector: proxy_pod_selector }] }]\n\n                kube_system_namespace_selector = {\n                  matchLabels: {\n                    \"kubernetes.io/metadata.name\": \"kube-system\"\n                  }\n                }\n                egress = [\n                  {\n                    ports: [{ port: 53, protocol: \"TCP\" }, { port: 53, protocol: \"UDP\" }],\n                    to: [{ namespaceSelector: kube_system_namespace_selector }]\n                  }\n                ]\n                egress_ip_rules.each do |egress_rule|\n                  egress.append(\n                    { to: [{ ipBlock: { cidr: egress_rule[:allow], except: egress_rule[:except] } }] }\n                  )\n                end",
    "comment": "@param [Array] desired_config_array @param [String] gitlab_workspaces_proxy_namespace @param [String] name @param [String] namespace @param [Boolean] network_policy_enabled @param [Array] network_policy_egress @param [Hash] labels @param [Hash] annotations @return [void]",
    "label": "",
    "id": "2528"
  },
  {
    "raw_code": "def self.append_scripts_resources(\n                desired_config_array:,\n                processed_devfile_yaml:,\n                name:,\n                namespace:,\n                labels:,\n                annotations:\n              )\n                desired_config_array => [\n                  *_,\n                  {\n                    kind: \"Deployment\",\n                    spec: {\n                      template: {\n                        spec: {\n                          containers: Array => containers,\n                          volumes: Array => volumes\n                        }\n                      }\n                    }\n                  },\n                  *_\n                ]\n\n                processed_devfile = YAML.safe_load(processed_devfile_yaml).deep_symbolize_keys.to_h\n\n                devfile_commands = processed_devfile.fetch(:commands)\n                devfile_events = processed_devfile.fetch(:events)\n\n                # NOTE: This guard clause ensures we still support older running workspaces which were started before we\n                #       added support for devfile postStart events. In that case, we don't want to add any resources\n                #       related to the postStart script handling, because that would cause those existing workspaces\n                #       to restart because the deployment would be updated.\n                return unless devfile_events[:postStart].present?\n\n                BmScriptsConfigmapAppender.append(\n                  desired_config_array: desired_config_array,\n                  name: name,\n                  namespace: namespace,\n                  labels: labels,\n                  annotations: annotations,\n                  devfile_commands: devfile_commands,\n                  devfile_events: devfile_events\n                )\n\n                BmScriptsVolumeInserter.insert(\n                  configmap_name: name,\n                  containers: containers,\n                  volumes: volumes\n                )\n\n                BmKubernetesPoststartHookInserter.insert(\n                  containers: containers,\n                  devfile_commands: devfile_commands,\n                  devfile_events: devfile_events\n                )\n\n                nil\n              end",
    "comment": "@param [Array] desired_config_array @param [String] processed_devfile_yaml @param [String] name @param [String] namespace @param [Hash] labels @param [Hash] annotations @return [void]",
    "label": "",
    "id": "2529"
  },
  {
    "raw_code": "def self.append_resource_quota(\n                desired_config_array:,\n                name:,\n                namespace:,\n                labels:,\n                annotations:,\n                max_resources_per_workspace:,\n                shared_namespace:\n              )\n                return unless max_resources_per_workspace.present?\n                return if shared_namespace.present?\n\n                max_resources_per_workspace => {\n                  limits: {\n                    cpu: limits_cpu,\n                    memory: limits_memory\n                  },\n                  requests: {\n                    cpu: requests_cpu,\n                    memory: requests_memory\n                  }\n                }\n\n                resource_quota = {\n                  apiVersion: \"v1\",\n                  kind: \"ResourceQuota\",\n                  metadata: {\n                    annotations: annotations,\n                    labels: labels,\n                    name: name,\n                    namespace: namespace\n                  },\n                  spec: {\n                    hard: {\n                      \"limits.cpu\": limits_cpu,\n                      \"limits.memory\": limits_memory,\n                      \"requests.cpu\": requests_cpu,\n                      \"requests.memory\": requests_memory\n                    }\n                  }\n                }\n\n                desired_config_array.append(resource_quota)\n\n                nil\n              end",
    "comment": "@param [Array] desired_config_array @param [String] name @param [String] namespace @param [Hash] labels @param [Hash] annotations @param [Hash] max_resources_per_workspace @param [String] shared_namespace @return [void]",
    "label": "",
    "id": "2530"
  },
  {
    "raw_code": "def self.append_image_pull_secrets_service_account(\n                desired_config_array:,\n                name:,\n                namespace:,\n                labels:,\n                annotations:,\n                image_pull_secrets:\n              )\n                image_pull_secrets_names = image_pull_secrets.map { |secret| { name: secret.fetch(:name) } }\n\n                workspace_service_account_definition = {\n                  apiVersion: \"v1\",\n                  kind: \"ServiceAccount\",\n                  metadata: {\n                    name: name,\n                    namespace: namespace,\n                    annotations: annotations,\n                    labels: labels\n                  },\n                  automountServiceAccountToken: false,\n                  imagePullSecrets: image_pull_secrets_names\n                }\n\n                desired_config_array.append(workspace_service_account_definition)\n\n                nil\n              end",
    "comment": "@param [Array] desired_config_array @param [String] name @param [String] namespace @param [Hash] labels @param [Hash] annotations @param [Array] image_pull_secrets @return [void]",
    "label": "",
    "id": "2531"
  },
  {
    "raw_code": "def self.get(context) # rubocop:disable Metrics/MethodLength -- This method is complex but we don't need to break it up.\n                context => {\n                  logger: logger,\n                  processed_devfile_yaml: processed_devfile_yaml,\n                  workspace_inventory_annotations_for_partial_reconciliation:\n                  workspace_inventory_annotations_for_partial_reconciliation,\n                  domain_template: domain_template,\n                  labels: labels,\n                  workspace_name: workspace_name,\n                  workspace_namespace: workspace_namespace,\n                  replicas: replicas\n                }\n\n                begin\n                  context.merge(\n                    desired_config_yaml: Devfile::Parser.get_all(\n                      processed_devfile_yaml,\n                      workspace_name,\n                      workspace_namespace,\n                      YAML.dump(labels.deep_stringify_keys),\n                      YAML.dump(workspace_inventory_annotations_for_partial_reconciliation.deep_stringify_keys),\n                      replicas,\n                      domain_template,\n                      'none'\n                    )\n                  )\n                rescue Devfile::CliError => e\n                  error_message = <<~MSG.squish\n                    #{e.class}: A non zero return code was observed when invoking the devfile CLI\n                    executable from the devfile gem.\n                  MSG\n                  logger.warn(\n                    message: error_message,\n                    error_type: 'create_devfile_parser_error',\n                    workspace_name: workspace_name,\n                    workspace_namespace: workspace_namespace,\n                    devfile_parser_error: e.message\n                  )\n                  raise e\n                rescue StandardError => e\n                  error_message = <<~MSG.squish\n                    #{e.class}: An unrecoverable error occurred when invoking the devfile gem,\n                    this may hint that a gem with a wrong architecture is being used.\n                  MSG\n                  logger.warn(\n                    message: error_message,\n                    error_type: 'create_devfile_parser_error',\n                    workspace_name: workspace_name,\n                    workspace_namespace: workspace_namespace,\n                    devfile_parser_error: e.message\n                  )\n                  raise e\n                end",
    "comment": "@param [Hash] context @return [Hash]",
    "label": "",
    "id": "2532"
  },
  {
    "raw_code": "def self.extract(context) # rubocop:disable Metrics/AbcSize -- this is a won't fix\n                context => {\n                  workspace_id: workspace_id,\n                  workspace_name: workspace_name,\n                  workspace_desired_state_is_running: workspace_desired_state_is_running,\n                  workspaces_agent_id: workspaces_agent_id,\n                  workspaces_agent_config: workspaces_agent_config\n                }\n\n                domain_template = \"{{.port}}-#{workspace_name}.#{workspaces_agent_config.dns_zone}\"\n\n                max_resources_per_workspace =\n                  deep_sort_and_symbolize_hashes(workspaces_agent_config.max_resources_per_workspace)\n                max_resources_per_workspace_sha256 = OpenSSL::Digest::SHA256.hexdigest(max_resources_per_workspace.to_s)\n\n                default_resources_per_workspace_container =\n                  deep_sort_and_symbolize_hashes(workspaces_agent_config.default_resources_per_workspace_container)\n\n                shared_namespace = workspaces_agent_config.shared_namespace\n                # TODO: Fix this as part of https://gitlab.com/gitlab-org/gitlab/-/issues/541902\n                shared_namespace = \"\" if shared_namespace.nil?\n\n                workspace_inventory_name = \"#{workspace_name}#{WORKSPACE_INVENTORY}\"\n                secrets_inventory_name = \"#{workspace_name}#{SECRETS_INVENTORY}\"\n\n                extra_annotations = {\n                  \"workspaces.gitlab.com/host-template\": domain_template.to_s,\n                  \"workspaces.gitlab.com/id\": workspace_id.to_s,\n                  # NOTE: This annotation is added to cause the workspace to restart whenever the max resources change\n                  \"workspaces.gitlab.com/max-resources-per-workspace-sha256\": max_resources_per_workspace_sha256\n                }\n                partial_reconcile_annotation = { ANNOTATION_KEY_INCLUDE_IN_PARTIAL_RECONCILIATION => \"true\" }\n                agent_annotations = workspaces_agent_config.annotations\n                common_annotations = deep_sort_and_symbolize_hashes(agent_annotations.merge(extra_annotations))\n                common_annotations_for_partial_reconciliation =\n                  deep_sort_and_symbolize_hashes(common_annotations.merge(partial_reconcile_annotation))\n                secrets_inventory_annotations = deep_sort_and_symbolize_hashes(\n                  common_annotations.merge(\"config.k8s.io/owning-inventory\": secrets_inventory_name)\n                )\n                workspace_inventory_annotations = deep_sort_and_symbolize_hashes(\n                  common_annotations.merge(\"config.k8s.io/owning-inventory\": workspace_inventory_name)\n                )\n                workspace_inventory_annotations_for_partial_reconciliation = deep_sort_and_symbolize_hashes(\n                  workspace_inventory_annotations.merge(partial_reconcile_annotation)\n                )\n\n                agent_labels = workspaces_agent_config.labels\n                labels = agent_labels.merge({ \"agent.gitlab.com/id\": workspaces_agent_id.to_s })\n                # TODO: Unconditionally add this label in https://gitlab.com/gitlab-org/gitlab/-/issues/535197\n                labels[\"workspaces.gitlab.com/id\"] = workspace_id.to_s if shared_namespace.present?\n\n                scripts_configmap_name = \"#{workspace_name}-scripts-configmap\"\n\n                context.merge({\n                  # Please keep alphabetized\n                  allow_privilege_escalation: workspaces_agent_config.allow_privilege_escalation,\n                  common_annotations: common_annotations,\n                  common_annotations_for_partial_reconciliation: common_annotations_for_partial_reconciliation,\n                  default_resources_per_workspace_container: default_resources_per_workspace_container,\n                  default_runtime_class: workspaces_agent_config.default_runtime_class,\n                  domain_template: domain_template,\n                  env_secret_name: \"#{workspace_name}#{ENV_VAR_SECRET_SUFFIX}\",\n                  file_secret_name: \"#{workspace_name}#{FILE_SECRET_SUFFIX}\",\n                  gitlab_workspaces_proxy_namespace: workspaces_agent_config.gitlab_workspaces_proxy_namespace,\n                  image_pull_secrets: deep_sort_and_symbolize_hashes(workspaces_agent_config.image_pull_secrets),\n                  labels: deep_sort_and_symbolize_hashes(labels),\n                  max_resources_per_workspace: max_resources_per_workspace,\n                  network_policy_egress: deep_sort_and_symbolize_hashes(workspaces_agent_config.network_policy_egress),\n                  network_policy_enabled: workspaces_agent_config.network_policy_enabled,\n                  replicas: workspace_desired_state_is_running ? 1 : 0,\n                  scripts_configmap_name: scripts_configmap_name,\n                  secrets_inventory_annotations: secrets_inventory_annotations,\n                  secrets_inventory_name: secrets_inventory_name,\n                  shared_namespace: shared_namespace,\n                  use_kubernetes_user_namespaces: workspaces_agent_config.use_kubernetes_user_namespaces,\n                  workspace_inventory_annotations: workspace_inventory_annotations,\n                  workspace_inventory_annotations_for_partial_reconciliation:\n                    workspace_inventory_annotations_for_partial_reconciliation,\n                  workspace_inventory_name: workspace_inventory_name\n                }).sort.to_h\n              end",
    "comment": "rubocop:disable Metrics/MethodLength -- this is a won't fix @param [Hash] context @return [Hash]",
    "label": "",
    "id": "2533"
  },
  {
    "raw_code": "def self.deep_sort_and_symbolize_hashes(collection)\n                collection_to_return = Gitlab::Utils.deep_sort_hashes(collection)\n\n                # NOTE: deep_symbolize_keys! is not available on Array, so we wrap the collection in a\n                #       Hash in case it is an Array.\n                { to_symbolize: collection_to_return }.deep_symbolize_keys!\n                collection_to_return\n              end",
    "comment": "rubocop:enable Metrics/MethodLength @param [Array, Hash] collection @return [Array, Hash]",
    "label": "",
    "id": "2534"
  },
  {
    "raw_code": "def desired_state_running?\n            desired_state == RUNNING\n          end",
    "comment": "@return [Boolean]",
    "label": "",
    "id": "2535"
  },
  {
    "raw_code": "def workspaces_agent_config\n            agent.unversioned_latest_workspaces_agent_config\n          end",
    "comment": "@return [BackgroundMigration::Models::BmWorkspaceAgentConfig]",
    "label": "",
    "id": "2536"
  },
  {
    "raw_code": "def parse_job(job)\n        # Error information from the previous try is in the payload for\n        # displaying in the Sidekiq UI, but is very confusing in logs!\n        job = job.except(\n          'exception.backtrace', 'exception.class', 'exception.message', 'exception.sql',\n          'error_message', 'error_class', 'error_backtrace', 'failed_at'\n        )\n\n        job['class'] = job.delete('wrapped') if job['wrapped'].present?\n\n        job['job_size_bytes'] = Sidekiq.dump_json(job['args']).bytesize\n        job['args'] = ['[COMPRESSED]'] if ::Gitlab::SidekiqMiddleware::SizeLimiter::Compressor.compressed?(job)\n\n        # Add process id params\n        job['pid'] = ::Process.pid\n\n        job.delete('args') unless SidekiqLogArguments.enabled?\n\n        job\n      end",
    "comment": "NOTE: Arguments are truncated/stringified in sidekiq_logging/json_formatter.rb",
    "label": "",
    "id": "2537"
  },
  {
    "raw_code": "def suffix\n        Digest::SHA2.hexdigest(name.to_s).to_i(16).to_s(36).last(6)\n      end",
    "comment": "Slugifying a name may remove the uniqueness guarantee afforded by it being based on name (which must be unique). To compensate, we add a predictable 6-byte suffix in those circumstances. This is not *guaranteed* uniqueness, but the chance of collisions is vanishingly small",
    "label": "",
    "id": "2538"
  },
  {
    "raw_code": "def self.match(request)\n        Router::Graphql.match(request) || Router::Rails.match(request)\n      end",
    "comment": "Performing Rails routing match before GraphQL would be more expensive for the GraphQL requests because we need to traverse all of the RESTful route definitions before falling back to GraphQL.",
    "label": "",
    "id": "2539"
  },
  {
    "raw_code": "def self.all_routes\n          ROUTES\n        end",
    "comment": "Overridden in EE to add more routes",
    "label": "",
    "id": "2540"
  },
  {
    "raw_code": "def prefix_search(traversal_ids)\n        node = self\n        traversal_ids.each do |traversal_id|\n          return [] unless node.children[traversal_id]\n\n          node = node.children[traversal_id]\n        end",
    "comment": "Bring back all branches in the trie that match the prefix If trie contains [9970, 123] and [9970, 456] prefix_search([9970]) returns [[9970, 123], [9970, 456]]",
    "label": "",
    "id": "2541"
  },
  {
    "raw_code": "def covered?(traversal_ids)\n        current_node = self\n\n        traversal_ids.each do |traversal_id|\n          # If we've hit an end marker, it's covered\n          return true if current_node.end\n\n          # If the segment doesn't exist, it's not covered\n          return false unless current_node.children[traversal_id]\n\n          current_node = current_node.children[traversal_id]\n        end",
    "comment": "Check if traversal ID is already covered by a broader prefix or included in trie If trie contains [9970, 123] and [9970, 456] covered?([9970]) returns false covered?([9970, 123]) returns true covered?([9970, 123, 789]) returns true",
    "label": "",
    "id": "2542"
  },
  {
    "raw_code": "def insert(traversal_ids)\n        current_node = self\n\n        traversal_ids.each do |traversal_id|\n          # If we reach an end marker, this means a broader permission already exists\n          break if current_node.end\n\n          # Create new node for this segment if not present\n          current_node.children[traversal_id] ||= TrieNode.new\n          current_node = current_node.children[traversal_id]\n        end",
    "comment": "Insert traversal ID into the trie if it's not covered",
    "label": "",
    "id": "2543"
  },
  {
    "raw_code": "def additional_params\n        {}\n      end",
    "comment": "Overriden on Quality::Seeders::Insights::Issues",
    "label": "",
    "id": "2544"
  },
  {
    "raw_code": "def each_page(method, representation_type, *args)\n      options =\n        if args.last.is_a?(Hash)\n          args.last\n        else\n          {}\n        end",
    "comment": "Fetches data from the Bitbucket API and yields a Page object for every page of data, without loading all of them into memory.  method - The method name used for getting the data. representation_type - The representation type name used to wrap the result args - Arguments to pass to the method.",
    "label": "",
    "id": "2545"
  },
  {
    "raw_code": "def fetch_next_page\n      extra_query = { pagelen: max_per_page }\n      extra_query[:page] = page_number if page_number && limit\n\n      parsed_response = connection.get(next_url, extra_query)\n      Page.new(parsed_response, type)\n    end",
    "comment": "Note to self for specs: - Allowed pagelen to be set by limit instead of just using PAGE_LENGTH - Allow specifying a starting page to grab one page at a time, so PageCounter can be used for logging - Added over_limit? to make sure only one page is called.",
    "label": "",
    "id": "2546"
  },
  {
    "raw_code": "def initialize(api_url:, token:)\n      @api_url = api_url\n      @token = token\n    end",
    "comment": "@param api_url [String] Base URL of the Grafana instance @param token [String] Admin-level API token for instance",
    "label": "",
    "id": "2547"
  },
  {
    "raw_code": "def get_dashboard(uid:)\n      http_get(\"#{@api_url}/api/dashboards/uid/#{uid}\")\n    end",
    "comment": "@param uid [String] Unique identifier for a Grafana dashboard",
    "label": "",
    "id": "2548"
  },
  {
    "raw_code": "def get_datasource(name:)\n      # CGI#escape formats strings such that the Grafana endpoint\n      # will not recognize the dashboard name. Prefer Addressable::URI#encode_component.\n      http_get(\"#{@api_url}/api/datasources/name/#{Addressable::URI.encode_component(name)}\")\n    end",
    "comment": "@param name [String] Unique identifier for a Grafana datasource",
    "label": "",
    "id": "2549"
  },
  {
    "raw_code": "def proxy_datasource(datasource_id:, proxy_path:, query: {})\n      http_get(\"#{@api_url}/api/datasources/proxy/#{datasource_id}/#{proxy_path}\", query: query)\n    end",
    "comment": "@param datasource_id [String] Grafana ID for the datasource @param proxy_path [String] Path to proxy - ex) 'api/v1/query_range'",
    "label": "",
    "id": "2550"
  },
  {
    "raw_code": "def git_request?(query)\n      query.blank? ||\n        query == 'service=git-upload-pack' ||\n        query == 'service=git-receive-pack'\n    end",
    "comment": "Allow /info/refs, /info/refs?service=git-upload-pack, and /info/refs?service=git-receive-pack, but nothing else.",
    "label": "",
    "id": "2551"
  },
  {
    "raw_code": "def container_path?(path)\n      wiki_path?(path) ||\n        ProjectPathValidator.valid_path?(path) ||\n        path =~ Gitlab::PathRegex.full_snippets_repository_path_regex\n    end",
    "comment": "Check if the path matches any known repository containers.",
    "label": "",
    "id": "2552"
  },
  {
    "raw_code": "def wiki_path?(path)\n      NamespacePathValidator.valid_path?(path) && path.end_with?('.wiki')\n    end",
    "comment": "These also cover wikis, since a `.wiki` suffix is valid in project/group paths too.",
    "label": "",
    "id": "2553"
  },
  {
    "raw_code": "def force_created_at_from_iso8601(string_value)\n      date = parse_iso8601_string(string_value)\n      instance_variable_set(ivar(:memoized_created_at), date)\n    end",
    "comment": "this function will set and memoize a created_at to avoid a #config_blob call.",
    "label": "",
    "id": "2554"
  },
  {
    "raw_code": "def unsafe_delete\n      return unless digest\n\n      client.delete_repository_tag_by_digest(repository.path, digest)\n    end",
    "comment": "Deletes the image associated with this tag Note this will delete the image and all tags associated with it. Consider using DeleteTagsService instead.",
    "label": "",
    "id": "2555"
  },
  {
    "raw_code": "def supports_gitlab_api?\n      strong_memoize(:supports_gitlab_api) do\n        registry_features = Gitlab::CurrentSettings.container_registry_features || []\n        next true if ::Gitlab.com_except_jh? && registry_features.include?(REGISTRY_GITLAB_V1_API_FEATURE)\n\n        with_token_faraday do |faraday_client|\n          response = faraday_client.get('/gitlab/v1/')\n          response.success? || response.status == 401\n        end",
    "comment": "https://gitlab.com/gitlab-org/container-registry/-/blob/master/docs/spec/gitlab/api.md#compliance-check",
    "label": "",
    "id": "2556"
  },
  {
    "raw_code": "def repository_details(path, sizing: nil)\n      with_token_faraday do |faraday_client|\n        req = faraday_client.get(\"#{GITLAB_REPOSITORIES_PATH}/#{path}/\") do |req|\n          req.params['size'] = sizing if sizing\n        end",
    "comment": "https://gitlab.com/gitlab-org/container-registry/-/blob/master/docs/spec/gitlab/api.md#get-repository-details",
    "label": "",
    "id": "2557"
  },
  {
    "raw_code": "def tags(path, page_size: 100, last: nil, before: nil, name: nil, sort: nil, referrers: nil, referrer_type: nil)\n      limited_page_size = [page_size, MAX_TAGS_PAGE_SIZE].min\n      with_token_faraday do |faraday_client|\n        url = \"#{GITLAB_REPOSITORIES_PATH}/#{path}/tags/list/\"\n        response = faraday_client.get(url) do |req|\n          req.params['n'] = limited_page_size\n          req.params['last'] = last if last\n          req.params['before'] = before if before\n          req.params['name'] = name if name.present?\n          req.params['sort'] = sort if sort\n          req.params['referrers'] = 'true' if referrers\n          req.params['referrer_type'] = referrer_type if referrer_type\n        end",
    "comment": "https://gitlab.com/gitlab-org/container-registry/-/blob/master/docs/spec/gitlab/api.md#list-repository-tags",
    "label": "",
    "id": "2558"
  },
  {
    "raw_code": "def sub_repositories_with_tag(path, page_size: 100, last: nil)\n      limited_page_size = [page_size, MAX_REPOSITORIES_PAGE_SIZE].min\n\n      with_token_faraday do |faraday_client|\n        url = \"/gitlab/v1/repository-paths/#{path}/repositories/list/\"\n        response = faraday_client.get(url) do |req|\n          req.params['n'] = limited_page_size\n          req.params['last'] = last if last\n        end",
    "comment": "https://gitlab.com/gitlab-org/container-registry/-/blob/master/docs/spec/gitlab/api.md#list-sub-repositories",
    "label": "",
    "id": "2559"
  },
  {
    "raw_code": "def rename_base_repository_path(path, name:, dry_run: false)\n      patch_repository(path, { name: name }, dry_run: dry_run)\n    end",
    "comment": "Given a path 'group/subgroup/project' and name 'newname', with a successful rename, it will be 'group/subgroup/newname' https://gitlab.com/gitlab-org/container-registry/-/blob/master/docs/spec/gitlab/api.md#rename-base-repository",
    "label": "",
    "id": "2560"
  },
  {
    "raw_code": "def move_repository_to_namespace(path, namespace:, dry_run: false)\n      patch_repository(path, { namespace: namespace }, dry_run: dry_run)\n    end",
    "comment": "Given a path 'group/subgroup/project' and a namespace 'group/subgroup_2' with a successful move, it will be 'group/subgroup_2/project' https://gitlab.com/gitlab-org/container-registry/-/blob/master/docs/spec/gitlab/api.md#renamemove-origin-repository",
    "label": "",
    "id": "2561"
  },
  {
    "raw_code": "def statistics\n      with_token_faraday do |faraday_client|\n        req = faraday_client.get('/gitlab/v1/statistics/')\n        result = response_body(req)\n\n        break {} unless result.present?\n\n        {\n          features: result.dig('release', 'ext_features')&.split(',') || [],\n          version: result.dig('release', 'version'),\n          db_enabled: result.dig('database', 'enabled')\n        }\n      end",
    "comment": "https://gitlab.com/gitlab-org/container-registry/-/blob/master/docs/spec/gitlab/api.md?ref_type=heads#get-registry-statistics example output: {\"release\"=>{\"ext_features\"=>\"tag_delete\", \"version\"=>\"v4.20\"}, \"database\"=>{\"enabled\"=>true}}",
    "label": "",
    "id": "2562"
  },
  {
    "raw_code": "def configure_connection(conn)\n      conn.headers['Accept'] = [JSON_TYPE]\n\n      conn.response :json, content_type: JSON_TYPE\n    end",
    "comment": "overrides the default configuration",
    "label": "",
    "id": "2563"
  },
  {
    "raw_code": "def supports_tag_delete?\n      strong_memoize(:supports_tag_delete) do\n        registry_features = Gitlab::CurrentSettings.container_registry_features || []\n        next true if ::Gitlab.com_except_jh? && registry_features.include?(REGISTRY_TAG_DELETE_FEATURE)\n\n        response = faraday.run_request(:options, '/v2/name/manifests/tag', '', {})\n        response.success? && response.headers['allow']&.include?('DELETE')\n      end",
    "comment": "Check if the registry supports tag deletion. This is only supported by the GitLab registry fork. The fastest and safest way to check this is to send an OPTIONS request to /v2/<name>/manifests/<tag>, using a random repository name and tag (the registry won't check if they exist). Registries that support tag deletion will reply with a 200 OK and include the DELETE method in the Allow header. Others reply with an 404 Not Found.",
    "label": "",
    "id": "2564"
  },
  {
    "raw_code": "def initialize(path, project: nil)\n      @path = path.to_s.downcase\n      @project = project\n    end",
    "comment": "The 'project' argument is optional. If provided during initialization, it will limit the path to the specified project, potentially reducing the need for a database query.",
    "label": "",
    "id": "2565"
  },
  {
    "raw_code": "def nodes\n      raise InvalidRegistryPathError unless valid?\n\n      @nodes ||= components.size.downto(2).map do |length|\n        components.take(length).join('/')\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2566"
  },
  {
    "raw_code": "def has_project?\n      repository_project.present?\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2567"
  },
  {
    "raw_code": "def has_repository?\n      return false unless has_project?\n\n      repository_project.container_repositories\n        .where(name: repository_name).any?\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2568"
  },
  {
    "raw_code": "def root_repository?\n      @path == project_path\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2569"
  },
  {
    "raw_code": "def self.filter_subscription_name\n      \"#{HTML_PIPELINE_SUBSCRIPTION}_#{Thread.current.object_id}\"\n    end",
    "comment": "Use thread ID to make subscription unique per thread",
    "label": "",
    "id": "2570"
  },
  {
    "raw_code": "def insert_after(after_value, value)\n      i = index(after_value) || (length - 1)\n\n      insert(i + 1, value)\n    end",
    "comment": "Insert a value immediately after another value  If the preceding value does not exist, the new value is added to the end of the Array.",
    "label": "",
    "id": "2571"
  },
  {
    "raw_code": "def insert_before(before_value, value)\n      i = index(before_value) || -1\n\n      if i < 0\n        unshift(value)\n      else\n        insert(i, value)\n      end",
    "comment": "Insert a value immediately before another value  If the succeeding value does not exist, the new value is added to the beginning of the Array.",
    "label": "",
    "id": "2572"
  },
  {
    "raw_code": "def initialize(default_project = nil, current_user = nil, options: {})\n      @current_user = current_user\n      @projects = Hash.new(default_project)\n      @options = options\n    end",
    "comment": "default_project - The default project to use for all documents, if any. current_user - The user viewing the document, if any.",
    "label": "",
    "id": "2573"
  },
  {
    "raw_code": "def associate_document(document, object)\n      # XML nodes respond to \"document\" but will return a Document instance,\n      # even when they belong to a DocumentFragment.\n      document = document.document if document.fragment?\n\n      @projects[document] = object.project if object.respond_to?(:project)\n    end",
    "comment": "Associates an HTML document with a Project.  document - The HTML document to map to a Project. object - The object that produced the HTML document.",
    "label": "",
    "id": "2574"
  },
  {
    "raw_code": "def parent_from_ref(ref)\n      return context[:project] || context[:group] unless ref\n      return context[:project] if context[:project]&.full_path == ref\n      return context[:group] if context[:group]&.full_path == ref\n\n      if reference_cache.cache_loaded?\n        # optimization to reuse the parent_per_reference query information\n        reference_cache.parent_per_reference[ref || reference_cache.current_parent_path]\n      else\n        Namespace.find_by_full_path(ref) || Project.find_by_full_path(ref)\n      end",
    "comment": "Given a cross-namespace reference string, get the Namespace record.  Defaults to value of `context[:project]`, or `context[:group]` if: * No reference is given OR * Reference given doesn't exist  ref - String reference.  Returns a Namespace or Project, or nil if the reference can't be found",
    "label": "",
    "id": "2575"
  },
  {
    "raw_code": "def css(document, query, reference_options = {})\n      # When using \"a.foo\" Nokogiri compiles this to \"//a[...]\" but\n      # \"descendant::a[...]\" is quite a bit faster and achieves the same result.\n      xpath = Nokogiri::CSS.xpath_for(query)[0].gsub(%r{^//}, 'descendant::')\n      xpath = restrict_to_p_nodes_at_root(xpath) if filter_nodes_at_beginning?(reference_options)\n      nodes = document.xpath(xpath)\n\n      filter_nodes(nodes, reference_options)\n    end",
    "comment": "Searches a Nokogiri document using a CSS query, optionally optimizing it whenever possible.  document          - A document/element to search. query             - The CSS query to use. reference_options - A hash with nodes filter options  Returns an array of Nokogiri::XML::Element objects if location is specified in reference_options. Otherwise it would a Nokogiri::XML::NodeSet.",
    "label": "",
    "id": "2576"
  },
  {
    "raw_code": "def filter_nodes_at_beginning(nodes)\n      parents_and_nodes = nodes.group_by(&:parent)\n      filtered_nodes = []\n\n      parents_and_nodes.each do |parent, nodes|\n        children = parent.children\n        nodes    = nodes.to_a\n\n        children.each do |child|\n          next if child.text.blank?\n\n          node = nodes.shift\n          break unless node == child\n\n          filtered_nodes << node\n        end",
    "comment": "Selects child nodes if they are present in the beginning among other siblings.  nodes - A Nokogiri::XML::NodeSet.  Returns an array of Nokogiri::XML::Element objects.",
    "label": "",
    "id": "2577"
  },
  {
    "raw_code": "def self.render(text, context = {})\n      cache_key = context.delete(:cache_key)\n      cache_key = full_cache_key(cache_key, context[:pipeline])\n\n      if cache_key\n        Rails.cache.fetch(cache_key) do\n          cacheless_render(text, context)\n        end",
    "comment": "Convert a Markdown String into an HTML-safe String of HTML  Note that while the returned HTML will have been sanitized of dangerous HTML, it may post a risk of information leakage if it's not also passed through `post_process`.  Also note that the returned String is always HTML, not XHTML. Views requiring XHTML, such as Atom feeds, need to call `post_process` on the result, providing the appropriate `pipeline` option.  text     - Markdown String context  - Hash of context options passed to our HTML Pipeline  Returns an HTML-safe String",
    "label": "",
    "id": "2578"
  },
  {
    "raw_code": "def self.render_field(object, field, context = {})\n      unless object.respond_to?(:cached_markdown_fields)\n        return cacheless_render_field(object, field, context)\n      end",
    "comment": "Convert a Markdown-containing field on an object into an HTML-safe String of HTML. This method is analogous to calling render(object.field), but it can cache the rendered HTML in the object, rather than Redis.",
    "label": "",
    "id": "2579"
  },
  {
    "raw_code": "def self.cacheless_render_field(object, field, context = {})\n      text = object.__send__(field) # rubocop:disable GitlabSecurity/PublicSend\n      context = context.reverse_merge(object.banzai_render_context(field)) if object.respond_to?(:banzai_render_context)\n\n      cacheless_render(text, context)\n    end",
    "comment": "Same as +render_field+, but without consulting or updating the cache field",
    "label": "",
    "id": "2580"
  },
  {
    "raw_code": "def self.cache_collection_render(texts_and_contexts)\n      items_collection = texts_and_contexts.each do |item|\n        context = item[:context]\n\n        if context.key?(:rendered)\n          item[:rendered] = context.delete(:rendered)\n        else\n          # If the attribute didn't come in pre-rendered, let's prepare it for caching it in redis\n          cache_key = full_cache_multi_key(context.delete(:cache_key), context[:pipeline])\n          item[:cache_key] = cache_key if cache_key\n        end",
    "comment": "Perform multiple render from an Array of Markdown String into an Array of HTML-safe String of HTML.  The redis cache is completely obviated if we receive a `:rendered` key in the context, as it is assumed the item has been pre-rendered somewhere else and there is no need to cache it.  If no `:rendered` key is present in the context, as the rendered Markdown String can be already cached, read all the data from the cache using Rails.cache.read_multi operation. If the Markdown String is not in the cache or it's not cacheable (no cache_key entry is provided in the context) the Markdown String is rendered and stored in the cache so the next render call gets the rendered HTML-safe String from the cache.  For further explanation see #render method comments.  texts_and_contexts - An Array of Hashes that contains the Markdown String (:text) an options passed to our HTML Pipeline (:context)  If on the :context you specify a :cache_key entry will be used to retrieve it and cache the result of rendering the Markdown String.  Returns an Array containing HTML-safe String instances.  Example: texts_and_contexts => [{ text: '### Hello', context: { cache_key: [note, :note] } }]",
    "label": "",
    "id": "2581"
  },
  {
    "raw_code": "def self.post_process(html, context)\n      context = Pipeline[context[:pipeline]].transform_context(context)\n\n      # Use a passed class for the pipeline or default to PostProcessPipeline\n      pipeline = context.delete(:post_process_pipeline) || ::Banzai::Pipeline::PostProcessPipeline\n\n      instrument_filters do\n        if context[:xhtml]\n          pipeline.to_document(html, context).to_html(save_with: Nokogiri::XML::Node::SaveOptions::AS_XHTML)\n        else\n          pipeline.to_html(html, context)\n        end.html_safe\n      end",
    "comment": "Perform post-processing on an HTML String  This method is used to perform state-dependent changes to a String of HTML, such as removing references that the current user doesn't have permission to make (`ReferenceRedactorFilter`).  html     - String to process context  - Hash of options to customize output :pipeline  - Symbol pipeline type - for context transform only, defaults to :full :project   - Project :user      - User object :post_process_pipeline - pipeline to use for post_processing - defaults to PostProcessPipeline  Returns an HTML-safe String",
    "label": "",
    "id": "2582"
  },
  {
    "raw_code": "def self.full_cache_multi_key(cache_key, pipeline_name)\n      return unless cache_key\n\n      Rails.cache.__send__(:expanded_key, full_cache_key(cache_key, pipeline_name)) # rubocop:disable GitlabSecurity/PublicSend\n    end",
    "comment": "To map Rails.cache.read_multi results we need to know the Rails.cache.expanded_key. Other option will be to generate stringified keys on our side and don't delegate to Rails.cache.expanded_key method.",
    "label": "",
    "id": "2583"
  },
  {
    "raw_code": "def self.instrument_filters\n      service = ActiveSupport::Notifications\n      Banzai::PipelineBase.default_instrumentation_service = service\n      subscription_name = Banzai::PipelineBase.filter_subscription_name\n\n      service.monotonic_subscribe(subscription_name) do |_event, start, ending, _transaction_id, payload|\n        duration = ending - start\n        payload[:result][:pipeline_timing] = payload[:result][:pipeline_timing].to_f + duration\n\n        if payload[:context][:debug] || payload[:context][:debug_timing]\n          duration_str = formatted_duration(duration)\n          pipeline_timing_str = formatted_duration(payload[:result][:pipeline_timing])\n          filter_name = payload[:filter].delete_prefix('Banzai::Filter::')\n          pipeline_name = payload[:pipeline].delete_prefix('Banzai::Pipeline::')\n\n          logger = Logger.new($stdout)\n          logger.debug \"#{duration_str} (#{pipeline_timing_str}): #{filter_name} [#{pipeline_name}]\"\n\n          if payload[:context][:debug]\n            logger.debug(payload)\n          end",
    "comment": "Instrumentation is used for a couple purposes - tracking timing of filters and the pipeline to manage performance and security. See `PipelineTimingCheck` - outputting debug timing/information for the pipeline. Example usage: Banzai.render(markdown, project: nil, debug_timing: true) Banzai.render(markdown, project: Project.first, debug: true)",
    "label": "",
    "id": "2584"
  },
  {
    "raw_code": "def self.[](name)\n      const_get(\"#{name.to_s.camelize}Parser\", false)\n    rescue NameError\n      raise InvalidReferenceType\n    end",
    "comment": "Returns the reference parser class for the given type  Example:  Banzai::ReferenceParser['issue']  This would return the `Banzai::ReferenceParser::IssueParser` class.",
    "label": "",
    "id": "2585"
  },
  {
    "raw_code": "def initialize(default_project: nil, user: nil, redaction_context: {})\n      @context = RenderContext.new(default_project, user)\n      @redaction_context = base_context.merge(redaction_context)\n    end",
    "comment": "default_project - A default Project to use for redacting Markdown. user - The user viewing the Markdown/HTML documents, if any. redaction_context - A Hash containing extra attributes to use during redaction",
    "label": "",
    "id": "2586"
  },
  {
    "raw_code": "def render(objects, attribute)\n      documents = render_documents(objects, attribute)\n      documents = post_process_documents(documents, objects, attribute)\n      redacted = redact_documents(documents)\n\n      objects.each_with_index do |object, index|\n        redacted_data = redacted[index]\n        object.__send__(\"redacted_#{attribute}_html=\", redacted_data[:document].to_html(save_options).html_safe) # rubocop:disable GitlabSecurity/PublicSend\n        object.user_visible_reference_count = redacted_data[:visible_reference_count] if object.respond_to?(:user_visible_reference_count)\n        object.total_reference_count = redacted_data[:total_reference_count] if object.respond_to?(:total_reference_count)\n      end",
    "comment": "Renders and redacts an Array of objects.  objects - The objects to render. attribute - The attribute containing the raw Markdown to render.  Returns the same input objects.",
    "label": "",
    "id": "2587"
  },
  {
    "raw_code": "def redact_documents(documents)\n      redactor = ReferenceRedactor.new(context)\n\n      redactor.redact(documents)\n    end",
    "comment": "Redacts the list of documents.  Returns an Array containing the redacted documents.",
    "label": "",
    "id": "2588"
  },
  {
    "raw_code": "def context_for(document, object, attribute)\n      @redaction_context.merge(object.banzai_render_context(attribute)).merge(\n        project: context.project_for_node(document)\n      )\n    end",
    "comment": "Returns a Banzai context for the given object and attribute.",
    "label": "",
    "id": "2589"
  },
  {
    "raw_code": "def self.[](name)\n      name ||= FullPipeline\n\n      pipeline = case name\n                 when Class\n                   name\n                 when Symbol\n                   const_get(\"#{name.to_s.camelize}Pipeline\", false)\n                 end",
    "comment": "Resolve a pipeline by name  name - nil, Class or Symbol. The name to be resolved.  Examples: Pipeline[nil] # => Banzai::Pipeline::FullPipeline Pipeline[:label] # => Banzai::Pipeline::LabelPipeline Pipeline[StatusPage::Pipeline::PostProcessPipeline] # => StatusPage::Pipeline::PostProcessPipeline  Pipeline['label'] # => raises ArgumentError - unsupport type Pipeline[Project] # => raises ArgumentError - not a subclass of BasePipeline  Returns a pipeline class which is a subclass of Banzai::Pipeline::BasePipeline.",
    "label": "",
    "id": "2590"
  },
  {
    "raw_code": "def self.parse(text)\n      text if COLOR_FORMAT.match?(text)\n    end",
    "comment": "Public: Analyzes whether the String is a color code.  text - The String to be parsed.  Returns the recognized color String or nil if none was found.",
    "label": "",
    "id": "2591"
  },
  {
    "raw_code": "def initialize(context)\n      @context = context\n    end",
    "comment": "context - An instance of Banzai::RenderContext.",
    "label": "",
    "id": "2592"
  },
  {
    "raw_code": "def extract(documents)\n      nodes = documents.flat_map do |document|\n        document.xpath(query)\n      end",
    "comment": "Returns Hash in the form { node => issuable_instance }",
    "label": "",
    "id": "2593"
  },
  {
    "raw_code": "def initialize(context)\n      @context = context\n    end",
    "comment": "context - An instance of `Banzai::RenderContext`.",
    "label": "",
    "id": "2594"
  },
  {
    "raw_code": "def redact(documents)\n      redact_cross_project_references(documents) unless can_read_cross_project?\n\n      all_document_nodes = document_nodes(documents)\n      redact_document_nodes(all_document_nodes)\n    end",
    "comment": "Redacts the references in the given Array of documents.  This method modifies the given documents in-place.  documents - A list of HTML documents containing references to redact.  Returns the documents passed as the first argument.",
    "label": "",
    "id": "2595"
  },
  {
    "raw_code": "def redact_document_nodes(all_document_nodes)\n      all_nodes = all_document_nodes.flat_map { |x| x[:nodes] }\n      visible = nodes_visible_to_user(all_nodes)\n      metadata = []\n\n      all_document_nodes.each do |entry|\n        nodes_for_document = entry[:nodes]\n\n        doc_data = {\n          document: entry[:document],\n          total_reference_count: nodes_for_document.count,\n          visible_reference_count: nodes_for_document.count\n        }\n\n        metadata << doc_data\n\n        nodes_for_document.each do |node|\n          next if visible.include?(node)\n\n          doc_data[:visible_reference_count] -= 1\n          redacted_content = redacted_node_content(node)\n          node.replace(redacted_content)\n        end",
    "comment": "Redacts the given node documents  data - An Array of a Hashes mapping an HTML document to nodes to redact.",
    "label": "",
    "id": "2596"
  },
  {
    "raw_code": "def redacted_node_content(node)\n      original_content = node.attr('data-original')\n      original_content = CGI.escape_html(original_content) if original_content\n\n      # Build the raw <a> tag just with a link as href and content if\n      # it's originally a link pattern. We shouldn't return a plain text href.\n      original_link =\n        if node.attr('data-link-reference') == 'true'\n          href = node.attr('href')\n\n          %(<a href=\"#{href}\">#{original_content}</a>)\n        end",
    "comment": "Return redacted content of given node as either the original link (<a> tag), the original content (text), or the inner HTML of the node. ",
    "label": "",
    "id": "2597"
  },
  {
    "raw_code": "def nodes_visible_to_user(nodes)\n      per_type = Hash.new { |h, k| h[k] = [] }\n      visible = Set.new\n\n      nodes.each do |node|\n        per_type[node.attr('data-reference-type')] << node\n      end",
    "comment": "Returns the nodes visible to the current user.  nodes - The input nodes to check.  Returns a new Array containing the visible nodes.",
    "label": "",
    "id": "2598"
  },
  {
    "raw_code": "def parent_from_ref(ref)\n      return context[:project] || context[:group] unless ref\n      return context[:project] if context[:project]&.full_path == ref\n\n      if reference_cache.cache_loaded?\n        # optimization to reuse the parent_per_reference query information\n        reference_cache.parent_per_reference[ref || reference_cache.current_parent_path]\n      else\n        Project.find_by_full_path(ref)\n      end",
    "comment": "Given a cross-project reference string, get the Project record  Defaults to value of `context[:project]`, or `context[:group]` if: * No reference is given OR * Reference given doesn't exist  ref - String reference.  Returns a Project, or nil if the reference can't be found",
    "label": "",
    "id": "2599"
  },
  {
    "raw_code": "def readable_project_ids_for(user)\n        @project_ids_by_user ||= {}\n        @project_ids_by_user[user] ||=\n          Project.public_or_visible_to_user(user).where(projects: { id: @projects_for_nodes.values.map(&:id) }).pluck(:id)\n      end",
    "comment": "Returns an Array of Project ids that can be read by the given user.  user - The User for which to check the projects",
    "label": "",
    "id": "2600"
  },
  {
    "raw_code": "def nodes_visible_to_user(user, nodes)\n        snippets = lazy { grouped_objects_for_nodes(nodes, references_relation, self.class.data_attribute) }\n\n        nodes.select do |node|\n          if node.has_attribute?(self.class.data_attribute)\n            can_read_reference?(user, snippets[node])\n          else\n            true\n          end",
    "comment": "Returns all the nodes that are visible to the given user.",
    "label": "",
    "id": "2601"
  },
  {
    "raw_code": "def can_read_reference?(user, ref_attr, node)\n        true\n      end",
    "comment": "any user can be mentioned by username",
    "label": "",
    "id": "2602"
  },
  {
    "raw_code": "def can_read_group_reference?(node, user, groups)\n        node_group = groups[node]\n\n        node_group && can?(user, :read_group, node_group)\n      end",
    "comment": "Check if project belongs to a group which user can read.",
    "label": "",
    "id": "2603"
  },
  {
    "raw_code": "def can_read_reference?(user, ref_project, node)\n        true\n      end",
    "comment": "we extract only external issue trackers references here, we don't extract cross-project references, so we don't need to do anything here.",
    "label": "",
    "id": "2604"
  },
  {
    "raw_code": "def self.data_attribute\n        @data_attribute ||= \"data-#{reference_type.to_s.dasherize}\"\n      end",
    "comment": "Returns the attribute name containing the value for every object to be parsed by the current parser.  For example, for a parser class that returns \"Animal\" objects this attribute would be \"data-animal\".",
    "label": "",
    "id": "2605"
  },
  {
    "raw_code": "def self.reference_class\n        reference_type.to_s.classify.constantize\n      end",
    "comment": "Returns a model class to use as a reference. By default, the method does not take namespaces into account, thus parser classes can customize the reference class to use a model name with a namespace",
    "label": "",
    "id": "2606"
  },
  {
    "raw_code": "def initialize(context)\n        @context = context\n      end",
    "comment": "context - An instance of `Banzai::RenderContext`.",
    "label": "",
    "id": "2607"
  },
  {
    "raw_code": "def nodes_user_can_reference(user, nodes)\n        nodes\n      end",
    "comment": "Returns all the nodes containing references that the user can refer to.",
    "label": "",
    "id": "2608"
  },
  {
    "raw_code": "def nodes_visible_to_user(user, nodes)\n        projects = lazy { projects_for_nodes(nodes) }\n        groups = lazy { groups_for_nodes(nodes) }\n        project_attr = 'data-project'\n        group_attr = 'data-group'\n\n        preload_associations(projects, user)\n        preload_group_associations(groups, user)\n\n        nodes.select do |node|\n          if node.has_attribute?(project_attr)\n            can_read_reference?(user, projects[node], node)\n          elsif node.has_attribute?(group_attr)\n            can_read_reference?(user, groups[node], node)\n          else\n            true\n          end",
    "comment": "Returns all the nodes that are visible to the given user.",
    "label": "",
    "id": "2609"
  },
  {
    "raw_code": "def referenced_by(nodes, options = {})\n        ids = unique_attribute_values(nodes, self.class.data_attribute)\n\n        return ids if options.fetch(:ids_only, false)\n\n        if ids.empty?\n          references_relation.none\n        else\n          references_relation.where(id: ids)\n        end",
    "comment": "Returns an Array of objects referenced by any of the given HTML nodes.",
    "label": "",
    "id": "2610"
  },
  {
    "raw_code": "def references_relation\n        raise NotImplementedError,\n          \"#{self.class} does not implement #{__method__}\"\n      end",
    "comment": "Returns the ActiveRecord::Relation to use for querying references in the DB.",
    "label": "",
    "id": "2611"
  },
  {
    "raw_code": "def gather_attributes_per_project(nodes, attribute)\n        per_project = Hash.new { |hash, key| hash[key] = Set.new }\n\n        nodes.each do |node|\n          project_id = node.attr('data-project').to_i\n          id = node.attr(attribute)\n\n          per_project[project_id] << id if id\n        end",
    "comment": "Returns a Hash containing attribute values per project ID.  The returned Hash uses the following format:  { project id => [value1, value2, ...] }  nodes - An Array of HTML nodes to process. attribute - The name of the attribute (as a String) for which to gather values.  Returns a Hash.",
    "label": "",
    "id": "2612"
  },
  {
    "raw_code": "def grouped_objects_for_nodes(nodes, collection, attribute)\n        return {} if nodes.empty?\n\n        ids = unique_attribute_values(nodes, attribute)\n        collection_objects = collection_objects_for_ids(collection, ids)\n        objects_by_id = collection_objects.index_by(&:id)\n\n        nodes.each_with_object({}) do |node, hash|\n          if node.has_attribute?(attribute)\n            obj = objects_by_id[node.attr(attribute).to_i]\n            hash[node] = obj if obj\n          end",
    "comment": "Returns a Hash containing objects for an attribute grouped per the nodes that reference them.  The returned Hash uses the following format:  { node => row }  nodes - An Array of HTML nodes to process.  collection - The model or ActiveRecord relation to use for retrieving rows from the database.  attribute - The name of the attribute containing the primary key values for every row.  Returns a Hash.",
    "label": "",
    "id": "2613"
  },
  {
    "raw_code": "def unique_attribute_values(nodes, attribute)\n        values = Set.new\n\n        nodes.each do |node|\n          if node.has_attribute?(attribute)\n            values << node.attr(attribute)\n          end",
    "comment": "Returns an Array containing all unique values of an attribute of the given nodes.",
    "label": "",
    "id": "2614"
  },
  {
    "raw_code": "def collection_objects_for_ids(collection, ids)\n        if Gitlab::SafeRequestStore.active?\n          ids = ids.map(&:to_i).uniq\n\n          cache = collection_cache[collection_cache_key(collection)]\n          to_query = ids - cache.keys\n\n          unless to_query.empty?\n            collection.where(id: to_query).each { |row| cache[row.id] = row }\n          end",
    "comment": "Queries the collection for the objects with the given IDs.  If the RequestStore module is enabled this method will only query any objects that have not yet been queried. For objects that have already been queried the object is returned from the cache.",
    "label": "",
    "id": "2615"
  },
  {
    "raw_code": "def collection_cache_key(collection)\n        collection.respond_to?(:model) ? collection.model : collection\n      end",
    "comment": "Returns the cache key to use for a collection.",
    "label": "",
    "id": "2616"
  },
  {
    "raw_code": "def process(documents, ids_only: false)\n        type = self.class.reference_type\n        reference_options = self.class.reference_options\n\n        nodes = documents.flat_map do |document|\n          Querying.css(document, \"a[data-reference-type='#{type}'].gfm\", reference_options).to_a\n        end",
    "comment": "Processes the list of HTML documents and returns an Array containing all the references.",
    "label": "",
    "id": "2617"
  },
  {
    "raw_code": "def gather_references(nodes, ids_only: false)\n        nodes = nodes_user_can_reference(current_user, nodes)\n        visible = nodes_visible_to_user(current_user, nodes)\n\n        { visible: referenced_by(visible, ids_only: ids_only), nodes: nodes, visible_nodes: visible }\n      end",
    "comment": "Gathers the references for the given HTML nodes.  Returns visible references and a list of nodes which are not visible to the user",
    "label": "",
    "id": "2618"
  },
  {
    "raw_code": "def projects_for_nodes(nodes)\n        @projects_for_nodes ||= grouped_objects_for_nodes(nodes, Project.includes(:project_feature), 'data-project')\n      end",
    "comment": "Returns a Hash containing the projects for a given list of HTML nodes.  The returned Hash uses the following format:  { node => project } ",
    "label": "",
    "id": "2619"
  },
  {
    "raw_code": "def can_read_reference?(user, ref_project, node)\n        raise NotImplementedError\n      end",
    "comment": "When a feature is disabled or visible only for team members we should not allow team members see reference comments. Override this method on subclasses to check if user can read resource",
    "label": "",
    "id": "2620"
  },
  {
    "raw_code": "def preload_associations(projects, user)\n        ::Preloaders::ProjectPolicyPreloader.new(projects.values, user).execute\n      end",
    "comment": "For any preloading of project associations needed to avoid N+1s. Note: `projects` param is a hash of { node => project }. See #projects_for_nodes for more information.",
    "label": "",
    "id": "2621"
  },
  {
    "raw_code": "def self.reference_filters\n        [\n          Filter::References::UserReferenceFilter,\n          Filter::References::IssueReferenceFilter,\n          Filter::References::WorkItemReferenceFilter,\n          Filter::References::ExternalIssueReferenceFilter,\n          Filter::References::MergeRequestReferenceFilter,\n          Filter::References::SnippetReferenceFilter,\n          Filter::References::CommitRangeReferenceFilter,\n          Filter::References::CommitReferenceFilter,\n          Filter::References::AlertReferenceFilter,\n          Filter::References::FeatureFlagReferenceFilter,\n          Filter::References::WikiPageReferenceFilter\n        ]\n      end",
    "comment": "UserReferenceFilter is intentionally excluded to prevent generating a notification. This pipeline is mostly for titles.",
    "label": "",
    "id": "2622"
  },
  {
    "raw_code": "def self.filters\n        @filters ||= FilterArray[\n          Filter::CodeLanguageFilter,\n          Filter::JsonTableFilter, # process before sanitization\n          Filter::PlantumlFilter,\n          # Must always be before the SanitizationFilter/SanitizeLinkFilter to prevent XSS attacks\n          Filter::SpacedLinkFilter,\n          Filter::SanitizationFilter,\n          Filter::SanitizeLinkFilter,\n          Filter::EscapedCharFilter,\n          Filter::KrokiFilter,\n          Filter::GollumTagsFilter,\n          Filter::WikiLinkGollumFilter,\n          Filter::AssetProxyFilter,\n          Filter::MathFilter,\n          Filter::ColorFilter,\n          Filter::MermaidFilter,\n          Filter::AttributesFilter,\n          Filter::VideoLinkFilter,\n          Filter::AudioLinkFilter,\n          Filter::IframeLinkFilter,\n          Filter::TableOfContentsTagFilter,\n          Filter::AutolinkFilter,\n          Filter::SuggestionFilter,\n          Filter::FootnoteFilter,\n          Filter::InlineDiffFilter,\n          *reference_filters,\n          Filter::ImageLazyLoadFilter, # keep after reference filters\n          Filter::ImageLinkFilter, # keep after reference filters\n          Filter::ExternalLinkFilter, # keep after ImageLinkFilter\n          Filter::EmojiFilter,\n          Filter::CustomEmojiFilter,\n          Filter::TaskListFilter,\n          Filter::SetDirectionFilter,\n          Filter::SyntaxHighlightFilter # this filter should remain at the end\n        ]\n      end",
    "comment": "These filters transform GitLab Flavored Markdown (GFM) to HTML. The nodes and marks referenced in app/assets/javascripts/behaviors/markdown/editor_extensions.js consequently transform that same HTML to GFM to be copied to the clipboard. Every filter that generates HTML from GFM should have a node or mark in app/assets/javascripts/behaviors/markdown/editor_extensions.js. The GFM-to-HTML-to-GFM cycle is tested in spec/features/copy_as_gfm_spec.rb.",
    "label": "",
    "id": "2623"
  },
  {
    "raw_code": "def self.filters\n        @filters ||= FilterArray[\n          Filter::HtmlEntityFilter,\n          Filter::SanitizationFilter,\n          Filter::SanitizeLinkFilter,\n          Filter::EmojiFilter\n        ]\n      end",
    "comment": "These filters will only perform sanitization of the content, preventing XSS, and replace emoji.",
    "label": "",
    "id": "2624"
  },
  {
    "raw_code": "def parse_sourcepos(sourcepos)\n          start_pos, end_pos = sourcepos&.split('-')\n          start_row, start_col = start_pos&.split(':')\n          end_row, end_col = end_pos&.split(':')\n\n          return unless start_row && start_col && end_row && end_col\n\n          {\n            start: { row: [1, start_row.to_i].max - 1, col: [1, start_col.to_i].max - 1 },\n            end: { row: [1, end_row.to_i].max - 1, col: [1, end_col.to_i].max - 1 }\n          }\n        end",
    "comment": "Parses string representing a sourcepos in format \"start_row:start_column-end_row:end_column\" into 0-based attributes. For example, \"1:10-14:1\" becomes { start: { row: 0, col: 9 }, end: { row: 13, col: 0 } }",
    "label": "",
    "id": "2625"
  },
  {
    "raw_code": "def read_blob(ref, filename)\n        return error_message(filename, 'no repository') unless repository.try(:exists?)\n\n        target = resolve_target_path(filename)\n\n        return error_message(filename, 'not found') unless target\n\n        blob = repository&.blob_at(ref, target)\n\n        return error_message(filename, 'not found') unless blob\n        return error_message(filename, 'not readable') unless blob.readable_text?\n\n        if wiki?\n          Gitlab::WikiPages::FrontMatterParser.new(blob.data).parse.content\n        else\n          blob.data\n        end",
    "comment": "Gets Blob at a path for a specific revision. This method will check that the Blob exists and contains readable text.  revision - The String SHA1. path     - The String file path.  Returns a string containing the blob content",
    "label": "",
    "id": "2626"
  },
  {
    "raw_code": "def resolve_relative_path(path, base_path)\n        p = Pathname(base_path)\n        p = p.dirname unless p.extname.empty?\n        p += path\n\n        p.cleanpath.to_s\n      end",
    "comment": "Resolves the given relative path of file in repository into canonical path based on the specified base_path.  Examples:  # File in the same directory as the current path resolve_relative_path(\"users.adoc\", \"doc/api/README.adoc\") # => \"doc/api/users.adoc\"  # File in the same directory, which is also the current path resolve_relative_path(\"users.adoc\", \"doc/api\") # => \"doc/api/users.adoc\"  # Going up one level to a different directory resolve_relative_path(\"../update/7.14-to-8.0.adoc\", \"doc/api/README.adoc\") # => \"doc/update/7.14-to-8.0.adoc\"  Returns a String",
    "label": "",
    "id": "2627"
  },
  {
    "raw_code": "def process_tag(tag)\n        parts = tag.split('|')\n        return if parts.empty?\n\n        if parts.size == 1\n          reference = parts[0].strip\n        else\n          name, reference = *parts.compact.map(&:strip)\n        end",
    "comment": "Process a single tag into its final HTML form.  tag - The String tag contents (the stuff inside the double brackets).  Returns the String HTML version of the tag.",
    "label": "",
    "id": "2628"
  },
  {
    "raw_code": "def render(input, context)\n        max_includes = [::Gitlab::CurrentSettings.asciidoc_max_includes, context[:max_includes]].compact.min\n\n        extensions = proc do\n          include_processor ::Gitlab::Asciidoc::IncludeProcessor.new(context.merge(max_includes: max_includes))\n          block ::Gitlab::Asciidoc::MermaidBlockProcessor\n\n          ::Gitlab::Kroki.formats(Gitlab::CurrentSettings).each do |name|\n            block ::AsciidoctorExtensions::KrokiBlockProcessor, name\n          end",
    "comment": "Public: Converts the provided Asciidoc markup into HTML.  input         - the source text in Asciidoc format context       - :commit, :project, :ref, :requested_path ",
    "label": "",
    "id": "2629"
  },
  {
    "raw_code": "def emoji_name_element_unicode_filter(text)\n        Gitlab::Utils::Gsub\n          .gsub_with_limit(text, emoji_pattern, limit: Banzai::Filter::FILTER_ITEM_LIMIT) do |match_data|\n          emoji = TanukiEmoji.find_by_alpha_code(match_data[0])\n\n          process_emoji_tag(emoji, match_data[0])\n        end",
    "comment": "Replace :emoji: with corresponding gl-emoji unicode.  text - String text to replace :emoji: in.  Returns a String with :emoji: replaced with gl-emoji unicode.",
    "label": "",
    "id": "2630"
  },
  {
    "raw_code": "def emoji_unicode_element_unicode_filter(text)\n        Gitlab::Utils::Gsub\n          .gsub_with_limit(text, emoji_unicode_pattern, limit: Banzai::Filter::FILTER_ITEM_LIMIT) do |match_data|\n          if ignore_emoji?(match_data[0])\n            match_data[0]\n          else\n            emoji = TanukiEmoji.find_by_codepoints(match_data[0])\n\n            process_emoji_tag(emoji, match_data[0])\n          end",
    "comment": "Replace unicode emoji with corresponding gl-emoji unicode.  text - String text to replace unicode emoji in.  Returns a String with unicode emoji replaced with gl-emoji unicode.",
    "label": "",
    "id": "2631"
  },
  {
    "raw_code": "def self.emoji_pattern\n        @emoji_pattern ||= TanukiEmoji.index.alpha_code_pattern\n      end",
    "comment": "Build a regexp that matches all valid :emoji: names.",
    "label": "",
    "id": "2632"
  },
  {
    "raw_code": "def handle_line_breaks(node)\n        return unless node.content.strip.lines.length > 1\n\n        node.content = \"#{node.content.lines.first.chomp}...\"\n        @truncated = true\n      end",
    "comment": "Handle line breaks within a node",
    "label": "",
    "id": "2633"
  },
  {
    "raw_code": "def truncate_if_block(node)\n        return if truncated\n        return unless node.element? && (node.description&.block? || node.matches?(MATCH_CODE))\n\n        node.inner_html = \"#{node.inner_html}...\" if node.next_sibling\n        @truncated = true\n      end",
    "comment": "If `node` is the first block element, and the text hasn't already been truncated, then append \"...\" to the node contents and return true.  Otherwise return false.",
    "label": "",
    "id": "2634"
  },
  {
    "raw_code": "def allowlist\n        ALLOWLIST\n      end",
    "comment": "This completely overrides the BaseSanitizationFilter allowlist. We don't want to support math, spans, etc. Bare minimum markdown",
    "label": "",
    "id": "2635"
  },
  {
    "raw_code": "def process_image_link(node)\n        return unless image?(node[:href])\n\n        # checking for the existence of an image file tends to be slow. So limit it.\n        return if image_link_limit_exceeded?\n\n        path =\n          if url?(node[:href])\n            node[:href]\n          elsif wiki\n            @image_link_count += 1\n            wiki.find_file(node[:href], load_content: false)&.path\n          end",
    "comment": "Attempt to process the node as an image tag.",
    "label": "",
    "id": "2636"
  },
  {
    "raw_code": "def process_page_link(node)\n        return if node[:href].casecmp?('_toc_') && node.text.casecmp?('_toc_')\n\n        if url?(node[:href])\n          set_common_attributes(node)\n        elsif wiki\n          set_common_attributes(node)\n\n          node[:href] = ::File.join(wiki_base_path, node[:href])\n          node.add_class('gfm')\n          node.add_class('gfm-gollum-wiki-page')\n\n          node['data-reference-type'] = 'wiki_page'\n          node['data-project'] = context[:project].id if context[:project]\n          node['data-group'] = context[:group]&.id if context[:group]\n        end",
    "comment": "Attempt to process the node as a page link tag.",
    "label": "",
    "id": "2637"
  },
  {
    "raw_code": "def build_relative_path(path, request_path)\n        return request_path if path.empty?\n        return path unless request_path\n        return path[1..] if path.start_with?('/')\n\n        parts = request_path.split('/')\n\n        parts.pop if uri_type(request_path) != :tree\n\n        path.delete_prefix!('./')\n\n        while path.start_with?('../')\n          parts.pop\n          path.sub!('../', '')\n        end",
    "comment": "Convert a relative path into its correct location based on the currently requested path  path         - Relative path String request_path - Currently-requested path String  Examples:  # File in the same directory as the current path build_relative_path(\"users.md\", \"doc/api/README.md\") # => \"doc/api/users.md\"  # File in the same directory, which is also the current path build_relative_path(\"users.md\", \"doc/api\") # => \"doc/api/users.md\"  # Going up one level to a different directory build_relative_path(\"../update/7.14-to-8.0.md\", \"doc/api/README.md\") # => \"doc/update/7.14-to-8.0.md\"  Returns a String",
    "label": "",
    "id": "2638"
  },
  {
    "raw_code": "def merge_adjacent_text_nodes(node)\n        content = CGI.escapeHTML(node.content)\n\n        if text_node?(node.previous)\n          content.prepend(node.previous.to_html)\n          node.previous.remove\n        end",
    "comment": "Merge directly adjacent text nodes and replace existing node with the merged content. For example, the document could be #(Text \"~c_bug\"), #(Element:0x57724 { name = \"span\" }, children = [ #(Text \"_\")] })] Our reference processing requires a single string of text to match against. So even if it was #(Text \"~c_bug\"), #(Text \"_\") it wouldn't match.  Merging together will give #(Text \"~c_bug_\")",
    "label": "",
    "id": "2639"
  },
  {
    "raw_code": "def self.initialize_settings\n        application_settings           = Gitlab::CurrentSettings.current_application_settings\n        Gitlab.config['asset_proxy'] ||= GitlabSettings::Options.build({})\n\n        if application_settings.respond_to?(:asset_proxy_enabled)\n          Gitlab.config.asset_proxy['enabled']       = application_settings.asset_proxy_enabled\n          Gitlab.config.asset_proxy['url']           = application_settings.asset_proxy_url\n          Gitlab.config.asset_proxy['secret_key']    = application_settings.asset_proxy_secret_key\n          Gitlab.config.asset_proxy['allowlist']     = determine_allowlist(application_settings)\n          Gitlab.config.asset_proxy['domain_regexp'] = compile_allowlist(Gitlab.config.asset_proxy.allowlist)\n        else\n          Gitlab.config.asset_proxy['enabled']       = ::ApplicationSetting.defaults[:asset_proxy_enabled]\n        end",
    "comment": "called during an initializer. Caching the values in Gitlab.config significantly increased performance, rather than querying Gitlab::CurrentSettings.current_application_settings over and over.  However, this does mean that the Rails servers need to get restarted whenever the application settings are changed",
    "label": "",
    "id": "2640"
  },
  {
    "raw_code": "def returned_timeout_value\n        HTML::Pipeline.parse(TIMEOUT_MARKDOWN_MESSAGE)\n      end",
    "comment": "If sanitization times out, we can not return partial un-sanitized results. It's ok to allow any following filters to run since this is safe HTML.",
    "label": "",
    "id": "2641"
  },
  {
    "raw_code": "def expand_reference_with_title_and_state(node, issuable)\n        node.content = \"#{expand_emoji(issuable.title).truncate(50)} (#{node.content}\"\n        node.content += \" - #{issuable_state_text(issuable)}\" if VISIBLE_STATES.include?(issuable.state)\n        node.content += ')'\n      end",
    "comment": "Example: Issue Title (#123 - closed)",
    "label": "",
    "id": "2642"
  },
  {
    "raw_code": "def expand_reference_with_summary(node, issuable)\n        summary = []\n\n        summary << assignees_text(issuable) if issuable.supports_assignee?\n        summary << milestone_text(issuable.milestone) if issuable.supports_milestone?\n        summary << health_status_text(issuable.health_status) if issuable.supports_health_status?\n\n        node.content = [node.content, *summary].compact_blank.join('  ')\n      end",
    "comment": "rubocop:disable Style/AsciiComments Example: Issue Title (#123 - closed) assignee name 1, assignee name 2+  v15.9  On track",
    "label": "",
    "id": "2643"
  },
  {
    "raw_code": "def expand_reference_with_state(node, issuable)\n        node.content += \" (#{issuable_state_text(issuable)})\"\n      end",
    "comment": "rubocop:enable Style/AsciiComments Example: #123 (closed)",
    "label": "",
    "id": "2644"
  },
  {
    "raw_code": "def process_existing\n        doc.xpath(XPATH_MATH_STYLE).each do |node|\n          break if render_nodes_limit_reached?(@nodes_count)\n\n          node[:class] = MarkdownFilter.glfm_markdown?(context) ? TAG_CLASS : MATH_CLASSES\n\n          @nodes_count += 1\n        end",
    "comment": "Add necessary classes to existing math blocks",
    "label": "",
    "id": "2645"
  },
  {
    "raw_code": "def returned_timeout_value\n        HTML::Pipeline.parse(Banzai::Filter::SanitizeLinkFilter::TIMEOUT_MARKDOWN_MESSAGE)\n      end",
    "comment": "Since this filter does a level of sanitization, we can not return partial un-sanitized results. It's ok to allow any following filters to run since this is safe HTML.",
    "label": "",
    "id": "2646"
  },
  {
    "raw_code": "def node_src_attribute(node)\n        node['data-canonical-src'] ? 'data-canonical-src' : 'href'\n      end",
    "comment": "if this is a link to a proxied image, then `src` is already the correct proxied url, so work with the `data-canonical-src`",
    "label": "",
    "id": "2647"
  },
  {
    "raw_code": "def punycode_autolink_node!(uri, node)\n        return unless uri\n        return unless context[:emailable_links]\n\n        unencoded_uri_str = Addressable::URI.unencode(node_src(node))\n\n        if unencoded_uri_str == node.content && idn?(uri)\n          node.content = uri.normalize\n        end",
    "comment": "Only replace an autolink with an IDN with it's punycode version if we need emailable links.  Otherwise let it be shown normally and the tooltips will show the punycode version.",
    "label": "",
    "id": "2648"
  },
  {
    "raw_code": "def sanitize_link_text!(node)\n        node.inner_html = node.inner_html.gsub(RTLO, ENCODED_RTLO)\n      end",
    "comment": "escape any right-to-left (RTLO) characters in link text",
    "label": "",
    "id": "2649"
  },
  {
    "raw_code": "def add_malicious_tooltip!(uri, node)\n        if idn?(uri) || has_encoded_rtlo?(uri)\n          node.add_class('has-tooltip')\n          node.set_attribute('title', uri.normalize)\n        end",
    "comment": "If the domain is an international domain name (IDN), let's expose with a tooltip in case it's intended to be malicious. This is particularly useful for links where the link text is not the same as the actual link. We will continue to show the unicode version of the domain in autolinked link text, which could contain emojis, etc.  Also show the tooltip if the url contains the RTLO character, as this is an indicator of a malicious link",
    "label": "",
    "id": "2650"
  },
  {
    "raw_code": "def call\n        doc.xpath('descendant-or-self::img[not(ancestor::a) and not(@data-src = \"\")]').each do |img|\n          link_replaces_image = !!context[:link_replaces_image]\n          html_class = link_replaces_image ? 'with-attachment-icon' : 'no-attachment-icon'\n\n          link = doc.document.create_element(\n            'a',\n            class: html_class,\n            href: img['data-src'] || img['src'],\n            target: '_blank',\n            rel: 'noopener noreferrer'\n          )\n\n          # make sure the original non-proxied src carries over to the link\n          link['data-canonical-src'] = img['data-canonical-src'] if img['data-canonical-src']\n\n          if img['data-diagram'] && img['data-diagram-src']\n            link['data-diagram'] = img['data-diagram']\n            link['data-diagram-src'] = img['data-diagram-src']\n            img.remove_attribute('data-diagram')\n            img.remove_attribute('data-diagram-src')\n          end",
    "comment": "Find every image that isn't already wrapped in an `a` tag, create a new node (a link to the image source), copy the image as a child of the anchor, and then replace the img with the link-wrapped version.  If `link_replaces_image` context parameter is provided, the image is going to be replaced with a link to an image.",
    "label": "",
    "id": "2651"
  },
  {
    "raw_code": "def process_toc_tag(node)\n        build_toc\n\n        # Replace the entire paragraph containing the TOC tag\n        node.parent.replace(result[:toc].presence || '')\n      end",
    "comment": "Replace an entire `[TOC]` node",
    "label": "",
    "id": "2652"
  },
  {
    "raw_code": "def replace_placeholder_action(action)\n        replacement = action.call(context) || ''\n\n        node = Banzai::Filter::SanitizationFilter.new(replacement).call\n        CGI.escapeHTML(node.text)\n      end",
    "comment": "The action param represents the Proc to call in order to retrieve the value",
    "label": "",
    "id": "2653"
  },
  {
    "raw_code": "def returned_timeout_value\n        Banzai::PipelineBase.parse(COMPLEX_MARKDOWN_MESSAGE)\n      end",
    "comment": "If sanitization times out, we can not return partial un-sanitized results. It's ok to allow any following filters to run since this is safe HTML.",
    "label": "",
    "id": "2654"
  },
  {
    "raw_code": "def lex(lexer, code)\n        lexer.lex(code)\n      end",
    "comment": "Separate method so it can be instrumented.",
    "label": "",
    "id": "2655"
  },
  {
    "raw_code": "def replace_pre_element(pre_node, highlighted)\n        pre_node.replace(highlighted)\n      end",
    "comment": "Replace the `pre` element with the entire highlighted block",
    "label": "",
    "id": "2656"
  },
  {
    "raw_code": "def parse_lang_params(code_node)\n        pre_node = code_node.parent\n        language = pre_node.attr('lang') || code_node.attr('lang')\n\n        return unless language\n\n        language, language_params = language.split(LANG_PARAMS_DELIMITER, 2)\n\n        # markdown renderer places extra lang parameters into data-meta\n        language_params = [pre_node.attr('data-meta'), code_node.attr('data-meta'), language_params].compact.join(' ')\n\n        [language, language_params]\n      end",
    "comment": "the `full_info_string` render option works with the space delimiter. Which means the language specified on a code block is parsed with spaces. Anything after the first space is placed in the `data-meta` attribute. However GitLab recognizes `:` as an additional delimiter on the lang attribute. So parse out the extra parameter.  Original \"```suggestion:+1-10 more```\" -> '<pre data-canonical-lang=\"suggestion:+1-10\" data-lang-params=\"more\">'.  With extra parsing \"```suggestion:+1-10 more```\" -> '<pre data-canonical-lang=\"suggestion\" data-lang-params=\"+1-10 more\">'.",
    "label": "",
    "id": "2657"
  },
  {
    "raw_code": "def trailer_filter(text)\n        text.lines.map! do |line|\n          trailer, rest = line.split(':', 2)\n\n          next line unless trailer.downcase.end_with?('-by') && rest.present?\n\n          chunks = rest.split\n          author_email = chunks.pop.delete_prefix('&lt;').delete_suffix('&gt;')\n          next line unless Devise.email_regexp.match(author_email)\n\n          author_name = chunks.join(' ').strip\n          stripped_trailer = \"#{trailer.strip}:\"\n\n          \"#{trailer}: #{link_to_user_or_email(author_name, author_email, stripped_trailer)}\\n\"\n        end.join\n      end",
    "comment": "Replace trailer lines with links to GitLab users or mailto links to non GitLab users.  text - String text to replace trailers in.  Returns a String with all trailer lines replaced with links to GitLab users and mailto links to non GitLab users. All links have `data-trailer` and `data-user` attributes attached.  The code intentionally avoids using Regex for security and performance reasons: https://gitlab.com/gitlab-org/gitlab/-/issues/363734",
    "label": "",
    "id": "2658"
  },
  {
    "raw_code": "def link_to_user_or_email(name, email, trailer)\n        link_to_user User.with_public_email(email).first,\n          name: name,\n          email: email,\n          trailer: trailer\n      end",
    "comment": "Find a GitLab user using the supplied email and generate a valid link to them, otherwise, generate a mailto link.  name - String name used in the commit message for the user email - String email used in the commit message for the user trailer - String trailer used in the commit message  Returns a String with a link to the user.",
    "label": "",
    "id": "2659"
  },
  {
    "raw_code": "def apply_file_link_rules!\n          @uri = Addressable::URI.join(@slug, @uri) if @uri.extname.present?\n        end",
    "comment": "Of the form 'file.md'",
    "label": "",
    "id": "2660"
  },
  {
    "raw_code": "def apply_hierarchical_link_rules!\n          @uri = Addressable::URI.join(@slug, @uri) if @uri.to_s[0] == '.'\n        end",
    "comment": "Of the form `./link`, `../link`, or similar",
    "label": "",
    "id": "2661"
  },
  {
    "raw_code": "def apply_relative_link_rules!\n          if @uri.relative? && @uri.path.present?\n            link = @uri.path\n            link = ::File.join(@wiki_base_path, link) unless prefixed_with_base_path?(link)\n            link = \"#{link}##{@uri.fragment}\" if @uri.fragment\n            @uri = Addressable::URI.parse(link)\n          end",
    "comment": "Any link _not_ of the form `http://example.com/`",
    "label": "",
    "id": "2662"
  },
  {
    "raw_code": "def old_wiki_base_path\n          @wiki_base_path.sub('/-/', '/')\n        end",
    "comment": "before we added `/-/` to all our paths",
    "label": "",
    "id": "2663"
  },
  {
    "raw_code": "def load_references_per_parent(nodes)\n          @references_per_parent ||= {}\n\n          @references_per_parent[parent_type] ||= begin\n            refs = Hash.new { |hash, key| hash[key] = Set.new }\n\n            [filter.object_class.link_reference_pattern, filter.object_class.reference_pattern].each do |pattern|\n              next unless pattern\n\n              prepare_doc_for_scan.to_enum(:scan, pattern).each do\n                parent_path = if parent_type == :group\n                                full_group_path($~[:group])\n                              elsif parent_type == :namespace\n                                full_namespace_path($~)\n                              else\n                                full_project_path($~[:namespace], $~[:project], $~)\n                              end",
    "comment": "Loads all object references (e.g. issue IDs) per project/group they belong to.",
    "label": "",
    "id": "2664"
  },
  {
    "raw_code": "def load_parent_per_reference\n          @per_reference ||= {}\n\n          @per_reference[parent_type] ||= begin\n            refs = references_per_parent.keys.compact\n            parent_ref = {}\n\n            # if we already have a parent, no need to query it again\n            refs.each do |ref|\n              next unless ref\n\n              if context[:project]&.full_path == ref\n                parent_ref[ref] = context[:project]\n              elsif context[:group]&.full_path == ref\n                parent_ref[ref] = context[:group]\n              end",
    "comment": "Returns a Hash containing referenced projects grouped per their full path.",
    "label": "",
    "id": "2665"
  },
  {
    "raw_code": "def find_for_paths(paths, absolute_path = false)\n          return [] if paths.empty?\n\n          if Gitlab::SafeRequestStore.active?\n            cached_objects_for_paths(paths, absolute_path)\n          else\n            objects_for_paths(paths, absolute_path)\n          end",
    "comment": "Returns projects for the given paths.",
    "label": "",
    "id": "2666"
  },
  {
    "raw_code": "def references_in(text, pattern = object_class.reference_pattern)\n          Gitlab::Utils::Gsub.gsub_with_limit(text, pattern, limit: Banzai::Filter::FILTER_ITEM_LIMIT) do |match_data|\n            if ident = identifier(match_data)\n              yield match_data[0], ident, match_data.named_captures['project'], match_data.named_captures['namespace'],\n                match_data\n            else\n              match_data[0]\n            end",
    "comment": "Public: Find references in text (like `!123` for merge requests)  references_in(text) do |match, id, project_ref, matches| object = find_object(project_ref, id) \"<a href=...>#{object.to_reference}</a>\" end  text - String text to search.  Yields the String match, the Integer referenced object ID, an optional String of the external project reference, and all of the matchdata.  Returns a String replaced with the return of the block.",
    "label": "",
    "id": "2667"
  },
  {
    "raw_code": "def parse_symbol(symbol, match_data)\n          symbol.to_i\n        end",
    "comment": "Transform a symbol extracted from the text to a meaningful value In most cases these will be integers, so we call #to_i by default  This method has the contract that if a string `ref` refers to a record `record`, then `parse_symbol(ref) == record_identifier(record)`.",
    "label": "",
    "id": "2668"
  },
  {
    "raw_code": "def record_identifier(record)\n          record.id\n        end",
    "comment": "We assume that most classes are identifying records by ID.  This method has the contract that if a string `ref` refers to a record `record`, then `class.parse_symbol(ref) == record_identifier(record)`.",
    "label": "",
    "id": "2669"
  },
  {
    "raw_code": "def find_object(parent_object, id)\n          raise NotImplementedError, \"#{self.class} must implement method: #{__callee__}\"\n        end",
    "comment": "Implement in child class Example: project.merge_requests.find",
    "label": "",
    "id": "2670"
  },
  {
    "raw_code": "def find_object_from_link(parent_object, id)\n          find_object(parent_object, id)\n        end",
    "comment": "Override if the link reference pattern produces a different ID (global ID vs internal ID, for instance) to the regular reference pattern.",
    "label": "",
    "id": "2671"
  },
  {
    "raw_code": "def url_for_object(object, parent_object)\n          raise NotImplementedError, \"#{self.class} must implement method: #{__callee__}\"\n        end",
    "comment": "Implement in child class Example: project_merge_request_url",
    "label": "",
    "id": "2672"
  },
  {
    "raw_code": "def object_link_filter(text, pattern, link_content: nil, link_reference: false)\n          references_in(text, pattern) do |match, id, project_ref, namespace_ref, matches|\n            parent_path = if parent_type == :group\n                            reference_cache.full_group_path(namespace_ref)\n                          elsif parent_type == :namespace\n                            reference_cache.full_namespace_path(matches)\n                          else\n                            reference_cache.full_project_path(namespace_ref, project_ref, matches)\n                          end",
    "comment": "Replace references (like `!123` for merge requests) in text with links to the referenced object's details page.  text - String text to replace references in. pattern - Reference pattern to match against. link_content - Original content of the link being replaced. link_reference - True if this was using the link reference pattern, false otherwise.  Returns a String with references replaced with links. All links have `gfm` and `gfm-OBJECT_NAME` class names attached for styling.",
    "label": "",
    "id": "2673"
  },
  {
    "raw_code": "def references_in(text, pattern = object_reference_pattern)\n          raise NotImplementedError, \"#{self.class} must implement method: #{__callee__}\"\n        end",
    "comment": "Public: Find references in text (like `!123` for merge requests)  references_in(text) do |match, id, project_ref, matches| object = find_object(project_ref, id) \"<a href=...>#{object.to_reference}</a>\" end  text - String text to search.  Yields the String match, the Integer referenced object ID, an optional String of the external project reference, and all of the matchdata.  Returns a String replaced with the return of the block.",
    "label": "",
    "id": "2674"
  },
  {
    "raw_code": "def each_node\n          return to_enum(__method__) unless block_given?\n\n          doc.xpath(query).each do |node|\n            yield node\n          end",
    "comment": "Iterates over all <a> and text() nodes in a document.  Nodes are skipped whenever their ancestor is one of the nodes returned by `ignore_ancestor_query`. Link tags are not processed if they have a \"gfm\" class or the \"href\" attribute is empty.",
    "label": "",
    "id": "2675"
  },
  {
    "raw_code": "def nodes\n          @nodes ||= each_node.to_a\n        end",
    "comment": "Returns an Array containing all HTML nodes.",
    "label": "",
    "id": "2676"
  },
  {
    "raw_code": "def data_attribute(attributes = {})\n          attributes = attributes.reject { |_, v| v.nil? }\n\n          # \"data-reference-type=\" attribute got moved into a constant because we need\n          # to use it on ReferenceRewriter class to detect if the markdown contains any reference\n          reference_type_attribute = \"#{REFERENCE_TYPE_DATA_ATTRIBUTE}#{escape_once(self.class.reference_type)} \"\n\n          attributes[:container] ||= 'body'\n          attributes[:placement] ||= 'top'\n          attributes.delete(:original) if context[:no_original_data]\n\n          attributes.map do |key, value|\n            %(data-#{key.to_s.dasherize}=\"#{escape_once(value)}\")\n          end",
    "comment": "Returns a data attribute String to attach to a reference link  attributes - Hash, where the key becomes the data attribute name and the value is the data attribute value  Examples:  data_attribute(project: 1, issue: 2) # => \"data-reference-type=\\\"SomeReferenceFilter\\\" data-project=\\\"1\\\" data-issue=\\\"2\\\"\"  data_attribute(project: 3, merge_request: 4) # => \"data-reference-type=\\\"SomeReferenceFilter\\\" data-project=\\\"3\\\" data-merge-request=\\\"4\\\"\"  Returns a String",
    "label": "",
    "id": "2677"
  },
  {
    "raw_code": "def validate\n          needs :project unless skip_project_check?\n        end",
    "comment": "Ensure that a :project key exists in context  Note that while the key might exist, its value could be nil!",
    "label": "",
    "id": "2678"
  },
  {
    "raw_code": "def yield_valid_link(node)\n          link = unescape_link(node.attr('href').to_s)\n          inner_html = node.inner_html\n\n          return unless link.force_encoding('UTF-8').valid_encoding?\n\n          yield link, inner_html\n        end",
    "comment": "Yields the link's URL and inner HTML whenever the node is a valid <a> tag.",
    "label": "",
    "id": "2679"
  },
  {
    "raw_code": "def update_nodes!\n          # if we haven't loaded `nodes` yet, don't do it here\n          return unless nodes?\n\n          @new_nodes.sort_by { |index, _new_nodes| -index }.each do |index, new_nodes|\n            nodes[index, 1] = new_nodes\n          end",
    "comment": "Once Filter completes replacing nodes, we update nodes with @new_nodes",
    "label": "",
    "id": "2680"
  },
  {
    "raw_code": "def parse_symbol(symbol, match_data)\n          absolute_path = !!match_data&.named_captures&.fetch('absolute_path')\n\n          if symbol\n            # when parsing links, there is no `match_data[:milestone_iid]`, but `symbol`\n            # holds the iid\n            { milestone_iid: symbol.to_i, milestone_name: nil, absolute_path: absolute_path }\n          else\n            { milestone_iid: match_data[:milestone_iid]&.to_i, milestone_name: match_data[:milestone_name]&.tr('\"', ''), absolute_path: absolute_path }\n          end",
    "comment": "Transform a symbol extracted from the text to a meaningful value  This method has the contract that if a string `ref` refers to a record `record`, then `parse_symbol(ref) == record_identifier(record)`.  This contract is slightly broken here, as we only have either the milestone_iid or the milestone_name, but not both.  But below, we have both pieces of information. But it's accounted for in `find_object`",
    "label": "",
    "id": "2681"
  },
  {
    "raw_code": "def record_identifier(record)\n          { milestone_iid: record.iid, milestone_name: record.name }\n        end",
    "comment": "This method has the contract that if a string `ref` refers to a record `record`, then `class.parse_symbol(ref) == record_identifier(record)`. See note in `parse_symbol` above",
    "label": "",
    "id": "2682"
  },
  {
    "raw_code": "def parse_symbol(symbol, match_data)\n          absolute_path = !!match_data&.named_captures&.fetch('absolute_path')\n\n          {\n            label_id: match_data[:label_id]&.to_i,\n            label_name: match_data[:label_name]&.tr('\"', ''),\n            absolute_path: absolute_path\n          }\n        end",
    "comment": "Transform a symbol extracted from the text to a meaningful value  This method has the contract that if a string `ref` refers to a record `record`, then `parse_symbol(ref) == record_identifier(record)`.  This contract is slightly broken here, as we only have either the label_id or the label_name, but not both.  But below, we have both pieces of information. But it's accounted for in `find_object`",
    "label": "",
    "id": "2683"
  },
  {
    "raw_code": "def record_identifier(record)\n          { label_id: record.id, label_name: record.title }\n        end",
    "comment": "We assume that most classes are identifying records by ID.  This method has the contract that if a string `ref` refers to a record `record`, then `class.parse_symbol(ref) == record_identifier(record)`. See note in `parse_symbol` above",
    "label": "",
    "id": "2684"
  },
  {
    "raw_code": "def references_in(text, pattern = object_reference_pattern)\n          Gitlab::Utils::Gsub.gsub_with_limit(text, pattern, limit: Banzai::Filter::FILTER_ITEM_LIMIT) do |match_data|\n            yield match_data[0], \"#{match_data[:namespace]}/#{match_data[:project]}\"\n          end",
    "comment": "Public: Find `namespace/project>` project references in text  references_in(text) do |match, project| \"<a href=...>#{project}></a>\" end  text - String text to search.  Yields the String match, and the String project name.  Returns a String replaced with the return of the block.",
    "label": "",
    "id": "2685"
  },
  {
    "raw_code": "def object_link_filter(text, pattern, link_content: nil, link_reference: false)\n          references_in(text) do |match, project_path|\n            cached_call(:banzai_url_for_object, match, path: [Project, project_path.downcase]) do\n              if project = projects_hash[project_path.downcase]\n                link_to_project(project, link_content: link_content) || match\n              else\n                match\n              end",
    "comment": "Replace `namespace/project>` project references in text with links to the referenced project page.  text - String text to replace references in. link_content - Original content of the link being replaced.  Returns a String with `namespace/project>` references replaced with links. All links have `gfm` and `gfm-project` class names attached for styling.",
    "label": "",
    "id": "2686"
  },
  {
    "raw_code": "def projects_hash\n          @projects ||= Project.eager_load(:route, namespace: [:route])\n                               .allow_cross_joins_across_databases(url: \"https://gitlab.com/gitlab-org/gitlab/-/issues/420046\")\n                               .where_full_path_in(projects)\n                               .index_by(&:full_path)\n                               .transform_keys(&:downcase)\n        end",
    "comment": "Returns a Hash containing all Project objects for the project references in the current document.  The keys of this Hash are the project paths, the values the corresponding Project objects.",
    "label": "",
    "id": "2687"
  },
  {
    "raw_code": "def projects\n          refs = Set.new\n\n          nodes.each do |node|\n            node.to_html.scan(Project.markdown_reference_pattern) do\n              refs << \"#{$~[:namespace]}/#{$~[:project]}\"\n            end",
    "comment": "Returns all projects referenced in the current document.",
    "label": "",
    "id": "2688"
  },
  {
    "raw_code": "def parse_symbol(sha_hash, _match)\n          sha_hash\n        end",
    "comment": "The default behaviour is `#to_i` - we just pass the hash through.",
    "label": "",
    "id": "2689"
  },
  {
    "raw_code": "def references_in(text, pattern = object_reference_pattern)\n          case pattern\n          when Regexp\n            Gitlab::Utils::Gsub.gsub_with_limit(text, pattern, limit: Banzai::Filter::FILTER_ITEM_LIMIT) do |match_data|\n              yield match_data[0], match_data[:issue]\n            end",
    "comment": "Public: Find `JIRA-123` issue references in text  references_in(text, pattern) do |match, issue| \"<a href=...>##{issue}</a>\" end  text - String text to search.  Yields the String match and the String issue reference.  Returns a String replaced with the return of the block.",
    "label": "",
    "id": "2690"
  },
  {
    "raw_code": "def object_link_filter(text, pattern, link_content: nil, link_reference: false)\n          references_in(text) do |match, id|\n            url = url_for_issue(id)\n            klass = reference_class(:issue)\n            data  = data_attribute(project: project.id, external_issue: id)\n            content = link_content || match\n\n            %(<a href=\"#{url}\" #{data}\n                 title=\"#{escape_once(issue_title)}\"\n                 class=\"#{klass}\">#{content}</a>)\n          end",
    "comment": "Replace `JIRA-123` issue references in text with links to the referenced issue's details page.  text - String text to replace references in. link_content - Original content of the link being replaced.  Returns a String with `JIRA-123` references replaced with links. All links have `gfm` and `gfm-issue` class names attached for styling.",
    "label": "",
    "id": "2691"
  },
  {
    "raw_code": "def references_in(text, pattern = object_reference_pattern)\n          Gitlab::Utils::Gsub.gsub_with_limit(text, pattern, limit: Banzai::Filter::FILTER_ITEM_LIMIT) do |match_data|\n            yield match_data[0], match_data['user']\n          end",
    "comment": "Public: Find `@user` user references in text  references_in(text) do |match, username| \"<a href=...>@#{user}</a>\" end  text - String text to search.  Yields the String match, and the String user name.  Returns a String replaced with the return of the block.",
    "label": "",
    "id": "2692"
  },
  {
    "raw_code": "def object_link_filter(text, pattern, link_content: nil, link_reference: false)\n          references_in(text, pattern) do |match, username|\n            if Feature.disabled?(:disable_all_mention) && username == 'all' && !skip_project_check?\n              link_to_all(link_content: link_content)\n            else\n              cached_call(:banzai_url_for_object, match, path: [User, username.downcase]) do\n                # order is important: per-organization usernames should be checked before global namespace\n                if org_user_detail = org_user_details[username.downcase]\n                  link_to_org_user_detail(org_user_detail)\n                elsif namespace = namespaces[username.downcase]\n                  link_to_namespace(namespace, link_content: link_content) || match\n                else\n                  match\n                end",
    "comment": "Replace `@user` user references in text with links to the referenced user's profile page.  text - String text to replace references in. link_content - Original content of the link being replaced.  Returns a String with `@user` references replaced with links. All links have `gfm` and `gfm-project_member` class names attached for styling.",
    "label": "",
    "id": "2693"
  },
  {
    "raw_code": "def namespaces\n          Namespace.preload(:owner, :route)\n                   .where_full_path_in(usernames)\n                   .index_by(&:full_path)\n                   .transform_keys(&:downcase)\n        end",
    "comment": "Returns a Hash containing all Namespace objects for the username references in the current document.  The keys of this Hash are the namespace paths, the values the corresponding Namespace objects.",
    "label": "",
    "id": "2694"
  },
  {
    "raw_code": "def org_user_details\n          return {} unless Feature.enabled?(:organization_users_internal, organization)\n\n          Organizations::OrganizationUserDetail.for_references\n                                               .for_organization(organization)\n                                               .with_usernames(usernames)\n                                               .index_by(&:username)\n                                               .transform_keys(&:downcase)\n        end",
    "comment": "check for users that have an aliased name within an organization, for example the bot users created by Users::Internal",
    "label": "",
    "id": "2695"
  },
  {
    "raw_code": "def usernames\n          refs = Set.new\n\n          nodes.each do |node|\n            node.to_html.scan(User.reference_pattern) do\n              refs << $~[:user]\n            end",
    "comment": "Returns all usernames referenced in the current document.",
    "label": "",
    "id": "2696"
  },
  {
    "raw_code": "def get(session, path, options = {})\n      json_response session.get(path, options)\n    end",
    "comment": "Should be used in a session manually",
    "label": "",
    "id": "2697"
  },
  {
    "raw_code": "def post(session, path, options = {})\n      json_response session.post(path, options)\n    end",
    "comment": "Should be used in a session manually",
    "label": "",
    "id": "2698"
  },
  {
    "raw_code": "def pre_auth\n      @pre_auth ||= Doorkeeper::OAuth::PreAuthorization.new(\n        Doorkeeper.configuration, params)\n    end",
    "comment": "Next methods are needed for Doorkeeper",
    "label": "",
    "id": "2699"
  },
  {
    "raw_code": "def all\n      session_get(\"/api/v4/users/me/teams\")\n    end",
    "comment": "Returns all teams that the current user is a member of",
    "label": "",
    "id": "2700"
  },
  {
    "raw_code": "def create(name:, display_name:, type:)\n      session_post('/api/v4/teams', body: {\n        name: name,\n        display_name: display_name,\n        type: type\n      }.to_json)\n    end",
    "comment": "Creates a team on the linked Mattermost instance, the team admin will be the `current_user` passed to the Mattermost::Client instance",
    "label": "",
    "id": "2701"
  },
  {
    "raw_code": "def destroy(team_id:)\n      session_delete(\"/api/v4/teams/#{team_id}\")\n    end",
    "comment": "The deletion is done async, so the response is fast. On the mattermost side, this triggers an soft deletion",
    "label": "",
    "id": "2702"
  },
  {
    "raw_code": "def spam_action_response_fields(spammable)\n        {\n          spam: spammable.spam?,\n          # NOTE: These fields are intentionally named with 'captcha' instead of 'recaptcha', so\n          # that they can be applied to future alternative CAPTCHA implementations other than\n          # reCAPTCHA (such as FriendlyCaptcha) without having to change the response field name\n          # in the API.\n          needs_captcha_response: spammable.render_recaptcha?,\n          spam_log_id: spammable.spam_log&.id,\n          captcha_site_key: Gitlab::CurrentSettings.recaptcha_site_key\n        }\n      end",
    "comment": "spam_action_response_fields(spammable) -> hash  Takes a Spammable as an argument and returns response fields necessary to display a CAPTCHA on the client.",
    "label": "",
    "id": "2703"
  },
  {
    "raw_code": "def self.build(obj)\n      case obj\n      when Hash\n        new(obj.transform_values { |value| build(value) })\n      when Array\n        obj.map { |value| build(value) }\n      else\n        obj\n      end",
    "comment": "Recursively build GitlabSettings::Options",
    "label": "",
    "id": "2704"
  },
  {
    "raw_code": "def default\n      @options['default']\n    end",
    "comment": "Some configurations use the 'default' key, like: https://gitlab.com/gitlab-org/gitlab/-/blob/c4d5c77c87494bb320fa7fdf19b0e4d7d52af1d1/spec/support/helpers/stub_configuration.rb#L96 But since `default` is also a method in Hash, this can be confusing and raise an exception instead of returning nil, as expected in some places. To avoid that, we use #default always as a possible internal key",
    "label": "",
    "id": "2705"
  },
  {
    "raw_code": "def with_indifferent_access\n      to_hash.with_indifferent_access\n    end",
    "comment": "For backward compatibility, like: https://gitlab.com/gitlab-org/gitlab/-/blob/adf67e90428670aaa955731f3bdeafb8b3a874cd/lib/gitlab/database/health_status/indicators/patroni_apdex.rb#L58",
    "label": "",
    "id": "2706"
  },
  {
    "raw_code": "def stringify_keys!\n      error_msg = \"Warning: Do not mutate #{self.class} objects: `#{__method__}`\"\n\n      log_and_raise_dev_exception(error_msg, method: __method__)\n\n      to_hash.deep_stringify_keys\n    end",
    "comment": "Don't alter the internal keys",
    "label": "",
    "id": "2707"
  },
  {
    "raw_code": "def symbolize_keys!\n      error_msg = \"Warning: Do not mutate #{self.class} objects: `#{__method__}`\"\n\n      log_and_raise_dev_exception(error_msg, method: __method__)\n\n      to_hash.deep_symbolize_keys\n    end",
    "comment": "Don't alter the internal keys",
    "label": "",
    "id": "2708"
  },
  {
    "raw_code": "def log_and_raise_dev_exception(message, extra = {})\n      raise message unless Rails.env.production?\n\n      # Gitlab::BacktraceCleaner drops config/initializers, so we just limit the\n      # backtrace to the first 10 lines.\n      payload = extra.merge(message: message, caller: caller[0..10])\n      Gitlab::AppJsonLogger.warn(payload)\n    end",
    "comment": "We can't call Gitlab::ErrorTracking.track_and_raise_for_dev_exception because that method will attempt to load ApplicationContext and fail to load User since the Devise is not yet set up in `config/initialiers/8_devise.rb`.",
    "label": "",
    "id": "2709"
  },
  {
    "raw_code": "def extract(opts = {})\n      params = SafeZip::ExtractParams.new(**opts)\n\n      extract_with_ruby_zip(params)\n    end",
    "comment": "extract given files or directories from the archive into the destination path  @param [Hash] opts the options for extraction. @option opts [Array<String] :files list of files to be extracted @option opts [Array<String] :directories list of directories to be extracted @option opts [String] :to destination path  @raise [PermissionDeniedError] @raise [SymlinkSourceDoesNotExistError] @raise [UnsupportedEntryError] @raise [EntrySizeError] @raise [AlreadyExistsError] @raise [NoMatchingError] @raise [ExtractError]",
    "label": "",
    "id": "2710"
  },
  {
    "raw_code": "def issue_keys\n        BatchLoader.for(branch_name).batch do |branch_names, loader|\n          merge_requests = MergeRequest\n            .select(:description, :source_branch, :title)\n            .from_project(project)\n            .from_source_branches(branch_names)\n            .opened\n\n          branch_names.each do |branch_name|\n            related_merge_request = merge_requests.find { |mr| mr.source_branch == branch_name }\n\n            key_sources = [branch_name, related_merge_request&.title, related_merge_request&.description].compact\n            issue_keys = JiraIssueKeyExtractor.new(key_sources).issue_keys\n\n            loader.call(branch_name, issue_keys)\n          end",
    "comment": "Extract Jira issue keys from the branch name and associated open merge request. Use BatchLoader to load this data without N+1 queries when serializing multiple branches in `Atlassian::JiraConnect::Serializers::BranchEntity`.",
    "label": "",
    "id": "2711"
  },
  {
    "raw_code": "def user_info(account_id)\n        r = get('/rest/api/3/user', { accountId: account_id, expand: 'groups' })\n\n        JiraUser.new(r.parsed_response) if r.code == 200\n      end",
    "comment": "Fetch user information for the given account. https://developer.atlassian.com/cloud/jira/platform/rest/v3/api-group-users/#api-rest-api-3-user-get",
    "label": "",
    "id": "2712"
  },
  {
    "raw_code": "def issue_keys_from_commits_since_last_deploy\n          commits = commits_since_last_deploy.without_merge_commits\n\n          commits.flat_map do |commit|\n            JiraIssueKeyExtractor.new(commit.message).issue_keys\n          end.compact\n        end",
    "comment": "Extract Jira issue keys from commits made to the deployment's branch or tag since the last successful deployment was made to the environment.",
    "label": "",
    "id": "2713"
  },
  {
    "raw_code": "def summary(strategies = flag.strategies)\n          {\n            url: edit_project_feature_flag_url(flag.project, flag),\n            lastUpdated: flag.updated_at.iso8601,\n            status: {\n              enabled: flag.active,\n              defaultValue: '',\n              rollout: {\n                percentage: strategies.map do |s|\n                  s.parameters['rollout'] || s.parameters['percentage']\n                end.compact.first&.to_f,\n                text: strategies.map { |s| STRATEGY_NAMES[s.name] }.compact.join(', ')\n              }.compact\n            }\n          }\n        end",
    "comment": "The summary does not map very well to our FeatureFlag model.  We allow feature flags to have multiple strategies, depending on the environment. Jira expects a single rollout strategy.  Also, we don't actually support showing a single flag, so we use the edit path as an interim solution.",
    "label": "",
    "id": "2714"
  },
  {
    "raw_code": "def state\n          case pipeline.status\n          when 'scheduled', 'created', 'pending', 'preparing', 'waiting_for_resource' then 'pending'\n          when 'running' then 'in_progress'\n          when 'success' then 'successful'\n          when 'failed' then 'failed'\n          when 'canceled', 'skipped' then 'cancelled'\n          else\n            'unknown'\n          end",
    "comment": "translate to Jira status",
    "label": "",
    "id": "2715"
  },
  {
    "raw_code": "def pipeline_mrs_issue_keys\n          pipeline.all_merge_requests.flat_map do |mr|\n            src = \"#{mr.source_branch} #{mr.title} #{mr.description}\"\n            JiraIssueKeyExtractor.new(src).issue_keys\n          end",
    "comment": "Extract Jira issue keys from either the source branch/ref, merge request title or merge request description.",
    "label": "",
    "id": "2716"
  },
  {
    "raw_code": "def delete(resource, path, body)\n      url = delete_url(resource, path)\n\n      response = retry_with_delay do\n        Import::Clients::HTTP.delete(url, basic_auth: auth, headers: post_headers, body: body)\n      end",
    "comment": "We need to support two different APIs for deletion:  /rest/api/1.0/projects/{projectKey}/repos/{repositorySlug}/branches/default /rest/branch-utils/1.0/projects/{projectKey}/repos/{repositorySlug}/branches",
    "label": "",
    "id": "2717"
  },
  {
    "raw_code": "def merge_event?\n        action == 'MERGED'\n      end",
    "comment": "TODO Move this into MergeEvent",
    "label": "",
    "id": "2718"
  },
  {
    "raw_code": "def new_pos\n        return if removed?\n        return unless line_position\n\n        line_position[1]\n      end",
    "comment": "There are three line comment types: added, removed, or context.  1. An added type means a new line was inserted, so there is no old position. 2. A removed type means a line was removed, so there is no new position. 3. A context type means the line was unmodified, so there is both a old and new position.",
    "label": "",
    "id": "2719"
  },
  {
    "raw_code": "def line_position\n        @line_position ||= diff_hunks.each do |hunk|\n          segments = hunk.fetch('segments', [])\n          segments.each do |segment|\n            lines = segment.fetch('lines', [])\n            lines.each do |line|\n              if line['commentIds']&.include?(id)\n                return [line['source'], line['destination']]\n              end",
    "comment": "Each comment contains the following information about the diff:  hunks: [ { segments: [ { \"lines\": [ { \"commentIds\": [ N ], \"source\": X, \"destination\": Y }, ... ] ....  To determine the line position of a comment, we search all the lines entries until we find this comment ID.",
    "label": "",
    "id": "2720"
  },
  {
    "raw_code": "def comments\n        @comments ||= flatten_comments\n      end",
    "comment": "Bitbucket Server supports the ability to reply to any comment and created multiple threads. It represents these as a linked list of comments within comments. For example:  \"comments\": [ { \"author\" : ... \"comments\": [ { \"author\": ...  Since GitLab only supports a single thread, we flatten all these comments into a single discussion.",
    "label": "",
    "id": "2721"
  },
  {
    "raw_code": "def flatten_comments\n        comments = raw_comment['comments']\n        workset =\n          if comments\n            [CommentNode.new(comments, self)]\n          else\n            []\n          end",
    "comment": "In order to provide context for each reply, we need to track the parent of each comment. This method works as follows:  1. Insert the root comment into the workset. The root element is the current note. 2. For each node in the workset: a. Examine if it has replies to that comment. If it does, insert that node into the workset. b. Parse that note into a Comment structure and add it to a flat list.",
    "label": "",
    "id": "2722"
  },
  {
    "raw_code": "def initialize(query_builder:, connection:, min_value: nil, min_max_strategy: :min_max)\n      @query_builder = query_builder\n      @connection = connection\n      @min_value = min_value\n      @min_max_strategy = min_max_strategy\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- this is a ClickHouse query builder class usin Arel",
    "label": "",
    "id": "2723"
  },
  {
    "raw_code": "def migrate(direction)\n      return unless respond_to?(direction)\n\n      case direction\n      when :up   then announce 'migrating'\n      when :down then announce 'reverting'\n      end",
    "comment": "Execute this migration in the named direction",
    "label": "",
    "id": "2724"
  },
  {
    "raw_code": "def migrate_without_lock\n        raise ClickHouse::MigrationSupport::Errors::UnknownMigrationVersionError, @target_version if invalid_target?\n\n        runnable.each(&method(:execute_migration)) # rubocop: disable Performance/MethodObjectAsBlock -- Execute through proxy\n      end",
    "comment": "Used for running multiple migrations up to or down to a certain value.",
    "label": "",
    "id": "2725"
  },
  {
    "raw_code": "def invalid_target?\n        return unless @target_version\n        return if @target_version == 0\n\n        !target\n      end",
    "comment": "Return true if a valid version is not provided.",
    "label": "",
    "id": "2726"
  },
  {
    "raw_code": "def mean_duration_in_seconds\n          select(\n            round(\n              ms_to_s(query_builder.avg(:duration))\n            ).as('mean_duration_in_seconds'),\n            aggregate: true\n          )\n        end",
    "comment": "Aggregation methods",
    "label": "",
    "id": "2727"
  },
  {
    "raw_code": "def file_name(key_path)\n      \"#{Time.now.utc.strftime('%Y%m%d%H%M%S')}_#{metric_name(key_path)}\"\n    end",
    "comment": "Example of file name  20210201124931_g_project_management_issue_title_changed_weekly.yml",
    "label": "",
    "id": "2728"
  },
  {
    "raw_code": "def create_migration_file\n        set_local_assigns!\n        validate_file_name!\n        migration_template \"migration.rb.template\", File.join(db_migrate_path, \"#{file_name}.rb\")\n      end",
    "comment": "Override create_migration_file to ensure custom template is used",
    "label": "",
    "id": "2729"
  },
  {
    "raw_code": "def source_paths\n      super + [self.class.superclass.default_source_root]\n    end",
    "comment": "Override to find templates from superclass as well",
    "label": "",
    "id": "2730"
  },
  {
    "raw_code": "def search_type_requires_pipe_detection?\n      true\n    end",
    "comment": "Overridden in EE to exclude zoekt exact search mode",
    "label": "",
    "id": "2731"
  },
  {
    "raw_code": "def use_elasticsearch_finder?\n        false\n      end",
    "comment": "Overwritten in ee/lib/ee/search/advanced_finders/work_items_finder.rb",
    "label": "",
    "id": "2732"
  },
  {
    "raw_code": "def weak_for_user?(password, user)\n        user_info_in_password?(password, user) || common_phrases_in_password?(password)\n      end",
    "comment": "Returns true when the password is on a list of weak passwords, or contains predictable substrings derived from user attributes. Case insensitive.",
    "label": "",
    "id": "2733"
  },
  {
    "raw_code": "def contains_predicatable_substring?(password, substrings)\n        return unless password.length < PASSWORD_SUBSTRING_CHECK_MAX_LENGTH\n\n        substrings = substrings.filter_map do |substring|\n          substring.downcase if substring.length >= MINIMUM_SUBSTRING_SIZE\n        end",
    "comment": "Case-insensitively checks whether a password includes a dynamic list of substrings. Substrings which are too short are not predictable and may occur randomly, and therefore not checked. Similarly passwords which are long enough to inadvertently and randomly include a substring are not checked.",
    "label": "",
    "id": "2734"
  },
  {
    "raw_code": "def self.feature_enabled_from_application_settings?\n      Gitlab::CurrentSettings.vscode_extension_marketplace_enabled?\n    end",
    "comment": "Returns true if the ExtensionMarketplace feature is enabled from application settings  @return [Boolean]",
    "label": "",
    "id": "2735"
  },
  {
    "raw_code": "def self.marketplace_home_url(user:)\n      Gitlab::SafeRequestStore.fetch(:vscode_extension_marketplace_home_url) do\n        Settings.get_single_setting(:vscode_extension_marketplace_home_url, user: user)\n      end",
    "comment": "This value is used when the end-user is accepting the third-party extension marketplace integration.  @param user [User] Current user for context @return [String] URL of the VSCode Extension Marketplace home",
    "label": "",
    "id": "2736"
  },
  {
    "raw_code": "def self.help_preferences_url\n      ::Gitlab::Routing.url_helpers.help_page_url('user/profile/preferences.md',\n        anchor: 'integrate-with-the-extension-marketplace')\n    end",
    "comment": "@return [String] URL of the help page for the user preferences for Extensions Marketplace opt-in",
    "label": "",
    "id": "2737"
  },
  {
    "raw_code": "def self.webide_extension_marketplace_settings(user:)\n      Settings.get_single_setting(:vscode_extension_marketplace_view_model, user: user)\n    end",
    "comment": "This returns a value to be used in the Web IDE config `extensionsGallerySettings` It should match the type expected by the Web IDE:  - https://gitlab.com/gitlab-org/gitlab-web-ide/-/blob/51f9e91f890752596e7a3ef51f436fea07885eff/packages/web-ide-types/src/config.ts#L109  @param [User] user The current user @return [Hash]",
    "label": "",
    "id": "2738"
  },
  {
    "raw_code": "def self.generate(context)\n        return context unless context.fetch(:requested_setting_names).include?(:vscode_extension_marketplace)\n\n        context[:settings][:vscode_extension_marketplace] = extension_marketplace_from_application_settings\n        context\n      end",
    "comment": "@param [Hash] context @return [Hash]",
    "label": "",
    "id": "2739"
  },
  {
    "raw_code": "def self.extension_marketplace_from_application_settings\n        settings = Gitlab::CurrentSettings.vscode_extension_marketplace\n        preset_key = settings.fetch(\"preset\", ::WebIde::ExtensionMarketplacePreset.open_vsx.key)\n\n        if preset_key == ::WebIde::ExtensionMarketplacePreset::CUSTOM_KEY\n          settings.fetch(\"custom_values\").deep_symbolize_keys\n        else\n          preset = ::WebIde::ExtensionMarketplacePreset.all.find { |x| x.key == preset_key }\n          preset&.values\n        end",
    "comment": "@param [User, nil] user @return [Hash]",
    "label": "",
    "id": "2740"
  },
  {
    "raw_code": "def self.generate(context)\n        return context unless context.fetch(:requested_setting_names).include?(:vscode_extension_marketplace_home_url)\n\n        context[:settings][:vscode_extension_marketplace_home_url] = home_url(context)\n        context\n      end",
    "comment": "@param [Hash] context @return [Hash]",
    "label": "",
    "id": "2741"
  },
  {
    "raw_code": "def self.home_url(context)\n        context => {\n          settings: {\n            vscode_extension_marketplace: Hash => vscode_settings,\n          }\n        }\n\n        item_url = vscode_settings&.fetch(:item_url, nil)\n\n        return \"\" unless item_url\n\n        base_url = ::Gitlab::UrlHelpers.normalized_base_url(item_url)\n\n        # NOTE: It's possible for `normalized_base_url` to return something like `://` so let's go ahead and check\n        #       that we actually start with `http` or `https`.\n        return base_url if /^https?:/.match?(base_url)\n\n        \"\"\n      end",
    "comment": "@param [Hash] context @return [String] The URL to use for the extension marketplace home",
    "label": "",
    "id": "2742"
  },
  {
    "raw_code": "def self.validate(context)\n        unless context.fetch(:requested_setting_names).intersect?([:vscode_extension_marketplace])\n          return Gitlab::Fp::Result.ok(context)\n        end",
    "comment": "@param [Hash] context @return [Gitlab::Fp::Result]",
    "label": "",
    "id": "2743"
  },
  {
    "raw_code": "def self.validate_against_schema(hash_to_validate)\n        schema = {\n          \"required\" =>\n            %w[\n              service_url\n              item_url\n              resource_url_template\n            ],\n          \"properties\" => {\n            \"service_url\" => {\n              \"type\" => \"string\"\n            },\n            \"item_url\" => {\n              \"type\" => \"string\"\n            },\n            \"resource_url_template\" => {\n              \"type\" => \"string\"\n            },\n            \"control_url\" => {\n              \"type\" => \"string\"\n            },\n            \"nls_base_url\" => {\n              \"type\" => \"string\"\n            },\n            \"publisher_url\" => {\n              \"type\" => \"string\"\n            }\n          }\n        }\n\n        schemer = JSONSchemer.schema(schema)\n        errors = schemer.validate(hash_to_validate)\n        errors.map { |error| JSONSchemer::Errors.pretty(error) }\n      end",
    "comment": "@param [Hash] hash_to_validate @return [Array]",
    "label": "",
    "id": "2744"
  },
  {
    "raw_code": "def self.generate(context)\n        return context unless context.fetch(:requested_setting_names).include?(:vscode_extension_marketplace_view_model)\n\n        context[:settings][:vscode_extension_marketplace_view_model] = build_view_model(context)\n\n        context\n      end",
    "comment": "@param [Hash] context @return [Hash]",
    "label": "",
    "id": "2745"
  },
  {
    "raw_code": "def self.build_view_model(context)\n        context => {\n          options: {\n            user: ::User => user\n          },\n          settings: {\n            vscode_extension_marketplace: Hash => vscode_settings,\n            vscode_extension_marketplace_metadata: Hash => metadata\n          }\n        }\n\n        return { enabled: true, vscode_settings: vscode_settings } if metadata.fetch(:enabled)\n\n        disabled_reason = metadata.fetch(:disabled_reason)\n\n        result = { enabled: false, reason: disabled_reason, help_url: help_url }\n\n        result.merge(gallery_disabled_extra_attributes(disabled_reason: disabled_reason, user: user))\n      end",
    "comment": "Builds the value for :vscode_extension_marketplace_view_model  @param [Hash] context The settings railway context @return [Hash] value for :vscode_extension_marketplace_view_model",
    "label": "",
    "id": "2746"
  },
  {
    "raw_code": "def self.gallery_disabled_extra_attributes(disabled_reason:, user:)\n        return { user_preferences_url: user_preferences_url } if disabled_reason == :opt_in_unset\n        return { user_preferences_url: user_preferences_url } if disabled_reason == :opt_in_disabled\n\n        {}\n      end",
    "comment": "Returns extra attributes for the view model when the extensions marketplace is disabled  Overridden in EE  @param [Symbol] disabled_reason The reason why the gallery is disabled @param [User] user The current user (only used in EE override) @return [Hash] Extra attributes for the view model  rubocop:disable Lint/UnusedMethodArgument -- `user:` param is used in EE",
    "label": "",
    "id": "2747"
  },
  {
    "raw_code": "def self.help_url\n        ::Gitlab::Routing.url_helpers.help_page_url('user/project/web_ide/_index.md', anchor: 'extension-marketplace')\n      end",
    "comment": "rubocop:enable Lint/UnusedMethodArgument Returns help url for Web IDE extensions marketplace  @return [String]",
    "label": "",
    "id": "2748"
  },
  {
    "raw_code": "def self.user_preferences_url\n        # noinspection RubyResolve -- Rubymine is not correctly recognizing indirectly referenced route helper\n        ::Gitlab::Routing.url_helpers.profile_preferences_url(anchor: 'integrations')\n      end",
    "comment": "Returns user preferences url for changing the user's opt-in status for VSCode extensions marketplace  @return [String]",
    "label": "",
    "id": "2749"
  },
  {
    "raw_code": "def self.generate(context)\n        return context unless context.fetch(:requested_setting_names).include?(:vscode_extension_marketplace_metadata)\n\n        context => {\n          options: Hash => options,\n          settings: {\n            vscode_extension_marketplace_home_url: String => marketplace_home_url,\n          }\n        }\n        options_with_defaults = { user: nil }.merge(options)\n        options_with_defaults => {\n          user: ::User | NilClass => user\n        }\n\n        extension_marketplace_metadata = build_metadata(\n          user: user,\n          marketplace_home_url: marketplace_home_url\n        )\n\n        context[:settings][:vscode_extension_marketplace_metadata] = extension_marketplace_metadata\n        context\n      end",
    "comment": "@param [Hash] context @return [Hash]",
    "label": "",
    "id": "2750"
  },
  {
    "raw_code": "def self.build_metadata(user:, marketplace_home_url:)\n        return metadata_disabled(:no_user) unless user\n\n        unless ::WebIde::ExtensionMarketplace.feature_enabled_from_application_settings?\n          return metadata_disabled(:instance_disabled)\n        end",
    "comment": "@param [User, nil] user @param [Boolean, nil] flag_enabled @return [Hash]",
    "label": "",
    "id": "2751"
  },
  {
    "raw_code": "def self.build_metadata_for_user(user:, marketplace_home_url:)\n        # noinspection RubyNilAnalysis -- RubyMine doesn't realize user can't be nil because of guard clause above\n        opt_in_status = ::WebIde::ExtensionMarketplaceOptIn.opt_in_status(\n          user: user,\n          marketplace_home_url: marketplace_home_url\n        ).to_sym\n\n        case opt_in_status\n        when :enabled\n          metadata_enabled\n        when :unset\n          metadata_disabled(:opt_in_unset)\n        when :disabled\n          metadata_disabled(:opt_in_disabled)\n        else\n          # This is an internal bug due to an enumeration mismatch/inconsistency with the model\n          raise \"Invalid user.extensions_marketplace_opt_in_status: '#{opt_in_status}'. \" \\\n            \"Supported statuses are: #{Enums::WebIde::ExtensionsMarketplaceOptInStatus.statuses.keys}.\"\n        end",
    "comment": "note: This is overridden in EE  @param [User] user @return [Hash]",
    "label": "",
    "id": "2752"
  },
  {
    "raw_code": "def self.metadata_enabled\n        { enabled: true }\n      end",
    "comment": "@return [Hash]",
    "label": "",
    "id": "2753"
  },
  {
    "raw_code": "def self.metadata_disabled(reason)\n        { enabled: false, disabled_reason: disabled_reasons.fetch(reason) }\n      end",
    "comment": "@param [symbol] reason @return [Hash]",
    "label": "",
    "id": "2754"
  },
  {
    "raw_code": "def self.init(context)\n        context => { requested_setting_names: Array => requested_setting_names }\n\n        # NOTE: We override the requested_setting_names to include *all* nested setting dependencies.\n        requested_setting_names = Gitlab::Fp::Settings::SettingsDependencyResolver.resolve(\n          requested_setting_names,\n          SETTINGS_DEPENDENCIES\n        )\n        context[:requested_setting_names] = requested_setting_names\n\n        context[:settings], context[:setting_types] = Gitlab::Fp::Settings::DefaultSettingsParser.parse(\n          module_name: \"Web IDE\",\n          requested_setting_names: requested_setting_names,\n          default_settings: DefaultSettings.default_settings\n        )\n\n        # NOTE: This is context which is required by shared Gitlab::Fp::Settings::EnvVarOverrideProcessor class\n        context[:env_var_prefix] = \"GITLAB_WEB_IDE\"\n        context[:env_var_failed_message_class] =\n          WebIde::Settings::Messages::SettingsEnvironmentVariableOverrideFailed\n\n        context\n      end",
    "comment": "@param [Hash] context @return [Hash] @raise [RuntimeError]",
    "label": "",
    "id": "2755"
  },
  {
    "raw_code": "def self.default_settings\n        {\n          vscode_extension_marketplace: [\n            {}, # NOTE: There is no default, the value is always generated by ExtensionMarketplaceGenerator\n            Hash\n          ],\n          vscode_extension_marketplace_metadata: [\n            { enabled: false, disabled_reason: :instance_disabled },\n            Hash\n          ],\n          vscode_extension_marketplace_home_url: [\n            \"\",\n            String\n          ],\n          vscode_extension_marketplace_view_model: [\n            { enabled: false, reason: :instance_disabled, help_url: '' },\n            Hash\n          ]\n        }\n      end",
    "comment": "ALL WEB IDE SETTINGS ARE DECLARED HERE. @return [Hash]",
    "label": "",
    "id": "2756"
  },
  {
    "raw_code": "def self.validate(context)\n        unless context.fetch(:requested_setting_names).include?(:vscode_extension_marketplace_metadata)\n          return Gitlab::Fp::Result.ok(context)\n        end",
    "comment": "@param [Hash] context @return [Gitlab::Fp::Result]",
    "label": "",
    "id": "2757"
  },
  {
    "raw_code": "def self.make_hash_validatable_by_json_schemer(hash)\n        hash\n          .deep_stringify_keys\n          .transform_values { |v| v.is_a?(Symbol) ? v.to_s : v }\n      end",
    "comment": "@param [Hash] hash @return [Hash]",
    "label": "",
    "id": "2758"
  },
  {
    "raw_code": "def self.validate_against_schema(hash_to_validate)\n        schema = {\n          \"properties\" => {\n            \"enabled\" => {\n              \"type\" => \"boolean\"\n            }\n          },\n          # do conditional check that \"enabled\" is boolean type\n          \"if\" => {\n            \"properties\" => {\n              \"enabled\" => {\n                \"type\" => \"boolean\"\n              }\n            }\n          },\n          \"then\" => {\n            # \"enabled\" is boolean, do conditional check for \"enabled\" value\n            \"if\" => {\n              \"properties\" => {\n                \"enabled\" => {\n                  \"const\" => true\n                }\n              }\n            },\n            \"then\" => {\n              # \"enabled\" is true, \"disabled_reason\" is not required\n              \"required\" => %w[enabled]\n            },\n            \"else\" => {\n              # \"enabled\" is false, \"disabled_reason\" is required\n              \"required\" => %w[enabled disabled_reason],\n              \"properties\" => {\n                \"disabled_reason\" => {\n                  \"type\" => \"string\"\n                }\n              }\n            }\n          }\n        }\n\n        schemer = JSONSchemer.schema(schema)\n        errors = schemer.validate(hash_to_validate)\n        errors.map { |error| JSONSchemer::Errors.pretty(error) }\n      end",
    "comment": "@param [Hash] hash_to_validate @return [Array]",
    "label": "",
    "id": "2759"
  },
  {
    "raw_code": "def self.get_settings(context)\n        initial_result = Gitlab::Fp::Result.ok(context)\n\n        # TODO: Add instance-level setting for extension marketplace settings.\n        #       See https://gitlab.com/gitlab-org/gitlab/-/issues/451871\n        result =\n          initial_result\n            .map(SettingsInitializer.method(:init))\n            .map(ExtensionMarketplaceGenerator.method(:generate))\n            .map(ExtensionMarketplaceHomeUrlGenerator.method(:generate))\n            .map(ExtensionMarketplaceMetadataGenerator.method(:generate))\n            # NOTE: EnvVarOverrideProcessor is inserted here to easily override settings for local or temporary testing\n            #       it should happen **before** validators.\n            .and_then(Gitlab::Fp::Settings::EnvVarOverrideProcessor.method(:process))\n            .and_then(ExtensionMarketplaceValidator.method(:validate))\n            .and_then(ExtensionMarketplaceMetadataValidator.method(:validate))\n            # NOTE: ViewModel generator happens near the end since it depends on other settings.\n            .map(ExtensionMarketplaceViewModelGenerator.method(:generate))\n            .map(\n              # As the final step, return the settings in a SettingsGetSuccessful message\n              ->(context) do\n                SettingsGetSuccessful.new(settings: context.fetch(:settings))\n              end",
    "comment": "@param [Hash] context @return [Hash] @raise [Gitlab::Fp::UnmatchedResultError]",
    "label": "",
    "id": "2760"
  },
  {
    "raw_code": "def self.type_name\n        raise NotImplementedError\n      end",
    "comment": "Human readable type used in error messages",
    "label": "",
    "id": "2761"
  },
  {
    "raw_code": "def required?\n        !spec.key?(:default)\n      end",
    "comment": "An input specification without a default value is required. For example: ```yaml spec: inputs: website: ```",
    "label": "",
    "id": "2762"
  },
  {
    "raw_code": "def validate_type(_value, _default)\n        raise NotImplementedError\n      end",
    "comment": "Type validations are done separately for different input types.",
    "label": "",
    "id": "2763"
  },
  {
    "raw_code": "def validate_options(_value)\n        return unless options\n\n        error('Options can only be used with string and number inputs')\n      end",
    "comment": "Options can be either StringInput or NumberInput and are validated accordingly.",
    "label": "",
    "id": "2764"
  },
  {
    "raw_code": "def validate_regex(_value, _default)\n        return unless regex_provided?\n\n        error('RegEx validation can only be used with string inputs')\n      end",
    "comment": "Regex can be only be a StringInput and is validated accordingly.",
    "label": "",
    "id": "2765"
  },
  {
    "raw_code": "def initialize(specs)\n        @inputs = []\n        @errors = []\n\n        return unless valid_specs?(specs)\n\n        build_inputs!(specs.to_h)\n      end",
    "comment": "@param specs [Hash] A hash containing inputs specifications from `spec:inputs` config header",
    "label": "",
    "id": "2766"
  },
  {
    "raw_code": "def enqueue_and_track_last!(project_ids)\n        return if project_ids.empty?\n\n        with_redis do |redis|\n          redis.pipelined do |pipeline|\n            pipeline.rpush(QUEUE_KEY, project_ids)\n            pipeline.set(LAST_QUEUED_KEY, project_ids.last, ex: REDIS_EXPIRATION_TIME)\n          end",
    "comment": " Add new work to the queue and keep track of last item.  existing[1, 2, 3] << new[4, 5, 6] ==> [1, 2, 3, 4, 5, 6] last: 6",
    "label": "",
    "id": "2767"
  },
  {
    "raw_code": "def routable_payload(job)\n          {\n            c: Gitlab.config.cell.id,\n            o: job.project.organization_id,\n            u: job.user_id,\n            p: job.project_id,\n            g: job.project.group&.id\n          }.compact_blank.transform_values { |id| id.to_s(36) }\n        end",
    "comment": "Creating routing information for routable tokens https://handbook.gitlab.com/handbook/engineering/architecture/design-documents/cells/routable_tokens/",
    "label": "",
    "id": "2768"
  },
  {
    "raw_code": "def sorts_in_same_order_as_index?\n        (index.columns & order_attributes) == order_attributes\n      end",
    "comment": "All order attributes exist in the query in the same order as they are queried.",
    "label": "",
    "id": "2769"
  },
  {
    "raw_code": "def no_filtering_after_sort_columns?\n        return true if order_attributes.empty?\n\n        (index.columns.split(order_attributes.last).last & filter_attributes).empty?\n      end",
    "comment": "All order attributes exist in the query in the same order as they are queried. We rely on sort order to be the same here to assume that anything following the last sort should not be filtered on.",
    "label": "",
    "id": "2770"
  },
  {
    "raw_code": "def unused_columns_at_end_of_index?\n        remaining_columns = (index.columns - combined_attributes)\n\n        (index.columns.last(remaining_columns.size) - remaining_columns).empty?\n      end",
    "comment": "We assume there are <= attributes than columns because filter_attributes_covered has already passed. So we check the count of unqueried columns, take that number from the end of the index, and compare to ensure that any unqueried columns are only at the end of the index. This also helps ensure there are no gaps in the used columns of the index.",
    "label": "",
    "id": "2771"
  },
  {
    "raw_code": "def rewrite\n      log_rewrite\n\n      return filter_query unless primary_key_present?\n\n      index_only_filter_query\n    end",
    "comment": "Rewrites the given ActiveRecord::Relation object to utilize the DB indices efficiently.  Currently Postgres will produce inefficient query plans which use a `filter_predicate` instead of a `access_predicate` to filter by IN clause contents. This behaviour does a table read of the data for filtering, disregarding the structure of the index and losing any benefit from any sorting applied to the index as it will have to resort the table read data.  Rewriting the query using the `unnest` command induces Postgres into using the appropriate index search behaviour for each column in the index by generating a cartesian product between the individual items of the IN filter items and queried table. This means each read column will maintain the sort order provided by the index, avoiding a memory sort node in the final query plan.  This will not work if queried columns are not all present in the index, or if unqueried columns exist in the index that are not at the end, as this makes that part of the index useless to Postgres and will result in a table scan anyways from that point.   Example usage;  relation = Vulnerabilities::Read.where(state: [1, 4]) relation = relation.order(severity: :desc, vulnerability_id: :desc)  rewriter = UnnestedInFilters::Rewriter.new(relation) optimized_relation = rewriter.rewrite  In the above example. the `relation` object would produce the following SQL query;  SELECT \"vulnerability_reads\".* FROM \"vulnerability_reads\" WHERE \"vulnerability_reads\".\"state\" IN (1, 4) ORDER BY \"vulnerability_reads\".\"severity\" DESC, \"vulnerability_reads\".\"vulnerability_id\" DESC LIMIT 20;  And the `optimized_relation` object would would produce the following query to utilize the index on (state, severity, vulnerability_id);  SELECT \"vulnerability_reads\".* FROM unnest('{1, 4}'::smallint[]) AS \"states\" (\"state\"), LATERAL ( SELECT \"vulnerability_reads\".* FROM \"vulnerability_reads\" WHERE (vulnerability_reads.\"state\" = \"states\".\"state\") ORDER BY \"vulnerability_reads\".\"severity\" DESC, \"vulnerability_reads\".\"vulnerability_id\" DESC LIMIT 20) AS vulnerability_reads ORDER BY \"vulnerability_reads\".\"severity\" DESC, \"vulnerability_reads\".\"vulnerability_id\" DESC LIMIT 20  If one of the columns being used for filtering or ordering is the primary key, then the query will be further optimized to use an index-only scan for initial filtering before selecting all columns using the primary key.  Using the prior query as an example, where `vulnerability_id` is the primary key, This will be rewritten to:  SELECT \"vulnerability_reads\".* FROM \"vulnerability_reads\" WHERE \"vulnerability_reads\".\"vulnerability_id\" IN ( SELECT \"vulnerability_reads\".\"vulnerability_id\" FROM unnest('{1, 4}'::smallint[]) AS \"states\" (\"state\"), LATERAL ( SELECT \"vulnerability_reads\".\"vulnerability_id\" FROM \"vulnerability_reads\" WHERE (vulnerability_reads.\"state\" = \"states\".\"state\") ORDER BY \"vulnerability_reads\".\"severity\" DESC, \"vulnerability_reads\".\"vulnerability_id\" DESC LIMIT 20 ) AS vulnerability_reads ) ORDER BY \"vulnerability_reads\".\"severity\" DESC, \"vulnerability_reads\".\"vulnerability_id\" DESC LIMIT 20",
    "label": "",
    "id": "2772"
  },
  {
    "raw_code": "def arel_in_nodes\n      where_clause_arel_nodes\n        .select { |arel_node| in_predicate?(arel_node) }\n        .select { |arel_node| model_column_names.include?(arel_node.left.name) }\n    end",
    "comment": "Actively filter any nodes that don't belong to the primary queried table to prevent sql type resolution issues Context: https://gitlab.com/gitlab-org/gitlab/-/issues/370271#note_1151019824",
    "label": "",
    "id": "2773"
  },
  {
    "raw_code": "def where_clause_arel_nodes\n      return [where_clause_ast] unless where_clause_ast.respond_to?(:children)\n\n      where_clause_ast.children\n    end",
    "comment": "`ActiveRecord::WhereClause#ast` is returning a single node when there is only one predicate but returning an `Arel::Nodes::And` node if there are more than one predicates. This is why we are checking the returned object responds to `children` or not.",
    "label": "",
    "id": "2774"
  },
  {
    "raw_code": "def initialize(context, entry)\n      @context = context\n      @entity = context.entity\n      @entry = entry\n    end",
    "comment": "@param [BulkImports::Pipeline::Context] context @param [ApplicationRecord] entry",
    "label": "",
    "id": "2775"
  },
  {
    "raw_code": "def url\n      return unless entry.is_a?(ApplicationRecord)\n      return unless iid\n      return unless ALLOWED_RELATIONS.include?(relation)\n\n      File.join(source_instance_url, group_prefix, source_full_path, '-', relation, iid.to_s)\n    end",
    "comment": "Builds a source URL for the given entry if iid is present",
    "label": "",
    "id": "2776"
  },
  {
    "raw_code": "def group_prefix\n      return '' if entity.project?\n\n      entity.pluralized_name\n    end",
    "comment": "Group milestone (or epic) url is /groups/:group_path/-/milestones/:iid Project milestone url is /:project_path/-/milestones/:iid",
    "label": "",
    "id": "2777"
  },
  {
    "raw_code": "def persist!(tracker)\n        counters = summary(tracker)\n\n        return unless counters\n\n        tracker.update!(\n          source_objects_count: counters[SOURCE_COUNTER],\n          fetched_objects_count: counters[FETCHED_COUNTER],\n          imported_objects_count: counters[IMPORTED_COUNTER]\n        )\n      end",
    "comment": "Commits counters from redis to the database",
    "label": "",
    "id": "2778"
  },
  {
    "raw_code": "def initialize(configuration)\n      @configuration = configuration\n    end",
    "comment": "@param [BulkImports::Configuration] configuration",
    "label": "",
    "id": "2779"
  },
  {
    "raw_code": "def fetch_ghost_user\n      attempt = 0\n\n      begin\n        attempt += 1\n        query = <<~GRAPHQL\n          {\n            users(usernames: [\"ghost\", \"ghost1\", \"ghost2\", \"ghost3\", \"ghost4\", \"ghost5\", \"ghost6\"], humans: false) {\n              nodes {\n                id\n                username\n                type\n              }\n            }\n          }\n        GRAPHQL\n\n        response = client.execute(query: query)\n        response = response.dig('data', 'users', 'nodes') || []\n        response.find { |user| user[\"type\"] == 'GHOST' }\n      rescue StandardError => e\n        if attempt < MAX_RETRIES\n          delay = 2**attempt # Exponential backoff (2, 4, 8...)\n          sleep(delay)\n          retry\n        end",
    "comment": "@return [Hash, nil]",
    "label": "",
    "id": "2780"
  },
  {
    "raw_code": "def set_ghost_user_id\n      return if Gitlab::Cache::Import::Caching.read(cache_key).present?\n\n      ghost_user = fetch_ghost_user\n      return unless ghost_user # since fetch_ghost_user returns nil if it's not in the API response\n\n      model_id = GlobalID.parse(ghost_user['id']).model_id\n\n      Gitlab::Cache::Import::Caching.write(cache_key, model_id)\n    rescue StandardError => e\n      Gitlab::ErrorTracking.track_exception(e,\n        { message: \"Failed to set source ghost user ID\", bulk_import_id: configuration.bulk_import_id }\n      )\n      nil\n    end",
    "comment": "@return [String, nil]",
    "label": "",
    "id": "2781"
  },
  {
    "raw_code": "def cached_ghost_user_id\n      Gitlab::Cache::Import::Caching.read(cache_key)\n    end",
    "comment": "Returns the cached ID of the ghost user from the source instance, if it exists.  @return [String, nil]",
    "label": "",
    "id": "2782"
  },
  {
    "raw_code": "def visibility_level(entity, namespace, visibility_string)\n      requested = requested_visibility_level(entity, visibility_string)\n      namespace_level = namespace&.visibility_level\n\n      lowest_level = [requested, namespace_level].compact.min\n\n      closet_allowed_level(lowest_level)\n    end",
    "comment": "Calculates visbility level based on the source and the destination namespace visbility levels If there are visibility_level restrictions on the destination instance, the highest allowed level less than the calculated level is returned",
    "label": "",
    "id": "2783"
  },
  {
    "raw_code": "def create_import_source_users(relation_key, relation_hash)\n        relation_factory::USER_REFERENCES.each do |reference|\n          next unless relation_hash[reference]\n\n          # Skip creating placeholder users for these relations.\n          # These may reference users that no longer exist in the source instance\n          # as they lack a foreign key constraint.\n          next if IGNORE_PLACEHOLDER_USER_CREATION[relation_key]&.include?(reference)\n\n          # Skip creating placeholder users for imported ghost users.\n          # Ghost user contributions are assigned directly to the destination ghost user\n          next if context.source_ghost_user_id.to_s == relation_hash[reference].to_s\n\n          source_user_mapper.find_or_create_source_user(\n            source_name: nil,\n            source_username: nil,\n            source_user_identifier: relation_hash[reference]\n          )\n        end",
    "comment": "Creates an Import::SourceUser objects for each source_user_identifier found in the relation_hash and associate it with the ImportUser.  For example, if the relation_hash is:  { \"title\": \"Title\", \"author_id\": 100, \"updated_by_id\": 101 }  Import::SourceUser records with source_user_identifier 100 and 101 will be created if none are found in the database, along with a placeholder user for each record.",
    "label": "",
    "id": "2784"
  },
  {
    "raw_code": "def push_placeholder_references(original_users_map)\n        original_users_map.each do |object, user_references|\n          next unless object.persisted?\n\n          user_references.each do |attribute, source_user_identifier|\n            source_user = source_user_mapper.find_source_user(source_user_identifier)\n            next unless source_user\n\n            # Do not create a reference if the object is already associated\n            # with a real user.\n            next if source_user.accepted_status? && object[attribute] == source_user.reassign_to_user_id\n\n            ::Import::PlaceholderReferences::PushService.from_record(\n              import_source: ::Import::SOURCE_DIRECT_TRANSFER,\n              import_uid: context.bulk_import_id,\n              record: object,\n              source_user: source_user,\n              user_reference_column: attribute.to_sym\n            ).execute\n          end",
    "comment": "Pushes a placeholder reference for each source_user_identifier contained in the original_users_map.  The `original_users_map` is a hash where the key is an object built by the RelationFactory, and the value is another hash. This second hash maps attributes that reference user IDs to the user IDs from the source instance, essentially the information present in the NDJSON file.  For example, below is an example of `original_users_map`:  { #<Issue:0x0001: {\"author_id\"=>1, \"updated_by_id\"=>2, \"last_edited_by_id\"=>2, \"closed_by_id\"=>2 }, #<ResourceStateEvent:0x0002: {\"user_id\"=>1\"]}, #<ResourceStateEvent:0x0003: {\"user_id\"=>2\"]}, #<ResourceStateEvent:0x0004: {\"user_id\"=>2\"]}, #<Note:0x0005: {\"author_id\"=>1\"]}, #<Note:0x0006: {\"author_id\"=>2\"]} }",
    "label": "",
    "id": "2785"
  },
  {
    "raw_code": "def with_entity(entity)\n      @entity = entity\n      self\n    end",
    "comment": "Extract key information from a provided entity and include it in log entries created from this logger instance. @param entity [BulkImports::Entity]",
    "label": "",
    "id": "2786"
  },
  {
    "raw_code": "def with_tracker(tracker)\n      with_entity(tracker.entity)\n      @tracker = tracker\n      self\n    end",
    "comment": "Extract key information from a provided tracker and its entity and include it in log entries created from this logger instance. @param tracker [BulkImports::Tracker]",
    "label": "",
    "id": "2787"
  },
  {
    "raw_code": "def extractor\n        @extractor ||= self.respond_to?(:extract) ? self : instantiate(self.class.get_extractor)\n      end",
    "comment": "Fetch pipeline extractor. An extractor is defined either by instance `#extract(context)` method or by using `extractor` DSL.  @example class MyPipeline extractor MyExtractor, foo: :bar end  class MyPipeline def extract(context) puts 'Fetch some data' end end  If pipeline implements instance method `extract` - use it and ignore class `extractor` method implementation.",
    "label": "",
    "id": "2788"
  },
  {
    "raw_code": "def transformers\n        strong_memoize(:transformers) do\n          defined_transformers = self.class.transformers.map(&method(:instantiate))\n\n          transformers = []\n          transformers << self if respond_to?(:transform)\n          transformers.concat(defined_transformers)\n          transformers\n        end",
    "comment": "Fetch pipeline transformers.  A transformer can be defined using: - `transformer` class method - `transform` instance method  Multiple transformers can be defined within a single pipeline and run sequentially for each record in the following order: - Instance method `transform` - Transformers defined using `transformer` class method  Instance method `transform` is always the last to run.  @example class MyPipeline transformer MyTransformerOne, foo: :bar transformer MyTransformerTwo, foo: :bar  def transform(context, data) # perform transformation here end end  In the example above `#transform` is the first to run and `MyTransformerTwo` method is the last.",
    "label": "",
    "id": "2789"
  },
  {
    "raw_code": "def loader\n        @loader ||= self.respond_to?(:load) ? self : instantiate(self.class.get_loader)\n      end",
    "comment": "Fetch pipeline loader. A loader is defined either by instance method `#load(context, data)` or by using `loader` DSL.  @example class MyPipeline loader MyLoader, foo: :bar end  class MyPipeline def load(context, data) puts 'Load some data' end end  If pipeline implements instance method `load` - use it and ignore class `loader` method implementation.",
    "label": "",
    "id": "2790"
  },
  {
    "raw_code": "def config\n        base_config = {\n          group: {\n            pipeline: BulkImports::Groups::Pipelines::GroupPipeline,\n            stage: 0\n          },\n          group_attributes: {\n            pipeline: BulkImports::Groups::Pipelines::GroupAttributesPipeline,\n            stage: 1\n          },\n          namespace_settings: {\n            pipeline: BulkImports::Groups::Pipelines::NamespaceSettingsPipeline,\n            stage: 1,\n            minimum_source_version: '15.0.0'\n          },\n          labels: {\n            pipeline: BulkImports::Common::Pipelines::LabelsPipeline,\n            stage: 1\n          },\n          milestones: {\n            pipeline: BulkImports::Common::Pipelines::MilestonesPipeline,\n            stage: 1\n          },\n          badges: {\n            pipeline: BulkImports::Common::Pipelines::BadgesPipeline,\n            stage: 1\n          },\n          boards: {\n            pipeline: BulkImports::Common::Pipelines::BoardsPipeline,\n            stage: 2\n          },\n          uploads: {\n            pipeline: BulkImports::Common::Pipelines::UploadsPipeline,\n            stage: 2\n          },\n          subgroups: {\n            pipeline: BulkImports::Groups::Pipelines::SubgroupEntitiesPipeline,\n            stage: 3 # SubGroup Entities must be imported in later stage\n            # to Project Entities to avoid `full_path` naming conflicts.\n          },\n          finisher: {\n            pipeline: BulkImports::Common::Pipelines::EntityFinisher,\n            stage: 4\n          }\n        }\n\n        base_config\n          .merge(project_entities_pipeline)\n          .merge(members_pipeline)\n      end",
    "comment": "To skip the execution of a pipeline in a specific source instance version, define the attributes `minimum_source_version` and `maximum_source_version`.  Use the `minimum_source_version` to inform that the pipeline needs to run when importing from source instances version greater than or equal to the specified minimum source version. For example, if the `minimum_source_version` is equal to 15.1.0, the pipeline will be executed when importing from source instances running versions 15.1.0, 15.1.1, 15.2.0, 16.0.0, etc. And it won't be executed when the source instance version is 15.0.1, 15.0.0, 14.10.0, etc.  Use the `maximum_source_version` to inform that the pipeline needs to run when importing from source instance versions less than or equal to the specified maximum source version. For example, if the `maximum_source_version` is equal to 15.1.0, the pipeline will be executed when importing from source instances running versions 15.1.1 (patch), 15.1.0, 15.0.1, 15.0.0, 14.10.0, etc. And it won't be executed when the source instance version is 15.2.0, 15.2.1, 16.0.0, etc.  SubGroup Entities must be imported in later stage than Project Entities to avoid `full_path` naming conflicts.",
    "label": "",
    "id": "2791"
  },
  {
    "raw_code": "def transform(context, data)\n          import_entity = context.entity\n\n          if import_entity.destination_namespace.present?\n            namespace = Namespace.find_by_full_path(import_entity.destination_namespace)\n          end",
    "comment": "rubocop: disable Style/IfUnlessModifier",
    "label": "",
    "id": "2792"
  },
  {
    "raw_code": "def request(method, resource, options = {}, &block)\n        with_error_handling do\n          Import::Clients::HTTP.public_send(\n            method,\n            resource_url(resource),\n            request_options(options),\n            &block\n          )\n        end",
    "comment": "rubocop:disable GitlabSecurity/PublicSend",
    "label": "",
    "id": "2793"
  },
  {
    "raw_code": "def request_options(options)\n        default_options.merge(options)\n      end",
    "comment": "rubocop:enable GitlabSecurity/PublicSend",
    "label": "",
    "id": "2794"
  },
  {
    "raw_code": "def with_error_handling\n        response = yield\n\n        return response if response.success?\n\n        raise ::BulkImports::NetworkError.new(\"Unsuccessful response #{response.code} from #{response.request.path.path}. Body: #{response.parsed_response}\", response: response)\n      rescue *Gitlab::HTTP::HTTP_ERRORS => e\n        raise ::BulkImports::NetworkError, e\n      end",
    "comment": "@raise [BulkImports::NetworkError] when unsuccessful",
    "label": "",
    "id": "2795"
  },
  {
    "raw_code": "def already_processed?(*)\n        false\n      end",
    "comment": "Overridden by child pipelines with different caching strategies",
    "label": "",
    "id": "2796"
  },
  {
    "raw_code": "def remote_filename\n        @remote_filename ||= begin\n          pattern = BulkImports::FileDownloads::FilenameFetch::REMOTE_FILENAME_PATTERN\n          name = response_headers['content-disposition'].to_s\n            .match(pattern)                                # matches the filename pattern\n            .then { |match| match&.named_captures || {} }  # ensures the match is a hash\n            .fetch('filename')                             # fetches the 'filename' key or raise KeyError\n\n          name = File.basename(name) # Ensures to remove path from the filename (../ for instance)\n          ensure_filename_size(name) # Ensures the filename is within the FILENAME_SIZE_LIMIT\n        end",
    "comment": "Fetch the remote filename information from the request content-disposition header - Raises if the filename does not exist - If the filename is longer then 255 chars truncate it to be a total of 255 chars (with the extension) rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "2797"
  },
  {
    "raw_code": "def ensure_filename_size(filename)\n        limit = BulkImports::FileDownloads::FilenameFetch::FILENAME_SIZE_LIMIT\n        return filename if filename.length <= limit\n\n        extname = File.extname(filename)\n        basename = File.basename(filename, extname)[0, limit]\n        \"#{basename}#{extname}\"\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "2798"
  },
  {
    "raw_code": "def config\n        base_config = {\n          project: {\n            pipeline: BulkImports::Projects::Pipelines::ProjectPipeline,\n            stage: 0\n          },\n          repository: {\n            pipeline: BulkImports::Projects::Pipelines::RepositoryPipeline,\n            maximum_source_version: '15.0.0',\n            stage: 1\n          },\n          repository_bundle: {\n            pipeline: BulkImports::Projects::Pipelines::RepositoryBundlePipeline,\n            minimum_source_version: '15.1.0',\n            stage: 1\n          },\n          project_attributes: {\n            pipeline: BulkImports::Projects::Pipelines::ProjectAttributesPipeline,\n            stage: 1\n          },\n          labels: {\n            pipeline: BulkImports::Common::Pipelines::LabelsPipeline,\n            stage: 2\n          },\n          milestones: {\n            pipeline: BulkImports::Common::Pipelines::MilestonesPipeline,\n            stage: 2\n          },\n          badges: {\n            pipeline: BulkImports::Common::Pipelines::BadgesPipeline,\n            stage: 2\n          },\n          issues: {\n            pipeline: BulkImports::Projects::Pipelines::IssuesPipeline,\n            stage: 3\n          },\n          snippets: {\n            pipeline: BulkImports::Projects::Pipelines::SnippetsPipeline,\n            stage: 3\n          },\n          snippets_repository: {\n            pipeline: BulkImports::Projects::Pipelines::SnippetsRepositoryPipeline,\n            stage: 4\n          },\n          boards: {\n            pipeline: BulkImports::Common::Pipelines::BoardsPipeline,\n            stage: 4\n          },\n          merge_requests: {\n            pipeline: BulkImports::Projects::Pipelines::MergeRequestsPipeline,\n            stage: 4\n          },\n          external_pull_requests: {\n            pipeline: BulkImports::Projects::Pipelines::ExternalPullRequestsPipeline,\n            stage: 4\n          },\n          protected_branches: {\n            pipeline: BulkImports::Projects::Pipelines::ProtectedBranchesPipeline,\n            stage: 4\n          },\n          project_feature: {\n            pipeline: BulkImports::Projects::Pipelines::ProjectFeaturePipeline,\n            stage: 4\n          },\n          container_expiration_policy: {\n            pipeline: BulkImports::Projects::Pipelines::ContainerExpirationPolicyPipeline,\n            stage: 4\n          },\n          service_desk_setting: {\n            pipeline: BulkImports::Projects::Pipelines::ServiceDeskSettingPipeline,\n            stage: 4\n          },\n          releases: {\n            pipeline: BulkImports::Projects::Pipelines::ReleasesPipeline,\n            stage: 4\n          },\n          ci_pipelines: {\n            pipeline: BulkImports::Projects::Pipelines::CiPipelinesPipeline,\n            stage: 5\n          },\n          commit_notes: {\n            pipeline: BulkImports::Projects::Pipelines::CommitNotesPipeline,\n            minimum_source_version: '15.10.0',\n            stage: 5\n          },\n          wiki: {\n            pipeline: BulkImports::Common::Pipelines::WikiPipeline,\n            stage: 5\n          },\n          uploads: {\n            pipeline: BulkImports::Common::Pipelines::UploadsPipeline,\n            stage: 5\n          },\n          lfs_objects: {\n            pipeline: BulkImports::Common::Pipelines::LfsObjectsPipeline,\n            stage: 5\n          },\n          design: {\n            pipeline: BulkImports::Projects::Pipelines::DesignBundlePipeline,\n            minimum_source_version: '15.1.0',\n            stage: 5\n          },\n          auto_devops: {\n            pipeline: BulkImports::Projects::Pipelines::AutoDevopsPipeline,\n            stage: 5\n          },\n          pipeline_schedules: {\n            pipeline: BulkImports::Projects::Pipelines::PipelineSchedulesPipeline,\n            stage: 5\n          },\n          references: {\n            pipeline: BulkImports::Projects::Pipelines::ReferencesPipeline,\n            stage: 5\n          },\n          finisher: {\n            pipeline: BulkImports::Common::Pipelines::EntityFinisher,\n            stage: 7\n          }\n        }\n\n        base_config.merge(members_pipeline)\n      end",
    "comment": "To skip the execution of a pipeline in a specific source instance version, define the attributes `minimum_source_version` and `maximum_source_version`.  Use the `minimum_source_version` to inform that the pipeline needs to run when importing from source instances version greater than or equal to the specified minimum source version. For example, if the `minimum_source_version` is equal to 15.1.0, the pipeline will be executed when importing from source instances running versions 15.1.0, 15.1.1, 15.2.0, 16.0.0, etc. And it won't be executed when the source instance version is 15.0.1, 15.0.0, 14.10.0, etc.  Use the `maximum_source_version` to inform that the pipeline needs to run when importing from source instance versions less than or equal to the specified maximum source version. For example, if the `maximum_source_version` is equal to 15.1.0, the pipeline will be executed when importing from source instances running versions 15.1.1 (patch), 15.1.0, 15.0.1, 15.0.0, 14.10.0, etc. And it won't be executed when the source instance version is 15.2.0, 15.2.1, 16.0.0, etc.",
    "label": "",
    "id": "2799"
  },
  {
    "raw_code": "def after_run(_)\n          ::Repositories::HousekeepingService.new(context.portable, :gc).execute\n        end",
    "comment": "The initial fetch can bring in lots of loose refs and objects. Running a `git gc` will make importing merge requests faster.",
    "label": "",
    "id": "2800"
  },
  {
    "raw_code": "def load(_context, file_path)\n          Gitlab::PathTraversal.check_path_traversal!(file_path)\n          Gitlab::PathTraversal.check_allowed_absolute_path!(file_path, [Dir.tmpdir])\n\n          return if tar_filepath?(file_path)\n          return if lfs_json_filepath?(file_path)\n          return if File.directory?(file_path)\n          return if Gitlab::Utils::FileInfo.linked?(file_path)\n\n          size = File.size(file_path)\n          oid = LfsObject.calculate_oid(file_path)\n\n          lfs_object = LfsObject.find_or_initialize_by(oid: oid, size: size)\n          lfs_object.file = File.open(file_path) unless lfs_object.file&.exists?\n          lfs_object.save! if lfs_object.changed?\n\n          repository_types(oid)&.each do |type|\n            create_lfs_objects_project(lfs_object, type)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2801"
  },
  {
    "raw_code": "def after_run(_)\n          FileUtils.rm_rf(tmpdir)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2802"
  },
  {
    "raw_code": "def enabled?\n      license_operational_metric_enabled? || ::Gitlab::CurrentSettings.usage_ping_enabled?\n    end",
    "comment": "If it is EE and license operational metric is true, then we will show enable service ping checkbox checked, as it will always send service ping",
    "label": "",
    "id": "2803"
  },
  {
    "raw_code": "def create_issue_link(integration_id, sentry_issue_id, issue)\n        return create_plugin_link(sentry_issue_id, issue) unless integration_id\n\n        create_global_integration_link(integration_id, sentry_issue_id, issue)\n      end",
    "comment": "Creates a link in Sentry corresponding to the provided Sentry issue and GitLab issue @param integration_id [Integer, nil] Representing a global GitLab integration in Sentry. Nil for plugins. @param sentry_issue_id [Integer] Id for an issue from Sentry @param issue [Issue] Issue for which the link should be created",
    "label": "",
    "id": "2804"
  },
  {
    "raw_code": "def initialize(progress, max_parallelism: nil, storage_parallelism: nil, incremental: false, server_side: false)\n      @progress = progress\n      @max_parallelism = max_parallelism\n      @storage_parallelism = storage_parallelism\n      @incremental = incremental\n      @server_side = server_side\n    end",
    "comment": "@param [StringIO] progress IO interface to output progress @param [Integer] max_parallelism max parallelism when running backups @param [Integer] storage_parallelism max parallelism per storage (is affected by max_parallelism) @param [Boolean] incremental if incremental backups should be created. @param [Boolean] server_side if server-side backups should be used.",
    "label": "",
    "id": "2805"
  },
  {
    "raw_code": "def schedule_backup_job(repository, always_create:)\n      json_job = {\n        storage_name: repository.storage,\n        relative_path: repository.relative_path,\n        gl_project_path: repository.gl_project_path,\n        always_create: always_create\n      }.to_json\n\n      @input_stream.puts(json_job)\n    end",
    "comment": "Schedule a new backup job through a non-blocking JSON based pipe protocol  @see https://gitlab.com/gitlab-org/gitaly/-/blob/master/doc/gitaly-backup.md",
    "label": "",
    "id": "2806"
  },
  {
    "raw_code": "def remote_directory\n      Gitlab.config.backup.upload.remote_directory\n    end",
    "comment": "The remote 'directory' to store your backups. For S3, this would be the bucket name. @example Configuration setting the S3 bucket name remote_directory: 'my.s3.bucket'",
    "label": "",
    "id": "2807"
  },
  {
    "raw_code": "def tar_file\n      @tar_file ||= \"#{backup_id}#{Backup::Manager::FILE_NAME_SUFFIX}\"\n    end",
    "comment": "TODO: This is a temporary workaround for bad design in Backup::Manager",
    "label": "",
    "id": "2808"
  },
  {
    "raw_code": "def backup_id\n      # Eventually the backup ID should only be fetched from\n      # backup_information, but we must have a fallback so that older backups\n      # can still be used.\n      if backup_information[:backup_id].present?\n        backup_information[:backup_id]\n      elsif options.backup_id.present?\n        File.basename(options.backup_id)\n      else\n        \"#{backup_information[:backup_created_at].strftime('%s_%Y_%m_%d_')}#{backup_information[:gitlab_version]}\"\n      end",
    "comment": "TODO: This is a temporary workaround for bad design in Backup::Manager",
    "label": "",
    "id": "2809"
  },
  {
    "raw_code": "def backup_path\n      Pathname(Gitlab.config.backup.path)\n    end",
    "comment": "TODO: This is a temporary workaround for bad design in Backup::Manager",
    "label": "",
    "id": "2810"
  },
  {
    "raw_code": "def create\n      # Deprecation: Using backup_id (ENV['BACKUP']) to specify previous backup was deprecated in 15.0\n      previous_backup = options.previous_backup || options.backup_id\n\n      run_unpack(previous_backup) if options.incremental?\n\n      create_all_tasks_result = run_all_create_tasks\n\n      logger.warn \"Warning: Your gitlab.rb and gitlab-secrets.json files contain sensitive data \\n\" \\\n           \"and are not included in this backup. You will need these files to restore a backup.\\n\" \\\n           \"Please back them up manually.\"\n      logger.info \"Backup #{backup_id} is done.\"\n      create_all_tasks_result\n    end",
    "comment": "@return [Boolean] whether all tasks succeeded",
    "label": "",
    "id": "2811"
  },
  {
    "raw_code": "def run_create_task(task)\n      build_backup_information\n\n      unless task.enabled?\n        logger.info \"Dumping #{task.human_name} ... \" + \"[DISABLED]\"\n        return true\n      end",
    "comment": "@param [Gitlab::Backup::Tasks::Task] task @return [Boolean] whether the task succeeded",
    "label": "",
    "id": "2812"
  },
  {
    "raw_code": "def verify!\n      run_unpack(options.backup_id)\n      read_backup_information\n\n      preconditions = Backup::Restore::Preconditions.new(\n        backup_information: backup_information,\n        logger: logger\n      )\n\n      preconditions.validate_backup_version!\n    ensure\n      cleanup\n    end",
    "comment": "Verify whether a backup is compatible with current GitLab's version",
    "label": "",
    "id": "2813"
  },
  {
    "raw_code": "def run_restore_task(task)\n      read_backup_information\n\n      restore_process = Backup::Restore::Process.new(\n        backup_id: backup_id,\n        backup_task: task,\n        backup_path: backup_path,\n        logger: logger\n      )\n\n      restore_process.execute!\n    end",
    "comment": "@param [Gitlab::Backup::Tasks::Task] task",
    "label": "",
    "id": "2814"
  },
  {
    "raw_code": "def find_task(task_id)\n      backup_tasks[task_id].tap do |task|\n        raise ArgumentError, \"Cannot find task with name: #{task_id}\" unless task\n      end",
    "comment": "Finds a task by id  @param [String] task_id @return [Backup::Tasks::Task]",
    "label": "",
    "id": "2815"
  },
  {
    "raw_code": "def backup_tasks\n      @backup_tasks ||= {\n        Backup::Tasks::Database.id => Backup::Tasks::Database.new(progress: progress, options: options),\n        Backup::Tasks::Repositories.id => Backup::Tasks::Repositories.new(progress: progress, options: options,\n          server_side_callable: -> { backup_information[:repositories_server_side] }),\n        Backup::Tasks::Uploads.id => Backup::Tasks::Uploads.new(progress: progress, options: options),\n        Backup::Tasks::Builds.id => Backup::Tasks::Builds.new(progress: progress, options: options),\n        Backup::Tasks::Artifacts.id => Backup::Tasks::Artifacts.new(progress: progress, options: options),\n        Backup::Tasks::Pages.id => Backup::Tasks::Pages.new(progress: progress, options: options),\n        Backup::Tasks::Lfs.id => Backup::Tasks::Lfs.new(progress: progress, options: options),\n        Backup::Tasks::TerraformState.id => Backup::Tasks::TerraformState.new(progress: progress, options: options),\n        Backup::Tasks::Registry.id => Backup::Tasks::Registry.new(progress: progress, options: options),\n        Backup::Tasks::Packages.id => Backup::Tasks::Packages.new(progress: progress, options: options),\n        Backup::Tasks::CiSecureFiles.id => Backup::Tasks::CiSecureFiles.new(progress: progress, options: options),\n        Backup::Tasks::ExternalDiffs.id => Backup::Tasks::ExternalDiffs.new(progress: progress, options: options)\n      }.freeze\n    end",
    "comment": "@return [Hash<String, Backup::Tasks::Task>]",
    "label": "",
    "id": "2816"
  },
  {
    "raw_code": "def initialize(\n      backup_id: nil, previous_backup: nil, incremental: false, force: false, strategy: Strategy::STREAM,\n      skippable_tasks: SkippableTasks.new, skippable_operations: SkippableOperations.new,\n      max_parallelism: nil, max_storage_parallelism: nil,\n      repository_storages: [], repository_paths: [], skip_repository_paths: [],\n      repositories_server_side_backup: false, remote_directory: nil,\n      compression_options: CompressionOptions.new, gzip_rsyncable: false, container_registry_bucket: nil,\n      service_account_file: nil)\n      @backup_id = backup_id\n      @previous_backup = previous_backup\n      @incremental = incremental\n      @force = force\n      @strategy = strategy\n      @skippable_tasks = skippable_tasks\n      @skippable_operations = skippable_operations\n      @max_parallelism = max_parallelism\n      @max_storage_parallelism = max_storage_parallelism\n      @remote_directory = remote_directory\n      @repositories_server_side_backup = repositories_server_side_backup\n      @repositories_storages = repository_storages\n      @repositories_paths = repository_paths\n      @skip_repositories_paths = skip_repository_paths\n      @compression_options = compression_options\n      @gzip_rsyncable = gzip_rsyncable\n      @container_registry_bucket = container_registry_bucket\n      @service_account_file = service_account_file\n    end",
    "comment": "rubocop:disable Metrics/ParameterLists -- This is a data object with all possible CMD options",
    "label": "",
    "id": "2817"
  },
  {
    "raw_code": "def extract_from_env!\n      # We've used lowercase `force` as the key while ENV normally is defined using UPPERCASE letters\n      # This provides a fallback when the user defines using expected standards, while not breaking compatibility\n      force_value = ENV.key?('FORCE') ? ENV['FORCE'] : ENV['force']\n\n      self.backup_id = ENV['BACKUP']\n      self.previous_backup = ENV['PREVIOUS_BACKUP']\n      self.incremental = Gitlab::Utils.to_boolean(ENV['INCREMENTAL'], default: incremental)\n      self.force = Gitlab::Utils.to_boolean(force_value, default: force)\n      self.strategy = Strategy::COPY if ENV['STRATEGY'] == 'copy'\n      self.max_parallelism = ENV['GITLAB_BACKUP_MAX_CONCURRENCY']&.to_i\n      self.max_storage_parallelism = ENV['GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY']&.to_i\n      self.remote_directory = ENV['DIRECTORY']\n      self.repositories_server_side_backup = Gitlab::Utils.to_boolean(ENV['REPOSITORIES_SERVER_SIDE'],\n        default: repositories_server_side_backup)\n      self.repositories_storages = ENV.fetch('REPOSITORIES_STORAGES', '').split(',').uniq\n      self.repositories_paths = ENV.fetch('REPOSITORIES_PATHS', '').split(',').uniq\n      self.skip_repositories_paths = ENV.fetch('SKIP_REPOSITORIES_PATHS', '').split(',').uniq\n      compression_options.compression_cmd = ENV['COMPRESS_CMD']\n      compression_options.decompression_cmd = ENV['DECOMPRESS_CMD']\n      self.gzip_rsyncable = Gitlab::Utils.to_boolean(ENV['GZIP_RSYNCABLE'], default: gzip_rsyncable)\n\n      extract_skippables!(ENV['SKIP']) if ENV['SKIP'].present?\n    end",
    "comment": "rubocop:enable Metrics/ParameterLists rubocop:disable Metrics/AbcSize -- TODO: Complexity will be solved in the Unified Backup implementation (https://gitlab.com/groups/gitlab-org/-/epics/11635) Extract supported options from defined ENV variables",
    "label": "",
    "id": "2818"
  },
  {
    "raw_code": "def update_from_backup_information!(backup_information)\n      self.repositories_storages += backup_information[:repositories_storages]&.split(',') || []\n      self.repositories_storages.uniq!\n      self.repositories_storages.compact!\n\n      self.repositories_paths += backup_information[:repositories_paths]&.split(',') || []\n      self.repositories_paths.uniq!\n      self.repositories_paths.compact!\n\n      self.skip_repositories_paths += backup_information[:skip_repositories_paths]&.split(',') || []\n      self.skip_repositories_paths.uniq!\n      self.skip_repositories_paths.compact!\n\n      extract_skippables!(backup_information[:skipped]) if backup_information[:skipped]\n    end",
    "comment": "rubocop:enable Metrics/AbcSize",
    "label": "",
    "id": "2819"
  },
  {
    "raw_code": "def serialize_skippables\n      list = []\n      list << 'tar' if skippable_operations.archive\n      list << 'remote' if skippable_operations.remote_storage\n      list << 'db' if skippable_tasks.db\n      list << 'uploads' if skippable_tasks.uploads\n      list << 'builds' if skippable_tasks.builds\n      list << 'artifacts' if skippable_tasks.artifacts\n      list << 'lfs' if skippable_tasks.lfs\n      list << 'terraform_state' if skippable_tasks.terraform_state\n      list << 'registry' if skippable_tasks.registry\n      list << 'pages' if skippable_tasks.pages\n      list << 'repositories' if skippable_tasks.repositories\n      list << 'packages' if skippable_tasks.packages\n      list << 'ci_secure_files' if skippable_tasks.ci_secure_files\n      list << 'external_diffs' if skippable_tasks.external_diffs\n      list.join(',')\n    end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity, Metrics/PerceivedComplexity -- TODO: Complexity will be solved in the Unified Backup implementation (https://gitlab.com/groups/gitlab-org/-/epics/11635) Return a String with a list of skippables, separated by commas  @return [String] a list of skippables",
    "label": "",
    "id": "2820"
  },
  {
    "raw_code": "def extract_skippables!(field)\n      list = field.split(',').uniq\n\n      extract_skippable_operations!(list)\n      extract_skippable_tasks(list)\n    end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity, Metrics/PerceivedComplexity Extract skippables from provided data field Current callers will provide either ENV['SKIP'] or backup_information[:skipped] content  The first time the method is executed it will setup `true` or `false` to each field subsequent executions will preserve `true` values and evaluate again only when previously set to `false`  @param [String] field contains a list separated by comma without surrounding spaces",
    "label": "",
    "id": "2821"
  },
  {
    "raw_code": "def initialize(connection_name)\n      @database_configuration = Backup::DatabaseConfiguration.new(connection_name)\n      @backup_model = backup_model\n      @snapshot_id = nil\n\n      configure_backup_model\n    end",
    "comment": "Initializes a database connection  @param [String] connection_name the key from `database.yml` for multi-database connection configuration",
    "label": "",
    "id": "2822"
  },
  {
    "raw_code": "def export_snapshot!\n      disable_timeouts!\n\n      connection.begin_transaction(isolation: :repeatable_read)\n      @snapshot_id = connection.select_value(\"SELECT pg_export_snapshot()\")\n    end",
    "comment": "Start a new transaction and run pg_export_snapshot() Returns the snapshot identifier  @return [String] snapshot identifier",
    "label": "",
    "id": "2823"
  },
  {
    "raw_code": "def release_snapshot!\n      return unless snapshot_id\n\n      connection.rollback_transaction\n      @snapshot_id = nil\n    end",
    "comment": "Rollback the transaction to release the effects of pg_export_snapshot()",
    "label": "",
    "id": "2824"
  },
  {
    "raw_code": "def backup_model\n      klass_name = connection_name.camelize\n\n      return \"#{self.class.name}::#{klass_name}\".constantize if self.class.const_defined?(klass_name.to_sym, false)\n\n      self.class.const_set(klass_name, Class.new(ApplicationRecord))\n    end",
    "comment": "Creates a disposable model to be used to host the Backup connection only",
    "label": "",
    "id": "2825"
  },
  {
    "raw_code": "def initialize(connection_name)\n      @connection_name = connection_name\n      @source_model = Gitlab::Database.database_base_models_with_gitlab_shared[connection_name] ||\n        Gitlab::Database.database_base_models_with_gitlab_shared['main']\n      @activerecord_database_config = ActiveRecord::Base.configurations.find_db_config(connection_name) ||\n        ActiveRecord::Base.configurations.find_db_config('main')\n    end",
    "comment": "Initializes configuration  @param [String] connection_name the key from `database.yml` for multi-database connection configuration",
    "label": "",
    "id": "2826"
  },
  {
    "raw_code": "def activerecord_configuration\n      ActiveRecord::DatabaseConfigurations::HashConfig.new(\n        @activerecord_database_config.env_name,\n        connection_name,\n        activerecord_variables\n      )\n    end",
    "comment": "Return the HashConfig for the database  @return [ActiveRecord::DatabaseConfigurations::HashConfig]",
    "label": "",
    "id": "2827"
  },
  {
    "raw_code": "def pg_env_variables\n      process_config_overrides! unless @pg_env_variables\n\n      @pg_env_variables\n    end",
    "comment": "Return postgres ENV variable values for current database with overrided values  @return[Hash<String,String>] hash of postgres ENV variables",
    "label": "",
    "id": "2828"
  },
  {
    "raw_code": "def activerecord_variables\n      process_config_overrides! unless @activerecord_variables\n\n      @activerecord_variables\n    end",
    "comment": "Return activerecord configuration values for current database with overrided values  @return[Hash<String,String>] activerecord database.yml configuration compatible values",
    "label": "",
    "id": "2829"
  },
  {
    "raw_code": "def original_activerecord_config\n      @activerecord_database_config.configuration_hash.dup\n    end",
    "comment": "Return the database configuration from rails config/database.yml file in the format expected by ActiveRecord::DatabaseConfigurations::HashConfig  @return [Hash] configuration hash",
    "label": "",
    "id": "2830"
  },
  {
    "raw_code": "def load!\n      return @backup_information unless @backup_information.nil?\n\n      manifest_data = load_from_file\n\n      @backup_information = BackupInformation.new(**manifest_data)\n    end",
    "comment": "Load #BackupInformation from a YAML manifest file on disk",
    "label": "",
    "id": "2831"
  },
  {
    "raw_code": "def save!\n      Dir.chdir(File.dirname(manifest_filepath)) do\n        File.open(manifest_filepath, 'w+') do |file|\n          file << backup_information.to_h.to_yaml.gsub(/^---\\n/, '')\n        end",
    "comment": "Save content from #BackupInformation into a manifest YAML file on disk",
    "label": "",
    "id": "2832"
  },
  {
    "raw_code": "def update(**data)\n      @backup_information ||= BackupInformation.new\n\n      data.each_pair do |key, value|\n        backup_information[key] = value\n      end",
    "comment": "Update backup information with provided data  @param [Hash] data arguments matching #BackupInformation keyword arguments",
    "label": "",
    "id": "2833"
  },
  {
    "raw_code": "def dump(output_file_path, pg_dump)\n        compress_rd, compress_wr = IO.pipe\n\n        compress_pid = spawn(compress_cmd, in: compress_rd, out: [output_file_path, 'w', FILE_PERMISSION])\n        compress_rd.close\n\n        dump_pid = pg_dump.spawn(output: compress_wr)\n        compress_wr.close\n\n        [compress_pid, dump_pid].all? do |pid|\n          Process.waitpid(pid)\n          $?.success?\n        end",
    "comment": "Triggers PgDump and outputs to the provided file path  @param [String] output_file_path full path to the output destination @param [Gitlab::Backup::Cli::Utils::PgDump] pg_dump @return [Boolean] whether pg_dump finished with success",
    "label": "",
    "id": "2834"
  },
  {
    "raw_code": "def dump(path, backup_id)\n        raise NotImplementedError\n      end",
    "comment": "dump task backup to `path`  @param [String] path fully qualified backup task destination @param [String] backup_id unique identifier for the backup",
    "label": "",
    "id": "2835"
  },
  {
    "raw_code": "def restore(path, backup_id)\n        raise NotImplementedError\n      end",
    "comment": "restore task backup from `path`",
    "label": "",
    "id": "2836"
  },
  {
    "raw_code": "def initialize(progress, storage_path, options:, excludes: [])\n        super(progress, options: options)\n\n        @storage_path = storage_path\n        @excludes = excludes\n      end",
    "comment": "@param [IO] progress @param [String] storage_path @param [::Backup::Options] options @param [Array] excludes",
    "label": "",
    "id": "2837"
  },
  {
    "raw_code": "def with_transient_pg_env(extended_env)\n        ENV.merge!(extended_env)\n        result = yield\n        ENV.reject! { |k, _| extended_env.key?(k) }\n\n        result\n      end",
    "comment": "@deprecated This will be removed when restore operation is refactored to use extended_env directly",
    "label": "",
    "id": "2838"
  },
  {
    "raw_code": "def initialize(progress, strategy:, options:, storages: [], paths: [], skip_paths: [])\n        super(progress, options: options)\n\n        @strategy = strategy\n        @storages = storages\n        @paths = paths\n        @skip_paths = skip_paths\n        @logger = Gitlab::BackupLogger.new(progress)\n      end",
    "comment": "@param [IO] progress IO interface to output progress @param [Object] :strategy Fetches backups from gitaly @param [Array<String>] :storages Filter by specified storage names. Empty means all storages. @param [Array<String>] :paths Filter by specified project paths. Empty means all projects, groups, and snippets. @param [Array<String>] :skip_paths Skip specified project paths. Empty means all projects, groups, and snippets.",
    "label": "",
    "id": "2839"
  },
  {
    "raw_code": "def initialize(backup_id:, backup_path:, manifest_filepath:, options:, logger:)\n        @backup_id = backup_id\n        @backup_path = backup_path\n        @manifest_filepath = manifest_filepath\n        @options = options\n        @logger = logger\n      end",
    "comment": "Unpacks a tar file from a previous or current backup  @param [String] backup_id Current or previous backup ID @param [Pathname] backup_path Backup path defined by Gitlab settings @param [Pathname] manifest_filepath Filepath of backup_information.yml @param [Backup::Options] options Backup options @param [Gitlab::BackupLogger] logger interfaces",
    "label": "",
    "id": "2840"
  },
  {
    "raw_code": "def initialize(backup_id:, backup_task:, backup_path:, logger:)\n        @backup_id = backup_id\n        @backup_task = backup_task\n        @backup_path = backup_path\n        @logger = logger\n      end",
    "comment": "Restore process class, dedicated to perform restore tasks  @param [String] backup_id Current or previous backup ID @param [Backup::Tasks::Task] backup_task Task to restore @param [Pathname] backup_path Pathname of the backup @param [Gitlab::BackupLogger] logger interface",
    "label": "",
    "id": "2841"
  },
  {
    "raw_code": "def initialize(backup_information:, logger:)\n        @backup_information = backup_information\n        @logger = logger\n      end",
    "comment": "Check preconditions before restoring a backup task  @param [Struct] backup_information Backup information @param [Gitlab::BackupLogger] logger interface",
    "label": "",
    "id": "2842"
  },
  {
    "raw_code": "def ensure_supported_backup_version!\n        gitlab_version_mismatch! unless gitlab_backup_same_version?\n      end",
    "comment": "Ensure Backup version is compatible with current GitLab installation  Currently we only allow restoring a Backup in the same GitLab version it was created  We do this because restoring from an older version requires migration steps to be executed and the upgrade path is only checked during GitLab's upgrade process  Trying to restore a newer backup on an older GitLab installation will always fail",
    "label": "",
    "id": "2843"
  },
  {
    "raw_code": "def validate_backup_version!\n        gitlab_backup_same_version? ? gitlab_version_matches! : gitlab_version_mismatch!\n      end",
    "comment": "Validate and report whether Backup version is compatible with current GitLab installation",
    "label": "",
    "id": "2844"
  },
  {
    "raw_code": "def gitlab_backup_same_version?\n        backup_information[:gitlab_version] == Gitlab::VERSION\n      end",
    "comment": "Check whether backup version matches gitlab installation  @return [Boolean] whether they are the same version",
    "label": "",
    "id": "2845"
  },
  {
    "raw_code": "def gitlab_version_mismatch!\n        logger.error(<<~HEREDOC)\n          GitLab version mismatch:\n            Your current GitLab version (#{Gitlab::VERSION}) differs from the GitLab version in the backup!\n            Please switch to the following version and try again:\n            version: #{backup_information[:gitlab_version]}\n        HEREDOC\n        logger.error \"Hint: git checkout v#{backup_information[:gitlab_version]}\"\n        exit 1\n      end",
    "comment": "Display a message for when version mismatches and exit 1",
    "label": "",
    "id": "2846"
  },
  {
    "raw_code": "def gitlab_version_matches!\n        logger.info(<<~HEREDOC)\n          GitLab version matches:\n            Your current GitLab version (#{Gitlab::VERSION}) matches the GitLab version in the backup.\n        HEREDOC\n        exit 0\n      end",
    "comment": "Display a message for when version matches and exit 0",
    "label": "",
    "id": "2847"
  },
  {
    "raw_code": "def self.id\n        raise NotImplementedError\n      end",
    "comment": "Identifier used as parameter in the CLI to skip from executing",
    "label": "",
    "id": "2848"
  },
  {
    "raw_code": "def backup!(backup_path, backup_id)\n        backup_output = backup_path.join(destination_path)\n\n        target.dump(backup_output, backup_id)\n      end",
    "comment": "Initiate a backup  @param [Pathname] backup_path a path where to store the backups @param [String] backup_id",
    "label": "",
    "id": "2849"
  },
  {
    "raw_code": "def id = self.class.id\n\n      # Name of the task used for logging.\n      def human_name\n        raise NotImplementedError\n      end\n\n      # Where to put the backup content\n      # It can be either an archive file or a directory containing multiple data\n      def destination_path\n        raise NotImplementedError\n      end\n\n      # Path to remove after a successful backup, uses #destination_path when not specified\n      def cleanup_path = destination_path\n\n      # `true` if the destination might not exist on a successful backup\n      def destination_optional = false\n\n      # `true` if the task can be used\n      def enabled = true\n\n      def enabled? = enabled\n\n      # a string returned here will be displayed to the user before calling #restore\n      def pre_restore_warning = nil\n\n      # a string returned here will be displayed to the user after calling #restore\n      def post_restore_warning = nil\n\n      private\n\n      # The target factory method\n      def target\n        raise NotImplementedError\n      end\n    end\n  end\nend",
    "comment": "Key string that identifies the task",
    "label": "",
    "id": "2850"
  },
  {
    "raw_code": "def process_media_single(element, ast_node)\n        new_element = Element.new(:p)\n        element.children << new_element\n\n        process_content(new_element, ast_node, %w[media])\n      end",
    "comment": "wraps a single media element. Currently ignore attrs.layout and attrs.width",
    "label": "",
    "id": "2851"
  },
  {
    "raw_code": "def process_media_group(element, ast_node)\n        ul_element = Element.new(:ul)\n        element.children << ul_element\n\n        ast_node['content'].each do |node|\n          next unless node['type'] == 'media'\n\n          li_element = Element.new(:li)\n          ul_element.children << li_element\n\n          process_media(li_element, node)\n        end",
    "comment": "wraps a group media element. Currently ignore attrs.layout and attrs.width",
    "label": "",
    "id": "2852"
  },
  {
    "raw_code": "def process_panel(element, ast_node)\n        panel_type = ast_node.dig('attrs', 'panelType')\n        return unless %w[info note warning success error].include?(panel_type)\n\n        panel_header_text = \"#{PANEL_EMOJIS[panel_type.to_sym]} \"\n        panel_header_element = Element.new(:text, panel_header_text)\n\n        new_element = Element.new(:blockquote)\n        new_element.children << panel_header_element\n        element.children << new_element\n\n        process_content(new_element, ast_node, PANEL_NODES)\n      end",
    "comment": "since we don't have something similar, then put <hr> around it and add a bolded status text (eg: \"Error:\") to the front of it.",
    "label": "",
    "id": "2853"
  },
  {
    "raw_code": "def process_table_cell(element, ast_node)\n        new_element = Element.new(:td)\n        element.children << new_element\n\n        process_content(new_element, ast_node, TABLE_CELL_NODES)\n      end",
    "comment": "we ignore the attributes, attrs.background, attrs.colspan, attrs.colwidth, and attrs.rowspan",
    "label": "",
    "id": "2854"
  },
  {
    "raw_code": "def process_table_header(element, ast_node)\n        new_element = Element.new(:th)\n        element.children << new_element\n\n        process_content(new_element, ast_node, TABLE_CELL_NODES)\n      end",
    "comment": "we ignore the attributes, attrs.background, attrs.colspan, attrs.colwidth, and attrs.rowspan",
    "label": "",
    "id": "2855"
  },
  {
    "raw_code": "def apply_marks(element, ast_node, allowed_types)\n        return element unless ast_node['marks']\n\n        new_element = element\n\n        ast_node['marks'].each do |mark|\n          next unless allowed_types.include?(mark['type'])\n\n          case mark['type']\n          when 'code'\n            new_element = Element.new(:codespan, ast_node['text'])\n          when 'em'\n            new_element = wrap_element(new_element, :em)\n          when 'link'\n            attrs = { 'href' => mark.dig('attrs', 'href') }\n            attrs['title'] = mark.dig('attrs', 'title')\n            new_element = wrap_element(new_element, :a, nil, attrs)\n          when 'strike'\n            new_element = wrap_element(new_element, :html_element, 'del', {}, category: :span)\n          when 'strong'\n            new_element = wrap_element(new_element, :strong)\n          when 'subsup'\n            type = mark.dig('attrs', 'type')\n\n            case type\n            when 'sub'\n              new_element = wrap_element(new_element, :html_element, 'sub', {}, category: :span)\n            when 'sup'\n              new_element = wrap_element(new_element, :html_element, 'sup', {}, category: :span)\n            else\n              next\n            end",
    "comment": "ADF marks are an attribute on the node.  For kramdown, we have to wrap the node with an element for the mark.",
    "label": "",
    "id": "2856"
  },
  {
    "raw_code": "def support_bot_id\n        new.support_bot.id\n      end",
    "comment": "Checks against this bot are now included in every issue and work item detail and list page rendering and in GraphQL queries (especially for determining the web_url of an issue/work item). Because the bot never changes once created, we can memoize it for the lifetime of the application process. It also doesn't matter that different nodes may have different object instances of the bot. We only memoize the id because this is the information we check against.",
    "label": "",
    "id": "2857"
  },
  {
    "raw_code": "def initialize(organization: nil)\n      case organization\n      when ::Organizations::Organization\n        @organization = organization\n        @organization_id = organization.id\n      else\n        @organization_id = organization\n      end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord -- Need to instantiate a record here",
    "label": "",
    "id": "2858"
  },
  {
    "raw_code": "def ghost\n      email = 'ghost%s@example.com'\n      unique_internal(User.where(user_type: :ghost), 'ghost', email) do |u|\n        u.bio = _('This is a \"Ghost User\", created to hold all issues authored by users that have ' \\\n                  'since been deleted. This user cannot be removed.')\n        u.name = 'Ghost'\n      end",
    "comment": "Return (create if necessary) the ghost user. The ghost user owns records previously belonging to deleted users.",
    "label": "",
    "id": "2859"
  },
  {
    "raw_code": "def unique_internal(scope, username, email_pattern, &block)\n      if @organization_id && organization_users_internal_enabled? # rubocop:disable Style/IfUnlessModifier -- exceeds line length\n        scope = scope.where(organization_id: @organization_id)\n      end",
    "comment": "NOTE: This method is patched in spec/spec_helper.rb to allow use of exclusive lease in RSpec's :before_all scope to keep the specs DRY.",
    "label": "",
    "id": "2860"
  },
  {
    "raw_code": "def all_active_routes\n      @all_active_routes ||=\n        ([active_routes] + renderable_items.map(&:active_routes)).flatten.each_with_object({}) do |pairs, hash|\n          pairs.each do |k, v|\n            hash[k] ||= []\n            hash[k] += Array(v)\n            hash[k].uniq!\n          end",
    "comment": "This method normalizes the information retrieved from the submenus and this menu Value from menus is something like: [{ path: 'foo', path: 'bar', controller: :foo }] This method filters the information and returns: { path: ['foo', 'bar'], controller: :foo }",
    "label": "",
    "id": "2861"
  },
  {
    "raw_code": "def has_items?\n      @items.any?\n    end",
    "comment": "Returns whether the menu has any menu item, no matter whether it is renderable or not",
    "label": "",
    "id": "2862"
  },
  {
    "raw_code": "def renderable_items\n      @renderable_items ||= @items.select(&:render?)\n    end",
    "comment": "Returns all renderable menu items",
    "label": "",
    "id": "2863"
  },
  {
    "raw_code": "def separated?\n      false\n    end",
    "comment": "Defines whether menu is separated from others with a top separator",
    "label": "",
    "id": "2864"
  },
  {
    "raw_code": "def serialize_for_super_sidebar\n      items = serialize_items_for_super_sidebar\n      is_active = @context.route_is_active.call(active_routes) || items.any? { |item| item[:is_active] }\n\n      {\n        id: self.class.name.demodulize.underscore,\n        title: title,\n        icon: sprite_icon,\n        avatar: avatar,\n        avatar_shape: avatar_shape,\n        entity_id: entity_id,\n        link: link,\n        is_active: is_active,\n        pill_count: has_pill? ? pill_count : nil,\n        pill_count_field: has_pill? ? pill_count_field : nil,\n        items: items,\n        separated: separated?\n      }.compact\n    end",
    "comment": "Returns a tree-like representation of itself and all renderable menu entries, with additional information on whether the item(s) have an active route",
    "label": "",
    "id": "2865"
  },
  {
    "raw_code": "def serialize_items_for_super_sidebar\n      # All renderable menu entries\n      renderable_items.map do |entry|\n        entry.serialize_for_super_sidebar.tap do |item|\n          active_routes = item.delete(:active_routes)\n          item[:is_active] = active_routes ? @context.route_is_active.call(active_routes) : false\n        end",
    "comment": "Returns an array of renderable menu entries, with additional information on whether the item has an active route",
    "label": "",
    "id": "2866"
  },
  {
    "raw_code": "def has_renderable_items?\n      renderable_items.any?\n    end",
    "comment": "Returns whether the menu has any renderable menu item",
    "label": "",
    "id": "2867"
  },
  {
    "raw_code": "def serialize_as_menu_item_args\n      {\n        title: title,\n        link: link,\n        active_routes: active_routes,\n        container_html_options: container_html_options\n      }\n    end",
    "comment": "Sometimes we want to convert a top-level Menu (e.g. Wiki/Snippets) to a MenuItem. This serializer is used in order to enable that conversion",
    "label": "",
    "id": "2868"
  },
  {
    "raw_code": "def initialize(\n      title:, link:, active_routes:, item_id: nil, container_html_options: {}, sprite_icon: nil,\n      sprite_icon_html_options: {}, has_pill: false, pill_count_dynamic: false, pill_count: nil,\n      pill_count_field: nil, super_sidebar_parent: nil, avatar: nil, entity_id: nil\n    )\n      @title = title\n      @link = link\n      @active_routes = active_routes\n      @item_id = item_id\n      @container_html_options = { aria: { label: title } }.merge(container_html_options)\n      @sprite_icon = sprite_icon\n      @sprite_icon_html_options = sprite_icon_html_options\n      @avatar = avatar\n      @entity_id = entity_id\n      @has_pill = has_pill\n      @pill_count = pill_count\n      @pill_count_field = pill_count_field\n      @pill_count_dynamic = pill_count_dynamic\n      @super_sidebar_parent = super_sidebar_parent\n    end",
    "comment": "rubocop: disable Metrics/ParameterLists",
    "label": "",
    "id": "2869"
  },
  {
    "raw_code": "def render?\n      return true if @render.nil?\n\n      @render\n    end",
    "comment": "rubocop: enable Metrics/ParameterLists",
    "label": "",
    "id": "2870"
  },
  {
    "raw_code": "def super_sidebar_menu_items\n      @super_sidebar_menu_items ||= renderable_menus\n        .flat_map(&:serialize_for_super_sidebar)\n    end",
    "comment": "Serializes every renderable menu and returns a flattened result",
    "label": "",
    "id": "2871"
  },
  {
    "raw_code": "def pick_from_old_menus(old_menus)\n        old_menus.select! do |menu|\n          next true unless menu.pick_into_super_sidebar?\n\n          add_menu(menu)\n          false\n        end",
    "comment": "Picks menus from a list and adds them to the current menu list if they should be picked into the super sidebar",
    "label": "",
    "id": "2872"
  },
  {
    "raw_code": "def add_menu_item_to_super_sidebar_parent(menus, menu_item)\n        parent = menu_item.super_sidebar_parent || ::Sidebars::UncategorizedMenu\n        return if parent == ::Sidebars::NilMenuItem\n\n        idx = index_of(menus, parent) || index_of(menus, ::Sidebars::UncategorizedMenu)\n        return unless idx\n\n        menus[idx].replace_placeholder(menu_item)\n      end",
    "comment": "Finds a menu_items super sidebar parent and adds the item to that menu Handles: - parent == nil, or parent not being part of the panel: we assume that the menu item hasn't been categorized yet - parent == ::Sidebars::NilMenuItem, the item explicitly is supposed to be removed",
    "label": "",
    "id": "2873"
  },
  {
    "raw_code": "def container_html_options\n        {\n          aria: { label: title }\n        }.merge(extra_container_html_options)\n      end",
    "comment": "The attributes returned from this method will be applied to helper methods like `link_to` or the div containing the container.",
    "label": "",
    "id": "2874"
  },
  {
    "raw_code": "def extra_container_html_options\n        {}\n      end",
    "comment": "Classes will override mostly this method and not `container_html_options`.",
    "label": "",
    "id": "2875"
  },
  {
    "raw_code": "def render?\n        true\n      end",
    "comment": "This method will control whether the menu or menu_item should be rendered. It will be overriden by specific classes.",
    "label": "",
    "id": "2876"
  },
  {
    "raw_code": "def active_routes\n        {}\n      end",
    "comment": "This method will indicate for which paths or controllers, the menu or menu item should be set as active.  The returned values are passed to the `nav_link` helper method, so the params can be either `path`, `page`, `controller`. Param 'action' is not supported.",
    "label": "",
    "id": "2877"
  },
  {
    "raw_code": "def index_of(list, element)\n        raise NotImplementedError\n      end",
    "comment": "Classes including this method will have to define the way to identify elements through this method",
    "label": "",
    "id": "2878"
  },
  {
    "raw_code": "def pill_count; end\n\n      # The GraphQL field name from `SidebarType` that will be used\n      # as the pill count for this menu item.\n      # This is used when the count is expensive and we want to fetch it separately\n      # from GraphQL.\n      def pill_count_field; end\n\n      def pill_html_options\n        {}\n      end\n\n      def format_cached_count(threshold, count)\n        if count > threshold\n          number_to_human(\n            count,\n            units: { thousand: 'k', million: 'm' }, precision: 1, significant: false, format: '%n%u'\n          )\n        else\n          number_with_delimiter(count)\n        end\n      end\n    end",
    "comment": "In this method we will need to provide the query to retrieve the elements count",
    "label": "",
    "id": "2879"
  },
  {
    "raw_code": "def link_html_options\n        container_html_options.tap do |html_options|\n          html_options[:class] = [*html_options[:class], 'gl-link'].join(' ')\n        end",
    "comment": "add on specific items as the pertain to `link_to` objects specifically",
    "label": "",
    "id": "2880"
  },
  {
    "raw_code": "def gitlab_shell_path\n      Gitlab.config.gitlab_shell.path\n    end",
    "comment": "Helper methods ",
    "label": "",
    "id": "2881"
  },
  {
    "raw_code": "def initialize(component)\n      raise ArgumentError unless component.is_a? String\n\n      @component = component\n      @checks = Set.new\n    end",
    "comment": "@param [String] component name of the component relative to the checks being executed",
    "label": "",
    "id": "2882"
  },
  {
    "raw_code": "def <<(check)\n      raise ArgumentError unless check.is_a?(Class) && check < BaseCheck\n\n      @checks << check\n    end",
    "comment": "Add a check to be executed  @param [BaseCheck] check class",
    "label": "",
    "id": "2883"
  },
  {
    "raw_code": "def execute\n      start_checking(component)\n\n      @checks.each do |check|\n        run_check(check)\n      end",
    "comment": "Executes defined checks in the specified order and outputs confirmation or error information",
    "label": "",
    "id": "2884"
  },
  {
    "raw_code": "def run_check(check_klass)\n      print_display_name(check_klass)\n\n      check = check_klass.new\n\n      # When implements skip method, we run it first, and if true, skip the check\n      if check.can_skip? && check.skip?\n        $stdout.puts Rainbow(check.skip_reason || check_klass.skip_reason).magenta\n        return\n      end",
    "comment": "Executes a single check  @param [SystemCheck::BaseCheck] check_klass",
    "label": "",
    "id": "2885"
  },
  {
    "raw_code": "def start_checking(component)\n      $stdout.puts \"Checking #{Rainbow(component).yellow} ...\"\n      $stdout.puts ''\n    end",
    "comment": "Prints header content for the series of checks to be executed for this component  @param [String] component name of the component relative to the checks being executed",
    "label": "",
    "id": "2886"
  },
  {
    "raw_code": "def finished_checking(component)\n      $stdout.puts ''\n      $stdout.puts \"Checking #{Rainbow(component).yellow} ... #{Rainbow('Finished').green}\"\n      $stdout.puts ''\n    end",
    "comment": "Prints footer content for the series of checks executed for this component  @param [String] component name of the component relative to the checks being executed",
    "label": "",
    "id": "2887"
  },
  {
    "raw_code": "def fix_and_rerun\n      $stdout.puts Rainbow('  Please fix the error above and rerun the checks.').red\n    end",
    "comment": "Display a message telling to fix and rerun the checks",
    "label": "",
    "id": "2888"
  },
  {
    "raw_code": "def for_more_information(*sources)\n      $stdout.puts Rainbow('  For more information see:').blue\n      sources.each do |source|\n        $stdout.puts \"  #{source}\"\n      end",
    "comment": "Display a formatted list of references (documentation or links) where to find more information  @param [Array<String>] sources one or more references (documentation or links)",
    "label": "",
    "id": "2889"
  },
  {
    "raw_code": "def finished_checking(component)\n      $stdout.puts ''\n      $stdout.puts \"Checking #{Rainbow(component).yellow} ... #{Rainbow('Finished').green}\"\n      $stdout.puts ''\n    end",
    "comment": "@deprecated This will no longer be used when all checks were executed using SystemCheck",
    "label": "",
    "id": "2890"
  },
  {
    "raw_code": "def start_checking(component)\n      $stdout.puts \"Checking #{Rainbow(component).yellow} ...\"\n      $stdout.puts ''\n    end",
    "comment": "@deprecated This will no longer be used when all checks were executed using SystemCheck",
    "label": "",
    "id": "2891"
  },
  {
    "raw_code": "def try_fixing_it(*steps)\n      steps = steps.shift if steps.first.is_a?(Array)\n\n      $stdout.puts Rainbow('  Try fixing it:').blue\n      steps.each do |step|\n        $stdout.puts \"  #{step}\"\n      end",
    "comment": "Display a formatted list of instructions on how to fix the issue identified by the #check?  @param [Array<String>] steps one or short sentences with help how to fix the issue",
    "label": "",
    "id": "2892"
  },
  {
    "raw_code": "def self.set_check_pass(term)\n      @check_pass = term\n    end",
    "comment": "Define a custom term for when check passed  @param [String] term used when check passed (default: 'yes')",
    "label": "",
    "id": "2893"
  },
  {
    "raw_code": "def self.set_check_fail(term)\n      @check_fail = term\n    end",
    "comment": "Define a custom term for when check failed  @param [String] term used when check failed (default: 'no')",
    "label": "",
    "id": "2894"
  },
  {
    "raw_code": "def self.set_name(name)\n      @name = name\n    end",
    "comment": "Define the name of the SystemCheck that will be displayed during execution  @param [String] name of the check",
    "label": "",
    "id": "2895"
  },
  {
    "raw_code": "def self.set_skip_reason(reason)\n      @skip_reason = reason\n    end",
    "comment": "Define the reason why we skipped the SystemCheck  This is only used if subclass implements `#skip?`  @param [String] reason to be displayed",
    "label": "",
    "id": "2896"
  },
  {
    "raw_code": "def self.check_pass\n      call_or_return(@check_pass) || 'yes'\n    end",
    "comment": "Term to be displayed when check passed  @return [String] term when check passed ('yes' if not re-defined in a subclass)",
    "label": "",
    "id": "2897"
  },
  {
    "raw_code": "def self.check_fail\n      call_or_return(@check_fail) || 'no'\n    end",
    "comment": "Term to be displayed when check failed  @return [String] term when check failed ('no' if not re-defined in a subclass)",
    "label": "",
    "id": "2898"
  },
  {
    "raw_code": "def self.display_name\n      call_or_return(@name) || self.name\n    end",
    "comment": "Name of the SystemCheck defined by the subclass  @return [String] the name",
    "label": "",
    "id": "2899"
  },
  {
    "raw_code": "def self.skip_reason\n      call_or_return(@skip_reason) || 'skipped'\n    end",
    "comment": "Skip reason defined by the subclass  @return [String] the reason",
    "label": "",
    "id": "2900"
  },
  {
    "raw_code": "def can_repair?\n      self.class.instance_methods(false).include?(:repair!)\n    end",
    "comment": "Does the check support automatically repair routine?  @return [Boolean] whether check implemented `#repair!` method or not",
    "label": "",
    "id": "2901"
  },
  {
    "raw_code": "def check?\n      raise NotImplementedError\n    end",
    "comment": "Execute the check routine  This is where you should implement the main logic that will return a boolean at the end  You should not print any output to STDOUT here, use the specific methods instead  @return [Boolean] whether check passed or failed",
    "label": "",
    "id": "2902"
  },
  {
    "raw_code": "def multi_check\n      raise NotImplementedError\n    end",
    "comment": "Execute a custom check that cover multiple unities  When using multi_check you have to provide the output yourself",
    "label": "",
    "id": "2903"
  },
  {
    "raw_code": "def show_error\n      raise NotImplementedError\n    end",
    "comment": "Prints troubleshooting instructions  This is where you should print detailed information for any error found during #check?  You may use helper methods to help format the output:  @see #try_fixing_it @see #fix_and_rerun @see #for_more_information",
    "label": "",
    "id": "2904"
  },
  {
    "raw_code": "def repair!\n      raise NotImplementedError\n    end",
    "comment": "When implemented by a subclass, will attempt to fix the issue automatically",
    "label": "",
    "id": "2905"
  },
  {
    "raw_code": "def skip?\n      raise NotImplementedError\n    end",
    "comment": "When implemented by a subclass, will evaluate whether check should be skipped or not  @return [Boolean] whether or not this check should be skipped",
    "label": "",
    "id": "2906"
  },
  {
    "raw_code": "def systemd_get_wants(unitname)\n      stdout, _stderr, status = Open3.capture3(\"systemctl\", \"--no-pager\", \"show\", unitname)\n\n      unless status\n        return []\n      end",
    "comment": "Return the Wants= of a unit, empty if the unit doesn't exist",
    "label": "",
    "id": "2907"
  },
  {
    "raw_code": "def get_url\n      if config.google?\n        connection.get_object_https_url(bucket_name, object_name, expire_at)\n      else\n        connection.get_object_url(bucket_name, object_name, expire_at)\n      end",
    "comment": "Implements https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html",
    "label": "",
    "id": "2908"
  },
  {
    "raw_code": "def delete_url\n      connection.delete_object_url(bucket_name, object_name, expire_at)\n    end",
    "comment": "Implements https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html",
    "label": "",
    "id": "2909"
  },
  {
    "raw_code": "def store_url\n      connection.put_object_url(bucket_name, object_name, expire_at, upload_options)\n    end",
    "comment": "Implements https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html",
    "label": "",
    "id": "2910"
  },
  {
    "raw_code": "def multipart_part_upload_url(part_number)\n      connection.signed_url({\n        method: 'PUT',\n        bucket_name: bucket_name,\n        object_name: object_name,\n        query: { 'uploadId' => upload_id, 'partNumber' => part_number },\n        headers: upload_options\n      }, expire_at)\n    end",
    "comment": "Implements https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html",
    "label": "",
    "id": "2911"
  },
  {
    "raw_code": "def multipart_complete_url\n      connection.signed_url({\n        method: 'POST',\n        bucket_name: bucket_name,\n        object_name: object_name,\n        query: { 'uploadId' => upload_id },\n        headers: { 'Content-Type' => 'application/xml' }\n      }, expire_at)\n    end",
    "comment": "Implements https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html",
    "label": "",
    "id": "2912"
  },
  {
    "raw_code": "def multipart_abort_url\n      connection.signed_url({\n        method: 'DELETE',\n        bucket_name: bucket_name,\n        object_name: object_name,\n        query: { 'uploadId' => upload_id }\n      }, expire_at)\n    end",
    "comment": "Implements https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadAbort.html",
    "label": "",
    "id": "2913"
  },
  {
    "raw_code": "def aws?\n      provider == AWS_PROVIDER\n    end",
    "comment": "AWS-specific options",
    "label": "",
    "id": "2914"
  },
  {
    "raw_code": "def azure_storage_domain\n      credentials[:azure_storage_domain]\n    end",
    "comment": "End AWS-specific options Begin Azure-specific options",
    "label": "",
    "id": "2915"
  },
  {
    "raw_code": "def google?\n      provider == GOOGLE_PROVIDER\n    end",
    "comment": "End Azure-specific options",
    "label": "",
    "id": "2916"
  },
  {
    "raw_code": "def aws_server_side_encryption_headers\n      {\n        'x-amz-server-side-encryption' => server_side_encryption,\n        'x-amz-server-side-encryption-aws-kms-key-id' => server_side_encryption_kms_key_id\n      }\n    end",
    "comment": "This returns a Hash of HTTP encryption headers to send along to S3.  They can also be passed in as Fog::AWS::Storage::File attributes, since there are aliases defined for them: https://github.com/fog/fog-aws/blob/ab288f29a0974d64fd8290db41080e5578be9651/lib/fog/aws/models/storage/file.rb#L24-L25",
    "label": "",
    "id": "2917"
  },
  {
    "raw_code": "def recover_invalid_record\n      create_discussion_note_on_missing_diff || record\n\n      # As we support more types, we can start to follow this pattern:\n      # case record\n      # when DiffNote\n      #   create_discussion_note_on_missing_diff\n      # when Issue\n      #  prepare_issue\n      # end || record\n    end",
    "comment": "If we notice this is being used for many models in the future we should consider refactoring, so each model has its own preparer. We can use metaprogramming to infer the preparer class.",
    "label": "",
    "id": "2918"
  },
  {
    "raw_code": "def resumes_work_when_interrupted!\n        sidekiq_options max_retries_after_interruption: MAX_RETRIES_AFTER_INTERRUPTION\n      end",
    "comment": "We can increase the number of times a worker is retried after being interrupted if the importer it executes can restart exactly from where it left off.  It is not safe to call this method if the importer loops over its data from the beginning when restarted, even if it skips data that is already imported inside the loop, as there is a possibility the importer will never reach the end of the loop.  Examples of stage workers that call this method are ones that execute services that:  - Continue paging an endpoint from where it left off: https://gitlab.com/gitlab-org/gitlab/-/blob/487521cc/lib/gitlab/github_import/parallel_scheduling.rb#L114-117 - Continue their loop from where it left off: https://gitlab.com/gitlab-org/gitlab/-/blob/024235ec/lib/gitlab/github_import/importer/pull_requests/review_requests_importer.rb#L15",
    "label": "",
    "id": "2919"
  },
  {
    "raw_code": "def limit_name\n      LIMIT_TIER_1\n    end",
    "comment": "Overridden in EE to return limit names based on licensed seats",
    "label": "",
    "id": "2920"
  },
  {
    "raw_code": "def push_references_by_ids(project, ids, model, attribute, source_user_identifier)\n        return unless allowed_to_push?(project, source_user_identifier)\n\n        ids.each do |id|\n          source_user = source_user_mapper(project).find_source_user(source_user_identifier)\n\n          next if source_user.nil?\n          next if source_user.accepted_status?\n\n          ::Import::PlaceholderReferences::PushService.new(\n            import_source: import_type,\n            import_uid: project.import_state.id,\n            source_user_id: source_user.id,\n            source_user_namespace_id: source_user.namespace_id,\n            model: model,\n            user_reference_column: attribute,\n            numeric_key: id).execute\n        end",
    "comment": "This is used for records created using legacy_bulk_insert which can return the ids of records created, but not the records themselves",
    "label": "",
    "id": "2921"
  },
  {
    "raw_code": "def push_reference_with_composite_key(project, record, attribute, composite_key, source_user_identifier)\n        return unless allowed_to_push?(project, source_user_identifier)\n\n        source_user = source_user_mapper(project).find_source_user(source_user_identifier)\n\n        # Do not create a reference if the object is already associated with a real user.\n        return if source_user_mapped_to_human?(record, attribute, source_user)\n\n        ::Import::PlaceholderReferences::PushService.new(\n          import_source: import_type,\n          import_uid: project.import_state.id,\n          source_user_id: source_user.id,\n          source_user_namespace_id: source_user.namespace_id,\n          model: record.class,\n          user_reference_column: attribute,\n          composite_key: composite_key\n        ).execute\n      end",
    "comment": "Pushes a placeholder reference using a composite key. This is used when the record requires a composite key for the reference.",
    "label": "",
    "id": "2922"
  },
  {
    "raw_code": "def disk_stats\n      disk_statistics_storage_status\n    end",
    "comment": "Simple convenience method for when obtaining both used and available statistics at once is preferred.",
    "label": "",
    "id": "2923"
  },
  {
    "raw_code": "def self.verify_backup\n        lock_backup do\n          ::Backup::Manager.new(backup_progress).verify!\n        end",
    "comment": "Verify backup file to ensure it is compatible with current GitLab's version",
    "label": "",
    "id": "2924"
  },
  {
    "raw_code": "def self.reset_pool_repositories!\n        ::Backup::Restore::PoolRepositories.reinitialize_pools! do |pool_result|\n          puts pool_result.to_h.to_json\n        end",
    "comment": "A Backup only includes regular repositories, after a restore we need to reinitialize their respective pools. This process is done by changing its original state to 'none' and scheduling its creation process again",
    "label": "",
    "id": "2925"
  },
  {
    "raw_code": "def rouge_formatter(lexer)\n        Formatters::HTMLLegacy.new(css_class: \"highlight #{lexer.tag}\")\n      end",
    "comment": "override this method for custom formatting behavior",
    "label": "",
    "id": "2926"
  },
  {
    "raw_code": "def initialize(options = {})\n        @tag = options[:tag]\n        @line_number = options[:line_number] || 1\n        @fix_attributes = options[:fix_attributes]\n        @ellipsis_indexes = options[:ellipsis_indexes] || []\n        @ellipsis_svg = options[:ellipsis_svg]\n      end",
    "comment": "Creates a new <tt>Rouge::Formatter::HTMLGitlab</tt> instance.  [+tag+]          The tag (language) of the lexer used to generate the formatted tokens [+line_number+]  The line number used to populate line IDs",
    "label": "",
    "id": "2927"
  },
  {
    "raw_code": "def scheme\n      'pkg'\n    end",
    "comment": "The URL scheme, which has a constant value of `\"pkg\"`.",
    "label": "",
    "id": "2928"
  },
  {
    "raw_code": "def initialize(type:, name:, namespace: nil, version: nil, qualifiers: nil, subpath: nil)\n      @type = type&.downcase\n      @namespace = namespace\n      @name = name\n      @version = version\n      @qualifiers = qualifiers\n      @subpath = subpath\n\n      ArgumentValidator.new(self).validate!\n    end",
    "comment": "Constructs a package URL from its components @param type [String] The package type or protocol. @param namespace [String] A name prefix, specific to the type of package. @param name [String] The name of the package. @param version [String] The version of the package. @param qualifiers [Hash] Extra qualifying data for a package, specific to the type of package. @param subpath [String] An extra subpath within a package, relative to the package root.",
    "label": "",
    "id": "2929"
  },
  {
    "raw_code": "def self.parse(string)\n      Decoder.new(string).decode!\n    end",
    "comment": "Creates a new PackageUrl from a string. @param [String] string The package URL string. @raise [InvalidPackageUrl] If the string is not a valid package URL. @return [PackageUrl]",
    "label": "",
    "id": "2930"
  },
  {
    "raw_code": "def to_h\n      {\n        scheme: scheme,\n        type: @type,\n        namespace: @namespace,\n        name: @name,\n        version: @version,\n        qualifiers: @qualifiers,\n        subpath: @subpath\n      }\n    end",
    "comment": "Returns a hash containing the scheme, type, namespace, name, version, qualifiers, and subpath components of the package URL.",
    "label": "",
    "id": "2931"
  },
  {
    "raw_code": "def to_s\n      Encoder.new(self).encode\n    end",
    "comment": "Returns a string representation of the package URL. Package URL representations are created according to the instructions from https://github.com/package-url/purl-spec/blob/0b1559f76b79829e789c4f20e6d832c7314762c5/PURL-SPECIFICATION.rst#how-to-build-purl-string-from-its-components.",
    "label": "",
    "id": "2932"
  },
  {
    "raw_code": "def partition(string, sep, from: :left, require_separator: true)\n        value, separator, remainder = if from == :left\n                                        left, separator, right = string.partition(sep)\n                                        [left, separator, right]\n                                      else\n                                        left, separator, right = string.rpartition(sep)\n                                        [right, separator, left]\n                                      end",
    "comment": "Partition the given string on the separator. The side being partitioned from is returned as the value, with the opposing side being returned as the remainder.  If a block is given, then the (value, remainder) are given to the block, and the return value of the block is used as the value.  If `require_separator` is true, then a nil value will be returned if the separator is not present.",
    "label": "",
    "id": "2933"
  },
  {
    "raw_code": "def find_pages_domain!\n        user_project.pages_domains.find_by(domain: params[:domain]) || not_found!('PagesDomain')\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2934"
  },
  {
    "raw_code": "def pages_domain\n        @pages_domain ||= find_pages_domain!\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2935"
  },
  {
    "raw_code": "def verify_package_file(package_file, uploaded_file)\n        stored_sha256 = Digest::SHA256.hexdigest(package_file.file_sha1)\n        expected_sha256 = uploaded_file.sha256\n\n        if stored_sha256 == expected_sha256\n          no_content!\n        else\n          conflict!\n        end",
    "comment": "The sha verification done by the maven api is between: - the sha256 set by workhorse helpers - the sha256 of the sha1 of the uploaded package file",
    "label": "",
    "id": "2936"
  },
  {
    "raw_code": "def geo_proxy_response\n        { geo_enabled: false }\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "2937"
  },
  {
    "raw_code": "def create_group\n        ::Groups::CreateService\n          .new(current_user, translate_params_for_compatibility)\n          .execute\n      end",
    "comment": "This is a separate method so that EE can extend its behaviour, without having to modify this code directly. ",
    "label": "",
    "id": "2938"
  },
  {
    "raw_code": "def update_group(group)\n        safe_params = translate_params_for_compatibility\n        return unauthorized! unless authorized_params?(group, safe_params)\n\n        ::Groups::UpdateService\n          .new(group, current_user, safe_params)\n          .execute\n      end",
    "comment": "This is a separate method so that EE can extend its behaviour, without having to modify this code directly. ",
    "label": "",
    "id": "2939"
  },
  {
    "raw_code": "def handle_similarity_order(group, projects)\n        if params[:search].present?\n          projects.sorted_by_similarity_desc(params[:search])\n        else\n          order_options = { name: :asc }\n          order_options['id'] ||= params[:sort] || 'asc'\n          projects.reorder(order_options)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2940"
  },
  {
    "raw_code": "def authorize_group_creation!\n        authorize! :create_group\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2941"
  },
  {
    "raw_code": "def check_query_limit; end\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "2942"
  },
  {
    "raw_code": "def sufficient?(scopes, request)\n      scopes.include?(self.name) && verify_if_condition(request)\n    end",
    "comment": "Are the `scopes` passed in sufficient to adequately authorize the passed request for the scope represented by the current instance of this class?",
    "label": "",
    "id": "2943"
  },
  {
    "raw_code": "def allow_access_with_scope(scopes, options = {})\n        Array(scopes).each do |scope|\n          allowed_scopes << Scope.new(scope, options)\n        end",
    "comment": "Set the authorization scope(s) allowed for an API endpoint.  A call to this method maps the given scope(s) to the current API endpoint class. If this method is called multiple times on the same class, the scopes are all aggregated.",
    "label": "",
    "id": "2944"
  },
  {
    "raw_code": "def scopes_registered_for_endpoint\n        @scopes_registered_for_endpoint ||=\n          begin\n            endpoint_classes = [options[:for].presence, ::API::API].compact\n            endpoint_classes.reduce([]) do |memo, endpoint|\n              if endpoint.respond_to?(:allowed_scopes)\n                memo.concat(endpoint.allowed_scopes)\n              else\n                memo\n              end",
    "comment": "An array of scopes that were registered (using `allow_access_with_scope`) for the current endpoint class. It also returns scopes registered on `API::API`, since these are meant to apply to all API routes.",
    "label": "",
    "id": "2945"
  },
  {
    "raw_code": "def find_merge_requests(args = {})\n        args = declared_params.merge(args)\n        args[:milestone_title] = args.delete(:milestone)\n        args[:not][:milestone_title] = args[:not]&.delete(:milestone)\n        args[:label_name] = args.delete(:labels)\n        args[:not][:label_name] = args[:not]&.delete(:labels)\n        args[:sort] = \"#{args[:order_by]}_#{args[:sort]}\"\n        args[:scope] = args[:scope].underscore if args[:scope]\n\n        parent_type = args[:project_id] ? :project : :group\n        args[:\"attempt_#{parent_type}_search_optimizations\"] = true\n\n        merge_requests = MergeRequestsFinder.new(current_user, args).execute\n        merge_requests = paginate(merge_requests)\n                           .preload(:source_project, :target_project)\n\n        return merge_requests if args[:view] == 'simple'\n\n        merge_requests\n          .with_api_entity_associations\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2946"
  },
  {
    "raw_code": "def render_merge_requests(merge_requests, options, skip_cache: false)\n        return present merge_requests, options if skip_cache\n\n        cache_context = ->(mr) do\n          [\n            current_user&.cache_key,\n            mr.merge_status,\n            mr.labels.map(&:cache_key),\n            mr.merge_request_assignees.map(&:cache_key),\n            mr.merge_request_reviewers.map(&:cache_key)\n          ].join(\":\")\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2947"
  },
  {
    "raw_code": "def order_and_sort_statuses(statuses)\n          sort_direction = params[:sort].presence || DEFAULT_SORT_DIRECTION\n          order_column = ALLOWED_SORT_VALUES.include?(params[:order_by]) ? params[:order_by] : DEFAULT_SORT_VALUE\n          statuses.order(order_column => sort_direction)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- Better code maintainability here, this won't be reused anywhere",
    "label": "",
    "id": "2948"
  },
  {
    "raw_code": "def readable_discussion_notes(noteable, discussion_ids)\n        notes = noteable.notes\n          .with_discussion_ids(discussion_ids)\n          .inc_relations_for_view(noteable)\n          .includes(:noteable)\n          .order_created_at_id_asc\n\n        # Without RendersActions#prepare_notes_for_rendering,\n        # Note#system_note_visible_for? will attempt to render\n        # Markdown references mentioned in the note to see whether they\n        # should be redacted. For notes that reference a commit, this\n        # would also incur a Gitaly call to verify the commit exists.\n        #\n        # With prepare_notes_for_rendering, we can avoid Gitaly calls\n        # because notes are redacted if they point to projects that\n        # cannot be accessed by the user.\n        notes = prepare_notes_for_rendering(notes)\n        notes.select { |n| n.readable_by?(current_user) }\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2949"
  },
  {
    "raw_code": "def apply_filters(projects)\n        projects = projects.with_statistics if params[:statistics]\n        projects = projects.joins(:statistics) if params[:order_by].include?('project_statistics') # rubocop: disable CodeReuse/ActiveRecord\n        projects = projects.created_by(current_user).imported.with_import_state if params[:imported]\n\n        lang = params[:with_programming_language]\n        projects = projects.with_programming_language(lang) if lang\n\n        projects\n      end",
    "comment": "EE::API::Projects would override this method",
    "label": "",
    "id": "2950"
  },
  {
    "raw_code": "def preload_relation(projects_relation, _options = {})\n      projects_relation\n    end",
    "comment": "This is overridden by the specific Entity class to preload assocations that it needs",
    "label": "",
    "id": "2951"
  },
  {
    "raw_code": "def execute_batch_counting(projects_relation); end\n\n    def preload_repository_cache(projects_relation)\n      repositories = repositories_for_preload(projects_relation)\n\n      Gitlab::RepositoryCache::Preloader.new(repositories).preload( # rubocop:disable CodeReuse/ActiveRecord\n        %i[exists? root_ref has_visible_content? avatar readme_path]\n      )\n    end\n\n    def repositories_for_preload(projects_relation)\n      projects_relation.map(&:repository)\n    end\n\n    # For all projects except those in a user namespace, the `namespace`\n    # and `group` are identical. Preload the group when it's not a user namespace.\n    def preload_groups(projects_relation)\n      group_projects = projects_for_group_preload(projects_relation)\n      groups = group_projects.map(&:namespace)\n\n      ::Namespaces::Preloaders::GroupRootAncestorPreloader.new(groups).execute\n\n      group_projects.each do |project|\n        project.group = project.namespace\n      end\n    end",
    "comment": "This is overridden by the specific Entity class to batch load certain counts",
    "label": "",
    "id": "2952"
  },
  {
    "raw_code": "def current_authenticated_job\n      if try(:namespace_inheritable, :authentication)\n        ci_build_from_namespace_inheritable\n      else\n        @current_authenticated_job # rubocop:disable Gitlab/ModuleWithInstanceVariables\n      end",
    "comment": "Returns the job associated with the token provided for authentication, if any",
    "label": "",
    "id": "2953"
  },
  {
    "raw_code": "def current_user\n      return @current_user if defined?(@current_user)\n\n      @current_user = initial_current_user\n\n      Gitlab::I18n.locale = @current_user&.preferred_language\n\n      sudo!\n\n      unless sudo?\n        token = validate_and_save_access_token!(scopes: scopes_registered_for_endpoint)\n\n        if token\n          result = ::Authz::Tokens::AuthorizeGranularScopesService.new(\n            boundary: boundary_for_endpoint, permissions: permissions_for_endpoint, token: token\n          ).execute\n\n          raise Gitlab::Auth::GranularPermissionsError, result.message if result.error?\n        end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables We can't rewrite this with StrongMemoize because `sudo!` would actually write to `@current_user`, and `sudo?` would immediately call `current_user` again which reads from `@current_user`. We should rewrite this in a way that using StrongMemoize is possible",
    "label": "",
    "id": "2954"
  },
  {
    "raw_code": "def set_current_organization(user: current_user)\n      ::Current.organization = Gitlab::Current::Organization.new(\n        params: {},\n        user: user\n      ).organization\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "2955"
  },
  {
    "raw_code": "def find_project(id)\n      return unless id\n\n      projects = find_project_scopes\n\n      if id.is_a?(Integer) || id =~ INTEGER_ID_REGEX\n        projects.find_by(id: id)\n      elsif id.include?(\"/\")\n        projects.find_by_full_path(id, follow_redirects: true)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2956"
  },
  {
    "raw_code": "def find_project_scopes\n      Project.without_deleted.not_hidden\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Can be overriden by API endpoints",
    "label": "",
    "id": "2957"
  },
  {
    "raw_code": "def find_pipeline(id)\n      return unless id\n\n      if INTEGER_ID_REGEX.match?(id.to_s)\n        ::Ci::Pipeline.find_by(id: id)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2958"
  },
  {
    "raw_code": "def find_pipeline!(id)\n      pipeline = find_pipeline(id)\n      check_pipeline_access(pipeline)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2959"
  },
  {
    "raw_code": "def find_group(id, organization: nil)\n      collection = organization.present? ? Group.in_organization(organization) : Group.all\n\n      if INTEGER_ID_REGEX.match?(id.to_s)\n        collection.find_by(id: id)\n      else\n        collection.find_by_full_path(id)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2960"
  },
  {
    "raw_code": "def find_group!(id, organization: nil)\n      group = find_group(id, organization: organization)\n      # We need to ensure the namespace is in the context since\n      # it's possible a method such as bypass_session! might log\n      # a message before @group is set.\n      ::Gitlab::ApplicationContext.push(namespace: group) if group\n      check_group_access(group)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2961"
  },
  {
    "raw_code": "def find_namespace(id)\n      if INTEGER_ID_REGEX.match?(id.to_s)\n        # We need to stick to an up-to-date replica or primary db here in order to properly observe the namespace\n        # recently created by GitlabSubscriptions::Trials::UltimateCreateService.\n        # See https://gitlab.com/gitlab-org/customers-gitlab-com/-/issues/9808\n        ::Namespace.sticking.find_caught_up_replica(:namespace, id)\n\n        Namespace.without_project_namespaces.find_by(id: id)\n      else\n        find_namespace_by_path(id)\n      end",
    "comment": "find_namespace returns the namespace regardless of user access level on the namespace rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2962"
  },
  {
    "raw_code": "def find_namespace!(id)\n      check_namespace_access(find_namespace(id))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord find_namespace! returns the namespace if the current user can read the given namespace Otherwise, returns a not_found! error",
    "label": "",
    "id": "2963"
  },
  {
    "raw_code": "def find_project_issue(iid, project_id = nil)\n      project = project_id ? find_project!(project_id) : user_project\n\n      ::IssuesFinder.new(\n        current_user,\n        project_id: project.id,\n        issue_types: WorkItems::Type.allowed_types_for_issues\n      ).find_by!(iid: iid)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2964"
  },
  {
    "raw_code": "def find_project_merge_request(iid, project: user_project)\n      MergeRequestsFinder.new(current_user, project_id: project.id).find_by!(iid: iid)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2965"
  },
  {
    "raw_code": "def find_project_commit(id)\n      user_project.commit_by(oid: id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2966"
  },
  {
    "raw_code": "def find_merge_request_with_access(iid, access_level = :read_merge_request)\n      merge_request = user_project.merge_requests.find_by!(iid: iid)\n      authorize! access_level, merge_request\n      merge_request\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2967"
  },
  {
    "raw_code": "def find_build!(id)\n      user_project.builds.find(id.to_i)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2968"
  },
  {
    "raw_code": "def required_attributes!(keys)\n      keys.each do |key|\n        bad_request_missing_attribute!(key) unless params[key].present?\n      end",
    "comment": "Checks the occurrences of required attributes, each attribute must be present in the params hash or a Bad Request error is invoked.  Parameters: keys (required) - A hash consisting of keys that must be present",
    "label": "",
    "id": "2969"
  },
  {
    "raw_code": "def filter_by_title(items, title)\n      items.where(title: title)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2970"
  },
  {
    "raw_code": "def filter_by_search(items, text)\n      items.search(text)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2971"
  },
  {
    "raw_code": "def redirect!(location_url)\n      raise ::API::API::MovedPermanentlyError, location_url\n    end",
    "comment": "An error is raised to interrupt user's request and redirect them to the right route. The error! helper behaves similarly, but it cannot be used because it formats the response message.",
    "label": "",
    "id": "2972"
  },
  {
    "raw_code": "def forbidden!(reason = nil)\n      render_api_error_with_reason!(403, '403 Forbidden', reason)\n    end",
    "comment": "error helpers",
    "label": "",
    "id": "2973"
  },
  {
    "raw_code": "def reorder_projects(projects)\n      projects.reorder(order_options_with_tie_breaker)\n    end",
    "comment": "project helpers rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2974"
  },
  {
    "raw_code": "def project_finder_params\n      project_finder_params_ce.merge(project_finder_params_ee)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2975"
  },
  {
    "raw_code": "def present_disk_file!(path, filename, content_type: nil, extra_response_headers: {})\n      filename ||= File.basename(path)\n      extra_response_headers.compact_blank.each { |k, v| header[k] = v }\n      header['Content-Disposition'] = ActionDispatch::Http::ContentDisposition.format(disposition: 'attachment', filename: filename)\n      header['Content-Transfer-Encoding'] = 'binary'\n      content_type(content_type || 'application/octet-stream')\n\n      # Support download acceleration\n      case headers['X-Sendfile-Type']\n      when 'X-Sendfile'\n        header['X-Sendfile'] = path\n        body '' # to avoid an error from API::APIGuard::ResponseCoercerMiddleware\n      else\n        sendfile path\n      end",
    "comment": "file helpers",
    "label": "",
    "id": "2976"
  },
  {
    "raw_code": "def present_carrierwave_file!(file, supports_direct_download: true, content_disposition: nil, content_type: nil, extra_response_headers: {}, extra_send_url_params: {})\n      return not_found! unless file&.exists?\n\n      if content_disposition\n        response_disposition = ActionDispatch::Http::ContentDisposition.format(disposition: content_disposition, filename: file.filename)\n      end",
    "comment": "Return back the given file depending on the object storage configuration. For disabled mode, the disk file is returned. For enabled mode, the response depends on the direct download support: * direct download supported by the uploader class: a redirect to the file signed url is returned. * direct download not supported: a workhorse send_url response is returned.  Params: @file the carrierwave file. @supports_direct_download set to false to force a workhorse send_url response. true by default. @content_disposition controls the Content-Disposition response header. nil by default. Forced to attachment for object storage disabled mode. @content_type controls the Content-Type response header. By default, it will rely on the 'application/octet-stream' value or the content type detected by carrierwave. @extra_response_headers. Set additional response headers. Not used in the direct download supported case. @extra_send_url_params. Additional parameters to send to workhorse send_url call. See Gitlab::Workhorse.send_url for more information",
    "label": "",
    "id": "2977"
  },
  {
    "raw_code": "def increment_unique_values(event_name, values)\n      return unless values.present?\n\n      Gitlab::UsageDataCounters::HLLRedisCounter.track_event(event_name, values: values)\n    rescue StandardError => error\n      Gitlab::AppLogger.warn(\"Redis tracking event failed for event: #{event_name}, message: #{error.message}\")\n    end",
    "comment": "@param event_name [String] the event name @param values [Array|String] the values counted",
    "label": "",
    "id": "2978"
  },
  {
    "raw_code": "def project_finder_params_ee\n      {}\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "2979"
  },
  {
    "raw_code": "def initial_current_user\n      return @initial_current_user if defined?(@initial_current_user)\n\n      begin\n        @initial_current_user = Gitlab::Auth::UniqueIpsLimiter.limit_user! { find_current_user! }\n      rescue Gitlab::Auth::UnauthorizedError\n        unauthorized!\n\n        # Explicitly return `nil`, otherwise an instance of `Rack::Response` is returned when reporting an error\n        nil\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "2980"
  },
  {
    "raw_code": "def sudo!\n      return unless sudo_identifier\n\n      unauthorized! unless initial_current_user\n\n      unless initial_current_user.can_admin_all_resources?\n        forbidden!('Must be admin to use sudo')\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "2981"
  },
  {
    "raw_code": "def legacy_send_artifacts_entry(file, entry)\n      header(*Gitlab::Workhorse.send_artifacts_entry(file, entry))\n\n      body ''\n    end",
    "comment": "Deprecated. Use `send_artifacts_entry` instead.",
    "label": "",
    "id": "2982"
  },
  {
    "raw_code": "def define_params_for_grape_middleware\n      self.define_singleton_method(:request) { ActionDispatch::Request.new(env) }\n      self.define_singleton_method(:params) { request.params.symbolize_keys }\n    end",
    "comment": "The Grape Error Middleware only has access to `env` but not `params` nor `request`. We workaround this by defining methods that returns the right values.",
    "label": "",
    "id": "2983"
  },
  {
    "raw_code": "def report_exception?(exception)\n      return true unless exception.respond_to?(:status)\n\n      exception.status == 500\n    end",
    "comment": "We could get a Grape or a standard Ruby exception. We should only report anything that is clearly an error.",
    "label": "",
    "id": "2984"
  },
  {
    "raw_code": "def find_by_deploy_key(project, key_id)\n        project.deploy_keys_projects.find_by!(deploy_key: key_id)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2985"
  },
  {
    "raw_code": "def reorder_users(users)\n          # Users#search orders by exact matches and handles pagination,\n          # so we should prioritize that, unless the user specifies some custom\n          # sort.\n          if params[:search] && !custom_order_by_or_sort?\n            users\n          else\n            params[:order_by] ||= 'id'\n            params[:sort] ||= 'desc'\n            users.reorder(order_options_with_tie_breaker)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2986"
  },
  {
    "raw_code": "def custom_namespace_present_options\n        {}\n      end",
    "comment": "EE::API::Namespaces would override this method",
    "label": "",
    "id": "2987"
  },
  {
    "raw_code": "def present_entity(result)\n          representation = entity_representation_for(::API::Entities::BatchedBackgroundMigration, result, {})\n          json_representation = Gitlab::Json.dump(representation)\n\n          body Gitlab::Json::PrecompiledJson.new(json_representation)\n        end",
    "comment": "Force progress evaluation to occur now while we're using the right connection",
    "label": "",
    "id": "2988"
  },
  {
    "raw_code": "def finder_params\n              {\n                packages_class: ::Packages::TerraformModule::Package,\n                package_type: :terraform_module,\n                package_name: \"#{params[:module_name]}/#{params[:module_system]}\",\n                package_version: params[:module_version],\n                exact_name: true,\n                within_public_package_registry: true\n              }.compact\n            end",
    "comment": "TODO: Remove `packages_class` with the rollout of the FF packages_refactor_group_packages_finder https://gitlab.com/gitlab-org/gitlab/-/issues/568923",
    "label": "",
    "id": "2989"
  },
  {
    "raw_code": "def self.postload_relation(projects_relation, options = {})\n        options[:project_members] = options[:current_user]\n          .project_members\n          .where(source_id: projects_relation.subquery(:id))\n          .preload(:source, user: [notification_settings: :source])\n\n        project_group_ids = projects_relation.subquery(:namespace_id)\n        options[:group_members] = ::GroupMember\n          .max_access_members(project_group_ids, options[:current_user])\n          .preload(:source, user: [notification_settings: :source])\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2990"
  },
  {
    "raw_code": "def self.preload_resource(project)\n        ActiveRecord::Associations::Preloader.new(records: [project], associations: { project_group_links: { group: :route } }).call\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2991"
  },
  {
    "raw_code": "def self.execute_batch_counting(projects_relation)\n        # Call the count methods on every project, so the BatchLoader would load them all at\n        # once when the entities are rendered\n        projects_relation.each(&:open_issues_count)\n\n        super\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2992"
  },
  {
    "raw_code": "def lazy_issuable_metadata\n        BatchLoader.for(object).batch(key: [current_user, :issuable_metadata]) do |models, loader, args|\n          current_user = args[:key].first\n\n          issuable_metadata = Gitlab::IssuableMetadata.new(current_user, models)\n          metadata_by_id = issuable_metadata.data\n\n          models.each do |issuable|\n            loader.call(issuable, metadata_by_id[issuable.id])\n          end",
    "comment": "This method will preload the `issuable_metadata` for the current entity according to the current top-level entity options, such as the current_user.",
    "label": "",
    "id": "2993"
  },
  {
    "raw_code": "def self.preload_relation(projects_relation, options = {})\n        # Preloading topics, should be done with using only `:topics`,\n        # as `:topics` are defined as: `has_many :topics, through: :project_topics`\n        # N+1 is solved then by using `subject.topics.map(&:name)`\n        # MR describing the solution: https://gitlab.com/gitlab-org/gitlab-foss/merge_requests/20555\n        projects_relation.preload(:project_feature, :route)\n                         .preload(:import_state, :topics)\n                         .preload(:auto_devops)\n                         .preload(namespace: [:route, :owner, :namespace_settings_with_ancestors_inherited_settings])\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2994"
  },
  {
    "raw_code": "def self.execute_batch_counting(projects_relation)\n        # Call the count methods on every project, so the BatchLoader would load them all at\n        # once when the entities are rendered\n        projects_relation.each(&:forks_count)\n\n        super\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2995"
  },
  {
    "raw_code": "def self.preload_relation(projects_relation, _options = {})\n        super(projects_relation).preload(\n          project_group_links: { group: :route },\n          fork_network: :root_project,\n          fork_network_member: :forked_from_project,\n          forked_from_project: [:route, :topics, :group, :project_feature, { namespace: [:route, :owner] }])\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2996"
  },
  {
    "raw_code": "def self.execute_batch_counting(projects_relation)\n        # Call the count methods on every project, so the BatchLoader would load them all at\n        # once when the entities are rendered\n        projects_relation.filter_map(&:forked_from_project).each(&:forks_count)\n\n        super\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "2997"
  },
  {
    "raw_code": "def model_version_uri\n            path = api_v4_projects_packages_ml_models_files___path___path(\n              id: object.project.id, model_version_id: object.model_version_id, path: '', file_name: ''\n            )\n\n            path.delete_suffix('(/path/)')\n          end",
    "comment": "Example: http://127.0.0.1:3000/api/v4/projects/20/packages/ml_models/1/files/",
    "label": "",
    "id": "2998"
  },
  {
    "raw_code": "def ml_model_candidate_uri\n            path = api_v4_projects_packages_ml_models_files___path___path(\n              id: object.project.id, model_version_id: \"#{CANDIDATE_PREFIX}#{object.iid}\", path: '', file_name: ''\n            )\n\n            path.delete_suffix('(/path/)')\n          end",
    "comment": "Example: http://127.0.0.1:3000/api/v4/projects/20/packages/ml_models/1/files/",
    "label": "",
    "id": "2999"
  },
  {
    "raw_code": "def generic_package_uri\n            path = api_v4_projects_packages_generic_package_version___path___path(\n              id: object.project.id, package_name: '', file_name: ''\n            )\n            path = path.delete_suffix('/package_version/(/path/)')\n\n            [path, object.artifact_root].join('')\n          end",
    "comment": "Example: http://127.0.0.1:3000/api/v4/projects/20/packages/generic/ml_experiment_1/1/ Note: legacy format",
    "label": "",
    "id": "3000"
  },
  {
    "raw_code": "def pipeline_schedule\n          @pipeline_schedule ||=\n            user_project\n              .pipeline_schedules\n              .preload(:owner)\n              .find_by(id: params.delete(:pipeline_schedule_id)).tap do |pipeline_schedule|\n                unless can?(current_user, :read_pipeline_schedule, pipeline_schedule)\n                  not_found!('Pipeline Schedule')\n                end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3001"
  },
  {
    "raw_code": "def pipeline_schedule_variable\n          @pipeline_schedule_variable ||=\n            pipeline_schedule.variables.find_by(key: params[:key]).tap do |pipeline_schedule_variable|\n              unless pipeline_schedule_variable\n                not_found!('Pipeline Schedule Variable')\n              end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3002"
  },
  {
    "raw_code": "def filter_builds(builds, scope)\n          return builds if scope.nil? || scope.empty?\n\n          available_statuses = ::CommitStatus::AVAILABLE_STATUSES\n\n          unknown = scope - available_statuses\n          render_api_error!('Scope contains invalid value(s)', 400) unless unknown.empty?\n\n          builds.where(status: available_statuses && scope)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3003"
  },
  {
    "raw_code": "def validate_current_authenticated_job\n          # current_authenticated_job will be nil if user is using\n          # a valid authentication (like PRIVATE-TOKEN) that is not CI_JOB_TOKEN\n          not_found!('Job') unless current_authenticated_job\n\n          ::Gitlab::ApplicationContext.push(job: current_authenticated_job)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3004"
  },
  {
    "raw_code": "def authenticate_job!(heartbeat_runner: false)\n          # 404 is not returned here because we want to terminate the job if it's\n          # running. A 404 can be returned from anywhere in the networking stack which is why\n          # we are explicit about a 403, we should improve this in\n          # https://gitlab.com/gitlab-org/gitlab/-/issues/327703\n          forbidden! unless current_job\n\n          # Ensure we go through the Ci::AuthJobFinder as part of this authentication\n          begin\n            job = job_from_token\n\n            forbidden! unless job\n          rescue ::Ci::AuthJobFinder::DeletedProjectError\n            forbidden!('Project has been deleted!')\n          rescue ::Ci::AuthJobFinder::ErasedJobError\n            forbidden!('Job has been erased!')\n          rescue ::Ci::AuthJobFinder::NotRunningJobError\n            # Pass current_job solely to load actual status of the job.\n            # AuthJobFinder currently returns no details.\n            job_forbidden!(current_job, 'Job is not processing on runner')\n          end",
    "comment": "HTTP status codes to terminate the job on GitLab Runner: - 403",
    "label": "",
    "id": "3005"
  },
  {
    "raw_code": "def current_job\n          id = params[:id]\n\n          load_balancer_stick_request(::Ci::Build, :build, id) if id\n\n          strong_memoize(:current_job) do\n            ::Ci::Build.find_by_id(id)\n          end",
    "comment": "current_job is queried by URL :id param with no authentication",
    "label": "",
    "id": "3006"
  },
  {
    "raw_code": "def job_token\n          @job_token ||= (params[JOB_TOKEN_PARAM] || env[JOB_TOKEN_HEADER]).to_s\n        end",
    "comment": "The token used by runner to authenticate a request. In most cases, the runner uses the token belonging to the requested job. However, when requesting for job artifacts, the runner would use the token that belongs to downstream jobs that depend on the job that owns the artifacts.",
    "label": "",
    "id": "3007"
  },
  {
    "raw_code": "def lazy_job_execution_status(object:, key:)\n          BatchLoader.for(object.id).batch(key: key) do |object_ids, loader|\n            statuses = object.class.id_in(object_ids).with_executing_builds.index_by(&:id)\n\n            object_ids.each do |id|\n              loader.call(id, statuses[id] ? :active : :idle)\n            end",
    "comment": "Efficiently determines job execution status for multiple runners using BatchLoader to avoid N+1 queries. Returns :active if runner has executing builds, :idle otherwise.",
    "label": "",
    "id": "3008"
  },
  {
    "raw_code": "def hook_test_service(hook, _)\n          TestHooks::ProjectService.new(hook, current_user, params[:trigger])\n        end",
    "comment": "EE::API::Hooks::TriggerTest overrides this helper",
    "label": "",
    "id": "3009"
  },
  {
    "raw_code": "def self.verify!(request)\n          return false unless Gitlab::CurrentSettings.slack_app_signing_secret\n\n          timestamp, signature = request.headers.values_at(\n            VERIFICATION_TIMESTAMP_HEADER,\n            VERIFICATION_SIGNATURE_HEADER\n          )\n\n          return false if timestamp.nil? || signature.nil?\n          return false if Time.current.to_i - timestamp.to_i >= VERIFICATION_TIMESTAMP_EXPIRY\n\n          request.body.rewind\n\n          basestring = [\n            VERIFICATION_VERSION,\n            timestamp,\n            request.body.read\n          ].join(VERIFICATION_DELIMITER)\n\n          hmac_digest = OpenSSL::HMAC.hexdigest(\n            VERIFICATION_HMAC_ALGORITHM,\n            Gitlab::CurrentSettings.slack_app_signing_secret,\n            basestring\n          )\n\n          # Signature will look like: 'v0=a2114d57b48eac39b9ad189dd8316235a7b4a8d21a10bd27519666489c69b503'\n          ActiveSupport::SecurityUtils.secure_compare(\n            signature,\n            \"#{VERIFICATION_VERSION}=#{hmac_digest}\"\n          )\n        end",
    "comment": "Verify the request by comparing the given request signature in the header with a signature value that we compute according to the steps in: https://api.slack.com/authentication/verifying-requests-from-slack.",
    "label": "",
    "id": "3010"
  },
  {
    "raw_code": "def check_allowed(params)\n          # This is a separate method so that EE can alter its behaviour more\n          # easily.\n\n          check_rate_limit!(:gitlab_shell_operation, scope: [params[:action], params[:project], actor.key_or_user])\n\n          rate_limiter = Gitlab::Auth::IpRateLimiter.new(request.ip)\n\n          unless rate_limiter.trusted_ip?\n            check_rate_limit!(:gitlab_shell_operation, scope: [params[:action], params[:project], rate_limiter.ip])\n          end",
    "comment": "rubocop: disable Metrics/AbcSize",
    "label": "",
    "id": "3011"
  },
  {
    "raw_code": "def validate_actor(actor)\n          return 'Could not find the given key' unless actor.key\n\n          'Could not find a user for the given key' unless actor.user\n        end",
    "comment": "rubocop: enable Metrics/AbcSize",
    "label": "",
    "id": "3012"
  },
  {
    "raw_code": "def paginate_with_strategies(relation, request_scope = nil, paginator_params: {})\n        paginator = paginator(relation, request_scope)\n\n        result = if block_given?\n                   yield(paginator.paginate(relation, **paginator_params))\n                 else\n                   paginator.paginate(relation, **paginator_params)\n                 end",
    "comment": "paginator_params are only currently supported with offset pagination",
    "label": "",
    "id": "3013"
  },
  {
    "raw_code": "def find_user_by_id(params)\n        id = params[:user_id] || params[:id]\n        User.find_by(id: id) || not_found!('User')\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3014"
  },
  {
    "raw_code": "def repo_type\n        parse_repo_path unless defined?(@repo_type)\n        @repo_type\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "3015"
  },
  {
    "raw_code": "def access_check_result\n        with_admin_mode_bypass!(actor.user&.id) do\n          access_check!(actor, params)\n        end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "3016"
  },
  {
    "raw_code": "def access_check!(actor, params)\n        access_checker = access_checker_for(actor, params[:protocol])\n        access_checker.check(params[:action], params[:changes]).tap do |result|\n          break result if @project || !repo_type.project?\n\n          # If we have created a project directly from a git push\n          # we have to assign its value to both @project and @container\n          @project = @container = access_checker.container\n        end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "3017"
  },
  {
    "raw_code": "def access_checker_for(actor, protocol)\n        access_checker_klass.new(actor.key_or_user, container, protocol,\n          authentication_abilities: ssh_authentication_abilities,\n          repository_path: repository_path,\n          redirected_path: redirected_path,\n          push_options: params[:push_options],\n          gitaly_context: gitaly_context(params)\n        )\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "3018"
  },
  {
    "raw_code": "def parse_repo_path\n        @container, @project, @repo_type, @redirected_path =\n          if params[:gl_repository]\n            Gitlab::GlRepository.parse(params[:gl_repository])\n          elsif params[:project]\n            Gitlab::RepoPath.parse(params[:project])\n          end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "3019"
  },
  {
    "raw_code": "def gl_repository\n        repo_type.identifier_for_container(container)\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables Repository id to pass between components that don't share/don't have access to the same filesystem mounts",
    "label": "",
    "id": "3020"
  },
  {
    "raw_code": "def repository\n        @repository ||= repo_type.repository_for(container)\n      end",
    "comment": "Return the repository for the detected type and container  @returns [Repository]",
    "label": "",
    "id": "3021"
  },
  {
    "raw_code": "def gitaly_payload(action)\n        return unless %w[git-receive-pack git-upload-pack git-upload-archive].include?(action)\n\n        {\n          repository: repository.gitaly_repository.to_h,\n          address: Gitlab::GitalyClient.address(repository.shard),\n          token: Gitlab::GitalyClient.token(repository.shard),\n          features: Feature::Gitaly.server_feature_flags(\n            user: ::Feature::Gitaly.user_actor(actor.user),\n            repository: repository,\n            project: ::Feature::Gitaly.project_actor(repository.container),\n            group: ::Feature::Gitaly.group_actor(repository.container)\n          )\n        }\n      end",
    "comment": "Return the Gitaly Address if it is enabled",
    "label": "",
    "id": "3022"
  },
  {
    "raw_code": "def present_cached(obj_or_collection, with:, cache_context: ->(_) { current_user&.cache_key }, expires_in: Gitlab::Cache::Helpers::DEFAULT_EXPIRY, **presenter_args)\n        json =\n          if obj_or_collection.is_a?(Enumerable)\n            cached_collection(\n              obj_or_collection,\n              presenter: with,\n              presenter_args: presenter_args,\n              context: cache_context,\n              expires_in: expires_in\n            )\n          else\n            cached_object(\n              obj_or_collection,\n              presenter: with,\n              presenter_args: presenter_args,\n              context: cache_context,\n              expires_in: expires_in\n            )\n          end",
    "comment": "This is functionally equivalent to the standard `#present` used in Grape endpoints, but the JSON for the object, or for each object of a collection, will be cached.  With a collection all the keys will be fetched in a single call and the Entity rendered for those missing from the cache, which are then written back into it.  Both the single object, and all objects inside a collection, must respond to `#cache_key`.  To override the Grape formatter we return a custom wrapper in `Gitlab::Json::PrecompiledJson` which tells the `Gitlab::Json::GrapeFormatter` to export the string without conversion.  A cache context can be supplied to add more context to the cache key. This defaults to including the `current_user` in every key for safety, unless overridden.  @param obj_or_collection [Object, Enumerable<Object>] the object or objects to render @param with [Grape::Entity] the entity to use for rendering @param cache_context [Proc] a proc to call for each object to provide more context to the cache key @param expires_in [ActiveSupport::Duration, Integer] an expiry time for the cache entry @param presenter_args [Hash] keyword arguments to be passed to the entity @return [Gitlab::Json::PrecompiledJson]",
    "label": "",
    "id": "3023"
  },
  {
    "raw_code": "def cache_action(key, **custom_cache_opts)\n        cache_opts = apply_default_cache_options(custom_cache_opts)\n\n        json, cached_headers = cache.fetch(key, **cache_opts) do\n          response = yield\n\n          cached_body = response.is_a?(Gitlab::Json::PrecompiledJson) ? response.to_s : Gitlab::Json.dump(response.as_json)\n          cached_headers = header.slice(*PAGINATION_HEADERS)\n\n          [cached_body, cached_headers]\n        end",
    "comment": "Action caching implementation  This allows you to wrap an entire API endpoint call in a cache, useful for short TTL caches to effectively rate-limit an endpoint. The block will be converted to JSON and cached, and returns a `Gitlab::Json::PrecompiledJson` object which will be exported without secondary conversion.  @param key [Object] any object that can be converted into a cache key @param expires_in [ActiveSupport::Duration, Integer] an expiry time for the cache entry @return [Gitlab::Json::PrecompiledJson]",
    "label": "",
    "id": "3024"
  },
  {
    "raw_code": "def cache_action_if(conditional, *opts, **kwargs)\n        if conditional\n          cache_action(*opts, **kwargs) do\n            yield\n          end",
    "comment": "Conditionally cache an action  Perform a `cache_action` only if the conditional passes",
    "label": "",
    "id": "3025"
  },
  {
    "raw_code": "def cache_action_unless(conditional, *opts, **kwargs)\n        cache_action_if(!conditional, *opts, **kwargs) do\n          yield\n        end",
    "comment": "Conditionally cache an action  Perform a `cache_action` unless the conditional passes",
    "label": "",
    "id": "3026"
  },
  {
    "raw_code": "def awardable\n        @awardable ||=\n          if params.include?(:note_id)\n            note_id = params.delete(:note_id)\n\n            awardable.notes.find(note_id)\n          elsif params.include?(:issue_iid)\n            user_project.issues.find_by!(iid: params[:issue_iid])\n          elsif params.include?(:merge_request_iid)\n            user_project.merge_requests.find_by!(iid: params[:merge_request_iid])\n          elsif params.include?(:snippet_id)\n            user_project.snippets.find(params[:snippet_id])\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3027"
  },
  {
    "raw_code": "def with_custom_attributes(collection_or_resource, options = {})\n            options = options.merge(\n              with_custom_attributes: params[:with_custom_attributes] &&\n                can?(current_user, :read_custom_attribute)\n            )\n\n            if options[:with_custom_attributes] && collection_or_resource.is_a?(ActiveRecord::Relation)\n              collection_or_resource = collection_or_resource.includes(:custom_attributes)\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3028"
  },
  {
    "raw_code": "def user_project_with_read_package\n        project = find_project(params[:id])\n\n        return forbidden! unless authorized_project_scope?(project)\n\n        project && authorize_job_token_policies!(project) && return\n\n        return project if can?(current_user, :read_package, project&.packages_policy_subject)\n        # guest users can have :read_project but not :read_package\n        return forbidden! if can?(current_user, :read_project, project)\n        return unauthorized! if authenticate_non_public?\n\n        not_found!('Project')\n      end",
    "comment": "This function is similar to the `find_project!` function, but it considers the `read_package` ability.",
    "label": "",
    "id": "3029"
  },
  {
    "raw_code": "def retrieve_members(source, params:, deep: false)\n        members = deep ? find_all_members(source) : source_members(source).connected_to_user\n        members = members.allow_cross_joins_across_databases(url: \"https://gitlab.com/gitlab-org/gitlab/-/issues/417456\")\n        members = members.includes(:user)\n        members = members.references(:user).merge(User.search(params[:query], use_minimum_char_limit: false)) if params[:query].present?\n        members = members.where(user_id: params[:user_ids]) if params[:user_ids].present?\n        members\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3030"
  },
  {
    "raw_code": "def find_all_members(source)\n        members = source.is_a?(Project) ? find_all_members_for_project(source) : find_all_members_for_group(source)\n        members.non_invite.non_request\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "3031"
  },
  {
    "raw_code": "def expose_url(path)\n        url_options = Gitlab::Application.routes.default_url_options\n        protocol, host, port, script_name = url_options.values_at(:protocol, :host, :port, :script_name)\n\n        # Using a blank component at the beginning of the join we ensure\n        # that the resulted path will start with '/'. If the resulted path\n        # does not start with '/', URI::Generic#new will fail\n        path_with_script_name = File.join('', [script_name, path].select(&:present?))\n\n        URI::Generic.new(protocol, nil, host, port, nil, path_with_script_name, nil, nil, nil, URI::RFC3986_PARSER, true).to_s\n      end",
    "comment": "This function should only be used to expose an API path or URL. It should not be used with Rails See https://gitlab.com/gitlab-org/gitlab/-/merge_requests/183415#note_2387443102",
    "label": "",
    "id": "3032"
  },
  {
    "raw_code": "def coerce_nil_params_to_array!\n        keys_to_coerce = params_with_array_types\n\n        params.each do |key, val|\n          params[key] = Array(val) if val.nil? && keys_to_coerce.include?(key)\n        end",
    "comment": "Grape v1.3.3 no longer automatically coerces an Array type to an empty array if the value is nil.",
    "label": "",
    "id": "3033"
  },
  {
    "raw_code": "def registry_base_url(package_type)\n          REGISTRY_BASE_URLS[package_type]\n        end",
    "comment": "Override in JiHu repo",
    "label": "",
    "id": "3034"
  },
  {
    "raw_code": "def find_personal_access_token\n            PersonalAccessToken.active.find_by_token(access_token_from_request)\n          end",
    "comment": "We override this method from auth_finders because we need to extract the token from the Conan JWT which is specific to the Conan API",
    "label": "",
    "id": "3035"
  },
  {
    "raw_code": "def find_oauth_access_token; end\n\n          def find_personal_access_token_from_conan_jwt\n            token = decode_oauth_token_from_jwt\n\n            return unless token\n\n            token.access_token_id\n          end\n\n          def find_deploy_token_from_conan_jwt\n            token = decode_oauth_token_from_jwt\n\n            return unless token\n\n            deploy_token = DeployToken.active.find_by_token(token.access_token_id.to_s)\n            # note: uesr_id is not a user record id, but is the attribute set on ConanToken\n            return if token.user_id != deploy_token&.username\n\n            deploy_token\n          end\n\n          def find_job_from_conan_jwt\n            token = decode_oauth_token_from_jwt\n\n            return unless token\n\n            ::Ci::AuthJobFinder.new(token: token.access_token_id.to_s).execute\n          end\n\n          def decode_oauth_token_from_jwt\n            jwt = Doorkeeper::OAuth::Token.from_bearer_authorization(current_request)\n\n            return unless jwt\n\n            token = ::Gitlab::ConanToken.decode(jwt)\n\n            return unless token && token.access_token_id && token.user_id\n\n            token\n          end\n\n          def package_scope\n            params[:id].present? ? :project : :instance\n          end\n\n          def search_project\n            project\n          end\n        end",
    "comment": "We need to override this one because it looks into Bearer authorization header",
    "label": "",
    "id": "3036"
  },
  {
    "raw_code": "def extract!\n      qualified_id, @ref, @path = extract_ref_path\n      @id = @override_id || qualified_id\n      @repo = repository_container.repository\n      raise InvalidPathError if @ref.match?(/\\s/)\n\n      return unless @ref.present?\n\n      @commit = if ref_type\n                  @fully_qualified_ref = self.class.qualify_ref(@ref, ref_type)\n                  @repo.commit(@fully_qualified_ref)\n                else\n                  @repo.commit(@ref)\n                end",
    "comment": "Extracts common variables for views working with Git tree-ish objects  Assignments are:  - @id     - A string representing the joined ref and path Assigns @override_id if it is present. - @ref    - A string representing the ref (e.g., the branch, tag, or commit SHA) - @path   - A string representing the filesystem path - @commit - A Commit representing the commit from the given ref - @fully_qualified_ref - A string representing the fully qualifed ref (e.g., refs/tags/v1.1)  If the :id parameter appears to be requesting a specific response format, that will be handled as well.",
    "label": "",
    "id": "3037"
  },
  {
    "raw_code": "def extract_ref(id)\n      pair = extract_raw_ref(id)\n\n      [\n        pair[0].strip,\n        pair[1].delete_prefix('/').delete_suffix('/')\n      ]\n    end",
    "comment": "Given a string containing both a Git tree-ish, such as a branch or tag, and a filesystem path joined by forward slashes, attempts to separate the two.  Expects a repository_container method that returns the active repository object. This is used to check the input against a list of valid repository refs.  Examples  # No repository_container available extract_ref('master') # => ['', '']  extract_ref('master') # => ['master', '']  extract_ref(\"f4b14494ef6abf3d144c28e4af0c20143383e062/CHANGELOG\") # => ['f4b14494ef6abf3d144c28e4af0c20143383e062', 'CHANGELOG']  extract_ref(\"v2.0.0/README.md\") # => ['v2.0.0', 'README.md']  extract_ref('master/app/models/project.rb') # => ['master', 'app/models/project.rb']  extract_ref('issues/1234/app/models/project.rb') # => ['issues/1234', 'app/models/project.rb']  # Given an invalid branch, we fall back to just splitting on the first slash extract_ref('non/existent/branch/README.md') # => ['non', 'existent/branch/README.md']  Returns an Array where the first value is the tree-ish and the second is the path",
    "label": "",
    "id": "3038"
  },
  {
    "raw_code": "def sensitive_fields\n        token_fields - [@expires_at_field]\n      end",
    "comment": "The expires_at field is not considered sensitive",
    "label": "",
    "id": "3039"
  },
  {
    "raw_code": "def ensure_token!(token_owner_record)\n        reset_token!(token_owner_record) unless token_set?(token_owner_record)\n        get_token(token_owner_record)\n      end",
    "comment": "Returns a token, but only saves when the database is in read & write mode",
    "label": "",
    "id": "3040"
  },
  {
    "raw_code": "def reset_token!(token_owner_record)\n        write_new_token(token_owner_record)\n        token_owner_record.save! if Gitlab::Database.read_write?\n      end",
    "comment": "Resets the token, but only saves when the database is in read & write mode",
    "label": "",
    "id": "3041"
  },
  {
    "raw_code": "def prefix_for(token_owner_record)\n        case prefix_option = options[:format_with_prefix]\n        when nil\n          nil\n        when Symbol\n          token_owner_record.send(prefix_option) # rubocop:disable GitlabSecurity/PublicSend -- We allow specifying a private method for `:format_with_prefix`.\n        else\n          raise NotImplementedError\n        end",
    "comment": "If a `format_with_prefix` option is provided, it applies and returns the formatted token. Otherwise, default implementation returns the token as-is",
    "label": "",
    "id": "3042"
  },
  {
    "raw_code": "def in_qa_file?(node)\n      path = node.location.expression.source_buffer.name\n\n      path.start_with?(File.join(Dir.pwd, 'qa'))\n    end",
    "comment": "Returns true if the given node originated from the qa/ directory.",
    "label": "",
    "id": "3043"
  },
  {
    "raw_code": "def const_pattern(name, parent: '{nil? cbase}')\n      name.split('::').inject(parent) { |memo, name_part| \"(const #{memo} :#{name_part})\" }\n    end",
    "comment": "Returns a nested `(const ...)` node pattern for a full qualified +name+.  @examples const_pattern 'Foo::Bar' # => (const (const {nil? cbase} :Foo) :Bar) const_pattern 'Foo::Bar', parent: ':Baz' # => (const (const :Baz :Foo) :Bar)",
    "label": "",
    "id": "3044"
  },
  {
    "raw_code": "def self.config_checksum\n      @config_checksum ||= Digest::SHA256.file(CONFIG_PATH).hexdigest\n    end",
    "comment": "Used by RuboCop to invalidate its cache if the contents of config/feature_categories.yml changes. Define a method called `external_dependency_checksum` and call this method to use it.",
    "label": "",
    "id": "3045"
  },
  {
    "raw_code": "def in_migration?(node)\n      in_deployment_migration?(node) || in_post_deployment_migration?(node)\n    end",
    "comment": "Returns true if the given node originated from the db/migrate directory.",
    "label": "",
    "id": "3046"
  },
  {
    "raw_code": "def time_enforced?(node)\n      return false unless enforced_since\n\n      version(node) > enforced_since\n    end",
    "comment": "Returns true if we've defined an 'EnforcedSince' variable in rubocop.yml and the migration version is greater.",
    "label": "",
    "id": "3047"
  },
  {
    "raw_code": "def array_column?(node)\n      node.each_descendant(:pair).any? do |pair_node|\n        pair_node.child_nodes[0].sym_type? && # Searching for a RuboCop::AST::SymbolNode\n          pair_node.child_nodes[0].value == :array && # Searching for a (pair (sym :array) (true)) node\n          pair_node.child_nodes[1].type == :true # RuboCop::AST::Node uses symbols for types, even when that is a :true\n      end",
    "comment": "Returns true if a column definition is for an array, like { array: true }  @example add_column :table, :ids, :integer, array: true, default: []  rubocop:disable Lint/BooleanSymbol",
    "label": "",
    "id": "3048"
  },
  {
    "raw_code": "def initialize(\n      formatter: Formatters::Asciidoc.new, cops_registry: RuboCop::Cop::Registry.global,\n      departments: [], extra_info: {}, base_dir: Dir.pwd\n    )\n      @departments = departments.map(&:to_sym).sort!\n      @extra_info = extra_info\n      @formatter = formatter\n      @cops = cops_registry\n      @config = RuboCop::ConfigLoader.default_configuration\n      @base_dir = base_dir\n      @docs_path = \"#{base_dir}/docs/modules/ROOT\"\n      FileUtils.mkdir_p(\"#{@docs_path}/pages\")\n    end",
    "comment": "rubocop:enable Layout/HashAlignment This class will only generate documentation for cops that belong to one of the departments given in the `departments` array. E.g. if we only wanted documentation for Lint cops:  CopsDocumentationGenerator.new(departments: ['Lint']).call  You can append additional information:  callback = ->(data) { required_rails_version(data.cop) } CopsDocumentationGenerator.new(extra_info: { ruby_version: callback }).call  This will insert the string returned from the lambda _after_ the section from RuboCop itself. See `CopsDocumentationGenerator::STRUCTURE` for available sections. ",
    "label": "",
    "id": "3049"
  },
  {
    "raw_code": "def properties(cop)\n      header = [\n        'Enabled by default', 'Safe', 'Supports autocorrection', 'Version Added',\n        'Version Changed'\n      ]\n      autocorrect = if cop.support_autocorrect? # rubocop:disable Cop/LineBreakAroundConditionalBlock -- Rule differs in upstream repo\n                      context = cop.new.always_autocorrect? ? 'Always' : 'Command-line only'\n\n                      \"#{context}#{' (Unsafe)' unless cop.new(config).safe_autocorrect?}\"\n                    else\n                      'No'\n                    end",
    "comment": "rubocop:disable Metrics/MethodLength -- Exception already existed in upstream repo",
    "label": "",
    "id": "3050"
  },
  {
    "raw_code": "def cop_header(cop)\n      content = +\"\\n\"\n      content << formatter.to_anchor(to_anchor(cop.cop_name))\n      content << formatter.to_header(cop.cop_name, level: 2)\n      content\n    end",
    "comment": "rubocop:enable Metrics/MethodLength",
    "label": "",
    "id": "3051"
  },
  {
    "raw_code": "def configurable_values(pars, name)\n      case name\n      when /^Enforced/\n        supported_style_name = RuboCop::Cop::Util.to_supported_styles(name)\n        format_table_value(pars[supported_style_name])\n      when 'IndentationWidth'\n        'Integer'\n      when 'Database'\n        format_table_value(pars['SupportedDatabases'])\n      else\n        case pars[name]\n        when String\n          'String'\n        when Integer\n          'Integer'\n        when Float\n          'Float'\n        when true, false\n          'Boolean'\n        when Array\n          'Array'\n        else\n          ''\n        end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity,Metrics/MethodLength  -- Exception already existed in upstream repo",
    "label": "",
    "id": "3052"
  },
  {
    "raw_code": "def format_table_value(val) # rubocop:disable Metrics/MethodLength -- Rule differs in upstream repo\n      value =\n        case val\n        when Array\n          if val.empty?\n            '`[]`'\n          else\n            val.map { |config| format_table_value(config) }.join(', ')\n          end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity,Metrics/MethodLength",
    "label": "",
    "id": "3053"
  },
  {
    "raw_code": "def print_cops_of_department(department)\n      selected_cops = cops_of_department(department)\n      content = formatter.to_comment(+<<~HEADER)\n        Do NOT edit this file by hand directly, as it is automatically generated.\n\n        Please make any necessary changes to the cop documentation within the source files themselves.\n      HEADER\n      content += formatter.to_header(department)\n      selected_cops.each { |cop| content << print_cop_with_doc(cop) }\n      content << footer_for_department(department)\n      file_name = formatter.to_filename(\"#{docs_path}/pages/#{department_to_basename(department)}\")\n      File.open(file_name, 'w') do |file|\n        puts \"* generated #{file_name}\"\n        file.write(\"#{content.strip}\\n\")\n      end",
    "comment": "rubocop:disable Metrics/MethodLength -- Exception already existed in upstream repo",
    "label": "",
    "id": "3054"
  },
  {
    "raw_code": "def print_cop_with_doc(cop) # rubocop:todo Metrics/AbcSize, Metrics/MethodLength -- Exception already existed in upstream repo\n      cop_config = config.for_cop(cop)\n      non_display_keys = %w[\n        AutoCorrect Description Enabled StyleGuide Reference Safe SafeAutoCorrect VersionAdded\n        VersionChanged\n      ]\n      pars = cop_config.reject { |k| non_display_keys.include? k }\n      description = 'No documentation'\n      example_objects = safety_objects = see_objects = []\n      cop_code(cop) do |code_object|\n        description = code_object.docstring unless code_object.docstring.blank?\n        example_objects = code_object.tags('example')\n        safety_objects = code_object.tags('safety')\n        see_objects = code_object.tags('see')\n      end",
    "comment": "rubocop:enable Metrics/MethodLength",
    "label": "",
    "id": "3055"
  },
  {
    "raw_code": "def to_anchor(title)\n      title.delete('/').tr(' ', '-').gsub(/[^a-zA-Z0-9-]/, '_').downcase\n    end",
    "comment": "HTML anchor are somewhat limited in what characters they can contain, just accept a known-good subset. As long as it's consistent it doesn't matter.  Style/AccessModifierDeclarations => styleaccessmodifierdeclarations OnlyFor: [] (default) => onlyfor_-__-_default_",
    "label": "",
    "id": "3056"
  },
  {
    "raw_code": "def initialize(directory, inflector: ActiveSupport::Inflector)\n      @directory = directory\n      @inflector = inflector\n    end",
    "comment": "Instantiates a TodoDir.  @param directory [String] base directory where all TODO YAML files are written to. @param inflector [ActiveSupport::Inflector, #underscore] an object which supports converting a string to its underscored version.",
    "label": "",
    "id": "3057"
  },
  {
    "raw_code": "def read(cop_name)\n      path = path_for(cop_name)\n\n      File.read(path) if File.exist?(path)\n    end",
    "comment": "Reads content of TODO YAML for given +cop_name+.  @param cop_name [String] name of the cop rule  @return [String, nil] content of the TODO YAML file if it exists",
    "label": "",
    "id": "3058"
  },
  {
    "raw_code": "def write(cop_name, content)\n      path = path_for(cop_name)\n\n      FileUtils.mkdir_p(File.dirname(path))\n      File.write(path, content)\n\n      path\n    end",
    "comment": "Saves +content+ for given +cop_name+ to TODO YAML file.  @return [String] path of the written TODO YAML file",
    "label": "",
    "id": "3059"
  },
  {
    "raw_code": "def inspect(cop_name)\n      path = path_for(cop_name)\n\n      if File.exist?(path)\n        FileUtils.mv(path, \"#{path}#{SUFFIX_INSPECT}\")\n        true\n      else\n        false\n      end",
    "comment": "Marks a TODO YAML file for inspection by renaming the original TODO YAML and appending the suffix +.inspect+ to it.  @return [Boolean] +true+ a file was marked for inspection successfully.",
    "label": "",
    "id": "3060"
  },
  {
    "raw_code": "def inspect_all\n      pattern = File.join(@directory, \"**/*#{SUFFIX_YAML}\")\n\n      Dir.glob(pattern).count do |path|\n        FileUtils.mv(path, \"#{path}#{SUFFIX_INSPECT}\")\n      end",
    "comment": "Marks all TODO YAML files for inspection.  @return [Integer] number of renamed YAML TODO files.  @see inspect",
    "label": "",
    "id": "3061"
  },
  {
    "raw_code": "def list_inspect\n      pattern = File.join(@directory, \"**/*#{SUFFIX_YAML}#{SUFFIX_INSPECT}\")\n\n      Dir.glob(pattern)\n    end",
    "comment": "Returns a list of TODO YAML files which are marked for inspection.  @return [Array<String>] list of paths  @see inspect @see inspect_all",
    "label": "",
    "id": "3062"
  },
  {
    "raw_code": "def delete_inspected\n      list_inspect.count do |path|\n        File.delete(path)\n      end",
    "comment": "Deletes a list of TODO yaml files which were marked for inspection.  @return [Integer] number of deleted YAML TODO files.  @see #inspect @see #inspect_all",
    "label": "",
    "id": "3063"
  },
  {
    "raw_code": "def send_to_constant?(node)\n      node.type == :send && node.children&.first&.type == :const\n    end",
    "comment": "Returns true for a `(send const ...)` node.",
    "label": "",
    "id": "3064"
  },
  {
    "raw_code": "def send_receiver_name_ends_with?(node, suffix)\n      return false unless send_to_constant?(node)\n\n      receiver_name = name_of_receiver(node)\n\n      receiver_name != suffix &&\n        receiver_name.end_with?(suffix)\n    end",
    "comment": "Returns `true` if the name of the receiving constant ends with a given `String`.",
    "label": "",
    "id": "3065"
  },
  {
    "raw_code": "def file_path_for_node(node)\n      node.location.expression.source_buffer.name\n    end",
    "comment": "Returns the file path (as a `String`) for an AST node.",
    "label": "",
    "id": "3066"
  },
  {
    "raw_code": "def name_of_constant(node)\n      node.children[1]\n    end",
    "comment": "Returns the name of a constant node.  Given the AST node `(const nil? :Foo)`, this method will return `:Foo`.",
    "label": "",
    "id": "3067"
  },
  {
    "raw_code": "def in_finder?(node)\n      in_app_directory?(node, 'finders')\n    end",
    "comment": "Returns true if the given node resides in app/finders or ee/app/finders.",
    "label": "",
    "id": "3068"
  },
  {
    "raw_code": "def in_model?(node)\n      in_app_directory?(node, 'models')\n    end",
    "comment": "Returns true if the given node resides in app/models or ee/app/models.",
    "label": "",
    "id": "3069"
  },
  {
    "raw_code": "def in_service_class?(node)\n      in_app_directory?(node, 'services')\n    end",
    "comment": "Returns true if the given node resides in app/services or ee/app/services.",
    "label": "",
    "id": "3070"
  },
  {
    "raw_code": "def in_presenter?(node)\n      in_app_directory?(node, 'presenters')\n    end",
    "comment": "Returns true if the given node resides in app/presenters or ee/app/presenters.",
    "label": "",
    "id": "3071"
  },
  {
    "raw_code": "def in_serializer?(node)\n      in_app_directory?(node, 'serializers')\n    end",
    "comment": "Returns true if the given node resides in app/serializers or ee/app/serializers.",
    "label": "",
    "id": "3072"
  },
  {
    "raw_code": "def in_worker?(node)\n      in_app_directory?(node, 'workers')\n    end",
    "comment": "Returns true if the given node resides in app/workers or ee/app/workers.",
    "label": "",
    "id": "3073"
  },
  {
    "raw_code": "def in_controller?(node)\n      in_app_directory?(node, 'controllers')\n    end",
    "comment": "Returns true if the given node resides in app/controllers or ee/app/controllers.",
    "label": "",
    "id": "3074"
  },
  {
    "raw_code": "def in_graphql?(node)\n      in_app_directory?(node, 'graphql')\n    end",
    "comment": "Returns true if the given node resides in app/graphql or ee/app/graphql.",
    "label": "",
    "id": "3075"
  },
  {
    "raw_code": "def in_api?(node)\n      in_lib_directory?(node, 'api')\n    end",
    "comment": "Returns true if the given node resides in lib/api or ee/lib/api.",
    "label": "",
    "id": "3076"
  },
  {
    "raw_code": "def in_spec?(node)\n      file_path_for_node(node).start_with?(\n        ce_spec_directory,\n        ee_spec_directory\n      )\n    end",
    "comment": "Returns true if the given node resides in spec or ee/spec.",
    "label": "",
    "id": "3077"
  },
  {
    "raw_code": "def in_app_directory?(node, directory)\n      file_path_for_node(node).start_with?(\n        File.join(ce_app_directory, directory),\n        File.join(ee_app_directory, directory)\n      )\n    end",
    "comment": "Returns `true` if the given AST node resides in the given directory, relative to app and/or ee/app.",
    "label": "",
    "id": "3078"
  },
  {
    "raw_code": "def in_lib_directory?(node, directory)\n      file_path_for_node(node).start_with?(\n        File.join(ce_lib_directory, directory),\n        File.join(ee_lib_directory, directory)\n      )\n    end",
    "comment": "Returns `true` if the given AST node resides in the given directory, relative to lib and/or ee/lib.",
    "label": "",
    "id": "3079"
  },
  {
    "raw_code": "def in_graphql_directory?(node, directory)\n      in_app_directory?(node, \"graphql/#{directory}\") ||\n        in_app_directory?(node, \"graphql/ee/#{directory}\")\n    end",
    "comment": "Returns true if the given node resides in app/graphql/{directory}, ee/app/graphql/{directory}, or ee/app/graphql/ee/{directory}.",
    "label": "",
    "id": "3080"
  },
  {
    "raw_code": "def name_of_receiver(node)\n      name_of_constant(node.children.first).to_s\n    end",
    "comment": "Returns the receiver name of a send node.  For the AST node `(send (const nil? :Foo) ...)` this would return `'Foo'`.",
    "label": "",
    "id": "3081"
  },
  {
    "raw_code": "def each_class_method(node)\n      return to_enum(__method__, node) unless block_given?\n\n      # class << self\n      #   def foo\n      #   end\n      # end\n      node.each_descendant(:sclass) do |sclass|\n        sclass.each_descendant(:def) do |def_node|\n          yield def_node\n        end",
    "comment": "Yields every defined class method in the given AST node.",
    "label": "",
    "id": "3082"
  },
  {
    "raw_code": "def each_send_node(node, &block)\n      node.each_descendant(:send, &block)\n    end",
    "comment": "Yields every send node found in the given AST node.",
    "label": "",
    "id": "3083"
  },
  {
    "raw_code": "def disallow_send_to(node, suffix, message)\n      each_send_node(node) do |send_node|\n        next unless send_receiver_name_ends_with?(send_node, suffix)\n\n        add_offense(send_node, message: message)\n      end",
    "comment": "Registers a RuboCop offense for a `(send)` node with a receiver that ends with a given suffix.  node - The AST node to check. suffix - The suffix of the receiver name, such as \"Finder\". message - The message to use for the offense.",
    "label": "",
    "id": "3084"
  },
  {
    "raw_code": "def self.adjusted_exit_status(status)\n        return status unless status == RuboCop::CLI::STATUS_OFFENSES\n        return RuboCop::CLI::STATUS_SUCCESS if active_offenses == 0\n\n        status\n      end",
    "comment": "We consider this run a success without any active offenses.",
    "label": "",
    "id": "3085"
  },
  {
    "raw_code": "def on_class(node)\n        return if node.parent_class&.const_name != 'ApplicationExperiment'\n        return if covered_with_tests?(node)\n\n        add_offense(node, message: CLASS_OFFENSE)\n      end",
    "comment": "Validates classes inherited from ApplicationExperiment These classes are located under app/experiments or ee/app/experiments",
    "label": "",
    "id": "3086"
  },
  {
    "raw_code": "def on_block(node)\n        return unless node.method?(:experiment)\n        return if covered_with_tests?(node)\n\n        add_offense(node, message: BLOCK_OFFENSE)\n      end",
    "comment": "Validates experiments block in *.rb and *.haml files: experiment(:experiment_name) do |e| e.candidate { 'candidate' } e.run end",
    "label": "",
    "id": "3087"
  },
  {
    "raw_code": "def corrector(node)\n        ->(corrector) do\n          corrector.insert_after(\n            node,\n            \" # rubocop: disable #{cop_name}\"\n          )\n        end",
    "comment": "Automatically correcting these offenses is not always possible, as sometimes code needs to be refactored to make this work. As such, we only allow developers to easily denylist existing offenses.",
    "label": "",
    "id": "3088"
  },
  {
    "raw_code": "def on_send(node)\n          module_node, method_name, first_arg = feature_flag_method?(node)\n          return unless method_name\n\n          return if first_arg.sym_type?\n\n          message = format(MSG, module: module_node.source, method: method_name)\n\n          add_offense(first_arg, message: message) do |corrector|\n            autocorrect(corrector, first_arg)\n          end",
    "comment": "rubocop:disable InternalAffairs/OnSendWithoutOnCSend -- `Feature&.enabled?` is not possible",
    "label": "",
    "id": "3089"
  },
  {
    "raw_code": "def on_new_investigation\n          super\n\n          return if self.class.feature_flags_already_tracked\n\n          self.class.feature_flags_already_tracked = true\n        end",
    "comment": "Called before all on_... have been called When refining this method, always call `super`",
    "label": "",
    "id": "3090"
  },
  {
    "raw_code": "def external_dependency_checksum\n          self.class.external_dependency_checksum\n        end",
    "comment": "Used by RuboCop to invalidate its cache if the contents of `config_file_path` changes.",
    "label": "",
    "id": "3091"
  },
  {
    "raw_code": "def use_tag_pair(args)\n            return args unless args.all?(&:pair_type?)\n\n            pair_hash = args.to_h { |pair| [pair.key, pair.value] }\n            seen = Hash.new { |hash, tag| hash[tag] = [] }\n            tag_pairs = []\n\n            args.each do |pair|\n              # We only care about { a: '<b>'.html_safe }. Ignore the rest\n              next unless pair.value.send_type? && pair.value.method?(:html_safe) && pair.value.receiver.str_type?\n\n              # Extract the tag from `<b>` or `</b>`.\n              tag = pair.value.receiver.value[%r{^</?(\\w+)>}, 1]\n              next unless tag\n\n              seen[tag] << pair\n              next unless seen[tag].size == 2\n\n              closing_tag, opening_tag = seen[tag].sort_by { |pair| pair.value.receiver.value }\n              pair_hash.delete(closing_tag.key)\n              pair_hash.delete(opening_tag.key)\n\n              keys = [opening_tag, closing_tag].map { |pair| pair.key.value.inspect }.join(\", \")\n              tag_pairs << \"tag_pair(tag.#{tag}, #{keys})\"\n\n              seen[tag].clear\n            end",
    "comment": "Turns `open: '<b>'.html_safe, close: '</b>'.html_safe` into `tag_pair(tag.b, :open, :close)`.",
    "label": "",
    "id": "3092"
  },
  {
    "raw_code": "def get_anchors_in_markdown(docs_file_path)\n            self.class.anchors_by_docs_file.fetch(docs_file_path) do\n              docs_content = File.read(docs_file_path)\n              headers = docs_content.scan(ATX_HEADER_MATCH)\n              counters = Hash.new(0)\n\n              self.class.anchors_by_docs_file[docs_file_path] = headers.map do |header|\n                _level, text, id = header\n\n                id || generate_anchor(text, counters)\n              end",
    "comment": "This methods extracts anchors from a Markdown file. The logic in here replicates our custom Kramdown header parser at https://gitlab.com/gitlab-org/ruby/gems/gitlab_kramdown/-/blob/bbc5ac439a2e6af60cbcce9a157283b2c5b59b38/lib/gitlab_kramdown/parser/header.rb. The logic is documented here: https://docs.gitlab.com/ee/user/markdown.html#heading-ids-and-links. There a special undocumnented syntax that makes it possible to set custom IDs, eg: ```md ### My heading {#my-custom-id} ``` This would result in a `my-custom-id` anchor instead of `my-heading`. We are also handling this special syntax in here.",
    "label": "",
    "id": "3093"
  },
  {
    "raw_code": "def requires_task?(file)\n          file.source.include?('task')\n        end",
    "comment": "Allow `require \"foo/rake_task\"`",
    "label": "",
    "id": "3094"
  },
  {
    "raw_code": "def on_send(node)\n          return unless in_migration?(node)\n\n          add_offense(node.loc.selector) if method_name(node) == :add_timestamps\n        end",
    "comment": "Check methods.",
    "label": "",
    "id": "3095"
  },
  {
    "raw_code": "def on_def(node)\n          return unless in_migration?(node)\n\n          node.each_descendant(:send) do |send_node|\n            add_offense(send_node.loc.selector) if method_name(send_node) == :timestamps\n          end",
    "comment": "Check methods in table creation.",
    "label": "",
    "id": "3096"
  },
  {
    "raw_code": "def external_dependency_checksum\n          @external_dependency_checksum ||= checksum_filenames('{,ee/}spec/migrations/**/*_spec.rb')\n        end",
    "comment": "Used by RuboCop to invalidate its cache if specs change.",
    "label": "",
    "id": "3097"
  },
  {
    "raw_code": "def on_def(node)\n          return unless in_migration?(node)\n\n          node.each_descendant(:send) do |send_node|\n            method_name = send_node.children[1]\n\n            if method_name == :datetime || method_name == :timestamp\n              add_offense(send_node.loc.selector, message: format(MSG, method_name))\n            end",
    "comment": "Check methods in table creation.",
    "label": "",
    "id": "3098"
  },
  {
    "raw_code": "def on_send(node)\n          return unless in_migration?(node)\n\n          node.each_descendant do |descendant|\n            next unless descendant.sym_type?\n\n            last_argument = descendant.children.last\n\n            if last_argument == :datetime || last_argument == :timestamp\n              add_offense(node, message: format(MSG, last_argument))\n            end",
    "comment": "Check methods.",
    "label": "",
    "id": "3099"
  },
  {
    "raw_code": "def old_version_migration_class?(class_node)\n          parent_class_node = class_node.parent_class\n          return false if parent_class_node.nil?\n          return false unless parent_class_node.send_type? && parent_class_node.last_argument.float_type?\n          return false unless parent_class_node.children[0].const_name == GITLAB_MIGRATION_CLASS\n\n          parent_class_node.first_argument.value < CURRENT_MIGRATION_VERSION\n        end",
    "comment": "Returns true for any parent class of format Gitlab::Database::Migration[version] if version < current_version",
    "label": "",
    "id": "3100"
  },
  {
    "raw_code": "def table_and_attribute_name(node)\n          migration_method = node.children[1]\n          table_name, attribute_name = ''\n\n          if migration_method == :text\n            # We are inside a node in a create/change table block\n            block_node = node.each_ancestor(:block).first\n            create_table_node = block_node\n                                  .children\n                                  .find { |n| TABLE_METHODS.include?(n.children[1]) }\n\n            if create_table_node\n              table_name = table_name_or_const_name(create_table_node.children[2])\n            else\n              # Guard against errors when a new table create/change migration\n              # helper is introduced and warn the author so that it can be\n              # added in TABLE_METHODS\n              table_name = 'unknown'\n              add_offense(\n                block_node.send_node.loc.selector,\n                message: 'Unknown table method. Please tweak `MigrationHelpers::TABLE_METHODS`.'\n              )\n            end",
    "comment": "For a given node, find the table and attribute this node is for  Simple when we have calls to `add_column_XXX` helper methods  A little bit more tricky when we have attributes defined as part of a create/change table block: - The attribute name is available on the node - Finding the table name requires to: * go up * find the first block the attribute def is part of * go back down to find the create_table node * fetch the table name from that node",
    "label": "",
    "id": "3101"
  },
  {
    "raw_code": "def text_limit_missing?(node, table_name, attribute_name)\n          return false if encrypted_attribute_name?(attribute_name)\n\n          limit_found = false\n\n          node.each_descendant(:send) do |send_node|\n            if set_text_limit?(send_node)\n              limit_found = matching_set_text_limit?(send_node, attribute_name)\n            elsif add_text_limit?(send_node)\n              limit_found = matching_add_text_limit?(send_node, table_name, attribute_name)\n            end",
    "comment": "Check if there is an `add_text_limit` call for the provided table and attribute name",
    "label": "",
    "id": "3102"
  },
  {
    "raw_code": "def locate_description(node)\n          description = description_kwarg(node)\n\n          return description unless description.nil? && enum?(node)\n\n          enum_style_description(node)\n        end",
    "comment": "Fields and arguments define descriptions using a `description` keyword argument. Enums may define descriptions this way, or as a second `String` param.",
    "label": "",
    "id": "3103"
  },
  {
    "raw_code": "def string?(description)\n          description.str_type?\n        end",
    "comment": "Returns true if `description` node is a `:str` (as opposed to a `#copy_field_description` call)",
    "label": "",
    "id": "3104"
  },
  {
    "raw_code": "def before_end_quote(string)\n          return string.source_range.adjust(end_pos: -1) unless string.heredoc?\n\n          heredoc_source = string.location.heredoc_body.source\n          adjust = heredoc_source.index(/\\s+\\Z/) - heredoc_source.length\n          string.location.heredoc_body.adjust(end_pos: adjust)\n        end",
    "comment": "Returns a `Parser::Source::Range` that ends just before the final `String` delimiter.",
    "label": "",
    "id": "3105"
  },
  {
    "raw_code": "def locate_this(string)\n          target = 'this'\n          range = string.heredoc? ? string.location.heredoc_body : string.source_range\n          index = range.source.index(target)\n          range.adjust(begin_pos: index, end_pos: (index + target.length) - range.length)\n        end",
    "comment": "Returns a `Parser::Source::Range` of the first `this` encountered",
    "label": "",
    "id": "3106"
  },
  {
    "raw_code": "def on_block(node)\n          check_redundant_type(node)\n        end",
    "comment": "For example: - `RSpec.describe ... do` - `context ... do`",
    "label": "",
    "id": "3107"
  },
  {
    "raw_code": "def on_send(node)\n          return unless env_assignment?(node)\n\n          add_offense(node, message: MESSAGE) do |corrector|\n            corrector.replace(node, stub_env(env_key(node), env_value(node)))\n          end",
    "comment": "Following is what node.children looks like on a match [s(:const, nil, :ENV), :[]=, s(:str, \"key\"), s(:str, \"value\")]",
    "label": "",
    "id": "3108"
  },
  {
    "raw_code": "def on_send(node)\n          return unless forbidden_factory_usage?(node)\n\n          method = node.children[1]\n\n          add_offense(node, message: MESSAGE % method)\n        end",
    "comment": "Following is what node.children looks like on a match: - Without FactoryBot namespace: [nil, :build, s(:sym, :user)] - With FactoryBot namespace: [s(:const, nil, :FactoryBot), :build, s(:sym, :user)]",
    "label": "",
    "id": "3109"
  },
  {
    "raw_code": "def should_scan?(node)\n          inherit_active_record_base?(node) || in_model?(node) || in_finder?(node) || in_service_class?(node)\n        end",
    "comment": "It limits the check to ActiveRecord, Models, Finders and Service classes",
    "label": "",
    "id": "3110"
  },
  {
    "raw_code": "def self.exit_on_failure?\n        true\n      end",
    "comment": "Exit with non 0 status code if any command fails  @return [Boolean]",
    "label": "",
    "id": "3111"
  },
  {
    "raw_code": "def container_exists?(name)\n          cmd = [\"docker\", \"ps\", \"-a\", \"-q\", \"-f\", \"name=^/#{name}$\"]\n          !execute_shell(cmd).strip.empty?\n        rescue StandardError => e\n          raise Error, \"Failed to check if container exists: #{e.message}\"\n        end",
    "comment": "Check if Docker container exists  @param [String] name @return [Boolean]",
    "label": "",
    "id": "3112"
  },
  {
    "raw_code": "def create_volume(name)\n          cmd = [\"docker\", \"volume\", \"create\", name]\n          execute_shell(cmd)\n        rescue StandardError => e\n          raise Error, \"Failed to create volume: #{e.message}\"\n        end",
    "comment": "Create Docker volume  @param [String] name @return [String]",
    "label": "",
    "id": "3113"
  },
  {
    "raw_code": "def pull_image(image)\n          cmd = [\"docker\", \"pull\", image]\n          execute_shell(cmd, live_output: true)\n\n        rescue StandardError => e\n          raise Error, \"Failed to pull Docker image: #{e.message}\"\n        end",
    "comment": "Pull Docker image with live output  @param [String] image @return [void]",
    "label": "",
    "id": "3114"
  },
  {
    "raw_code": "def run_container(\n          name:, image:, environment: {}, ports: {}, volumes: {}, restart: \"always\",\n          additional_options: [])\n          cmd = [\"docker\", \"run\", \"-d\", \"--name\", name]\n\n          environment&.each do |key, value|\n            cmd.push(\"-e\", \"#{key}=#{value}\")\n          end",
    "comment": "Run Docker container  @param [String] name @param [String] image @param [Hash] environment @param [Hash] ports @param [Hash] volumes @param [String] restart @param [Array<String>] additional_options @return [String]",
    "label": "",
    "id": "3115"
  },
  {
    "raw_code": "def exec(name, command)\n          cmd = [\"docker\", \"exec\", name, *command]\n          execute_shell(cmd)\n        rescue StandardError => e\n          raise Error, \"Failed to execute command in container: #{e.message}\"\n        end",
    "comment": "Execute command in Docker container  @param [String] name @param [Array<String>] command @return [String]",
    "label": "",
    "id": "3116"
  },
  {
    "raw_code": "def create\n          log(\"Creating docker container instance '#{name}'\", :info, bright: true)\n          run_pre_install_setup\n          run_install\n          run_post_install_setup\n\n          log(\"Installation successful and GitLab is available via: #{configuration.gitlab_url}\", :success,\n            bright: true)\n        rescue Gitlab::Orchestrator::Docker::Error\n          exit(1)\n        end",
    "comment": "Perform installation with all the additional setup  @return [void]",
    "label": "",
    "id": "3117"
  },
  {
    "raw_code": "def docker_client\n          @docker_client ||= Docker::Client.new\n        end",
    "comment": "Docker client instance  @return [Docker::Client]",
    "label": "",
    "id": "3118"
  },
  {
    "raw_code": "def env_values\n          return {} if extra_env.empty?\n\n          env = extra_env.map { |e| e.split(\"=\") }.reject { |e| e.size != 2 }.to_h\n          return {} if env.empty?\n\n          env\n        end",
    "comment": "Additional environment variables for container  @return [Hash]",
    "label": "",
    "id": "3119"
  },
  {
    "raw_code": "def run_pre_install_setup\n          Helpers::Spinner.spin(\"running pre-installation setup\") do\n            configuration.run_pre_installation_setup\n          end",
    "comment": "Execute pre-installation setup  @return [void]",
    "label": "",
    "id": "3120"
  },
  {
    "raw_code": "def run_install\n          values = configuration.values.deep_merge({ environment: env_values })\n          total_attempts = retry_attempts + 1\n\n          Helpers::Spinner.spin(\"creating docker container\") do\n            total_attempts.times do |attempt|\n              log(\"Pulling docker image: #{values[:image]}\", :info)\n              docker_client.pull_image(values[:image])\n\n              begin\n                docker_client.run_container(\n                  name: name,\n                  image: values[:image],\n                  environment: values[:environment],\n                  ports: values[:ports],\n                  volumes: values[:volumes] || {},\n                  restart: values[:restart],\n                  additional_options: [\"--shm-size\", \"256m\"]\n                )\n                break\n              rescue Gitlab::Orchestrator::Docker::Error => e\n                if attempt >= retry_attempts\n                  handle_install_failure(e)\n                else\n                  log(\"Installation failed, retrying...\", :warn)\n                  log(\"Error: #{e}\", :warn)\n                end",
    "comment": "Run Docker container creation  @return [void]",
    "label": "",
    "id": "3121"
  },
  {
    "raw_code": "def run_post_install_setup\n          configuration.run_post_installation_setup\n        end",
    "comment": "Execute post-installation setup  @return [void]",
    "label": "",
    "id": "3122"
  },
  {
    "raw_code": "def handle_install_failure(error)\n          log(\"Docker container creation failed!\", :error)\n          log(\"For more information on troubleshooting failures, see: '#{TROUBLESHOOTING_LINK}'\", :warn)\n          raise error\n        end",
    "comment": "Handle Docker container creation failure  @param [StandardError] error @return [void]",
    "label": "",
    "id": "3123"
  },
  {
    "raw_code": "def run_pre_installation_setup\n            # To be implemented by subclasses\n          end",
    "comment": "Run pre-installation setup  @return [void]",
    "label": "",
    "id": "3124"
  },
  {
    "raw_code": "def run_post_installation_setup\n            # To be implemented by subclasses\n          end",
    "comment": "Run post-installation setup  @return [void]",
    "label": "",
    "id": "3125"
  },
  {
    "raw_code": "def values\n            {}\n          end",
    "comment": "Configuration specific values  @return [Hash]",
    "label": "",
    "id": "3126"
  },
  {
    "raw_code": "def gitlab_url\n            raise NotImplementedError, \"#{self.class} must implement #gitlab_url\"\n          end",
    "comment": "Gitlab url  @return [String]",
    "label": "",
    "id": "3127"
  },
  {
    "raw_code": "def run_pre_installation_setup; end\n\n          # Run post-installation setup\n          #\n          # @return [void]\n          def run_post_installation_setup\n            wait_for_gitlab_ready\n          end\n\n          # Docker container configuration values\n          #\n          # @return [Hash]\n          def values\n            {\n              image: image,\n              environment: {\n                GITLAB_OMNIBUS_CONFIG: omnibus_config,\n                GITLAB_ROOT_PASSWORD: admin_password\n              },\n              ports: {\n                \"#{host_http_port}:80\" => nil\n              },\n              restart: \"always\"\n            }\n          end\n\n          # Gitlab url\n          #\n          # @return [String]\n          def gitlab_url\n            @gitlab_url ||= URI(\"http://#{gitlab_domain}:#{host_http_port}\").to_s\n          end\n\n          private\n\n          attr_reader :image, :admin_password, :host_http_port\n\n          # Wait for GitLab to be ready\n          #\n          # @return [void]\n          def wait_for_gitlab_ready\n            Helpers::Spinner.spin(\"Waiting for GitLab to be ready. This may take a while...\") do\n              gitlab_ready = false\n\n              30.times do\n                begin\n                  response = Net::HTTP.get_response(URI(\"#{gitlab_url}/users/sign_in\"))\n                  if response.code == \"200\"\n                    log(\"GitLab is ready! \", :success)\n                    gitlab_ready = true\n                    break\n                  end\n                rescue StandardError => e\n                  log(\"GitLab is not ready yet. Reason: #{e.message}\", :debug)\n                end",
    "comment": "Run pre-installation setup  @return [void]",
    "label": "",
    "id": "3128"
  },
  {
    "raw_code": "def docker_client\n            @docker_client ||= ::Gitlab::Orchestrator::Docker::Client.new\n          end",
    "comment": "Docker client instance  @return [Docker::Client]",
    "label": "",
    "id": "3129"
  },
  {
    "raw_code": "def omnibus_config\n            <<~RUBY\n              external_url 'http://#{gitlab_domain}';\n              gitlab_rails['gitlab_default_theme'] = 10;\n              gitlab_rails['gitlab_disable_animations'] = true;\n              gitlab_rails['application_settings_cache_seconds'] = 0;\n              gitlab_rails['env']['GITLAB_LICENSE_MODE'] = 'test';\n              gitlab_rails['env']['CUSTOMER_PORTAL_URL'] = 'https://customers.staging.gitlab.com';\n              gitlab_rails['env']['GITLAB_ALLOW_SEPARATE_CI_DATABASE'] = 'false';\n              gitlab_rails['env']['COVERBAND_ENABLED'] = 'false';\n            RUBY\n          end",
    "comment": "GitLab Omnibus configuration  @return [String]",
    "label": "",
    "id": "3130"
  },
  {
    "raw_code": "def generate(metric_type)\n          if data.empty?\n            log(\"No metrics data found in #{metrics_file}\", :warn)\n            return\n          end",
    "comment": "Generate and display graphs for specific metric type  @param metric_type [String] @return [void]",
    "label": "",
    "id": "3131"
  },
  {
    "raw_code": "def total_resource_allocation(metric, type)\n          data.sum { |_, pod| pod[metric][type] || 0 }\n        end",
    "comment": "Get total allocation across the whole deployment  @param metric [String] requests or limits @param type [String] cpu or memory @return [Integer]",
    "label": "",
    "id": "3132"
  },
  {
    "raw_code": "def graph_width\n          return @graph_width if @graph_width\n          return @graph_width = [[max_width, MIN_GRAPH_WIDTH].max, MAX_GRAPH_WIDTH].min if max_width.positive?\n\n          begin\n            width = IO.console.winsize[1]\n          rescue NoMethodError\n            width = DEFAULT_GRAPH_WIDTH\n          end",
    "comment": "Graph width based on terminal size or specified max width  @return [Integer]",
    "label": "",
    "id": "3133"
  },
  {
    "raw_code": "def load_metrics\n          raise(\"Metrics file not found at #{metrics_file}\") unless File.exist?(metrics_file)\n\n          JSON.parse(File.read(metrics_file))\n        end",
    "comment": "Load metrics from metrics file  @return [Hash]",
    "label": "",
    "id": "3134"
  },
  {
    "raw_code": "def generate_ascii_graph(values, title, unit, request, limit)\n          return \"No data available for #{title}\" if values.empty?\n\n          values_max = values.max_by { |v| v[:val] }[:val]\n          values_sum = values.sum { |v| v[:val] }.to_f\n          graph_max = [values_max, request || 0, limit || 0].max\n\n          statistics = [\"Avg: #{(values_sum / values.length).round(1)}#{unit}\", \"Max: #{values_max}#{unit}\"]\n\n          statistics << if request\n                          \"Request: #{request}#{unit} (#{((values_max.to_f / request) * 100).round(1)}%)\"\n                        else\n                          \"Request: #{colorize('N/A', :red, bright: true)}\"\n                        end",
    "comment": "Generate ASCII graphs  @param values [Array<Hash>] @param title [String] @param unit [String] @param request [Integer] @param limit [Integer] @return [void]",
    "label": "",
    "id": "3135"
  },
  {
    "raw_code": "def print_y_axis(values, request, limit, effective_width, graph_max)\n          scale = graph_max.to_f / GRAPH_HEIGHT\n          request_row = request ? (request / scale).ceil : 0\n          limit_row = limit ? (limit / scale).ceil : 0\n\n          # Print graph from top to bottom\n          GRAPH_HEIGHT.downto(1) do |row|\n            threshold = row * scale\n            print format(\"%6.0f \", threshold)\n            print \"\"\n\n            # Sample values to fit effective width\n            sample_size = [values.length, effective_width].min\n            step = values.length.to_f / sample_size\n\n            sample_size.times do |i|\n              index = (i * step).to_i\n              value = values[index][:val]\n\n              if !value.zero? && value >= threshold\n                # Use different characters based on proximity to limit\n                if limit && value > limit * 0.9\n                  print \"\"  # High usage (>90% of limit)\n                elsif limit && value > limit * 0.7\n                  print \"\"  # Medium usage (>70% of limit)\n                else\n                  print \"\"  # Normal usage\n                end",
    "comment": "Print Y axis and metrics  @param values [Array<Hash>] @param request [Integer] @param limit [Integer] @param effective_width [Integer] @param graph_max [Integer] @return [void]",
    "label": "",
    "id": "3136"
  },
  {
    "raw_code": "def print_markers(request_row, limit_row, current_row)\n          # Show resource markers at correct positions\n          markers = []\n          markers << \" LIMIT\" if limit_row == current_row\n          markers << \" REQUEST\" if request_row == current_row\n          return unless markers.any?\n\n          print \" #{markers.join(', ')}\"\n        end",
    "comment": "Print markers for request and limit definitions  @param request_row [Integer] @param limit_row [Integer] @param current_row [Integer] @return [void]",
    "label": "",
    "id": "3137"
  },
  {
    "raw_code": "def print_x_axis(values, effective_width, y_axis_width)\n          print \"     0 \"\n          print \"\"\n          print \"\" * effective_width\n          puts \"\"\n\n          # Time labels\n          if values.length > 1\n            first_entry_ts = Time.at(values.first[:ts]).utc.strftime(\"%H:%M:%S\")\n            last_entry_ts = Time.at(values.last[:ts]).utc.strftime(\"%H:%M:%S\")\n\n            print \" \" * y_axis_width\n            print first_entry_ts\n            # Calculate remaining space for last entry timestamp\n            remaining_space = effective_width - first_entry_ts.length - 7\n            print \" \" * [remaining_space, 0].max\n            print last_entry_ts\n            puts\n          end",
    "comment": "Print x-axis of the graph  @param values [Array<Hash>] @param effective_width [Integer] @param y_axis_width [Integer] @return [void]",
    "label": "",
    "id": "3138"
  },
  {
    "raw_code": "def add_helm_chart(name, url)\n          log(\"Adding helm chart '#{url}'\", :info)\n          puts(run_helm(%W[repo add #{name} #{url}]).tap do |output|\n            # when cache is present, the command will skip with 0 exit code but repo update still needs to be performed\n            raise(Error, output) if output.include?(\"already exists with the same configuration\")\n          end)\n        rescue Error => e\n          if e.message.include?(\"already exists\")\n            log(\"helm chart repo already exists, updating\", :warn)\n            return puts(run_helm(%W[repo update #{name}]))\n          end",
    "comment": "Add helm chart repository  @param [String] name @param [String] url @return [void]",
    "label": "",
    "id": "3139"
  },
  {
    "raw_code": "def add_gitlab_helm_chart(sha = nil)\n          return package_chart(sha) if sha\n\n          add_helm_chart(GITLAB_CHART_PREFIX, GITLAB_CHART_URL)\n          \"#{GITLAB_CHART_PREFIX}/gitlab\"\n        end",
    "comment": "Add helm chart and return reference  @param [String] sha fetch and package helm chart using specific repo sha @return [String] chart reference or path to packaged chart tgz",
    "label": "",
    "id": "3140"
  },
  {
    "raw_code": "def upgrade(name, chart, namespace:, timeout:, values:, args: [])\n          log(\"Upgrading helm release '#{name}' in namespace '#{namespace}'\", :info)\n          puts run_helm([\n            \"upgrade\", \"--install\", name, chart,\n            \"--namespace\", namespace,\n            \"--timeout\", timeout,\n            \"--values\", \"-\",\n            \"--wait\",\n            *args\n          ], values)\n        end",
    "comment": "Run helm upgrade command with --install argument  @param [String] name deployment name @param [String] chart helm chart reference @param [String] namespace deployment namespace @param [String] timeout timeout value like 5s, 10m @param [String] values yml string with helm values @param [Array] args extra arguments to pass to command @return [void]",
    "label": "",
    "id": "3141"
  },
  {
    "raw_code": "def uninstall(name, namespace:, timeout:)\n          log(\"Uninstalling helm release '#{name}' in namespace '#{namespace}'\", :info)\n          puts run_helm(%W[uninstall #{name} --namespace #{namespace} --timeout #{timeout} --wait])\n        end",
    "comment": "Uninstall helm release  @param [String] name @param [String] namespace @param [String] timeout @return [void]",
    "label": "",
    "id": "3142"
  },
  {
    "raw_code": "def status(name, namespace:)\n          run_helm(%W[status #{name} --namespace #{namespace}])\n        rescue Error => e\n          e.message.include?(\"release: not found\") ? nil : raise(e)\n        end",
    "comment": "Display status of helm release  @param [String] name @param [String] namespace @return [<String, nil>] status of helm release or nil if release is not found",
    "label": "",
    "id": "3143"
  },
  {
    "raw_code": "def repository_cache\n          @repository_cache ||= ENV[REPOSITORY_CACHE_VARIABLE_NAME] || \"\"\n        end",
    "comment": "Custom repository cache folder  @return [String]",
    "label": "",
    "id": "3144"
  },
  {
    "raw_code": "def tmp_dir\n          Helpers::Utils.tmp_dir\n        end",
    "comment": "Temporary directory for helm chart  @return [String]",
    "label": "",
    "id": "3145"
  },
  {
    "raw_code": "def package_chart(sha)\n          log(\"Packaging chart for git sha '#{sha}'\", :info)\n          chart_dir = fetch_chart_repo(sha)\n          chart_tar = \"gitlab-#{sha}.tgz\"\n          cached_chart_tar = File.join(repository_cache, chart_tar)\n\n          if repository_cache.present? && File.exist?(cached_chart_tar)\n            puts \"Cached version of chart found at #{cached_chart_tar}, skipping packaging\"\n            return cached_chart_tar\n          end",
    "comment": "Create chart package from specific chart repo sha  @param [String] sha @return [String] path to package",
    "label": "",
    "id": "3146"
  },
  {
    "raw_code": "def fetch_chart_repo(sha)\n          uri = URI(\"#{GITLAB_CHART_PROJECT_URL}/-/archive/#{sha}/gitlab-#{sha}.tar\")\n          res = Net::HTTP.get_response(uri)\n          raise \"Failed to download chart, got response code: #{res.code}\" unless res.code == \"200\"\n\n          tar = File.join(tmp_dir, \"gitlab-#{sha}.tar\").tap { |path| File.write(path, res.body) }\n          execute_shell([\"tar\", \"-xf\", tar, \"-C\", tmp_dir])\n          File.join(tmp_dir, \"gitlab-#{sha}\")\n        end",
    "comment": "Download and extract helm chart  @param [String] sha @return [String] path to extracted repo",
    "label": "",
    "id": "3147"
  },
  {
    "raw_code": "def run_helm(cmd, stdin = nil)\n          helm_cmd = [\"helm\", *cmd]\n          helm_cmd.push(\"--repository-cache\", repository_cache) if repository_cache.present?\n          execute_shell(helm_cmd, stdin_data: stdin)\n        rescue Helpers::Shell::CommandFailure => e\n          raise(Error, e.message)\n        end",
    "comment": "Run helm command  @param [Array] cmd @return [String]",
    "label": "",
    "id": "3148"
  },
  {
    "raw_code": "def get_namespace\n          execute_shell([\"kubectl\", \"get\", \"namespace\", namespace])\n        rescue Helpers::Shell::CommandFailure => e\n          raise(Error, e.message)\n        end",
    "comment": "Get namespace data  @return [String]",
    "label": "",
    "id": "3149"
  },
  {
    "raw_code": "def create_namespace\n          execute_shell([\"kubectl\", \"create\", \"namespace\", namespace])\n        rescue Helpers::Shell::CommandFailure => e\n          raise(Error, e.message)\n        end",
    "comment": "Create namespace  @return [String] command output",
    "label": "",
    "id": "3150"
  },
  {
    "raw_code": "def create_resource(resource)\n          run_in_namespace(\"apply\", args: [\"-f\", \"-\"], stdin_data: resource.json)\n        end",
    "comment": "Create kubernetes resource  @param [Resources::Base] resource @return [String] command output",
    "label": "",
    "id": "3151"
  },
  {
    "raw_code": "def delete_resource(resource_type, resource_name, ignore_not_found: true)\n          run_in_namespace(\"delete\", resource_type, resource_name, args: [\n            \"--ignore-not-found=#{ignore_not_found}\", \"--wait\"\n          ])\n        end",
    "comment": "Remove kubernetes resource  @param [String] resource_type @param [String] resource_name @param [Boolean] ignore_not_found @return [String] command output",
    "label": "",
    "id": "3152"
  },
  {
    "raw_code": "def execute(pod_name, command, container: nil)\n          args = [\"--\", *command]\n          args.unshift(\"-c\", container) if container\n\n          run_in_namespace(\"exec\", get_pod_name(pod_name), args: args)\n        end",
    "comment": "Execute command in a pod  @param [String] pod full or part of pod name @param [Array] command @param [String] container @return [String]",
    "label": "",
    "id": "3153"
  },
  {
    "raw_code": "def pod(pod_name)\n          output = run_in_namespace(\"get\", \"pod\", pod_name, args: [\"--output\", \"json\"])\n          JSON.parse(output, symbolize_names: true)\n        end",
    "comment": "Get pod data  @param pod_name [String] @return [Hash]",
    "label": "",
    "id": "3154"
  },
  {
    "raw_code": "def pod_logs(pods, since: \"1h\", containers: \"default\")\n          pod_data = JSON.parse(all_pods)[\"items\"]\n            .select { |pod| pods.empty? || pods.any? { |p| pod.dig(\"metadata\", \"name\").include?(p) } }\n            .each_with_object({}) { |pod, hash| hash[pod.dig(\"metadata\", \"name\")] = pod.slice(\"metadata\", \"spec\") }\n\n          if pod_data.empty?\n            raise Error, \"No pods matched: #{pods.join(', ')}\" unless pods.empty?\n\n            raise Error, \"No pods found in namespace '#{namespace}'\"\n          end",
    "comment": "Get pod logs  @param [Array<String>] pods @param [String] since @param [String] containers @return [Hash<String, String>]",
    "label": "",
    "id": "3155"
  },
  {
    "raw_code": "def top_pods\n          args = [\"--no-headers\", \"--containers\"]\n\n          output = run_in_namespace(\"top\", \"pods\", args: args).split(\"\\n\").map { |line| line.strip.split(/\\s+/) }\n          unless output.all? { |row| row.length == 4 }\n            raise Error, \"Unexpected top pods output: #{output}\\nExpected row format: POD_NAME CONTAINER CPU MEMORY\"\n          end",
    "comment": "Get resource consumption from top pods command  @return [Hash<Array>]",
    "label": "",
    "id": "3156"
  },
  {
    "raw_code": "def events(json_format: false)\n          args = [\"--sort-by=lastTimestamp\"]\n          args << \"--output=json\" if json_format\n          run_in_namespace(\"get\", \"events\", args: args)\n        end",
    "comment": "Get events  @param [Boolean] json_format @return [String]",
    "label": "",
    "id": "3157"
  },
  {
    "raw_code": "def patch(resource_type, resource_name, patch_data, patch_type: 'merge')\n          run_in_namespace(\"patch\", resource_type, resource_name, args: [\n            \"--type=#{patch_type}\",\n            \"-p\", patch_data\n          ])\n        end",
    "comment": "Patch kubernetes resource  @param [String] resource_type @param [String] resource_name @param [String] patch_data @param [String] patch_type default: 'merge' @return [String] command output",
    "label": "",
    "id": "3158"
  },
  {
    "raw_code": "def all_pods(output: \"json\")\n          run_in_namespace(\"get\", \"pods\", args: [\"--output\", output])\n        end",
    "comment": "Get all pods in namespace  @param [String] output --output type @return [String]",
    "label": "",
    "id": "3159"
  },
  {
    "raw_code": "def get_pod_name(name)\n          pod_name = all_pods(output: \"jsonpath={.items[*].metadata.name}\")\n            .split(\" \")\n            .find { |pod_name| pod_name.include?(name) }\n          raise Error, \"Pod '#{name}' not found\" unless pod_name\n\n          pod_name\n        end",
    "comment": "Get full pod name  @param [String] name @return [String]",
    "label": "",
    "id": "3160"
  },
  {
    "raw_code": "def run_in_namespace(*action, args:, stdin_data: nil)\n          execute_shell([\"kubectl\", *action, \"-n\", namespace, *args], stdin_data: stdin_data)\n        rescue Helpers::Shell::CommandFailure => e\n          raise(Error, e.message)\n        end",
    "comment": "Run kubectl command in namespace  @param [Array] *action @param [Array] args @param [String] stdin_data @return [String]",
    "label": "",
    "id": "3161"
  },
  {
    "raw_code": "def initialize(resource_name, key, data)\n            super(resource_name)\n\n            @key = key\n            @data = data\n          end",
    "comment": "Generic kubernetes secret resource  @param [String] resource_name @param [String] key @param [String] data",
    "label": "",
    "id": "3162"
  },
  {
    "raw_code": "def json\n            @json ||= {\n              kind: \"Secret\",\n              apiVersion: \"v1\",\n              metadata: {\n                name: resource_name\n              },\n              data: {\n                key => Base64.encode64(data)\n              }\n            }.to_json\n          end",
    "comment": "Secret kubernetes resource  @return [String] JSON representation of the secret resource",
    "label": "",
    "id": "3163"
  },
  {
    "raw_code": "def json\n            raise(NoMethodError)\n          end",
    "comment": "Kubectl resource json  @return [String]",
    "label": "",
    "id": "3164"
  },
  {
    "raw_code": "def ==(other)\n            self.class == other.class && json == other.json\n          end",
    "comment": "Object comparator  @param [Base] other @return [Booelan]",
    "label": "",
    "id": "3165"
  },
  {
    "raw_code": "def initialize(resource_name, key, value)\n            super(resource_name)\n\n            @key = key\n            @value = value\n          end",
    "comment": "Generic kubernetes configmap resource  @param [String] resource_name @param [String] key @param [String] value",
    "label": "",
    "id": "3166"
  },
  {
    "raw_code": "def json\n            @json ||= {\n              kind: \"ConfigMap\",\n              apiVersion: \"v1\",\n              metadata: {\n                name: resource_name\n              },\n              data: {\n                key => value\n              }\n            }.to_json\n          end",
    "comment": "Configmap kubernetes resource  @return [String] JSON representation of the configmap resource",
    "label": "",
    "id": "3167"
  },
  {
    "raw_code": "def kubectl\n            @kubectl ||= Client.new(namespace)\n          end",
    "comment": "Kubectl client  @return [Gitlab::Orchestrator::Kubectl::Client]",
    "label": "",
    "id": "3168"
  },
  {
    "raw_code": "def logger\n            @logger ||= Logger.new(output_file)\n          end",
    "comment": "Logger instance used by background process  @return [Logger]",
    "label": "",
    "id": "3169"
  },
  {
    "raw_code": "def setup\n            setup_output_dir\n            check_namespace\n            setup_signal_handlers\n          end",
    "comment": "Run initial setup  @return [void]",
    "label": "",
    "id": "3170"
  },
  {
    "raw_code": "def setup_output_dir\n            FileUtils.mkdir_p(output_dir)\n            log(\" Created output directory: #{output_dir}\")\n          end",
    "comment": "Create output directory if it does not exist  @return [void]",
    "label": "",
    "id": "3171"
  },
  {
    "raw_code": "def check_namespace\n            kubectl.get_namespace\n          rescue Kubectl::Client::Error\n            log(\" Namespace '#{namespace}' does not exist\", :error)\n            exit 1\n          end",
    "comment": "Check if namespace exists  @return [void]",
    "label": "",
    "id": "3172"
  },
  {
    "raw_code": "def setup_signal_handlers\n            Signal.trap(SHUTDOWN_SIGNAL) do\n              puts \"Shutting down metrics collector\"\n\n              @shutdown_timeout.times do\n                break unless @collection_running\n\n                sleep 1\n              end",
    "comment": "Setup signal handler for graceful shutdown  @return [void]",
    "label": "",
    "id": "3173"
  },
  {
    "raw_code": "def run_background_process\n            pid = Process.fork\n\n            if pid\n              Helpers::Spinner.spin(\"creating background process\") do\n                log(\" Saved process pid #{pid} to #{Helpers::Utils.metrics_pid_file}\")\n              end",
    "comment": "Daemonize metrics collector process  @return [void]",
    "label": "",
    "id": "3174"
  },
  {
    "raw_code": "def run\n            loop do\n              begin\n                collect_metrics\n              rescue StandardError => e\n                logger.error(\"Error during metrics collection: #{e.message}\")\n              end",
    "comment": "Run metrics collector  @return [void]",
    "label": "",
    "id": "3175"
  },
  {
    "raw_code": "def get_pod_resources(pod_name)\n            containers = kubectl.pod(pod_name).dig(:spec, :containers) || []\n\n            containers.each_with_object({}) do |container, hsh|\n              name = container[:name]\n              requests = container.dig(:resources, :requests) || {}\n              limits = container.dig(:resources, :limits) || {}\n\n              hsh[name] = {\n                requests: { cpu: parse_cpu_limits(requests[:cpu]), memory: parse_memory_limits(requests[:memory]) },\n                limits: { cpu: parse_cpu_limits(limits[:cpu]), memory: parse_memory_limits(limits[:memory]) }\n              }\n            end",
    "comment": "Get resource definitions for all containers in given pod  @param pod_name [String] @return [Hash]",
    "label": "",
    "id": "3176"
  },
  {
    "raw_code": "def collect_metrics\n            @collection_running = true\n            timestamp = Time.now.to_i\n\n            logger.info(\"Collecting metrics...\")\n            pod_metrics = kubectl.top_pods\n\n            if pod_metrics.empty?\n              logger.warn(\"No pods found in namespace or no metrics available\")\n              return\n            end",
    "comment": "Collect pod resource metrics  @return [void]",
    "label": "",
    "id": "3177"
  },
  {
    "raw_code": "def save_metrics\n            File.write(metrics_file, JSON.pretty_generate(metrics_data))\n            logger.info(\"Saved metrics to #{metrics_file}\")\n          rescue StandardError => e\n            logger.error(\"Failed to save metrics: #{e.message}\")\n          end",
    "comment": "Save metrics file  @return [void]",
    "label": "",
    "id": "3178"
  },
  {
    "raw_code": "def parse_cpu_limits(cpu_limit_str)\n            case cpu_limit_str\n            when /(\\d+)m$/\n              ::Regexp.last_match(1).to_i\n            when /(\\d+)n$/\n              (::Regexp.last_match(1).to_f / 1_000_000).round\n            when /(\\d+\\.?\\d*)$/\n              (::Regexp.last_match(1).to_f * 1000).to_i\n            else\n              0\n            end",
    "comment": "Convert limit value to milicores integer or float  @param cpu_limit_str [String] @return [Number]",
    "label": "",
    "id": "3179"
  },
  {
    "raw_code": "def parse_memory_limits(memory_limit_str)\n            case memory_limit_str\n            when /(\\d+)Mi$/\n              ::Regexp.last_match(1).to_i\n            when /(\\d+)Ki$/\n              (::Regexp.last_match(1).to_f / 1024).round\n            when /(\\d+)Gi$/\n              (::Regexp.last_match(1).to_f * 1024).to_i\n            when /(\\d+)M$/\n              (::Regexp.last_match(1).to_f * 0.953674).round  # MB to MiB conversion\n            when /(\\d+)G$/\n              (::Regexp.last_match(1).to_f * 953.674).round   # GB to MiB conversion\n            when /(\\d+)$/\n              (::Regexp.last_match(1).to_f / 1048576).round   # Bytes to MiB conversion\n            else\n              0\n            end",
    "comment": "Convert memory limit value to MiB integer or float  @param memory_limit_str [String] @return [Number]",
    "label": "",
    "id": "3180"
  },
  {
    "raw_code": "def self.uninstall(name, cleanup_configuration:, timeout:)\n          helm = Helm::Client.new\n          namespace = cleanup_configuration.namespace\n\n          log(\"Performing full deployment cleanup\", :info, bright: true)\n          return log(\"Helm release '#{name}' not found, skipping\", :warn) unless helm.status(name, namespace: namespace)\n\n          Helpers::Spinner.spin(\"uninstalling helm release '#{name}'\") do\n            helm.uninstall(name, namespace: namespace, timeout: timeout)\n\n            log(\"Removing license secret\", :info)\n            puts cleanup_configuration.kubeclient.delete_resource(\"secret\", LICENSE_SECRET)\n          end",
    "comment": "Delete installation  @param [String] name @param [Configurations::Cleanup::Base] cleanup_configuration @param [String] timeout @return [void]",
    "label": "",
    "id": "3181"
  },
  {
    "raw_code": "def create\n          log(\"Creating CNG deployment '#{name}'\", :info, bright: true)\n          chart_reference = run_pre_deploy_setup\n          run_deploy(chart_reference)\n          run_post_deploy_setup\n        # Exit on error to not duplicate error messages and exit cleanly when kubectl or helm related errors are raised\n        rescue Kubectl::Client::Error, Helm::Client::Error\n          exit(1)\n        end",
    "comment": "Perform deployment with all the additional setup  @return [void]",
    "label": "",
    "id": "3182"
  },
  {
    "raw_code": "def component_version_values\n          @component_version_values ||= DefaultValues.component_ci_versions.map { |k, v| \"#{k}=#{v}\" }\n        end",
    "comment": "Specific component version values used in CI  @return [String]",
    "label": "",
    "id": "3183"
  },
  {
    "raw_code": "def kubeclient\n          @kubeclient ||= Kubectl::Client.new(namespace)\n        end",
    "comment": "Kubectl client instance  @return [Kubectl::Client]",
    "label": "",
    "id": "3184"
  },
  {
    "raw_code": "def helm\n          @helm ||= Helm::Client.new\n        end",
    "comment": "Helm client instance  @return [Helm::Client]",
    "label": "",
    "id": "3185"
  },
  {
    "raw_code": "def license\n          @license ||= ENV[\"QA_EE_LICENSE\"] || ENV[\"EE_LICENSE\"]\n        end",
    "comment": "Gitlab license  @return [String]",
    "label": "",
    "id": "3186"
  },
  {
    "raw_code": "def license_values\n          return {} unless license\n\n          {\n            global: {\n              extraEnv: {\n                GITLAB_LICENSE_MODE: \"test\",\n                CUSTOMER_PORTAL_URL: \"https://customers.staging.gitlab.com\"\n              }\n            },\n            gitlab: {\n              license: {\n                secret: LICENSE_SECRET\n              }\n            }\n          }\n        end",
    "comment": "Helm values for license secret  @return [Hash]",
    "label": "",
    "id": "3187"
  },
  {
    "raw_code": "def env_values\n          return {} if extra_env.empty?\n\n          env = extra_env.map { |e| e.split(\"=\") }.reject { |e| e.size != 2 }.to_h\n          return {} if env.empty?\n\n          {\n            global: {\n              extraEnv: env\n            }\n          }\n        end",
    "comment": "Additional environment variables for deployment  @return [Hash]",
    "label": "",
    "id": "3188"
  },
  {
    "raw_code": "def run_pre_deploy_setup\n          Helpers::Spinner.spin(\"running pre-deployment setup\") do\n            chart_reference = helm.add_gitlab_helm_chart(chart_sha)\n            create_namespace\n            create_license\n\n            configuration.run_pre_deployment_setup\n\n            chart_reference\n          end",
    "comment": "Execute pre-deployment setup which consists of: * chart setup * namespace and license creation * optional configuration specific pre-deploy setup  @return [String] chart reference",
    "label": "",
    "id": "3189"
  },
  {
    "raw_code": "def run_deploy(chart_reference)\n          args = [\"--atomic\"]\n          args.push(*component_version_values.flat_map { |v| [\"--set\", v] }) if ci\n          args.push(\"--set\", cli_values.join(\",\")) unless cli_values.empty?\n          values = DefaultValues.common_values(gitlab_domain)\n            .deep_merge(license_values)\n            .deep_merge(env_values)\n            .deep_merge(configuration.values)\n            .deep_stringify_keys\n            .to_yaml\n\n          Helpers::Spinner.spin(\"running helm deployment\") do\n            opts = {\n              namespace: namespace,\n              timeout: timeout,\n              values: values,\n              # remove --atomic on last attempt so failed deployment is not removed on failure\n              args: @deployment_attempts == retry_attempts ? args.reject { |a| a == \"--atomic\" } : args\n            }\n            helm.upgrade(name, chart_reference, **opts)\n          rescue Helm::Client::Error => e\n            @deployment_attempts += 1\n            handle_deploy_failure(e) if @deployment_attempts > retry_attempts\n\n            log(\"Deployment failed, retrying...\", :warn)\n            log(\"Error: #{e}\", :warn)\n            retry\n          end",
    "comment": "Run helm deployment  @param [String] chart_reference @return [void]",
    "label": "",
    "id": "3190"
  },
  {
    "raw_code": "def run_post_deploy_setup\n          Helpers::Spinner.spin(\"running post-deployment setup\") { configuration.run_post_deployment_setup }\n        end",
    "comment": "Execute post-deployment setup  @return [void]",
    "label": "",
    "id": "3191"
  },
  {
    "raw_code": "def create_namespace\n          log(\"Creating namespace '#{namespace}'\", :info)\n          puts kubeclient.create_namespace\n        rescue Kubectl::Client::Error => e\n          return log(\"namespace already exists, skipping\", :warn) if e.message.include?(\"already exists\")\n\n          raise(e)\n        end",
    "comment": "Create namespace  @return [void]",
    "label": "",
    "id": "3192"
  },
  {
    "raw_code": "def create_license\n          log(\"Creating gitlab license secret\", :info)\n          return log(\"`QA_EE_LICENSE|EE_LICENSE` variable is not set, skipping\", :warn) unless license\n\n          secret = Kubectl::Resources::Secret.new(LICENSE_SECRET, \"license\", license)\n          puts mask_secrets(kubeclient.create_resource(secret), [license, Base64.encode64(license)])\n        end",
    "comment": "Create gitlab license  @return [void]",
    "label": "",
    "id": "3193"
  },
  {
    "raw_code": "def handle_deploy_failure(error)\n          log(\"Helm deployment failed!\", :error)\n          log(\"For more information on troubleshooting failures, see: '#{TROUBLESHOOTING_LINK}'\", :warn)\n\n          events = get_warning_events\n          if events\n            log(\"Following events of Warning type present in cluster:\", :warn)\n            log(events)\n          end",
    "comment": "Handle helm upgrade failure  @param [StandardError] error @return [void]",
    "label": "",
    "id": "3194"
  },
  {
    "raw_code": "def get_warning_events\n          items = JSON.parse(kubeclient.events(json_format: true), symbolize_names: true)[:items]\n\n          events = items\n            .select { |item| item[:kind] == \"Event\" && item[:type] == \"Warning\" }\n            .reject { |item| IGNORED_EVENTS.include?(item[:reason]) }\n            .map do |item|\n              object = item[:involvedObject]\n\n              {\n                **item.slice(:type, :reason),\n                name: \"#{object[:kind]}/#{object[:name]}\",\n                message: item[:message]\n              }\n            end",
    "comment": "Get cluster events with warning type  @return [String]",
    "label": "",
    "id": "3195"
  },
  {
    "raw_code": "def resource_values(preset_name)\n            raise ArgumentError, \"'#{preset_name}' is not a valid preset name\" unless PRESETS.include?(preset_name)\n\n            send(preset_name) # rubocop:disable GitlabSecurity/PublicSend -- send with user input is prevented by validating PRESETS\n          end",
    "comment": "Kubernetes resources values for given preset  @param [String] preset_name @return [Hash]",
    "label": "",
    "id": "3196"
  },
  {
    "raw_code": "def default\n            @default ||= {\n              gitlab: {\n                webservice: {\n                  workerProcesses: 2,\n                  minReplicas: 1,\n                  resources: resources(\"1500m\", \"3Gi\")\n                },\n                sidekiq: {\n                  concurrency: 20,\n                  minReplicas: 1,\n                  resources: resources(\"900m\", \"2Gi\"),\n                  hpa: {\n                    cpu: { targetAverageValue: \"800m\" }\n                  }\n                },\n                kas: {\n                  minReplicas: 1,\n                  resources: resources(\"40m\", \"96Mi\")\n                },\n                # TODO: if limits are defined, git operations start failing in e2e tests, investigate potential cause\n                # https://gitlab.com/gitlab-org/quality/quality-engineering/team-tasks/-/issues/3699\n                \"gitlab-shell\": {\n                  minReplicas: 1,\n                  resources: resources(\"30m\", \"16Mi\", no_limits: true)\n                },\n                gitaly: {\n                  resources: resources(\"300m\", \"300Mi\")\n                },\n                toolbox: {\n                  resources: resources(\"50m\", \"128Mi\", no_limits: true)\n                }\n              },\n              registry: {\n                resources: resources(\"40m\", \"96Mi\"),\n                hpa: {\n                  minReplicas: 1,\n                  **cpu_utilization\n                }\n              },\n              minio: {\n                resources: resources(\"30m\", \"32Mi\")\n              },\n              \"nginx-ingress\": {\n                controller: {\n                  resources: resources(\"30m\", \"256Mi\")\n                }\n              },\n              postgresql: {\n                primary: {\n                  resources: resources(\"400m\", \"1Gi\")\n                }\n              },\n              redis: {\n                master: {\n                  resources: resources(\"50m\", \"16Mi\")\n                }\n              }\n            }\n          end",
    "comment": "Default resource preset for local deployments  @return [Hash]",
    "label": "",
    "id": "3197"
  },
  {
    "raw_code": "def high\n            @high ||= {\n              gitlab: {\n                webservice: {\n                  workerProcesses: 4,\n                  minReplicas: 1,\n                  # See https://docs.gitlab.com/charts/charts/gitlab/webservice/#memory-requestslimits\n                  resources: resources(3, \"5Gi\", 3, \"7Gi\"),\n                  hpa: cpu_utilization\n                },\n                sidekiq: {\n                  concurrency: 30,\n                  minReplicas: 1,\n                  resources: resources(\"1200m\", \"2Gi\"),\n                  hpa: cpu_utilization\n                },\n                kas: {\n                  minReplicas: 1,\n                  resources: resources(\"60m\", \"96Mi\"),\n                  hpa: cpu_utilization\n                },\n                # TODO: if limits are defined, git operations start failing in e2e tests, investigate potential cause\n                # https://gitlab.com/gitlab-org/quality/quality-engineering/team-tasks/-/issues/3699\n                \"gitlab-shell\": {\n                  minReplicas: 2,\n                  resources: resources(\"60m\", \"32Mi\", no_limits: true),\n                  hpa: cpu_utilization\n                },\n                gitaly: {\n                  resources: resources(\"400m\", \"384Mi\")\n                },\n                # Toolbox create peak load during startup but then consumes very little\n                # Set high limit value but don't request full amount to avoid unnecessary lock\n                toolbox: {\n                  resources: resources(\"50m\", \"128Mi\", no_limits: true)\n                }\n              },\n              registry: {\n                resources: resources(\"50m\", \"128Mi\"),\n                hpa: {\n                  minReplicas: 1,\n                  **cpu_utilization\n                }\n              },\n              minio: {\n                resources: resources(\"50m\", \"32Mi\")\n              },\n              \"nginx-ingress\": {\n                controller: {\n                  resources: resources(\"30m\", \"256Mi\")\n                }\n              },\n              postgresql: {\n                primary: {\n                  resources: resources(\"600m\", \"1536Mi\")\n                }\n              },\n              redis: {\n                master: {\n                  resources: resources(\"100m\", \"16Mi\")\n                }\n              }\n            }\n          end",
    "comment": "High resource preset optimized for running e2e tests in parallel  @return [Hash]",
    "label": "",
    "id": "3198"
  },
  {
    "raw_code": "def performance\n            high.deep_merge({\n              redis: {\n                master: {\n                  resources: resources(\"200m\", \"128Mi\")\n                }\n              }\n            })\n          end",
    "comment": "Resource preset optimized for performance tests  @return [Hash]",
    "label": "",
    "id": "3199"
  },
  {
    "raw_code": "def resources(cpu_r, memory_r, cpu_l = nil, memory_l = nil, no_limits: false)\n            cpu_l ||= cpu_r\n            memory_l ||= memory_r\n\n            {\n              requests: {\n                cpu: cpu_r,\n                memory: memory_r\n              }\n            }.tap do |definition|\n              next if no_limits\n\n              definition[:limits] = {\n                cpu: cpu_l,\n                memory: memory_l\n              }\n            end",
    "comment": "Kubernetes resources configuration  Set limits equal to requests by default for simplicity  @param [<String, Integer>] cpu_r @param [String] memory_r @param [<String, Integer>] cpu_l @param [String] memory_l @param [Boolean] no_limits if true, skip limit definition @return [Hash]",
    "label": "",
    "id": "3200"
  },
  {
    "raw_code": "def cpu_utilization\n            @cpu_utilization ||= {\n              cpu: {\n                targetType: \"Utilization\",\n                targetAverageUtilization: 90\n              }\n            }\n          end",
    "comment": "Common hpa cpu utilization config  It is recommended to keep value high to avoid scaling entirely To improve test stability, prefer vertical scaling over horizontal  @return [Hash]",
    "label": "",
    "id": "3201"
  },
  {
    "raw_code": "def common_values(domain)\n            {\n              global: {\n                hosts: {\n                  domain: domain,\n                  https: false\n                },\n                ingress: {\n                  configureCertmanager: false,\n                  tls: {\n                    enabled: false\n                  }\n                },\n                appConfig: {\n                  applicationSettingsCacheSeconds: 0,\n                  dependencyProxy: {\n                    enabled: true\n                  }\n                }\n              },\n              postgresql: {\n                metrics: { enabled: false },\n                primary: {\n                  extraEnvVars: [\n                    { name: \"POSTGRESQL_MAX_CONNECTIONS\", value: \"200\" }\n                  ]\n                }\n              },\n              gitlab: {\n                \"gitlab-exporter\": { enabled: false },\n                \"gitlab-shell\": {\n                  sshDaemon: 'gitlab-sshd',\n                  config: { proxyProtocol: true }\n                }\n              },\n              redis: { metrics: { enabled: false } },\n              prometheus: { install: false },\n              \"gitlab-runner\": { install: false },\n              installCertmanager: false\n            }\n          end",
    "comment": "Main common chart values  @param [String] domain @return [Hash]",
    "label": "",
    "id": "3202"
  },
  {
    "raw_code": "def component_ci_versions\n            {\n              \"gitlab.gitaly.image.repository\" => \"#{IMAGE_REPOSITORY}/gitaly\",\n              \"gitlab.gitaly.image.tag\" => with_semver_prefix(gitaly_version),\n              \"gitlab.gitlab-shell.image.repository\" => \"#{IMAGE_REPOSITORY}/gitlab-shell\",\n              \"gitlab.gitlab-shell.image.tag\" => with_semver_prefix(gitlab_shell_version),\n              \"gitlab.migrations.image.repository\" => \"#{IMAGE_REPOSITORY}/gitlab-toolbox-ee\",\n              \"gitlab.migrations.image.tag\" => toolbox_version,\n              \"gitlab.toolbox.image.repository\" => \"#{IMAGE_REPOSITORY}/gitlab-toolbox-ee\",\n              \"gitlab.toolbox.image.tag\" => toolbox_version,\n              \"gitlab.sidekiq.annotations.commit\" => commit_short_sha,\n              \"gitlab.sidekiq.image.repository\" => \"#{IMAGE_REPOSITORY}/gitlab-sidekiq-ee\",\n              \"gitlab.sidekiq.image.tag\" => sidekiq_version,\n              \"gitlab.webservice.annotations.commit\" => commit_short_sha,\n              \"gitlab.webservice.image.repository\" => \"#{IMAGE_REPOSITORY}/gitlab-webservice-ee\",\n              \"gitlab.webservice.image.tag\" => webservice_version,\n              \"gitlab.webservice.workhorse.image\" => \"#{IMAGE_REPOSITORY}/gitlab-workhorse-ee\",\n              \"gitlab.webservice.workhorse.tag\" => workhorse_version,\n              \"gitlab.kas.image.repository\" => \"#{IMAGE_REPOSITORY}/gitlab-kas\",\n              \"gitlab.kas.image.tag\" => with_semver_prefix(kas_version),\n              \"gitlab.registry.image.repository\" => \"#{IMAGE_REPOSITORY}/gitlab-container-registry\",\n              \"gitlab.registry.image.tag\" => registry_version\n            }\n          end",
    "comment": "Key value pairs for ci specific component version values  This is defined as key value pairs to allow constructing example cli args for easier reproducibility  @return [Hash]",
    "label": "",
    "id": "3203"
  },
  {
    "raw_code": "def with_semver_prefix(version)\n            return version unless version.match?(/^[0-9]+\\.[0-9]+\\.[0-9]+(-rc[0-9]+)?(-ee)?$/)\n\n            \"v#{version}\"\n          end",
    "comment": "Semver compatible version  @param [String] version @return [Boolean]",
    "label": "",
    "id": "3204"
  },
  {
    "raw_code": "def initialize(**args)\n            super(**args.slice(:namespace, :ci, :gitlab_domain))\n\n            @admin_password = args[:admin_password]\n            @admin_token = args[:admin_token]\n            @host_http_port = args[:host_http_port]\n            @host_ssh_port = args[:host_ssh_port]\n            @host_registry_port = args[:host_registry_port]\n            @resource_preset = args[:resource_preset]\n          end",
    "comment": "Instance of kind deployment configuration  @param **args [Hash] @option args [String] :namespace Namespace used for deployment @option args [Boolean] :ci Run for CI environment @option args [String] :gitlab_domain Custom gitlab domain @option args [String] :admin_password Initial password for admin user @option args [String] :admin_token Initial PAT token for admin user @option args [Integer] :host_http_port HTTP port for gitlab pages @option args [Integer] :host_ssh_port SSH port for gitlab @option args [Integer] :host_registry_port Registry port for gitlab @option args [String] :resource_preset resource preset name",
    "label": "",
    "id": "3205"
  },
  {
    "raw_code": "def run_pre_deployment_setup\n            create_initial_root_password\n            create_pre_receive_hook\n          end",
    "comment": "Run pre-deployment setup  @return [void]",
    "label": "",
    "id": "3206"
  },
  {
    "raw_code": "def run_post_deployment_setup\n            patch_registry_svc_port\n            create_root_token\n          end",
    "comment": "Run post-deployment setup  @return [void]",
    "label": "",
    "id": "3207"
  },
  {
    "raw_code": "def values\n            {\n              global: {\n                shell: {\n                  port: host_ssh_port\n                },\n                pages: {\n                  port: host_http_port\n                },\n                registry: {\n                  port: host_registry_port\n                },\n                initialRootPassword: {\n                  secret: ADMIN_PASSWORD_SECRET\n                },\n                gitaly: {\n                  hooks: {\n                    preReceive: {\n                      configmap: PRE_RECEIVE_HOOK_CONFIGMAP_NAME\n                    }\n                  }\n                }\n              },\n              \"nginx-ingress\": {\n                controller: {\n                  replicaCount: 1,\n                  minAavailable: 1,\n                  service: {\n                    type: \"NodePort\",\n                    nodePorts: {\n                      \"gitlab-shell\": Orchestrator::Kind::Cluster.host_port_mapping(host_ssh_port),\n                      http: Orchestrator::Kind::Cluster.host_port_mapping(host_http_port),\n                      registry: Orchestrator::Kind::Cluster.host_port_mapping(host_registry_port)\n                    }\n                  }\n                }\n              }\n            }.deep_merge(ResourcePresets.resource_values(resource_preset))\n          end",
    "comment": "Helm chart values specific to kind deployment  @return [Hash]",
    "label": "",
    "id": "3208"
  },
  {
    "raw_code": "def gitlab_url\n            @gitlab_url ||= URI(\"http://gitlab.#{gitlab_domain}:#{host_http_port}\").to_s\n          end",
    "comment": "Gitlab url  @return [String]",
    "label": "",
    "id": "3209"
  },
  {
    "raw_code": "def admin_pat_seed\n            <<~RUBY\n              Gitlab::Seeder.quiet do\n                User.find_by(username: 'root').tap do |user|\n                  params = {\n                    scopes: Gitlab::Auth.all_available_scopes.map(&:to_s),\n                    name: 'seeded-api-token'\n                  }\n\n                  user.personal_access_tokens.build(params).tap do |pat|\n                    pat.expires_at = 365.days.from_now\n                    pat.set_token(\"#{admin_token}\")\n                    pat.organization = Organizations::Organization.default_organization\n                    pat.save!\n                  end",
    "comment": "Token seed script for root user  @return [String]",
    "label": "",
    "id": "3210"
  },
  {
    "raw_code": "def create_initial_root_password\n            log(\"Creating admin user initial password secret\", :info)\n            secret = Kubectl::Resources::Secret.new(ADMIN_PASSWORD_SECRET, \"password\", admin_password)\n            puts mask_secrets(kubeclient.create_resource(secret), [admin_password, Base64.encode64(admin_password)])\n          end",
    "comment": "Create initial root password  @return [void]",
    "label": "",
    "id": "3211"
  },
  {
    "raw_code": "def create_pre_receive_hook\n            log(\"Creating pre-receive hook\", :info)\n            configmap = Kubectl::Resources::Configmap.new(PRE_RECEIVE_HOOK_CONFIGMAP_NAME, \"hook.sh\", PRE_RECEIVE_HOOK)\n            puts kubeclient.create_resource(configmap)\n          end",
    "comment": "Create pre-receive hook  @return [void]",
    "label": "",
    "id": "3212"
  },
  {
    "raw_code": "def create_root_token\n            log(\"Creating admin user personal access token\", :info)\n            puts mask_secrets(\n              kubeclient.execute(\"toolbox\", [\"gitlab-rails\", \"runner\", admin_pat_seed], container: \"toolbox\"),\n              [admin_token]\n            ).strip\n          rescue Kubectl::Client::Error => e\n            token_exists_error = \"duplicate key value violates unique constraint \" \\\n              \"\\\"index_personal_access_tokens_on_token_digest\\\"\"\n            return log(\"Token already exists, skipping!\", :warn) if e.message.include?(token_exists_error)\n\n            raise e\n          end",
    "comment": "Create admin user personal access token  @return [void]",
    "label": "",
    "id": "3213"
  },
  {
    "raw_code": "def skip_pre_deployment_setup!\n              @skip_pre_deployment_setup = true\n            end",
    "comment": "Do not run pre deployment setup  @return [void]",
    "label": "",
    "id": "3214"
  },
  {
    "raw_code": "def skip_post_deployment_setup!\n              @skip_post_deployment_setup = true\n            end",
    "comment": "Do not run post deployment setup  @return [void]",
    "label": "",
    "id": "3215"
  },
  {
    "raw_code": "def run_pre_deployment_setup\n            return if self.class.skip_pre_deployment_setup\n\n            raise(NoMethodError, \"run_pre_deployment_setup not implemented\")\n          end",
    "comment": "Steps to be executed before performing helm deployment  @return [void]",
    "label": "",
    "id": "3216"
  },
  {
    "raw_code": "def run_post_deployment_setup\n            return if self.class.skip_post_deployment_setup\n\n            raise(NoMethodError, \"run_post_deployment_setup not implemented\")\n          end",
    "comment": "Steps to be executed after helm deployment has been performed  @return [void]",
    "label": "",
    "id": "3217"
  },
  {
    "raw_code": "def values\n            {}\n          end",
    "comment": "Values hash containing the values to be passed to helm chart install  @return [Hash]",
    "label": "",
    "id": "3218"
  },
  {
    "raw_code": "def gitlab_url\n            \"http://gitlab.#{gitlab_domain}\"\n          end",
    "comment": "Deployed app url  @return [String]",
    "label": "",
    "id": "3219"
  },
  {
    "raw_code": "def kubeclient\n            @kubeclient ||= Kubectl::Client.new(namespace)\n          end",
    "comment": "Instance of {Kubectl::Client}  @return [Kubectl::Client]",
    "label": "",
    "id": "3220"
  },
  {
    "raw_code": "def remove_password_secret\n              log(\"Removing secret '#{Configurations::Kind::ADMIN_PASSWORD_SECRET}'\", :info)\n              puts kubeclient.delete_resource(\"secret\", Configurations::Kind::ADMIN_PASSWORD_SECRET)\n            end",
    "comment": "Remove admin password secret  @return [void]",
    "label": "",
    "id": "3221"
  },
  {
    "raw_code": "def remove_hook_configmap\n              log(\"Removing configmap '#{Configurations::Kind::PRE_RECEIVE_HOOK_CONFIGMAP_NAME}'\", :info)\n              puts kubeclient.delete_resource('configmap', Configurations::Kind::PRE_RECEIVE_HOOK_CONFIGMAP_NAME)\n            end",
    "comment": "Remove pre-receive hook configmap  @return [void]",
    "label": "",
    "id": "3222"
  },
  {
    "raw_code": "def run\n              raise(NoMethodError, \"run not implemented\")\n            end",
    "comment": "Run cleanup  @return [void]",
    "label": "",
    "id": "3223"
  },
  {
    "raw_code": "def kubeclient\n              @kubeclient ||= Kubectl::Client.new(namespace)\n            end",
    "comment": "Instance of {Kubectl::Client}  @return [Kubectl::Client]",
    "label": "",
    "id": "3224"
  },
  {
    "raw_code": "def destroy\n            log(\"Destroying cluster '#{CLUSTER_NAME}'\", :info, bright: true)\n\n            unless execute_shell(%w[kind get clusters]).include?(CLUSTER_NAME)\n              return log(\"Cluster not found, skipping!\", :warn)\n            end",
    "comment": "Destroy kind cluster  @param [String] name @return [void]",
    "label": "",
    "id": "3225"
  },
  {
    "raw_code": "def host_port_mapping(port)\n            yml = YAML.safe_load(File.read(kind_config_file_name))\n\n            yml[\"nodes\"].first[\"extraPortMappings\"].find { |mapping| mapping[\"hostPort\"] == port }[\"containerPort\"]\n          end",
    "comment": "Get configured port mapping  @param [Integer] port @return [Integer]",
    "label": "",
    "id": "3226"
  },
  {
    "raw_code": "def kind_config_file_name\n            File.join(Helpers::Utils.config_dir, \"kind-config.yml\")\n          end",
    "comment": "Kind cluster configuration file  @return [String]",
    "label": "",
    "id": "3227"
  },
  {
    "raw_code": "def helm_client\n          @helm_client ||= Helm::Client.new\n        end",
    "comment": "Helm client instance  @return [Helm::Client]",
    "label": "",
    "id": "3228"
  },
  {
    "raw_code": "def create_cluster\n          Helpers::Spinner.spin(\"performing cluster creation\") do\n            puts execute_shell([\n              \"kind\",\n              \"create\",\n              \"cluster\",\n              \"--name\", name,\n              \"--wait\", \"30s\",\n              \"--config\", ci ? ci_config : default_config\n            ])\n          end",
    "comment": "Create kind cluster  @return [void]",
    "label": "",
    "id": "3229"
  },
  {
    "raw_code": "def install_metrics_server\n          Helpers::Spinner.spin(\"installing metrics server\", raise_on_error: false) do\n            helm_client.add_helm_chart(METRICS_CHART_NAME, METRICS_CHART_URL)\n            helm_client.upgrade(\n              METRICS_CHART_NAME,\n              \"#{METRICS_CHART_NAME}/#{METRICS_CHART_NAME}\",\n              namespace: \"kube-system\",\n              timeout: \"1m\",\n              values: { \"args\" => [\"--kubelet-insecure-tls\"] }.to_yaml,\n              # use atomic to avoid leaving broken state if install fails\n              args: [\"--atomic\", \"--version\", METRICS_CHART_VERSION]\n            )\n          end",
    "comment": "Install metrics-server on cluster  Avoids \"FailedGetResourceMetric\" cluster errors and adds support for resource monitoring  @return [void]",
    "label": "",
    "id": "3230"
  },
  {
    "raw_code": "def update_server_url\n          return unless docker_hostname\n\n          Helpers::Spinner.spin(\"updating kind cluster server url\") do\n            cluster_name = \"kind-#{name}\"\n            server = execute_shell([\n              \"kubectl\", \"config\", \"view\",\n              \"-o\", \"jsonpath={.clusters[?(@.name == \\\"#{cluster_name}\\\")].cluster.server}\"\n            ])\n            uri = URI.parse(server).tap { |uri| uri.host = docker_hostname }\n            execute_shell(%W[kubectl config set-cluster #{cluster_name} --server=#{uri}])\n          end",
    "comment": "Update server url in kubeconfig for kubectl to work correctly with remote docker  @return [void]",
    "label": "",
    "id": "3231"
  },
  {
    "raw_code": "def cluster_exists?\n          execute_shell(%w[kind get clusters]).split(\"\\n\").any? { |line| line.strip == name }\n        end",
    "comment": "Check if cluster exists  @return [Boolean]",
    "label": "",
    "id": "3232"
  },
  {
    "raw_code": "def kind_config_file(config_yml)\n          self.class.kind_config_file_name.tap { |path| File.write(path, config_yml) }\n        end",
    "comment": "Create kind config file and return it's path  @param [String] config_yml @return [String]",
    "label": "",
    "id": "3233"
  },
  {
    "raw_code": "def ci_config\n          config_yml = <<~YML\n            apiVersion: kind.x-k8s.io/v1alpha4\n            kind: Cluster\n            networking:\n              apiServerAddress: \"0.0.0.0\"\n            # use google container registry as a mirror to avoid dockerhub rate limits\n            containerdConfigPatches:\n              - |-\n                [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"]\n                  endpoint = [\"https://mirror.gcr.io\"]\n            nodes:\n              - role: control-plane\n                kubeadmConfigPatches:\n                  - |\n                    kind: InitConfiguration\n                    nodeRegistration:\n                      kubeletExtraArgs:\n                        node-labels: \"ingress-ready=true\"\n                  - |\n                    kind: ClusterConfiguration\n                    apiServer:\n                      certSANs:\n                        - \"#{docker_hostname}\"\n                extraPortMappings:\n                  - containerPort: #{http_port}\n                    hostPort: #{host_http_port}\n                    listenAddress: \"0.0.0.0\"\n                  - containerPort: #{ssh_port}\n                    hostPort: #{host_ssh_port}\n                    listenAddress: \"0.0.0.0\"\n                  - containerPort: #{registry_port}\n                    hostPort: #{host_registry_port}\n                    listenAddress: \"0.0.0.0\"\n          YML\n\n          kind_config_file(config_yml)\n        end",
    "comment": "Temporary ci specific kind configuration file  @return [String] file path",
    "label": "",
    "id": "3234"
  },
  {
    "raw_code": "def default_config\n          template = ERB.new(<<~YML, trim_mode: \"-\")\n            kind: Cluster\n            apiVersion: kind.x-k8s.io/v1alpha4\n            nodes:\n            - role: control-plane\n              kubeadmConfigPatches:\n                - |\n                  kind: InitConfiguration\n                  nodeRegistration:\n                    kubeletExtraArgs:\n                      node-labels: \"ingress-ready=true\"\n            <% if docker_hostname -%>\n                - |\n                  kind: ClusterConfiguration\n                  apiServer:\n                    certSANs:\n                      - \"<%= docker_hostname %>\"\n            <% end -%>\n              extraPortMappings:\n                - containerPort: #{http_port}\n                  hostPort: #{host_http_port}\n                  listenAddress: \"0.0.0.0\"\n                - containerPort: #{ssh_port}\n                  hostPort: #{host_ssh_port}\n                  listenAddress: \"0.0.0.0\"\n                - containerPort: #{registry_port}\n                  hostPort: #{host_registry_port}\n                  listenAddress: \"0.0.0.0\"\n          YML\n\n          kind_config_file(template.result(binding))\n        end",
    "comment": "Temporary kind configuration file  @return [String] file path",
    "label": "",
    "id": "3235"
  },
  {
    "raw_code": "def http_port\n          @http_port ||= rand(30000..31000)\n        end",
    "comment": "Random http port to expose outside cluster  @return [Integer]",
    "label": "",
    "id": "3236"
  },
  {
    "raw_code": "def registry_port\n          @registry_port ||= 32495\n        end",
    "comment": "Set container registry port to expose outside cluster  @return [Integer]",
    "label": "",
    "id": "3237"
  },
  {
    "raw_code": "def ssh_port\n          @ssh_port ||= rand(31001..32000)\n        end",
    "comment": "Random ssh port to expose outside cluster  @return [Integer]",
    "label": "",
    "id": "3238"
  },
  {
    "raw_code": "def self.spin(spinner_message, done_message: \"done\", raise_on_error: true, print_block_output: true, &block)\n          new(\n            spinner_message,\n            raise_on_error: raise_on_error,\n            print_block_output: print_block_output\n          ).spin(done_message, &block)\n        end",
    "comment": "Run code block inside spinner  @param [String] spinner_message message to print when spinner starts @param [String] done_message message to print when spinner finishes @param [Boolean] raise_on_error raise error if block raises an error @param [Boolean] print_block_output print output generated within spinner block including nested spinners @param [Proc] &block @return [Object]",
    "label": "",
    "id": "3239"
  },
  {
    "raw_code": "def spin(done_message = \"done\")\n          original_stdout = start_spinner\n          result = yield\n          spinner_success(done_message, original_stdout)\n\n          result\n        rescue StandardError => e\n          spinner_error(original_stdout)\n          error_message = [\n            \"\",\n            colorize(\"=== block '#{spinner_message}' error ===\", :magenta),\n            colorize(e.message&.strip, error_color),\n            colorize(\"=== block '#{spinner_message}' error ===\", :magenta)\n          ].join(\"\\n\")\n          return result unless raise_on_error\n\n          raise(e)\n        ensure\n          puts_with_offset(original_stdout, $stdout.string) if print_block_output && !$stdout.string.empty?\n          puts_with_offset(original_stdout, error_message) if error_message\n\n          $stdout = original_stdout\n          spinner_stack.pop\n        end",
    "comment": "Run code block inside spinner and capture any output  Spinner doesn't work with blocks producing output while it spins so output is captured and printed after spinner is done  @param [String] done_message @return [Object]",
    "label": "",
    "id": "3240"
  },
  {
    "raw_code": "def error_color\n          @error_color ||= raise_on_error ? :red : :yellow\n        end",
    "comment": "Error message color  @return [Symbol]",
    "label": "",
    "id": "3241"
  },
  {
    "raw_code": "def success_mark\n          @success_mark ||= colorize(TTY::Spinner::TICK, :green)\n        end",
    "comment": "Success mark  @return [String]",
    "label": "",
    "id": "3242"
  },
  {
    "raw_code": "def error_mark\n          colorize(TTY::Spinner::CROSS, error_color)\n        end",
    "comment": "Error mark  @return [String]",
    "label": "",
    "id": "3243"
  },
  {
    "raw_code": "def spinner\n          @spinner ||= TTY::Spinner.new(\n            \"[:spinner] #{spinner_message} ...\",\n            format: :dots,\n            success_mark: success_mark,\n            error_mark: error_mark\n          )\n        end",
    "comment": "Spinner instance  @return [TTY::Spinner]",
    "label": "",
    "id": "3244"
  },
  {
    "raw_code": "def spinner_stack\n          self.class.instance_variable_get(:@spinner_stack) || self.class.instance_variable_set(:@spinner_stack, [])\n        end",
    "comment": "Currently running spinner stack  @return [Array]",
    "label": "",
    "id": "3245"
  },
  {
    "raw_code": "def nested_spinner?\n          spinner_stack.size > 1\n        end",
    "comment": "Is currently running spinner nested  @return [Boolean]",
    "label": "",
    "id": "3246"
  },
  {
    "raw_code": "def tty?\n          spinner.send(:tty?) && !nested_spinner? # rubocop:disable GitlabSecurity/PublicSend -- method is public on master branch but not released yet\n        end",
    "comment": "Check tty and nested spinner Nested spinners override $stdout which won't be tty so we need to return false  @return [Boolean]",
    "label": "",
    "id": "3247"
  },
  {
    "raw_code": "def start_spinner\n          original_stdout = $stdout\n          $stdout = StringIO.new\n          spinner_stack << self\n\n          spinner.auto_spin if tty?\n\n          original_stdout\n        end",
    "comment": "Start spinner instance and reassing stdout  @return [IO]",
    "label": "",
    "id": "3248"
  },
  {
    "raw_code": "def spinner_success(done_message, io)\n          return spinner.success(done_message) if tty?\n\n          spinner.stop if spinner.spinning?\n          puts_with_offset(io, \"[#{success_mark}] #{spinner_message} ... #{done_message}\")\n        end",
    "comment": "Return spinner success  @param [String] done_message @param [IO] io @return [void]",
    "label": "",
    "id": "3249"
  },
  {
    "raw_code": "def spinner_error(io)\n          done_message = colorize(\"failed\", error_color)\n          return spinner.error(done_message) if tty?\n\n          spinner.stop if spinner.spinning?\n          puts_with_offset(io, \"[#{error_mark}] #{spinner_message} ... #{done_message}\")\n        end",
    "comment": "Return spinner error  @param [StandardError] error @param [IO] io @return [void]",
    "label": "",
    "id": "3250"
  },
  {
    "raw_code": "def puts_with_offset(io, message)\n          offset = nested_spinner? ? \"  \" : \"\"\n          io.puts(message.split(\"\\n\").map { |line| \"#{offset}#{line}\" }.join(\"\\n\"))\n        end",
    "comment": "Print output with a leading offset for correct nested spinner display  @param [IO] io @param [String] message @return [void]",
    "label": "",
    "id": "3251"
  },
  {
    "raw_code": "def register_commands(klass)\n          raise \"#{klass} is not a Thor class\" unless klass < ::Thor\n\n          klass.commands.each do |name, command|\n            raise \"Tried to register command '#{name}' but the command already exists\" if commands[name]\n\n            # check if the method takes arguments\n            pass_args = klass.new.method(name).arity != 0\n\n            commands[name] = command\n            define_method(name) do |*args|\n              pass_args ? invoke(klass, name, *args) : invoke(klass, name)\n            end",
    "comment": "Register all public methods of Thor class inside another Thor class as commands  @param [Thor] klass @return [void]",
    "label": "",
    "id": "3252"
  },
  {
    "raw_code": "def execute_shell(cmd, stdin_data: nil, raise_on_failure: true, env: {}, live_output: false)\n          raise \"System commands must be given as an array of strings\" unless cmd.is_a?(Array)\n\n          if cmd.one? && cmd.first.match?(/\\s/)\n            raise \"System commands must be split into an array of space-separated values\"\n          end",
    "comment": "Execute shell command  @param [Array] command @param [String] stdin_data @param [Boolean] raise_on_failure @param [Hash] env @param [Boolean] live_output whether to stream output in real-time @return [<String, Array>] return command output and status if raise_on_failure is false",
    "label": "",
    "id": "3253"
  },
  {
    "raw_code": "def rainbow\n            @rainbow ||= Rainbow.new.tap { |rb| rb.enabled = true if @force_color }\n          end",
    "comment": "Global instance of rainbow colorization class  @return [Rainbow]",
    "label": "",
    "id": "3254"
  },
  {
    "raw_code": "def force_color!\n            @force_color = true\n          end",
    "comment": "Force color output  @return [Boolean]",
    "label": "",
    "id": "3255"
  },
  {
    "raw_code": "def log(message, type = :default, bright: false)\n          puts colorize(message, LOG_COLOR.fetch(type), bright: bright)\n        end",
    "comment": "Print colorized log message to stdout  @param [String] message @param [Symbol] type @param [Boolean] bright @return [void]",
    "label": "",
    "id": "3256"
  },
  {
    "raw_code": "def exit_with_error(message)\n          log(message, :error, bright: true)\n          exit 1\n        end",
    "comment": "Exit with non zero exit code and print error message  @param [String] message @return [void]",
    "label": "",
    "id": "3257"
  },
  {
    "raw_code": "def colorize(message, color, bright: false)\n          Output.rainbow.wrap(message)\n            .then { |m| bright ? m.bright : m }\n            .then { |m| color ? m.color(color) : m }\n        end",
    "comment": "Colorize message string and output to stdout  @param [String] message @param [<Symbol, nil>] color @param [Boolean] bright @return [String]",
    "label": "",
    "id": "3258"
  },
  {
    "raw_code": "def mask_secrets(message, secrets)\n          # Add explicit arg validation to avoid values leaking to error outputs in case of errors\n          raise ArgumentError, \"message must be a string\" unless message.is_a?(String)\n          raise ArgumentError, \"secrets must be an array of strings\" unless secrets.is_a?(Array) && secrets.all?(String)\n\n          message.gsub(/#{secrets.join('|')}/, \"*****\")\n        end",
    "comment": "Remove sensitive data from message  @param [String] message @param [Array<String>] secrets @return [String]",
    "label": "",
    "id": "3259"
  },
  {
    "raw_code": "def self.tmp_dir\n          @tmp_dir ||= Dir.mktmpdir(\"orchestrator\")\n        end",
    "comment": "Global tmp dir for file operations  @return [String]",
    "label": "",
    "id": "3260"
  },
  {
    "raw_code": "def self.config_dir\n          @config_dir ||= File.join(Dir.home, \".gitlab-orchestrator\").tap { |dir| FileUtils.mkdir_p(dir) }\n        end",
    "comment": "gitlab-orchestrator configuration directory  @return [String]",
    "label": "",
    "id": "3261"
  },
  {
    "raw_code": "def self.metrics_pid_file\n          @metrics_pid_file ||= File.join(config_dir, \"collector.pid\")\n        end",
    "comment": "Background process PID file for metrics collector  @return [String]",
    "label": "",
    "id": "3262"
  },
  {
    "raw_code": "def kubeclient\n          @kubeclient ||= Kubectl::Client.new(options[:namespace])\n        end",
    "comment": "Kubectl client  @return [Kubectl::Client]",
    "label": "",
    "id": "3263"
  },
  {
    "raw_code": "def symbolized_options\n          @symbolized_options ||= options.transform_keys(&:to_sym)\n        end",
    "comment": "Options hash with symbolized keys  @return [Hash]",
    "label": "",
    "id": "3264"
  },
  {
    "raw_code": "def method_added(name)\n              option :namespace,\n                desc: \"Deployment namespace\",\n                default: \"gitlab\",\n                type: :string,\n                aliases: \"-n\"\n              option :set,\n                desc: \"Optional helm chart values \" \\\n                  \"(can specify multiple or separate values with commas: key1=val1,key2=val2)\",\n                type: :string,\n                repeatable: true\n              option :ci,\n                desc: \"Use CI specific configuration\",\n                default: false,\n                type: :boolean\n              option :timeout,\n                desc: \"Timeout for deployment\",\n                default: \"10m\",\n                type: :string\n              option :chart_sha,\n                desc: \"Specific sha of GitLab chart repository, latest release version is used by default. \" \\\n                  \"Requires 'tar' executable to be installed.\",\n                type: :string\n              option :env,\n                desc: \"Extra environment variables to set for rails containers \" \\\n                  \"(can specify multiple or separate values with commas: env1=val1,env2=val2)\",\n                type: :string,\n                repeatable: true,\n                aliases: \"-e\"\n              option :retry,\n                desc: \"Max number of retries for failed deployment\",\n                default: 0,\n                type: :numeric\n              option :resource_preset,\n                desc: \"Kubernetes resource definition preset\",\n                default: Gitlab::Orchestrator::Deployment::ResourcePresets::DEFAULT,\n                type: :string,\n                enum: Gitlab::Orchestrator::Deployment::ResourcePresets::PRESETS\n\n              super\n            end",
    "comment": "Add common deployment options for each deployment command defined as public method  @param [String] name @return [void]",
    "label": "",
    "id": "3265"
  },
  {
    "raw_code": "def installation(name, configuration)\n            Orchestrator::Deployment::Installation.new(\n              name, configuration: configuration,\n              **symbolized_options.slice(:namespace, :set, :ci, :gitlab_domain, :timeout, :chart_sha, :env, :retry)\n            )\n          end",
    "comment": "Installation instance  @param [String] name @param [Deployment::Configurations::Base] configuration @return [Deployment::Installation]",
    "label": "",
    "id": "3266"
  },
  {
    "raw_code": "def print_deploy_args(configuration)\n            ci_components = Orchestrator::Deployment::DefaultValues\n              .component_ci_versions\n              .flat_map do |component, version|\n                [\"--set\", \"#{component}=#{version}\"]\n              end",
    "comment": "Print example of deployment arguments and all CI component arguments  @param [String] configuration deployment configuration name @return [void]",
    "label": "",
    "id": "3267"
  },
  {
    "raw_code": "def symbolized_options\n            @symbolized_options ||= super.tap do |opts|\n              next unless opts[:gitlab_domain].nil?\n\n              # merge default option lazily to not fetch ip_address_list every time class is loaded\n              opts.merge!({ gitlab_domain: \"#{Socket.ip_address_list.detect(&:ipv4_private?).ip_address}.nip.io\" })\n            end",
    "comment": "Populate options with default gitlab domain if missing  @return [Hash]",
    "label": "",
    "id": "3268"
  },
  {
    "raw_code": "def method_added(name)\n              option :ci,\n                desc: \"Use CI specific configuration\",\n                default: false,\n                type: :boolean\n              option :timeout,\n                desc: \"Timeout for instance creation\",\n                default: \"10m\",\n                type: :string\n              option :env,\n                desc: \"Extra environment variables to set for containers \" \\\n                  \"(can specify multiple or separate values with commas: env1=val1,env2=val2)\",\n                type: :string,\n                repeatable: true,\n                aliases: \"-e\"\n              option :retry,\n                desc: \"Max number of retries for failed instance creation\",\n                default: 0,\n                type: :numeric\n\n              super\n            end",
    "comment": "Add common instance options for each instance command defined as public method  @param [String] name @return [void]",
    "label": "",
    "id": "3269"
  },
  {
    "raw_code": "def installation(name, configuration)\n            Orchestrator::Instance::Installation.new(\n              name, configuration: configuration,\n              **symbolized_options.slice(:ci, :gitlab_domain, :timeout, :env, :retry)\n            )\n          end",
    "comment": "Installation instance  @param [String] name @param [Instance::Configurations::Base] configuration @return [Instance::Installation]",
    "label": "",
    "id": "3270"
  },
  {
    "raw_code": "def print_config_args(configuration)\n            cmd = [\"orchestrator\", \"create\", \"instance\", configuration]\n            cmd.push(*options[:env].flat_map { |opt| [\"--env\", opt] }) if options[:env]\n\n            log(\"Received --print-config-args option, printing example of all instance arguments!\", :warn)\n            log(\"To reproduce CI instance, run orchestrator with following arguments:\")\n            log(\"  #{cmd.join(' ')}\")\n          end",
    "comment": "Print example of instance configuration arguments and all CI component arguments  @param [String] configuration instance configuration name @return [void]",
    "label": "",
    "id": "3271"
  },
  {
    "raw_code": "def symbolized_options\n            @symbolized_options ||= super.tap do |opts|\n              next unless opts[:gitlab_domain].nil?\n\n              opts.merge!({ gitlab_domain: \"localhost\" })\n            end",
    "comment": "Populate options with default gitlab domain if missing  @return [Hash]",
    "label": "",
    "id": "3272"
  },
  {
    "raw_code": "def invoke_command(command, args = [], options = {})\n    command_instance.invoke(command, args, options)\n  end",
    "comment": "Invoke command with args  @param [String] command @param [Array] args @return [void]",
    "label": "",
    "id": "3273"
  },
  {
    "raw_code": "def expect_command_to_include_attributes(command, attributes)\n    expect(described_class.commands[command].to_h).to include(attributes)\n  end",
    "comment": "Expect command to have attributes  @param [String] command @param [Hash] attributes @return [void]",
    "label": "",
    "id": "3274"
  },
  {
    "raw_code": "def perform_before_hooks\n          log_browser_versions\n\n          # Perform app readiness check before continuing with the whole test suite\n          Tools::ReadinessCheck.perform(wait: 180)\n\n          # Initialize global api admin client\n          initialize_admin_api_client!\n          # Initialize global test user and it's api client\n          initialize_test_user!\n\n          if Runtime::Env.rspec_retried?\n            Runtime::Logger.info('Skipping further global hooks due to retry process')\n            return false\n          end",
    "comment": "Perform global setup  @return [Boolean] returns true if hooks were performed successfully",
    "label": "",
    "id": "3275"
  },
  {
    "raw_code": "def initialize_test_user!\n          return unless Runtime::Env.running_on_live_env?\n\n          Runtime::User::Store.initialize_user_api_client\n          Runtime::User::Store.initialize_test_user\n        end",
    "comment": "Initialize test user and it's api client before test execution for live environments  @return [void]",
    "label": "",
    "id": "3276"
  },
  {
    "raw_code": "def approval_configuration\n        parse_body(api_get_from(api_approval_configuration_path))\n      end",
    "comment": "Approval configuration  @return [Hash]",
    "label": "",
    "id": "3277"
  },
  {
    "raw_code": "def update_approval_configuration(configuration)\n        api_post_to(api_approval_configuration_path, configuration)\n      end",
    "comment": "Update approvals configuration MR: https://docs.gitlab.com/ee/api/merge_request_approvals.html#change-approval-configuration Project: https://docs.gitlab.com/ee/api/merge_request_approvals.html#change-configuration  @param [Hash] configuration @return [Hash]",
    "label": "",
    "id": "3278"
  },
  {
    "raw_code": "def fetch_approval_rules\n        parse_body(api_get_from(api_approval_rules_path))\n      end",
    "comment": "Approval rules  @return [Array<Hash>]",
    "label": "",
    "id": "3279"
  },
  {
    "raw_code": "def create_approval_rules\n        raise(\"Trying to create approval rules but no rules set!\") unless approval_rules\n\n        rule = { approvals_required: 1, name: \"Approval rule for mr #{title}\" }\n        rule[:user_ids] = approval_rules[:users].map(&:id) if approval_rules[:users]\n        rule[:group_ids] = approval_rules[:group].map(&:full_path) if approval_rules[:groups]\n\n        api_post_to(api_approvals_path, rule)\n      end",
    "comment": "Create approval rules  @return [Hash]",
    "label": "",
    "id": "3280"
  },
  {
    "raw_code": "def api_client\n        @api_client ||= user.api_client\n      end",
    "comment": "Api client  Api client is set as public for MergeRequestFromFork resource to use correct client  @return [Runtime::API::Client]",
    "label": "",
    "id": "3281"
  },
  {
    "raw_code": "def to_s\n        token\n      end",
    "comment": "Return token value when implicitly converting this object to string  @return [String]",
    "label": "",
    "id": "3282"
  },
  {
    "raw_code": "def add_member(user, access_level = AccessLevel::DEVELOPER)\n        Support::Retrier.retry_until do\n          QA::Runtime::Logger.info(%(Adding user #{user.username} to #{full_path} #{self.class.name}))\n          response = post Runtime::API::Request.new(api_client, api_members_path).url,\n            { user_id: user.id, access_level: access_level }\n          break true if response.code == QA::Support::API::HTTP_STATUS_CREATED\n          break true if response.body.include?('Member already exists')\n        end",
    "comment": "Add single user to group or project  @param [Resource::User] user @param [Integer] access_level",
    "label": "",
    "id": "3283"
  },
  {
    "raw_code": "def add_members(*users)\n        users.each do |user|\n          add_member(user)\n        end",
    "comment": "Add multiple users to group or project with default access level  @param [Array<Resource::User>] users",
    "label": "",
    "id": "3284"
  },
  {
    "raw_code": "def update_member(user, access_level: AccessLevel::DEVELOPER)\n        Support::Retrier.retry_until do\n          QA::Runtime::Logger.info(%(Updating user #{user.username} in #{full_path} #{self.class.name}))\n          response = put Runtime::API::Request.new(api_client, \"#{api_members_path}/#{user.id}\").url,\n            { access_level: access_level }\n          next true if success?(response.code)\n        end",
    "comment": "Update the access level for single user in a group or project  @param [Resource::User] user @param [Integer] access_level (default Developer)",
    "label": "",
    "id": "3285"
  },
  {
    "raw_code": "def update_members(*users, access_level: AccessLevel::DEVELOPER)\n        users.each do |user|\n          update_member(user, access_level: access_level)\n        end",
    "comment": "Update the access level for multiple users in a group or project  @param [Array<Resource::User>] users @param [Integer] access_level (default Developer)",
    "label": "",
    "id": "3286"
  },
  {
    "raw_code": "def repository_storage=(name)\n        raise ArgumentError, \"Please provide a valid repository storage name\" if name.to_s.empty?\n\n        @repository_storage = name\n      end",
    "comment": "Sets the project's repository storage This feature requires admin access so be sure to fabricate the project as an admin user, and add the metadata `:requires_admin` to the test it's used in.",
    "label": "",
    "id": "3287"
  },
  {
    "raw_code": "def wait_for_pipeline(**kwargs)\n        wait_until(sleep_interval: 1, message: \"Wait for pipeline with '#{kwargs}' to be available\") do\n          result = pipelines(auto_paginate: true, **kwargs)\n          next unless result.present? && result.size == 1\n\n          result.first\n        end",
    "comment": "Waits for a pipeline to be available with the attributes as specified.  @param [Hash] **kwargs optional query arguments, see: https://docs.gitlab.com/ee/api/pipelines.html#list-project-pipelines @return [Hash] the pipeline",
    "label": "",
    "id": "3288"
  },
  {
    "raw_code": "def runners(**kwargs)\n        auto_paginated_response(request_url(api_runners_path, **kwargs))\n      end",
    "comment": "Fetch project runners  @param [Hash] **kwargs optional query arguments, see: https://docs.gitlab.com/ee/api/runners.html#list-projects-runners @return [Array]",
    "label": "",
    "id": "3289"
  },
  {
    "raw_code": "def wait_for_pull_mirroring\n        mirror_succeeded = Support::Retrier.retry_until(\n          max_duration: 360,\n          raise_on_failure: false,\n          sleep_interval: 1\n        ) do\n          reload!\n          api_resource[:import_status] == \"finished\"\n        end",
    "comment": "Uses the API to wait until a pull mirroring update is successful (pull mirroring is treated as an import)",
    "label": "",
    "id": "3290"
  },
  {
    "raw_code": "def perform_housekeeping\n        Runtime::Logger.debug(\"Calling API endpoint #{api_housekeeping_path}\")\n\n        response = post(request_url(api_housekeeping_path), nil)\n\n        return if response.code == HTTP_STATUS_CREATED\n\n        raise(\n          ResourceQueryError,\n          \"Could not perform housekeeping. Request returned (#{response.code}): `#{response.body}`.\"\n        )\n      end",
    "comment": "Calls the API endpoint that triggers the backend service that performs repository housekeeping (garbage collection and similar tasks).",
    "label": "",
    "id": "3291"
  },
  {
    "raw_code": "def statistics\n        response = get(request_url(\"#{api_get_path}?statistics=true\"))\n        data = parse_body(response)\n\n        raise \"Could not get project usage statistics\" unless data.key?(:statistics)\n\n        data[:statistics]\n      end",
    "comment": "Gets project statistics.  @return [Hash] the project usage data including repository size.",
    "label": "",
    "id": "3292"
  },
  {
    "raw_code": "def comparable\n        reload! if api_response.nil?\n\n        api_resource.slice(\n          :name,\n          :path,\n          :description,\n          :tag_list,\n          :archived,\n          :issues_enabled,\n          :merge_request_enabled,\n          :wiki_enabled,\n          :jobs_enabled,\n          :snippets_enabled,\n          :shared_runners_enabled,\n          :request_access_enabled,\n          :avatar_url,\n          :created_at\n        )\n      end",
    "comment": "Return subset of fields for comparing projects  @return [Hash]",
    "label": "",
    "id": "3293"
  },
  {
    "raw_code": "def comparable\n        reload! if api_response.nil?\n\n        api_resource.slice(\n          :title\n        )\n      end",
    "comment": "Return subset of fields for comparing work items  @return [Hash]",
    "label": "",
    "id": "3294"
  },
  {
    "raw_code": "def fabricate!(*args, &prepare_block)\n          if Specs::Helpers::ContextSelector.dot_com? || Runtime::Env.personal_access_tokens_disabled?\n            return fabricate_via_browser_ui!(*args, &prepare_block)\n          end",
    "comment": "Force top level group creation via UI if test is executed on dot_com environment",
    "label": "",
    "id": "3295"
  },
  {
    "raw_code": "def query_parameters\n        super.merge({ with_projects: false })\n      end",
    "comment": "Parameters included in the query URL  @return [Hash]",
    "label": "",
    "id": "3296"
  },
  {
    "raw_code": "def remove_via_api!\n        Support::Retrier.retry_until(max_duration: 60, sleep_interval: 1, message: \"Waiting for branch #{branch_name} to be protected\") do\n          # We confirm it exists before removal because there's no creation event when the default branch is automatically protected by GitLab itself, and there's a slight delay between creating the repo and protecting the default branch\n          exists?\n        end",
    "comment": "Remove the branch protection after confirming that it exists",
    "label": "",
    "id": "3297"
  },
  {
    "raw_code": "def fabricate_via_api!\n        unless api_support?\n          raise NotImplementedError, \"Resource #{self.class.name} does not support fabrication via the API!\"\n        end",
    "comment": "@return [String] the resource web url",
    "label": "",
    "id": "3298"
  },
  {
    "raw_code": "def exists?(**args)\n        request = Runtime::API::Request.new(api_client, api_get_path)\n        response = get(request.url, args)\n\n        response.code == HTTP_STATUS_OK\n      end",
    "comment": "Checks if a resource already exists  @return [Boolean] true if the resource returns HTTP status code 200",
    "label": "",
    "id": "3299"
  },
  {
    "raw_code": "def query_parameters\n        @query_parameters ||= {}\n      end",
    "comment": "Parameters included in the query URL  @return [Hash]",
    "label": "",
    "id": "3300"
  },
  {
    "raw_code": "def api_get\n        process_api_response(parse_body(api_get_from(api_get_path))).tap do\n          # Record method that was used to create certain resource\n          #  :get - resource already existed in GitLab instance and was fetched via get request\n          #  :post - resource was created from scratch using post request\n          #  :put - resource was created from scratch using put request\n          @api_fabrication_http_method ||= :get\n        end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables -- QA::Resource::Base specific implementation",
    "label": "",
    "id": "3301"
  },
  {
    "raw_code": "def api_get_from(get_path, q_params: query_parameters)\n        path = \"#{get_path}#{query_parameters_to_string(q_params)}\"\n        request = Runtime::API::Request.new(api_client, path)\n        response = get(request.url)\n\n        if response.code == HTTP_STATUS_NOT_FOUND\n          raise(ResourceNotFoundError, <<~MSG.strip)\n            Resource at #{request.mask_url} could not be found (#{response.code}): `#{response}`.\n            #{QA::Support::Loglinking.failure_metadata(response.headers[:x_request_id])}\n          MSG\n        elsif !success?(response.code)\n          raise(InternalServerError, <<~MSG.strip)\n            Failed to GET #{request.mask_url} - (#{response.code}): `#{response}`.\n            #{QA::Support::Loglinking.failure_metadata(response.headers[:x_request_id])}\n          MSG\n        end",
    "comment": "TODO: remove global query_parameters so this helper method does not depend on global variable It assumes that all resource related might have a common query which often is not the case",
    "label": "",
    "id": "3302"
  },
  {
    "raw_code": "def api_client\n        @api_client ||= Runtime::User::Store.default_api_client\n      end",
    "comment": "Api client to use for fabrications by default  @return [QA::Runtime::API::Client]",
    "label": "",
    "id": "3303"
  },
  {
    "raw_code": "def process_api_response(parsed_response)\n        self.api_response = parsed_response\n        self.api_resource = transform_api_resource(parsed_response.deep_dup)\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "3304"
  },
  {
    "raw_code": "def request_url(path, **opts)\n        Runtime::API::Request.new(api_client, path, **opts).url\n      end",
    "comment": "Get api request url  @param [String] path @return [String]",
    "label": "",
    "id": "3305"
  },
  {
    "raw_code": "def query_parameters_to_string(parameters)\n        parameters.each_with_object([]) do |(k, v), arr|\n          arr << \"#{k}=#{v}\"\n        end.join('&').prepend('?').chomp('?') # prepend `?` unless the string is blank\n      end",
    "comment": "Query parameters formatted as `?key1=value1&key2=value2...`  @param parameters [Hash<String, String>] @return [String]",
    "label": "",
    "id": "3306"
  },
  {
    "raw_code": "def wait_for_resource_availability(resource_web_url)\n        return unless Runtime::Address.valid?(resource_web_url)\n\n        Support::Retrier.retry_until(sleep_interval: 3, max_attempts: 5, raise_on_failure: false) do\n          # Until path based routing is supported in cells authenticated get requests are required\n          response_check = QA::Runtime::Env.running_against_cell? ? api_get_from(api_get_path) : get(resource_web_url)\n          Runtime::Logger.debug(\"Resource availability check for #{resource_web_url} ... #{response_check.code}\")\n          response_check.code == HTTP_STATUS_OK\n        end",
    "comment": "Given a URL, wait for the given URL to return 200 @param [String] resource_web_url the URL to check @example wait_for_resource_availability('https://gitlab.com/api/v4/projects/1234') @example wait_for_resource_availability(resource_web_url(create(:issue)))",
    "label": "",
    "id": "3307"
  },
  {
    "raw_code": "def api_get\n        process_api_response(\n          api_post_to(\n            api_get_path,\n            <<~GQL\n                query {\n                  issue(id: \"gid://gitlab/Issue/#{issue.id}\") {\n                    designCollection {\n                      design(filename: \"#{filename}\") {\n                        id\n                        fullPath\n                        image\n                        filename\n                      }\n                    }\n                  }\n                }\n            GQL\n          )\n        )\n      end",
    "comment": "Fetch design  @return [Hash]",
    "label": "",
    "id": "3308"
  },
  {
    "raw_code": "def api_post_body\n        query = <<~GQL\n          mutation ($files: [Upload!]!, $projectPath: ID!, $iid: ID!) {\n            designManagementUpload(input: { files: $files, projectPath: $projectPath, iid: $iid }) {\n              designs {\n                id\n                fullPath\n                image\n                filename\n                webUrl\n              }\n            }\n          }\n        GQL\n        operations = {\n          query: query,\n          variables: {\n            files: nil,\n            projectPath: issue.project.full_path,\n            iid: issue.iid\n          }\n        }\n\n        {\n          operations: JSON.dump(operations),\n          map: '{\"0\":[\"variables.files\"]}',\n          \"0\": ::File.new(filepath)\n        }\n      end",
    "comment": "Graphql mutation for design creation  @return [String]",
    "label": "",
    "id": "3309"
  },
  {
    "raw_code": "def api_post_to(post_path, post_body, args = {})\n        super(post_path, post_body, { content_type: 'multipart/form-data' })\n      end",
    "comment": "Override api_post_to method to add multipart request option  @param [String] post_path @param [Hash] post_body @param [Hash] args @return [Hash]",
    "label": "",
    "id": "3310"
  },
  {
    "raw_code": "def transform_api_resource(api_resource)\n        api_resource.key?(:designs) ? api_resource[:designs].first : api_resource\n      end",
    "comment": "Return first design from fabricated design array designManagementUpload mutation doesn't support returning single design  @param [Hash] api_resource @return [Hash]",
    "label": "",
    "id": "3311"
  },
  {
    "raw_code": "def init(&prepare_block)\n          new.tap(&prepare_block)\n        end",
    "comment": "Initialize new instance of class without fabrication  @yieldparam [self] instance of page object @return [self]",
    "label": "",
    "id": "3312"
  },
  {
    "raw_code": "def all(api_client = nil, **kwargs)\n          instance(api_client).all(**kwargs)\n        end",
    "comment": "All instances of the Resource  @return [Array<QA::Resource>]",
    "label": "",
    "id": "3313"
  },
  {
    "raw_code": "def fabricate_via_api_unless_fips!\n          if Runtime::Env.personal_access_tokens_disabled?\n            fabricate!\n          else\n            fabricate_via_api!\n          end",
    "comment": "TODO: remove, this method is redundant because normal fabricate! performs exactly the same check",
    "label": "",
    "id": "3314"
  },
  {
    "raw_code": "def uses_admin_api_client\n          define_method(:api_client) do\n            @api_client ||= Runtime::User::Store.admin_api_client\n          end",
    "comment": "Override api client definition to use admin api client  @return [void]",
    "label": "",
    "id": "3315"
  },
  {
    "raw_code": "def fabrication_type(resource, method)\n          return \"Built\" if method == :browser_ui || [:post, :put].include?(resource.api_fabrication_http_method)\n          return \"Retrieved\" if resource.api_fabrication_http_method == :get || resource.retrieved_from_cache\n\n          Runtime::Logger.warn(\"Resource fabrication http method has not been set properly, assuming :get value!\")\n          \"Built\"\n        end",
    "comment": "Fetch type of fabrication, either resource was built or fetched  @param [Resource] resource @param [Symbol] method @return [String]",
    "label": "",
    "id": "3316"
  },
  {
    "raw_code": "def attribute(name, &block)\n          (@attribute_names ||= []).push(name) # save added attributes\n\n          attr_writer(name)\n\n          define_method(name) do\n            return instance_variable_get(:\"@#{name}\") if instance_variable_defined?(:\"@#{name}\")\n\n            instance_variable_set(:\"@#{name}\", attribute_value(name, block))\n          end",
    "comment": "Define custom attribute  @param [Symbol] name @return [void]",
    "label": "",
    "id": "3317"
  },
  {
    "raw_code": "def attributes(*names)\n          names.each { |name| attribute(name) }\n        end",
    "comment": "Define multiple custom attributes  @param [Array] names @return [void]",
    "label": "",
    "id": "3318"
  },
  {
    "raw_code": "def all(**kwargs)\n        raise NotImplementedError\n      end",
    "comment": "To be overridden by Resource classes to return a list of all instances of the resource  @params [Hash] kwargs arguments to be used to query the API to search for resources with a specific criteria @return [Array]",
    "label": "",
    "id": "3319"
  },
  {
    "raw_code": "def ==(other)\n        other.is_a?(self.class) && comparable == other.comparable\n      end",
    "comment": "Object comparison  @param [QA::Resource::Base] other @return [Boolean]",
    "label": "",
    "id": "3320"
  },
  {
    "raw_code": "def inspect\n        JSON.pretty_generate(comparable)\n      end",
    "comment": "Override inspect for a better rspec failure diff output  @return [String]",
    "label": "",
    "id": "3321"
  },
  {
    "raw_code": "def comparable\n        raise(\"comparable method needs to be implemented in order to compare resources via '=='\")\n      end",
    "comment": "Custom resource comparison logic using resource attributes from api_resource  @return [Hash]",
    "label": "",
    "id": "3322"
  },
  {
    "raw_code": "def all_attributes\n        @all_attributes ||= self.class.ancestors\n                                .select { |clazz| clazz <= QA::Resource::Base }\n                                .map { |clazz| clazz.instance_variable_get(:@attribute_names) } # rubocop:disable Performance/FlatMap\n                                .flatten\n                                .compact\n      end",
    "comment": "Get all defined attributes across all parents  @return [Array<Symbol>]",
    "label": "",
    "id": "3323"
  },
  {
    "raw_code": "def close\n        api_put_to(api_put_path, state_event: \"close\")\n      end",
    "comment": "Close issue  @return [void]",
    "label": "",
    "id": "3324"
  },
  {
    "raw_code": "def related_merge_requests\n        parse_body(api_get_from(api_related_mrs_path))\n      end",
    "comment": "Related merge requests  @return [Array<Hash>]",
    "label": "",
    "id": "3325"
  },
  {
    "raw_code": "def comparable\n        reload! if api_response.nil?\n\n        api_resource.slice(\n          :state,\n          :description,\n          :type,\n          :title,\n          :labels,\n          :milestone,\n          :upvotes,\n          :downvotes,\n          :merge_requests_count,\n          :user_notes_count,\n          :due_date,\n          :has_tasks,\n          :task_status,\n          :confidential,\n          :discussion_locked,\n          :issue_type,\n          :task_completion_status,\n          :closed_at,\n          :created_at\n        )\n      end",
    "comment": "Return subset of fields for comparing issues  @return [Hash]",
    "label": "",
    "id": "3326"
  },
  {
    "raw_code": "def import_status\n        response = get(Runtime::API::Request.new(api_client, \"/bulk_imports/#{import_id}\").url)\n\n        unless response.code == HTTP_STATUS_OK\n          raise ResourceQueryError, \"Could not get import status. Request returned (#{response.code}): `#{response}`.\"\n        end",
    "comment": "Get import status  @return [String]",
    "label": "",
    "id": "3327"
  },
  {
    "raw_code": "def import_details\n        response = get(Runtime::API::Request.new(api_client, \"/bulk_imports/#{import_id}/entities\").url)\n\n        parse_body(response)\n      end",
    "comment": "Get import details  @return [Array]",
    "label": "",
    "id": "3328"
  },
  {
    "raw_code": "def fabricate!\n        fabricate_via_api!\n      end",
    "comment": "Initially we only support fabricate via API",
    "label": "",
    "id": "3329"
  },
  {
    "raw_code": "def fabricate_via_api!\n        api_get\n      rescue NoValueError => e\n        Runtime::Logger.info(\"Runner api_get exception caught and handled: #{e}\")\n        # Start container on initial fabrication and populate all attributes once id is known\n        # see: https://docs.gitlab.com/ee/api/runners.html#get-runners-details\n        start_container_and_register\n        # Temporary workaround for https://gitlab.com/gitlab-org/gitlab/-/issues/409089\n        Support::Retrier.retry_on_exception(max_attempts: 6, sleep_interval: 10,\n          message: \"Retrying GET for runners/:id\"\n        ) do\n          api_get\n        end",
    "comment": "Start container and register runner Fetch via API and populate attributes ",
    "label": "",
    "id": "3330"
  },
  {
    "raw_code": "def resource_web_url(resource)\n        super\n      rescue ResourceURLMissingError\n        # this particular resource does not expose a web_url property\n      end",
    "comment": "Resource web url  @param [Hash] resource @return [String]",
    "label": "",
    "id": "3331"
  },
  {
    "raw_code": "def api_post_body\n        {\n          name: title,\n          color: color,\n          description: description\n        }\n      end",
    "comment": "Params for label creation  @return [Hash]",
    "label": "",
    "id": "3332"
  },
  {
    "raw_code": "def comparable\n        reload! unless api_response\n\n        api_response.slice(\n          :name,\n          :description,\n          :description_html,\n          :color,\n          :text_color,\n          :subscribed\n        )\n      end",
    "comment": "Return subset of fields for comparing labels  @return [Hash]",
    "label": "",
    "id": "3333"
  },
  {
    "raw_code": "def api_get_path\n        \"/graphql\"\n      end",
    "comment": "All GraphQL queries and mutations use the same path, `/graphql`  @return [String]",
    "label": "",
    "id": "3334"
  },
  {
    "raw_code": "def projects(auto_paginate: false)\n        response = if auto_paginate\n                     auto_paginated_response(request_url(\"#{api_get_path}/projects\", per_page: '100'))\n                   else\n                     parse_body(api_get_from(\"#{api_get_path}/projects\"))\n                   end",
    "comment": "Get group projects  @return [Array<QA::Resource::Project>]",
    "label": "",
    "id": "3335"
  },
  {
    "raw_code": "def labels(auto_paginate: false)\n        response = if auto_paginate\n                     auto_paginated_response(request_url(\"#{api_get_path}/labels\", per_page: '100'))\n                   else\n                     parse_body(api_get_from(\"#{api_get_path}/labels\"))\n                   end",
    "comment": "Get group labels  @return [Array<QA::Resource::GroupLabel>]",
    "label": "",
    "id": "3336"
  },
  {
    "raw_code": "def milestones(auto_paginate: false)\n        response = if auto_paginate\n                     auto_paginated_response(request_url(\"#{api_get_path}/milestones\", per_page: '100'))\n                   else\n                     parse_body(api_get_from(\"#{api_get_path}/milestones\"))\n                   end",
    "comment": "Get group milestones  @return [Array<QA::Resource::GroupMilestone>]",
    "label": "",
    "id": "3337"
  },
  {
    "raw_code": "def badges(auto_paginate: false)\n        response = if auto_paginate\n                     auto_paginated_response(request_url(\"#{api_get_path}/badges\", per_page: '100'))\n                   else\n                     parse_body(api_get_from(\"#{api_get_path}/badges\"))\n                   end",
    "comment": "Get group badges  @return [Array<QA::Resource::GroupBadge>]",
    "label": "",
    "id": "3338"
  },
  {
    "raw_code": "def runners(**kwargs)\n        auto_paginated_response(request_url(api_runners_path, **kwargs))\n      end",
    "comment": "Get group runners  @param [Hash] **kwargs optional query arguments, see: https://docs.gitlab.com/ee/api/runners.html#list-groups-runners @return [Array]",
    "label": "",
    "id": "3339"
  },
  {
    "raw_code": "def api_get_path\n        raise NotImplementedError\n      end",
    "comment": "API get path  @return [String]",
    "label": "",
    "id": "3340"
  },
  {
    "raw_code": "def api_post_path\n        '/groups'\n      end",
    "comment": "API post path  @return [String]",
    "label": "",
    "id": "3341"
  },
  {
    "raw_code": "def api_put_path\n        \"/groups/#{id}\"\n      end",
    "comment": "API put path  @return [String]",
    "label": "",
    "id": "3342"
  },
  {
    "raw_code": "def api_delete_path\n        \"/groups/#{id}\"\n      end",
    "comment": "API delete path  @return [String]",
    "label": "",
    "id": "3343"
  },
  {
    "raw_code": "def api_runners_path\n        \"#{api_get_path}/runners\"\n      end",
    "comment": "API path to GET runners See https://docs.gitlab.com/ee/api/runners.html#list-groups-runners  @return [String]",
    "label": "",
    "id": "3344"
  },
  {
    "raw_code": "def audit_events\n        api_get_from(\"#{api_get_path}/audit_events\")\n      end",
    "comment": "Get group audit events  @return [Array]",
    "label": "",
    "id": "3345"
  },
  {
    "raw_code": "def set_ip_restriction_range(ip_ranges)\n        put_body = { ip_restriction_ranges: ip_ranges }\n        api_put_to(api_put_path, put_body)\n      end",
    "comment": "Set IP restriction for group  @param ip_ranges [String] Comma-separated list of IP addresses or subnet masks to restrict group access @return [void]",
    "label": "",
    "id": "3346"
  },
  {
    "raw_code": "def ==(other)\n        other.is_a?(GroupBase) && comparable == other.comparable\n      end",
    "comment": "Object comparison Override to make sure we are comparing descendands of GroupBase  @param [QA::Resource::GroupBase] other @return [Boolean]",
    "label": "",
    "id": "3347"
  },
  {
    "raw_code": "def comparable\n        reload! if api_response.nil?\n\n        api_resource.slice(\n          :name,\n          :path,\n          :description,\n          :emails_disabled,\n          :lfs_enabled,\n          :mentions_disabled,\n          :project_creation_level,\n          :request_access_enabled,\n          :require_two_factor_authentication,\n          :share_with_group_lock,\n          :subgroup_creation_level,\n          :two_factor_grace_period\n          # TODO: Add back visibility comparison once https://gitlab.com/gitlab-org/gitlab/-/issues/331252 is fixed\n          # :visibility\n        )\n      end",
    "comment": "Return subset of fields for comparing groups  @return [Hash]",
    "label": "",
    "id": "3348"
  },
  {
    "raw_code": "def expires_at\n        @expires_at || (Time.now.utc.to_date + 2)\n      end",
    "comment": "Expire in 2 days just in case the token is created just before midnight",
    "label": "",
    "id": "3349"
  },
  {
    "raw_code": "def reviews\n        parse_body(api_get_from(api_reviewers_path))\n      end",
    "comment": "Get merge request reviews  @return [Array<Hash>]",
    "label": "",
    "id": "3350"
  },
  {
    "raw_code": "def notes\n        QA::Runtime::Logger.info(\"Getting comments from MR: #{api_merge_request_notes_path}\")\n\n        response = get(Runtime::API::Request.new(api_client, api_merge_request_notes_path).url)\n\n        unless response.code == HTTP_STATUS_OK\n          raise ResourceQueryError, \"Could not get comments form MR: (#{response.code}): `#{response}`.\"\n        end",
    "comment": "Get the merge request notes  @return [Array<Hash>]",
    "label": "",
    "id": "3351"
  },
  {
    "raw_code": "def approve\n        api_post_to(api_approve_path, {})\n      end",
    "comment": "Approve merge request  Due to internal implementation of api client, project needs to have setting 'Prevent approval by merge request creator' set to false since we use same user that created merge request which is set through approval configuration  @return [void]",
    "label": "",
    "id": "3352"
  },
  {
    "raw_code": "def comparable\n        reload! if api_response.nil?\n\n        api_resource.except(\n          :id,\n          :web_url,\n          :project_id,\n          :source_project_id,\n          :target_project_id,\n          :detailed_merge_status,\n          # we consider mr to still be the same even if users changed\n          :author,\n          :reviewers,\n          :assignees,\n          # these can differ depending on user fetching mr\n          :user,\n          :subscribed,\n          :first_contribution\n        ).merge({ references: api_resource[:references].except(:full) })\n      end",
    "comment": "Return subset of fields for comparing merge requests  @return [Hash]",
    "label": "",
    "id": "3353"
  },
  {
    "raw_code": "def populate_target_and_source_if_required\n        return if @no_preparation\n\n        populate(:target) if create_target?\n        populate(:source)\n      end",
    "comment": "Create source and target and commits if necessary  @return [void]",
    "label": "",
    "id": "3354"
  },
  {
    "raw_code": "def create_target?\n        !(project.initialize_with_readme && target_branch == project.default_branch) && target_new_branch\n      end",
    "comment": "Check if target needs to be created  Return false if project was already initialized and mr target is default branch Return false if target_new_branch is explicitly set to false  @return [Boolean]",
    "label": "",
    "id": "3355"
  },
  {
    "raw_code": "def wait_until_mergable\n        return if Support::Waiter.wait_until(sleep_interval: 1, raise_on_failure: false, log: false) do\n          Runtime::Logger.debug(\"Merge Request detailed_merge_status: #{detailed_merge_status}\")\n\n          reload!.detailed_merge_status == 'mergeable'\n        end",
    "comment": "Wait until the merge request can be merged. Raises WaitExceededError if the MR can't be merged within 60 seconds  @return [void]",
    "label": "",
    "id": "3356"
  },
  {
    "raw_code": "def wait_for_preparation\n        return if Support::Waiter.wait_until(sleep_interval: 1, raise_on_failure: false, log: false) do\n          Runtime::Logger.debug(\"Merge Request detailed_merge_status: #{detailed_merge_status}\")\n\n          reload!.prepared_at && %w[preparing checking approvals_syncing].exclude?(detailed_merge_status)\n        end",
    "comment": "Wait until the merge request is prepared. Raises WaitExceededError if the MR is not prepared within 60 seconds https://docs.gitlab.com/ee/api/merge_requests.html#preparation-steps  @return [void]",
    "label": "",
    "id": "3357"
  },
  {
    "raw_code": "def expires_at\n        @expires_at || (Time.now.utc.to_date + 2)\n      end",
    "comment": "Expire in 2 days just in case the token is created just before midnight",
    "label": "",
    "id": "3358"
  },
  {
    "raw_code": "def fabricate_group!(group_name: nil)\n        Page::Main::Menu.perform(&:go_to_groups)\n        Page::Dashboard::Groups.perform do |groups|\n          groups.click_new_group\n\n          Page::Group::New.perform do |group_new|\n            group_new.click_create_group\n\n            group_new.set_path(group_name)\n\n            group_new.create\n          end",
    "comment": "Fabricate a Group using the UI from the Groups Dashboard page @param [String] group_name @return [Page::Group::New]",
    "label": "",
    "id": "3359"
  },
  {
    "raw_code": "def query_parameters\n        super.merge({ with_projects: false })\n      end",
    "comment": "Parameters included in the query URL  @return [Hash]",
    "label": "",
    "id": "3360"
  },
  {
    "raw_code": "def determine_full_path\n        determine_parent_group_paths(sandbox, path)\n      end",
    "comment": "Determine the path up to the root group.  This is equivalent to the full_path API attribute. We can't use the full_path attribute because it depends on the group being fabricated first, and we use this method to help _check_ if the group exists.  @param [QA::Resource::GroupBase] sandbox the immediate parent group of this group @param [String] path the path name of this group (the leaf, not the full path) @return [String]",
    "label": "",
    "id": "3361"
  },
  {
    "raw_code": "def determine_parent_group_paths(parent, path)\n        return \"#{parent.path}/#{path}\" unless parent.respond_to?(:sandbox)\n\n        determine_parent_group_paths(parent.sandbox, \"#{parent.path}/#{path}\")\n      end",
    "comment": "Recursively traverse the parents of this group up to the root group.  @param [QA::Resource::GroupBase] parent the immediate parent group @param [String] path the path traversed so far @return [String]",
    "label": "",
    "id": "3362"
  },
  {
    "raw_code": "def api_post_path\n        \"/projects/#{fork.id}/merge_requests\"\n      end",
    "comment": "Post path targeting fork project rather than target  @return [String]",
    "label": "",
    "id": "3363"
  },
  {
    "raw_code": "def api_client\n        @api_client ||= fork.api_client\n      end",
    "comment": "Api client for mr creations MR needs to be created using same api client used for fork creation to have the correct access rights  @return [Runtime::API::Client]",
    "label": "",
    "id": "3364"
  },
  {
    "raw_code": "def create_target?\n        false\n      end",
    "comment": "Target is upstream, in fork workflow it must not be populated  @return [Boolean]",
    "label": "",
    "id": "3365"
  },
  {
    "raw_code": "def comparable\n        reload! unless api_response\n\n        api_response.slice(\n          :title,\n          :description,\n          :state,\n          :due_date,\n          :start_date\n        )\n      end",
    "comment": "Return subset of fields for comparing milestones  @return [Hash]",
    "label": "",
    "id": "3366"
  },
  {
    "raw_code": "def github_client\n        @github_client ||= Octokit::Client.new(access_token: github_personal_access_token)\n      end",
    "comment": "Github client  @return [Octokit::Client]",
    "label": "",
    "id": "3367"
  },
  {
    "raw_code": "def api_delete_path\n        \"/users/#{id}?hard_delete=#{hard_delete_on_api_removal}\"\n      rescue NoValueError\n        \"/users/#{fetch_id(username)}?hard_delete=#{hard_delete_on_api_removal}\"\n      end",
    "comment": "TODO: implement separate delete method that only admin user can perform User resource can't delete itslef if it is using it's own pat",
    "label": "",
    "id": "3368"
  },
  {
    "raw_code": "def api_get_path\n        \"/user\"\n      end",
    "comment": "Default path to get full information on user object  @return [String]",
    "label": "",
    "id": "3369"
  },
  {
    "raw_code": "def get_user_ip_address(user_id)\n        raise \"Only admin can get user's ip address\" unless admin?\n\n        parse_body(api_get_from(\"/users/#{user_id}\"))[:last_sign_in_ip]\n      end",
    "comment": "Get users last sign in ip address  @param user_id [Integer] @return [String]",
    "label": "",
    "id": "3370"
  },
  {
    "raw_code": "def users(per_page: 100)\n        raise(\"This method can be called only on the Admin user!\") unless admin?\n\n        resp = get(Runtime::API::Request.new(api_client, '/users', per_page: per_page.to_s))\n        raise ResourceQueryError unless resp.code == Support::API::HTTP_STATUS_OK\n\n        parse_body(resp)\n      end",
    "comment": "Get all users  @param [Integer] per_page @return [Array<Hash>]",
    "label": "",
    "id": "3371"
  },
  {
    "raw_code": "def ldap_user?\n        @ldap_user\n      end",
    "comment": "User registered through LDAP protocol  @return [Boolean]",
    "label": "",
    "id": "3372"
  },
  {
    "raw_code": "def create_personal_access_token!(use_for_api_client: true)\n        user_id = begin\n          id\n        rescue NoValueError\n          nil\n        end",
    "comment": "Create new personal access token for user  @return [QA::Resource::PersonalAccessToken]",
    "label": "",
    "id": "3373"
  },
  {
    "raw_code": "def personal_access_token(revoked: false, active: true)\n        @personal_access_tokens.find { |pat| pat.revoked == revoked && pat.active == active }\n      end",
    "comment": "Get specific personal access token for user  @param [Boolean] revoked @param [Boolean] active @return [QA::Resource::PersonalAccessToken]",
    "label": "",
    "id": "3374"
  },
  {
    "raw_code": "def personal_access_tokens(revoked: false, active: true)\n        @personal_access_tokens.select { |pat| pat.revoked == revoked && pat.active == active }\n      end",
    "comment": "Get specific personal access tokens for user  @param [Boolean] revoked @param [Boolean] active @return [Array<QA::Resource::PersonalAccessToken>]",
    "label": "",
    "id": "3375"
  },
  {
    "raw_code": "def add_personal_access_token(pat)\n        return if @personal_access_tokens.any? { |p| p.id == pat.id }\n        raise \"Attempting to add token not belonging to this user\" if pat.user_id != id\n\n        @personal_access_tokens << pat\n      end",
    "comment": "Add personal access token to user  @param [QA::Resource::PersonalAccessToken] pat @return [void]",
    "label": "",
    "id": "3376"
  },
  {
    "raw_code": "def current_personal_access_token\n        api_client.personal_access_token\n      end",
    "comment": "Return personal access token currently used by user resource for all api operations  @return [String]",
    "label": "",
    "id": "3377"
  },
  {
    "raw_code": "def api_client\n        @api_client ||= Runtime::User::Store.admin_api_client || Runtime::User::Store.user_api_client\n      end",
    "comment": "Users can only be created by admin, use global admin api client if not explicitly set Still revert to user_api_client in order to perform get operation for fetching existing user  @return [QA::Runtime::API::Client]",
    "label": "",
    "id": "3378"
  },
  {
    "raw_code": "def comparable\n        [username, password]\n      end",
    "comment": "Compare users by username and password  @return [Array]",
    "label": "",
    "id": "3379"
  },
  {
    "raw_code": "def transform_api_resource(api_resource)\n        return api_resource if api_resource[:username] == username\n\n        path = begin\n          # /users/:id can't be used as default get path because it returns very limited response if admin token is\n          # not used\n          \"/users/#{id}\"\n        rescue NoValueError\n          # if id is not yet known, attempt to find the id based on username\n          \"/users/#{fetch_id(username)}\"\n        end",
    "comment": "Use id specific path in case api get action was not performed with the token that belongs to user of resource This will happen when user was created with admin token without option 'with_personal_access_token' set to true  @param api_resource [Hash] @return [Hash]",
    "label": "",
    "id": "3380"
  },
  {
    "raw_code": "def api_get_path\n        \"/groups/#{CGI.escape(group.full_path)}/badges/#{id}\"\n      end",
    "comment": "API get path  @return [String]",
    "label": "",
    "id": "3381"
  },
  {
    "raw_code": "def api_post_path\n        \"/groups/#{CGI.escape(group.full_path)}/badges\"\n      end",
    "comment": "API post path  @return [String]",
    "label": "",
    "id": "3382"
  },
  {
    "raw_code": "def api_post_body\n        {\n          link_url: link_url,\n          image_url: image_url\n        }\n      end",
    "comment": "Params for label creation  @return [Hash]",
    "label": "",
    "id": "3383"
  },
  {
    "raw_code": "def resource_web_url(_resource); end\n\n      protected\n\n      # Return subset of fields for comparing badges\n      #\n      # @return [Hash]\n      def comparable\n        reload! unless api_response\n\n        api_response.slice(\n          :name,\n          :link_url,\n          :image_url\n        )\n      end\n    end",
    "comment": "Override base method as this particular resource does not expose a web_url property  @param [Hash] resource @return [String]",
    "label": "",
    "id": "3384"
  },
  {
    "raw_code": "def api_comments_path\n        \"#{api_get_path}/notes\"\n      end",
    "comment": "Comments (notes) path  @return [String]",
    "label": "",
    "id": "3385"
  },
  {
    "raw_code": "def comments(auto_paginate: false, attempts: 0)\n        Runtime::Logger.debug(\"Fetching comments for #{self.class.name.black.bg(:white)} with path '#{api_get_path}'\")\n        return parse_body(api_get_from(api_comments_path)) unless auto_paginate\n\n        auto_paginated_response(\n          Runtime::API::Request.new(api_client, api_comments_path, per_page: '100').url,\n          attempts: attempts\n        )\n      end",
    "comment": "Get issue comments  @return [Array]",
    "label": "",
    "id": "3386"
  },
  {
    "raw_code": "def add_comment(body:, confidential: false)\n        api_post_to(api_comments_path, body: body, confidential: confidential)\n      end",
    "comment": "Create a new comment  @param [String] body @param [Boolean] confidential @return [Hash]",
    "label": "",
    "id": "3387"
  },
  {
    "raw_code": "def label_events(auto_paginate: false, attempts: 0)\n        events(\"label\", auto_paginate: auto_paginate, attempts: attempts)\n      end",
    "comment": "Issue label events  @param [Boolean] auto_paginate @param [Integer] attempts @return [Array<Hash>]",
    "label": "",
    "id": "3388"
  },
  {
    "raw_code": "def state_events(auto_paginate: false, attempts: 0)\n        events(\"state\", auto_paginate: auto_paginate, attempts: attempts)\n      end",
    "comment": "Issue state events  @param [Boolean] auto_paginate @param [Integer] attempts @return [Array<Hash>]",
    "label": "",
    "id": "3389"
  },
  {
    "raw_code": "def milestone_events(auto_paginate: false, attempts: 0)\n        events(\"milestone\", auto_paginate: auto_paginate, attempts: attempts)\n      end",
    "comment": "Issue milestone events  @param [Boolean] auto_paginate @param [Integer] attempts @return [Array<Hash>]",
    "label": "",
    "id": "3390"
  },
  {
    "raw_code": "def events(name, auto_paginate:, attempts:)\n        return parse_body(api_get_from(\"#{api_get_path}/resource_#{name}_events\")) unless auto_paginate\n\n        auto_paginated_response(\n          Runtime::API::Request.new(api_client, \"#{api_get_path}/resource_#{name}_events\", per_page: '100').url,\n          attempts: attempts\n        )\n      end",
    "comment": "Issue events  @param [String] name event name @param [Boolean] auto_paginate @param [Integer] attempts @return [Array<Hash>]",
    "label": "",
    "id": "3391"
  },
  {
    "raw_code": "def expires_at\n        @expires_at || (Time.now.utc.to_date + 2)\n      end",
    "comment": "Expire in 2 days just in case the token is created just before midnight",
    "label": "",
    "id": "3392"
  },
  {
    "raw_code": "def api_get_path\n        raise NotImplementedError, not_implemented_message(__callee__)\n      end",
    "comment": "@return [String] the api path to fetch the resource",
    "label": "",
    "id": "3393"
  },
  {
    "raw_code": "def api_post_path\n        raise NotImplementedError, not_implemented_message(__callee__)\n      end",
    "comment": "@return [String] the api path to create the resource",
    "label": "",
    "id": "3394"
  },
  {
    "raw_code": "def api_post_body\n        raise NotImplementedError, not_implemented_message(__callee__)\n      end",
    "comment": "@return [Hash] the payload needed to create the resource",
    "label": "",
    "id": "3395"
  },
  {
    "raw_code": "def self.fetch_direct_connection_details(token)\n          response = Support::API.post(\n            \"#{Runtime::Scenario.gitlab_address}/api/v4/code_suggestions/direct_access\",\n            nil,\n            headers: { Authorization: \"Bearer #{token}\", 'Content-Type': 'application/json' }\n          )\n          raise \"Unexpected status code #{response.code}\" unless response.code == Support::API::HTTP_STATUS_CREATED\n\n          direct_connection_details = Support::API.parse_body(response)\n\n          raise \"direct_connection[:base_url] should not be nil\" if direct_connection_details[:base_url].nil?\n          raise \"direct_connection[:token] should not be nil\" if direct_connection_details[:token].nil?\n          raise \"direct_connection[:headers] should not be nil\" if direct_connection_details[:headers].nil?\n\n          direct_connection_details\n        end",
    "comment": "https://docs.gitlab.com/ee/api/code_suggestions.html#fetch-direct-connection-information",
    "label": "",
    "id": "3396"
  },
  {
    "raw_code": "def fabricate_via_api!\n          return api_get if actions.empty?\n\n          super\n        rescue ResourceNotFoundError\n          result = super\n\n          project.wait_for_push(commit_message)\n\n          result\n        end",
    "comment": "If `actions` are specified, it performs the actions to create, update, or delete commits. If no actions are specified it gets existing commits.",
    "label": "",
    "id": "3397"
  },
  {
    "raw_code": "def add_files(files)\n          validate_files!(files)\n\n          actions.push(*files.map { |file| file.merge({ action: \"create\" }) })\n        end",
    "comment": "Add files Pass in array of new files like, example: [{ \"file_path\": \"foo/bar\", \"content\": \"some content\" }]  @param [Array<Hash>] files @return [void]",
    "label": "",
    "id": "3398"
  },
  {
    "raw_code": "def update_files(files)\n          validate_files!(files)\n\n          actions.push(*files.map { |file| file.merge({ action: \"update\" }) })\n        end",
    "comment": "Update files Pass in array of files and it's contents, example: [{ \"file_path\": \"foo/bar\", \"content\": \"some content\" }]  @param [Array<Hash>] files @return [void]",
    "label": "",
    "id": "3399"
  },
  {
    "raw_code": "def add_directory(dir)\n          raise \"Must set directory as a Pathname\" unless dir.is_a?(Pathname)\n\n          files_to_add = []\n\n          dir.each_child do |child|\n            case child.ftype\n            when \"directory\"\n              add_directory(child)\n            when \"file\"\n              files_to_add.push({ file_path: child.basename, content: child.read })\n            else\n              continue\n            end",
    "comment": "Add all files from directory  @param [Pathname] dir @return [void]",
    "label": "",
    "id": "3400"
  },
  {
    "raw_code": "def dataloss?\n        wait_until_shell_command_matches(dataloss_command, /Outdated repositories/)\n      end",
    "comment": "Executes the praefect `dataloss` command.  @return [Boolean] whether dataloss has occurred",
    "label": "",
    "id": "3401"
  },
  {
    "raw_code": "def wait_for_read_count_change(pre_read_data)\n        diff_found = false\n        Support::Waiter.wait_until(sleep_interval: 1, max_duration: 60) do\n          query_read_distribution.each_with_index do |data, index|\n            diff_found = true if data[:value] > value_for_node(pre_read_data, data[:node])\n          end",
    "comment": "Waits until there is an increase in the number of reads for any node compared to the number of reads provided. If a node has no pre-read data, consider it to have had zero reads.",
    "label": "",
    "id": "3402"
  },
  {
    "raw_code": "def omnibus_configuration(cell_url:)\n          <<~OMNIBUS\n          gitlab_rails['lfs_enabled'] = true;\n          gitlab_rails['initial_root_password']= '#{Runtime::Env.admin_password}'\n          external_url '#{cell_url}';\n          OMNIBUS\n        end",
    "comment": "Default omnibus configuration for a GitLab instance @param cell_url [String] the external url for the GitLab instance",
    "label": "",
    "id": "3403"
  },
  {
    "raw_code": "def set_gitlab_urls(instance)\n          Support::GitlabAddress.define_gitlab_address_attribute!(instance.external_url)\n          Runtime::Env.gitlab_url = instance.external_url\n          Runtime::Scenario.define(:gitlab_address, instance.external_url)\n        end",
    "comment": "Sets the gitlab_url values so that gitlab-qa flows work on one of the instances @param instance [DockerRun::GitLab object] the GitLab instance to be used",
    "label": "",
    "id": "3404"
  },
  {
    "raw_code": "def add_gitlab_instance(name:, url:, external_port:, internal_port: '80', omnibus_config: nil)\n          cell_url = \"http://#{url}/\"\n          external_url = \"http://#{url}:#{external_port}/\"\n          ports = \"#{external_port}:#{internal_port}\"\n          omnibus_config ||= omnibus_configuration(cell_url: cell_url)\n          @list << Service::DockerRun::Gitlab.new(\n            image: Runtime::Env.release,\n            name: name,\n            ports: ports,\n            omnibus_config: omnibus_config,\n            external_url: external_url).tap do |gitlab|\n            gitlab.login\n            gitlab.register!\n          end",
    "comment": "Creates a DockerRun::Gitlab instance and adds to the list of instances @param name [string] the name for the instance @param url [string] the URL for the instance @param external_port [string] the external port @param internal_port [string] the internal port to use instead of default (optional) @param omnibus_config [string] omnibus_configuration to use instead of default (optional) @return [Service::DockerRun::Gitlab] the last created GitLab instance",
    "label": "",
    "id": "3405"
  },
  {
    "raw_code": "def wait_for_instance(instance)\n          Support::Waiter.wait_until(max_duration: 900, sleep_interval: 10, raise_on_failure: true) do\n            instance.health == \"healthy\"\n          end",
    "comment": "Waits for an instance to be healthy @param instance [DockerRun::GitLab object] the GitLab instance to be checked",
    "label": "",
    "id": "3406"
  },
  {
    "raw_code": "def remove_instance(instance_name)\n          index = @list.index { |x| x.name == instance_name }\n          instance = @list.slice!(index)\n          instance.remove!\n        end",
    "comment": "Remove an instance with a given name @param instance_name [String] the name of the instance that was specified during initialization",
    "label": "",
    "id": "3407"
  },
  {
    "raw_code": "def local_storage_config\n          <<~YAML\n            ---\n            apiVersion: v1\n            kind: ServiceAccount\n            metadata:\n              name: storage-provisioner\n              namespace: kube-system\n            ---\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: ClusterRoleBinding\n            metadata:\n              name: storage-provisioner\n            roleRef:\n              apiGroup: rbac.authorization.k8s.io\n              kind: ClusterRole\n              name: system:persistent-volume-provisioner\n            subjects:\n              - kind: ServiceAccount\n                name: storage-provisioner\n                namespace: kube-system\n            ---\n            apiVersion: v1\n            kind: Pod\n            metadata:\n              name: storage-provisioner\n              namespace: kube-system\n            spec:\n              serviceAccountName: storage-provisioner\n              tolerations:\n              - effect: NoExecute\n                key: node.kubernetes.io/not-ready\n                operator: Exists\n                tolerationSeconds: 300\n              - effect: NoExecute\n                key: node.kubernetes.io/unreachable\n                operator: Exists\n                tolerationSeconds: 300\n              hostNetwork: true\n              containers:\n              - name: storage-provisioner\n                image: gcr.io/k8s-minikube/storage-provisioner:v1.8.1\n                command: [\"/storage-provisioner\"]\n                imagePullPolicy: IfNotPresent\n                volumeMounts:\n                - mountPath: /tmp\n                  name: tmp\n              volumes:\n              - name: tmp\n                hostPath:\n                  path: /tmp\n                  type: Directory\n            ---\n            kind: StorageClass\n            apiVersion: storage.k8s.io/v1\n            metadata:\n              name: standard\n              namespace: kube-system\n              annotations:\n                storageclass.kubernetes.io/is-default-class: \"true\"\n              labels:\n                addonmanager.kubernetes.io/mode: EnsureExists\n            provisioner: k8s.io/minikube-hostpath\n          YAML\n        end",
    "comment": "See https://github.com/rancher/k3d/issues/67",
    "label": "",
    "id": "3408"
  },
  {
    "raw_code": "def prove_airgap\n          begin\n            gitlab_ip = Resolv.getaddress 'registry.gitlab.com'\n          rescue Resolv::ResolvError => e\n            Runtime::Logger.debug(\"prove_airgap unable to get ip address for endpoint - #{e.message}\")\n            # If Resolv.getaddress fails, it implies we cannot access the URL in question\n            # This may occur in offline-environment/airgapped testing\n            return 'true'\n          end",
    "comment": "Ping Cloudflare DNS, should fail Ping Registry, should fail to resolve",
    "label": "",
    "id": "3409"
  },
  {
    "raw_code": "def authsources\n          @authsources ||= begin\n            authsources_filename = \"simplesamlphp_authsources.php\"\n            authsources_file = ERB.new(read_fixture('saml', \"#{authsources_filename}.erb\")).result(binding)\n            # On ci when using gitlab-qa gem, tests run in a separate container which uses it's own copy of `qa` code\n            # This makes mounting files with DinD setup impossible because docker commands are running on separate\n            #  docker service container which only has access to the checked out code at `ci_project_dir` location.\n            # Test container started by gitlab-qa shares a common folder with job environment via `rspec` folder so to\n            #  correctly mount it, the mount path needs to be changed to one docker will have access to\n            # This still won't work when running gitlab-qa locally because by default gitlab-qa does not have a volume\n            #  that allows to share files between host and test container yet it will mount docker socket by default\n            if Runtime::Env.running_in_ci? && Runtime::Path.qa_root == \"/home/gitlab/qa\"\n              ::File.join(Runtime::Path.qa_root, \"rspec\", authsources_filename).then do |path|\n                ::File.write(path, authsources_file)\n                path.gsub(Runtime::Path.qa_root, \"#{Runtime::Env.ci_project_dir}/qa\")\n              end",
    "comment": "Creates an authsources file in qa root in `tmp` or `rspec` directory (for ci) from a template at `qa/qa/fixtures/saml/simplesamlphp_authsources.php.erb` with group sync permitted by inserting the value of `@group` as the groups attribute and `@users` as the users attribute in the template This allows generating users for saml tests dynamically See: https://docs.gitlab.com/ee/user/group/saml_sso/group_sync.html#configure-saml-group-sync",
    "label": "",
    "id": "3410"
  },
  {
    "raw_code": "def create\n            container = new\n            container.register!\n            container.wait_for_running\n\n            container\n          rescue StandardError => e\n            Runtime::Logger.error(\"Failed to start smocker container, logs:\\n#{container.logs}\")\n            raise e\n          end",
    "comment": "Create new instance of smocker container with random name and ports  @return [QA::Service::DockerRun::Smocker]",
    "label": "",
    "id": "3411"
  },
  {
    "raw_code": "def init(wait: 10)\n            if @container.nil?\n              @container = create\n\n              @api = Vendor::Smocker::SmockerApi.new(\n                host: @container.host_name,\n                public_port: @container.public_port,\n                admin_port: @container.admin_port\n              )\n              @api.wait_for_ready(wait: wait)\n            end",
    "comment": "@param wait [Integer] seconds to wait for server @yieldparam [SmockerApi] the api object ready for interaction",
    "label": "",
    "id": "3412"
  },
  {
    "raw_code": "def wait_for_running\n          Support::Waiter.wait_until(max_duration: 10, reload_page: false) do\n            running?\n          end",
    "comment": "Wait for container to be running  @return [void]",
    "label": "",
    "id": "3413"
  },
  {
    "raw_code": "def register!\n          return if running?\n\n          command = %W[docker run -d --network #{network} --name #{name}]\n          # when host network is used, published ports are discarded and service in container runs as if on host\n          # make sure random open ports are fetched and configured for smocker server\n          command.push(\"-e\", \"SMOCKER_MOCK_SERVER_LISTEN_PORT=#{host_network? ? server_port : DEFAULT_SERVER_PORT}\")\n          command.push(\"-e\", \"SMOCKER_CONFIG_LISTEN_PORT=#{host_network? ? config_port : DEFAULT_CONFIG_PORT}\")\n          command.push(\"--publish-all\") unless host_network?\n          command.push(image)\n\n          shell command.join(\" \")\n        end",
    "comment": "Start smocker container  @return [void]",
    "label": "",
    "id": "3414"
  },
  {
    "raw_code": "def public_port\n          @public_port ||= if host_network?\n                             server_port\n                           elsif docker_network?\n                             DEFAULT_SERVER_PORT\n                           else\n                             fetch_published_port(DEFAULT_SERVER_PORT)\n                           end",
    "comment": "Server port  When running in contained docker network, return internal port because service is accessed using hostname:port  @return [Integer]",
    "label": "",
    "id": "3415"
  },
  {
    "raw_code": "def admin_port\n          @admin_port ||= if host_network?\n                            config_port\n                          elsif docker_network?\n                            DEFAULT_CONFIG_PORT\n                          else\n                            fetch_published_port(DEFAULT_CONFIG_PORT)\n                          end",
    "comment": "Admin port  When running in contained docker network, return internal port because service is accessed using hostname:port  @return [Integer]",
    "label": "",
    "id": "3416"
  },
  {
    "raw_code": "def server_port\n          @server_port ||= random_port\n        end",
    "comment": "Random open port for server  @return [Integer]",
    "label": "",
    "id": "3417"
  },
  {
    "raw_code": "def config_port\n          @config_port ||= random_port\n        end",
    "comment": "Random open port for server configuration  @return [Integer]",
    "label": "",
    "id": "3418"
  },
  {
    "raw_code": "def host_network?\n          network == \"host\"\n        end",
    "comment": "Host network used?  @return [Boolea]",
    "label": "",
    "id": "3419"
  },
  {
    "raw_code": "def docker_network?\n          host_name == \"#{name}.#{network}\"\n        end",
    "comment": "Running within custom docker network  @return [Boolean]",
    "label": "",
    "id": "3420"
  },
  {
    "raw_code": "def fetch_published_port(container_port)\n          port = published_ports.split(\"\\n\").find { |line| line.start_with?(container_port.to_s) }.split(':').last\n          raise(\"Could not find published #{container_port} port for container #{name}\") unless port\n\n          port.to_i\n        end",
    "comment": "Fetch published container port  @param [Integer] container_port @return [Integer]",
    "label": "",
    "id": "3421"
  },
  {
    "raw_code": "def published_ports\n          @published_ports ||= shell(\"docker port #{name}\").presence || raise(\n            \"Unable to fetch published ports for smocker container #{name}\"\n          )\n        end",
    "comment": "Published ports for smocker container  @return [String]",
    "label": "",
    "id": "3422"
  },
  {
    "raw_code": "def random_port\n          server = TCPServer.new('127.0.0.1', 0)\n          port = server.addr[1]\n          server.close\n          port\n        end",
    "comment": "Random unassigned port  @return [Integer]",
    "label": "",
    "id": "3423"
  },
  {
    "raw_code": "def login(registry, user:, password:, force: false)\n          return if self.class.authenticated_registries[registry] && !force\n\n          shell(\n            %(docker login --username \"#{user}\" --password \"#{password}\" #{registry}),\n            mask_secrets: [password]\n          )\n\n          self.class.authenticated_registries[registry] = true\n        end",
    "comment": "Authenticate against a container registry If authentication is successful, will cache registry  @param registry [String] registry to authenticate against @param user [String] @param password [String] @param force [Boolean] force authentication if already authenticated @return [Void]",
    "label": "",
    "id": "3424"
  },
  {
    "raw_code": "def host_name\n          @host_name ||= if network == \"host\" || network == \"bridge\"\n                           host_ip\n                         else\n                           \"#{@name}.#{network}\"\n                         end",
    "comment": "Host name of the container  If host or default bridge network is used, container can only be reached using ip address  @return [String]",
    "label": "",
    "id": "3425"
  },
  {
    "raw_code": "def host_ip\n          docker_host = shell(\"docker context inspect --format='{{json .Endpoints.docker.Host}}'\").delete('\"')\n          hostname = URI(docker_host).host\n          # The docker host could be bound to a Unix socket, in which case as a URI it has no host\n          host = hostname.presence || Socket.gethostname\n          ip = Addrinfo.tcp(host, nil).ip_address\n          ip == '0.0.0.0' ? '127.0.0.1' : ip\n        rescue SocketError\n          # If the host could not be resolved, fallback on localhost\n          '127.0.0.1'\n        end",
    "comment": "Returns the IP address of the docker host  @return [String]",
    "label": "",
    "id": "3426"
  },
  {
    "raw_code": "def copy(from:, to:)\n          shell(\"docker cp #{from} #{to}\")\n        end",
    "comment": "Copy files to/from the Docker container and the host  @param from the source path to copy files from @param to the destination path to copy files to",
    "label": "",
    "id": "3427"
  },
  {
    "raw_code": "def initialize(name:, omnibus_config: '', image: '', ports: '80:80', external_url: Runtime::Env.gitlab_url)\n          @image = image\n          @name = name\n          @omnibus_configuration = omnibus_config\n          @ports = ports\n          @external_url = external_url\n          super()\n        end",
    "comment": "@param [String] name @param [String] omnibus_config @param [String] image @param [String] ports Docker-formatted port exposition @see ports https://docs.docker.com/engine/reference/commandline/run/#publish @param [String] external_url",
    "label": "",
    "id": "3428"
  },
  {
    "raw_code": "def extract_service_logs\n          copy(from: \"#{@name}:/var/log/gitlab\", to: Runtime::Path.qa_tmp(@name))\n        end",
    "comment": "Copy logs for GitLab services from the Docker container to the test framework's tmp folder",
    "label": "",
    "id": "3429"
  },
  {
    "raw_code": "def authenticate_third_party(force: false)\n            raise_validation_error unless can_authenticate_third_party?\n\n            login(\n              third_party_registry,\n              user: third_party_registry_user,\n              password: third_party_registry_password,\n              force: force\n            )\n          end",
    "comment": "@return [Void]",
    "label": "",
    "id": "3430"
  },
  {
    "raw_code": "def pipeline_mappings(**kwargs)\n          @pipeline_mapping = kwargs\n        end",
    "comment": "Pipeline mapping for this scenario  Defines pipeline mapping hash for this scenario Mapping must use one of the pipeline types defined in QA::Ci::Tools::PipelineCreator::SUPPORTED_PIPELINES and an array of jobs which must exist in that pipeline  @example pipeline_mappings test_on_cng: ['cng-instance'], test_on_gdk: ['gdk-instance']  @return [Hash<String, Array<String>>]",
    "label": "",
    "id": "3431"
  },
  {
    "raw_code": "def spec_glob_pattern(pattern)\n          unless pattern.is_a?(String) && pattern.end_with?(\"_spec.rb\")\n            raise ArgumentError, \"Scenario #{self.class.name} defines pattern that is not matching only spec files\"\n          end",
    "comment": "Glob pattern limiting which specs scenario can run  @param pattern [String] @return [String]",
    "label": "",
    "id": "3432"
  },
  {
    "raw_code": "def define_gitlab_address(args)\n        address_from_opt = Runtime::Scenario.attributes[:gitlab_address]\n\n        return define_gitlab_address_attribute!(args.shift) if args.first && Runtime::Address.valid?(args.first)\n        return define_gitlab_address_attribute!(address_from_opt) if address_from_opt\n\n        define_gitlab_address_attribute!\n      end",
    "comment": "Define gitlab address attribute  Use first argument if a valid address, else use named argument or default to environment variable  @param [Array] args @return [void]",
    "label": "",
    "id": "3433"
  },
  {
    "raw_code": "def create_new(source_branch:)\n        if Page::Project::Show.perform(&:has_create_merge_request_button?)\n          Page::Project::Show.perform(&:new_merge_request)\n          return\n        end",
    "comment": "Opens the form to create a new merge request. It tries to use the \"Create merge request\" button that appears after a commit is pushed, but if that button isn't available, it uses the \"New merge request\" button on the page that lists merge requests.  @param [String] source_branch the branch to be merged",
    "label": "",
    "id": "3434"
  },
  {
    "raw_code": "def visit_latest_pipeline(status: nil, wait: 120, skip_wait: true)\n        Page::Project::Menu.perform(&:go_to_pipelines)\n        Page::Project::Pipeline::Index.perform do |index|\n          index.has_any_pipeline?(wait: wait)\n          index.wait_for_latest_pipeline(status: status, wait: wait) if status || !skip_wait\n          index.click_on_latest_pipeline\n        end",
    "comment": "Acceptable statuses: Canceled, Created, Failed, Manual, Passed Pending, Running, Skipped",
    "label": "",
    "id": "3435"
  },
  {
    "raw_code": "def wait_for_pipeline_creation_via_api(project:, size: 1, wait: 240)\n        Runtime::Logger.info(\"Waiting for #{project.name}'s latest pipeline to be created...\")\n        Support::Waiter.wait_until(message: 'Wait for pipeline to be created', max_duration: wait) do\n          project.pipelines.present? && project.pipelines.size >= size\n        end",
    "comment": "With pipeline creation is slow a known issue - https://gitlab.com/groups/gitlab-org/-/epics/7290, it might help reduce flakiness if we wait for pipeline to be created via API first before visiting it via the UI.  Trying to let it wait for up to 4 minutes, any longer than that is unacceptable in most scenarios.  Provide a different size when more than 1 pipelines are expected.",
    "label": "",
    "id": "3436"
  },
  {
    "raw_code": "def wait_for_latest_pipeline_to_finish(project:, wait: 240)\n        Runtime::Logger.info(\"Waiting for #{project.name}'s latest pipeline to finish...\")\n        Support::Waiter.wait_until(message: 'Wait for latest pipeline to run', max_duration: wait) do\n          pipeline = project.latest_pipeline\n          pipeline[:started_at].present? && pipeline[:finished_at].present?\n        end",
    "comment": "To wait for pipeline to complete regardless of status ",
    "label": "",
    "id": "3437"
  },
  {
    "raw_code": "def start_slack_install(project)\n          project.visit!\n\n          Page::Project::Menu.perform do |project_menu_page|\n            project_menu_page.click_project\n            project_menu_page.go_to_integrations_settings\n          end",
    "comment": "Need to sign in for this method @param [QA::Resource::Project]",
    "label": "",
    "id": "3438"
  },
  {
    "raw_code": "def start_gitlab_connect(project, channel: nil)\n          Vendor::Slack::Page::Chat.perform do |chat_page|\n            # sometimes Slack will present a blocking page\n            # for downloading the app instead of using a browser\n            chat_page.skip_download_screen\n\n            lines = [\"/staging-gitlab #{project.path_with_namespace} issue show 1\"]\n            chat_page.send_message_to_channel(lines, channel: channel)\n\n            # The only way to know if we are authorized is to send a slash command to the channel.\n            # If the account / chat_name is already authorized, the Slack app will try to look up the issue\n            # and return a 404 because it doesn't exist\n            Support::Waiter.wait_until(max_duration: 4, raise_on_failure: false) do\n              chat_page.messages.last.text =~ /connect your GitLab account|404 not found!/i\n            end",
    "comment": "@param [QA::Resource::Project] project @option [String | Nil] channel @return [Boolean] is this account already authorized?",
    "label": "",
    "id": "3439"
  },
  {
    "raw_code": "def asset_exists?(url)\n        page.execute_script <<~JS\n          xhr = new XMLHttpRequest();\n          xhr.open('GET', '#{url}', true);\n          xhr.send();\n        JS\n\n        return false unless wait_until(sleep_interval: 0.5, max_duration: 60, reload: false) do\n          page.evaluate_script('xhr.readyState == XMLHttpRequest.DONE')\n        end",
    "comment": "Returns true if successfully GETs the given URL Useful because `page.status_code` is unsupported by our driver, and we don't have access to the `response` to use `have_http_status`.",
    "label": "",
    "id": "3440"
  },
  {
    "raw_code": "def choose_element(name, click_by_js = false, **kwargs)\n        kwargs[:visible] = false unless kwargs.key?(:visible)\n        if find_element(name, **kwargs).checked?\n          QA::Runtime::Logger.debug(\"#{name} is already selected\")\n\n          return\n        end",
    "comment": "Method for selecting radios",
    "label": "",
    "id": "3441"
  },
  {
    "raw_code": "def click_element_coordinates(name, **kwargs)\n        page.driver.browser.action.move_to(find_element(name, **kwargs).native).click.perform\n      rescue Selenium::WebDriver::Error::StaleElementReferenceError => e\n        QA::Runtime::Logger.error(\"Element #{name} has become stale: #{e}\")\n      end",
    "comment": "Use this to simulate moving the pointer to an element's coordinate and sending a click event. This is a helpful workaround when there is a transparent element overlapping the target element and so, normal `click_element` on target would raise Selenium::WebDriver::Error::ElementClickInterceptedError",
    "label": "",
    "id": "3442"
  },
  {
    "raw_code": "def click_element(name, page = nil, **kwargs)\n        skip_finished_loading_check = kwargs.delete(:skip_finished_loading_check)\n        wait_for_requests(skip_finished_loading_check: skip_finished_loading_check)\n\n        wait = kwargs.delete(:wait) || Capybara.default_max_wait_time\n        text = kwargs.delete(:text)\n\n        begin\n          find(element_selector_css(name, kwargs), text: text, wait: wait).click\n        rescue Net::ReadTimeout => error\n          # In some situations due to perhaps a slow environment we can encounter errors\n          # where clicks are registered, but the calls to selenium-webdriver result in\n          # timeout errors. In these cases rescue from the error and attempt to continue in\n          # the test to avoid a flaky test failure. This should be safe as assertions in the\n          # tests will catch any case where the click wasn't actually registered.\n          QA::Runtime::Logger.warn \"click_element -- #{error} -- #{error.backtrace.inspect}\"\n          # There may be a 5xx error -- lets refresh the page like the warning page suggests\n          # and it if resolves itself we can avoid a flaky failure\n          refresh\n        end",
    "comment": "replace with (..., page = self.class)",
    "label": "",
    "id": "3443"
  },
  {
    "raw_code": "def act_via_capybara(method, locator, **kwargs)\n        page.public_send(method, locator, **kwargs)\n      end",
    "comment": "Uses capybara to locate and interact with an element instead of using `*_element`. This can be used when it's not possible to add a QA selector but we still want to log the action  @param [String] method the capybara method to use @param [String] locator the selector used to find the element @param [Hash] **kwargs optional arguments",
    "label": "",
    "id": "3444"
  },
  {
    "raw_code": "def fill_editor_element(name, content)\n        element = find_element name\n\n        if element.tag_name == 'textarea'\n          element.set content\n        else\n          mod = page.driver.browser.capabilities.platform_name.include?(\"mac\") ? :command : :control\n          prosemirror = element.find '[contenteditable].ProseMirror'\n          prosemirror.send_keys [mod, 'a']\n          prosemirror.send_keys :delete\n          prosemirror.send_keys content\n\n          # Wait for the hidden input field to be updated\n          # The hidden field contains markdown serialized by RTE\n          has_field?(type: 'hidden', with: content, wait: 3)\n        end",
    "comment": "fill in editor element, whether plain text or rich text",
    "label": "",
    "id": "3445"
  },
  {
    "raw_code": "def click_new_application_button\n          click_element 'new-application-button'\n        end",
    "comment": "rubocop:enable Layout/LineLength",
    "label": "",
    "id": "3446"
  },
  {
    "raw_code": "def get_id_of_application\n          find_element('id-of-application-field', visible: false).value\n        end",
    "comment": "Returns the ID of the resource",
    "label": "",
    "id": "3447"
  },
  {
    "raw_code": "def disable_initialize_with_sast\n          return unless has_element?('initialize-with-sast-checkbox', visible: false)\n\n          uncheck_element('initialize-with-sast-checkbox', true)\n        end",
    "comment": "Disable experiment for SAST at project creation https://gitlab.com/gitlab-org/gitlab/-/issues/333196",
    "label": "",
    "id": "3448"
  },
  {
    "raw_code": "def click_on_prometheus_integration\n            click_element('prometheus-link')\n          end",
    "comment": "rubocop:enable QA/ElementWithPattern",
    "label": "",
    "id": "3449"
  },
  {
    "raw_code": "def wait_for_latest_pipeline(status: nil, wait: nil, reload: false)\n            wait ||= Support::Repeater::DEFAULT_MAX_WAIT_TIME\n            finished_status = %w[passed failed canceled skipped manual warning]\n\n            wait_until(max_duration: wait, reload: reload, sleep_interval: 1, message: \"Wait for latest pipeline\") do\n              if status\n                latest_pipeline_status.casecmp(status) == 0\n              else\n                finished_status.include?(latest_pipeline_status.downcase)\n              end",
    "comment": "If no status provided, wait for pipeline to complete",
    "label": "",
    "id": "3450"
  },
  {
    "raw_code": "def wait_for_ide_to_load(file_name = nil)\n            page.driver.browser.switch_to.window(page.driver.browser.window_handles.last)\n            # On test environments we have a broadcast message that can cover the buttons\n            if has_element?('broadcast-notification-container', wait: 5)\n              within_element('broadcast-notification-container') do\n                click_element('close-button')\n              end",
    "comment": "Used for stability @param file_name [string] wait for file to be loaded (optional)",
    "label": "",
    "id": "3451"
  },
  {
    "raw_code": "def output(wait: 5)\n            result = ''\n\n            wait_until(reload: false, max_duration: wait, sleep_interval: 1) do\n              result = job_log.include?('Job') ? job_log : ''\n              result.present?\n            end",
    "comment": "Reminder: You may wish to wait for a particular job status before checking output",
    "label": "",
    "id": "3452"
  },
  {
    "raw_code": "def has_unlocked_artifact?(wait: 240)\n            wait_until(reload: true, max_duration: wait, sleep_interval: 1) do\n              has_element?('artifacts-unlocked-message-content')\n            end",
    "comment": "Artifact unlock is async and depends on queue size on target env",
    "label": "",
    "id": "3453"
  },
  {
    "raw_code": "def add_personal_access_token(personal_access_token)\n            # If for some reasons this process is retried, user cannot re-enter github token in the same group\n            # In this case skip this step and proceed to import project row\n            return unless has_element?('personal-access-token-field')\n\n            raise ArgumentError, \"No personal access token was provided\" if personal_access_token.empty?\n\n            fill_element('personal-access-token-field', personal_access_token)\n            click_element('authenticate-button')\n            finished_loading?\n          end",
    "comment": "Add personal access token  @param [String] personal_access_token @return [void]",
    "label": "",
    "id": "3454"
  },
  {
    "raw_code": "def import!(gh_project_name, target_group_path, project_name)\n            within_element('project-import-row', source_project: gh_project_name) do\n              click_element('target-namespace-dropdown')\n              click_element(\"listbox-item-#{target_group_path}\", wait: 10)\n              fill_element('project-path-field', project_name)\n\n              retry_until do\n                click_element('import-button')\n                # Make sure import started before waiting for completion\n                has_no_element?('import-status-indicator', text: \"Not started\", wait: 1)\n              end",
    "comment": "Import project  @param [String] source_project_name @param [String] target_group_path @return [void]",
    "label": "",
    "id": "3455"
  },
  {
    "raw_code": "def has_go_to_project_link?(gh_project_name)\n            within_element('project-import-row', source_project: gh_project_name) do\n              has_element?('go-to-project-link')\n            end",
    "comment": "Check Go to project button present  @param [String] gh_project_name @return [Boolean]",
    "label": "",
    "id": "3456"
  },
  {
    "raw_code": "def has_imported_project?(\n            gh_project_name,\n            wait: QA::Support::WaitForRequests::DEFAULT_MAX_WAIT_TIME,\n            allow_partial_import: false\n          )\n            within_element('project-import-row', source_project: gh_project_name, skip_finished_loading_check: true) do\n              wait_until(\n                max_duration: wait,\n                sleep_interval: 5,\n                reload: false,\n                skip_finished_loading_check_on_refresh: true\n              ) do\n                status_selector = 'import-status-indicator'\n\n                next has_element?(status_selector, text: \"Complete\", wait: 1) unless allow_partial_import\n\n                [\"Partially completed\", \"Complete\"].any? do |status|\n                  has_element?(status_selector, text: status, wait: 1)\n                end",
    "comment": "Check if import page has a successfully imported project  @param [String] source_project_name @param [Integer] wait @return [Boolean]",
    "label": "",
    "id": "3457"
  },
  {
    "raw_code": "def select_advanced_option(option_name)\n            check_element('advanced-settings-checkbox', true, option_name: option_name)\n          end",
    "comment": "Select advanced github import option  @param [Symbol] option_name @return [void]",
    "label": "",
    "id": "3458"
  },
  {
    "raw_code": "def switch_catalog_sorting_option(option)\n          click_element('catalog-sorting-option-button')\n          find(\"[data-testid='listbox-item-#{option}']\").click\n        end",
    "comment": "Current acceptable options: 'CREATED', 'RELEASED'",
    "label": "",
    "id": "3459"
  },
  {
    "raw_code": "def register_user(user)\n          raise ArgumentError, 'User must be of type Resource::User' unless user.is_a? Resource::User\n\n          fill_element 'new-user-first-name-field', user.first_name\n          fill_element 'new-user-last-name-field', user.last_name\n          fill_element 'new-user-username-field', user.username\n          fill_element 'new-user-email-field', user.email\n          fill_element 'new-user-password-field', user.password\n\n          Support::Waiter.wait_until(sleep_interval: 0.5) do\n            page.has_content?(\"Username is available.\")\n\n            network_password_requirements.each do |requirement|\n              page_has_success_requirement?(requirement) if page.has_content?(requirement, wait: 0.5)\n            end",
    "comment": "Register a user @param [Resource::User] user the user to register",
    "label": "",
    "id": "3460"
  },
  {
    "raw_code": "def expand_content(element_name)\n          within_element(element_name) do\n            # Because it is possible to click the button before the JS toggle code is bound\n            wait_until(reload: false, message: \"Waiting until content is expanded\") do\n              click_button class: 'settings-toggle' unless has_css?('button', text: 'Collapse', wait: 1)\n\n              has_content?('Collapse')\n            end",
    "comment": "Click the Expand button present in the specified section  @param [Symbol|String] element_name `element` name defined in a `view` block",
    "label": "",
    "id": "3461"
  },
  {
    "raw_code": "def within_new_item_menu\n          click_element('new-menu-toggle')\n\n          yield\n        end",
    "comment": "Opens the new item menu and yields to the block  @return [void]",
    "label": "",
    "id": "3462"
  },
  {
    "raw_code": "def open_submenu(parent_menu_name, sub_menu)\n          # If Project Studio is enabled, show the sidebar\n          expand_sidebar_if_collapsed if Runtime::Env.project_studio_enabled?\n\n          # prevent closing sub-menu if it was already open\n          unless has_element?('menu-section', section_name: parent_menu_name, wait: 0)\n            click_element('menu-section-button', section_name: parent_menu_name)\n          end",
    "comment": "Open sidebar navigation submenu  @param [String] parent_menu_name @param [String] sub_menu @return [void]",
    "label": "",
    "id": "3463"
  },
  {
    "raw_code": "def expand_sidebar_if_collapsed\n          click_element('sidebar-icon') if has_css?('.super-sidebar-is-icon-only', wait: 0)\n        end",
    "comment": "Expands the sidebar if it's in icon-only (collapsed) mode This is needed when Project Studio is enabled as it defaults to collapsed sidebar @return [void]",
    "label": "",
    "id": "3464"
  },
  {
    "raw_code": "def select_item(item_text, css: 'li.gl-new-dropdown-item')\n          if item_text\n            find(css, text: item_text, match: :prefer_exact).click\n          else\n            find(css, match: :first).click\n          end",
    "comment": "Find and click item using css selector and matching text If item_text is not provided, select the first item that matches the given css selector  @param [String] item_text @param [String] css - css selector of the item @return [void]",
    "label": "",
    "id": "3465"
  },
  {
    "raw_code": "def click_confirmation_ok_button_if_present\n          # In the case of changing access levels[1], the modal appears while there's a request in process, so we need\n          # to skip the loading check otherwise it will time out.\n          #\n          # [1]: https://gitlab.com/gitlab-org/gitlab/-/blob/4a99af809b86047ce3c8985e6582748bbd23fc84/qa/qa/page/component/members/members_table.rb#L54\n          return unless has_element?('confirmation-modal', skip_finished_loading_check: true)\n\n          click_element('confirm-ok-button', skip_finished_loading_check: true)\n        end",
    "comment": "Click the confirmation button if the confirmation modal is present Can be used when the modal may not always appear in a test. For example, if the modal is behind a feature flag  @return [void]",
    "label": "",
    "id": "3466"
  },
  {
    "raw_code": "def initialize(page, container)\n          @page = page\n          @container = container\n        end",
    "comment": "page      - A QA::Page::Base object container - CSS selector of the comment textarea's container",
    "label": "",
    "id": "3467"
  },
  {
    "raw_code": "def attach_file(attachment)\n          if QA::Page::Base.perform { |d| d.has_element?('content-editor', wait: 0.5) } # Rich text editor\n            filename = attachment.match(%r{([^/]+$)})[1]\n            page.find('[data-testid=\"file-upload-field\"]', visible: 'false').set attachment\n\n            # Wait for link to be appended to dropzone text\n            page.wait_until(reload: false) do\n              page.find(\"#{container} img\")['alt'].match?(filename)\n            end",
    "comment": "Not tested and not expected to work with multiple dropzones instantiated on one page because there is no distinguishing attribute per dropzone file field.",
    "label": "",
    "id": "3468"
  },
  {
    "raw_code": "def has_filtered_group?(name)\n          filter_group(name)\n\n          page.has_link?(name, wait: 0) # element containing link to group\n        end",
    "comment": "Check if a group exists in private or public tab @param name [String] group name @return [Boolean] whether a group with given name exists",
    "label": "",
    "id": "3469"
  },
  {
    "raw_code": "def filter_group(name)\n          filter_input = find_element('filtered-search-term-input')\n          filter_input.click\n          filter_input.set(name)\n          click_element 'search-button'\n          # Loading starts a moment after `return` is sent. We mustn't jump ahead\n          wait_for_requests if spinner_exists?\n          has_element?('nested-groups-projects-list', wait: 1)\n        end",
    "comment": "Filter by group name @param name [String] group name @return [Boolean] whether the filter returned any group",
    "label": "",
    "id": "3470"
  },
  {
    "raw_code": "def comment(text, attachment: nil, filter: :all_activities)\n          method(\"select_#{filter}_filter\").call\n          fill_editor_element('comment-field', \"#{text}\\n\")\n\n          unless attachment.nil?\n            QA::Page::Component::Dropzone.new(self, '.new-note')\n              .attach_file(attachment)\n          end",
    "comment": "Attachment option should be an absolute path",
    "label": "",
    "id": "3471"
  },
  {
    "raw_code": "def focused_board\n            find('.issue-boards-content.js-focus-mode-board.is-focused')\n          end",
    "comment": "The `focused_board` method does not use `find_element` with an element defined with the attribute `data-testid` since such element is not unique when the `is-focused` class is not set, and it was not possible to find a better solution.",
    "label": "",
    "id": "3472"
  },
  {
    "raw_code": "def comment(text, attachment: nil, filter: :all_activities)\n            method(:\"select_#{filter}_filter\").call\n            fill_editor_element('markdown-editor-form-field', \"#{text}\\n\")\n\n            unless attachment.nil?\n              QA::Page::Component::Dropzone.new(self, '.new-note')\n                .attach_file(attachment)\n            end",
    "comment": "Attachment option should be an absolute path",
    "label": "",
    "id": "3473"
  },
  {
    "raw_code": "def connect_gitlab_instance(gitlab_url, gitlab_token)\n          # Wait until element is present and refresh if not in case feature flag did not kick in\n          wait_until(max_duration: 10) { has_element?('import-gitlab-url', wait: 1) }\n\n          set_gitlab_url(gitlab_url)\n          set_gitlab_token(gitlab_token)\n\n          click_element('connect-instance-button')\n        end",
    "comment": "Connect gitlab instance  @param [String] gitlab_url @param [String] gitlab_token @return [void]",
    "label": "",
    "id": "3474"
  },
  {
    "raw_code": "def import_group(source_group_name, target_group_name)\n          finished_loading?\n\n          filter_group(source_group_name)\n\n          within_element('import-item', source_group: source_group_name) do\n            click_element('target-namespace-dropdown')\n            click_element(\"listbox-item-#{target_group_name}\")\n\n            retry_until(message: \"Triggering import\") do\n              click_element('import-group-button')\n              # Make sure import started before waiting for completion\n              has_no_element?('import-status-indicator', text: \"Not started\", wait: 1)\n            end",
    "comment": "Import source group in to target group  @param [String] source_group_name @param [String] target_group_name @return [void]",
    "label": "",
    "id": "3475"
  },
  {
    "raw_code": "def has_imported_group?(source_group_name, wait: QA::Support::WaitForRequests::DEFAULT_MAX_WAIT_TIME)\n          within_element('import-item', source_group: source_group_name) do\n            has_element?('import-status-indicator', text: \"Complete\", wait: wait)\n          end",
    "comment": "Check if import page has a successfully imported group  @param [String] source_group_name @param [Integer] wait @return [Boolean]",
    "label": "",
    "id": "3476"
  },
  {
    "raw_code": "def count_all_runners\n            find_element(\"runner-count-all\").text.to_i\n          end",
    "comment": "Returns total count of all runner types  @return [Integer]",
    "label": "",
    "id": "3477"
  },
  {
    "raw_code": "def count_group_runners\n            find_element(\"runner-count-group\").text.to_i\n          end",
    "comment": "Returns total count of group runner types  @return [Integer]",
    "label": "",
    "id": "3478"
  },
  {
    "raw_code": "def count_project_runners\n            find_element(\"runner-count-project\").text.to_i\n          end",
    "comment": "Returns total count of project runner types  @return [Integer]",
    "label": "",
    "id": "3479"
  },
  {
    "raw_code": "def count_online_runners\n            within_element(\"runner-stats-online\") do\n              find_element(\"non-animated-value\").text.to_i\n            end",
    "comment": "Returns count of online runners  @return [Integer]",
    "label": "",
    "id": "3480"
  },
  {
    "raw_code": "def revertible?\n          has_element?('revert-button', disabled: false, wait: 10)\n        end",
    "comment": "Waits up 10 seconds and returns false if the Revert button is not enabled",
    "label": "",
    "id": "3481"
  },
  {
    "raw_code": "def wait_until_ready_to_merge\n          wait_until(message: \"Waiting for ready to merge\", sleep_interval: 1) do\n            # changes in mr are rendered async, because of that mr can sometimes show no changes and there will be no\n            # merge button, in such case we must retry loop otherwise find_element will raise ElementNotFound error\n            next false unless has_element?('merge-button', wait: 1)\n\n            # If the widget shows \"Merge blocked: new changes were just added\" we can refresh the page and check again\n            next false if merge_blocked_by_new_changes?\n\n            break true unless find_element('merge-button').disabled?\n\n            QA::Runtime::Logger.debug(\"MR widget text: \\\"#{mr_widget_text}\\\"\")\n\n            false\n          end",
    "comment": "Waits up 60 seconds and raises an error if unable to merge.  If a state is encountered in which a user would typically refresh the page, this will refresh the page and then check again if it's ready to merge. For example, it will refresh if a new change was pushed and the page needs to be refreshed to show the change. ",
    "label": "",
    "id": "3482"
  },
  {
    "raw_code": "def merge_blocked_by_new_changes?\n          has_element?('head-mismatch-content', wait: 1)\n        end",
    "comment": "Returns true when widget shows \"Merge blocked: new changes were just added\"",
    "label": "",
    "id": "3483"
  },
  {
    "raw_code": "def canary?\n          has_element?('canary-badge-link')\n        end",
    "comment": "To verify whether the user has been directed to a canary web node @return [Boolean] result of checking existence of 'canary-badge-link' element @example: Menu.perform do |menu| expect(menu.canary?).to be(true) end",
    "label": "",
    "id": "3484"
  },
  {
    "raw_code": "def organization_name=(name)\n          fill_element('organization-name', name)\n        end",
    "comment": "Sets the organization name @param name [string] name of organization",
    "label": "",
    "id": "3485"
  },
  {
    "raw_code": "def initialize(git_uri)\n        @git_uri = git_uri\n        @uri =\n          if %r{\\A(?:ssh|http|https)://}.match?(git_uri)\n            URI.parse(git_uri)\n          else\n            *rest, path = git_uri.split(':')\n            # Host cannot have : so we'll need to escape it\n            user_host = rest.join('%3A').sub(/\\A\\[(.+)\\]\\z/, '\\1')\n            URI.parse(\"ssh://#{user_host}/#{path}\")\n          end",
    "comment": "See: config/initializers/1_settings.rb Settings#build_gitlab_shell_ssh_path_prefix",
    "label": "",
    "id": "3486"
  },
  {
    "raw_code": "def local_size\n        internal_refs = %w[\n          refs/keep-around/\n          refs/merge-requests/\n          refs/pipelines/\n          refs/remotes/\n          refs/tmp/\n          refs/environments/\n        ]\n        cmd = <<~CMD\n          git rev-list #{internal_refs.map { |r| \"--exclude='#{r}*'\" }.join(' ')} \\\n          --not --alternate-refs --not \\\n          --all --objects --use-bitmap-index --disk-usage\n        CMD\n\n        run_git(cmd).to_i\n      end",
    "comment": "Gets the size of the repository using `git rev-list --all --objects --use-bitmap-index --disk-usage` as Gitaly does (see https://gitlab.com/gitlab-org/gitlab/-/issues/357680)",
    "label": "",
    "id": "3487"
  },
  {
    "raw_code": "def run_gc\n        run_git('git gc')\n      end",
    "comment": "Performs garbage collection",
    "label": "",
    "id": "3488"
  },
  {
    "raw_code": "def log(msg, level = :info)\n            QA::Runtime::Logger.public_send(level, msg)\n          end",
    "comment": "Log message  @param [String] msg @param [Symbol] level @return [void]",
    "label": "",
    "id": "3489"
  },
  {
    "raw_code": "def create_issue(project, channel:, title:, description:)\n            lines = [\n              \"/staging-gitlab #{project.path_with_namespace} issue new #{title}\",\n              description\n            ]\n\n            send_message_to_channel(lines, channel: channel)\n          end",
    "comment": "@param [QA::Resource::Project] project @param [String] channel @param [String] title @param [String] description",
    "label": "",
    "id": "3490"
  },
  {
    "raw_code": "def move_issue(project, target, id:, channel:)\n            line = \"/staging-gitlab #{project.path_with_namespace} issue move #{id} to #{target.path_with_namespace}\"\n            send_message_to_channel([line], channel: channel)\n          end",
    "comment": "@param [QA::Resource::Project] project @param [QA::Resource::Project] target @param [String] id @param [String] channel",
    "label": "",
    "id": "3491"
  },
  {
    "raw_code": "def show_issue(project, id:, channel:)\n            send_message_to_channel(\n              [\"/staging-gitlab #{project.path_with_namespace} issue show #{id}\"],\n              channel: channel\n            )\n          end",
    "comment": "@param [QA::Resource::Project] project @param [String] id @param [String] channel",
    "label": "",
    "id": "3492"
  },
  {
    "raw_code": "def close_issue(project, id:, channel:)\n            send_message_to_channel(\n              [\"/staging-gitlab #{project.path_with_namespace} issue close #{id}\"],\n              channel: channel\n            )\n          end",
    "comment": "@param [QA::Resource::Project] project @param [String] id @param [String] channel",
    "label": "",
    "id": "3493"
  },
  {
    "raw_code": "def comment_on_issue(project, channel:, id:, comment:)\n            command = \"/staging-gitlab #{project.path_with_namespace} issue comment #{id}\"\n            send_message_to_channel([command, comment], channel: channel)\n          end",
    "comment": "@param [QA::Resource::Project] project @param [String] channel @param [String] id @param [String] comment",
    "label": "",
    "id": "3494"
  },
  {
    "raw_code": "def send_message_to_channel(lines, channel:)\n            go_to_channel(channel)\n\n            find(\"[data-qa='message_input']\").click\n\n            line = lines.shift\n\n            while line\n              send_keys(line)\n              wait_for_text(line)\n\n              send_keys([:shift, :enter]) unless lines.empty?\n\n              line = lines.shift\n            end",
    "comment": "@param [Array<String>] lines - messages to send @param [String] channel to send message to",
    "label": "",
    "id": "3495"
  },
  {
    "raw_code": "def wait_for_text(line)\n            Support::Waiter.wait_until(max_duration: 3, raise_on_failure: false) do\n              page.text.include?(line)\n            end",
    "comment": "@param [String] line of text to wait for in chat",
    "label": "",
    "id": "3496"
  },
  {
    "raw_code": "def go_to_channel(channel_name)\n            channel = messages.find do |msg|\n              msg.text == channel_name\n            end",
    "comment": "@param [String] channel_name to visit",
    "label": "",
    "id": "3497"
  },
  {
    "raw_code": "def messages(**opts)\n            find_all(\"[data-qa='virtual-list-item']\", **opts)\n          end",
    "comment": "@param [Hash] opts to include when finding all message elements",
    "label": "",
    "id": "3498"
  },
  {
    "raw_code": "def initialize(host, user:, password:, port: nil)\n          @host = host\n          @user = user\n          @password = password\n          @port = port\n          @cookies = {}\n        end",
    "comment": "@param host [String] the ip or hostname of the jenkins server @param user [String] the Jenkins admin user @param password [String] the Jenkins admin password @param port [Integer] the port that Jenkins is serving on",
    "label": "",
    "id": "3499"
  },
  {
    "raw_code": "def create_job(name)\n          job = Job.new(name, self)\n          yield job if block_given?\n          job.create\n          job\n        end",
    "comment": "Creates a new job in Jenkins  @param name [String] the name of the job @yieldparam job [Jenkins::Job] the job to be configured @return [Jenkins::Job] the created job in Jenkins",
    "label": "",
    "id": "3500"
  },
  {
    "raw_code": "def job_running?(name)\n          res = execute <<~GROOVY\n            project = Jenkins.instance.getProjects().find{p -> p.getName().equals('#{name}')}\n            build = project.getBuilds().find{b -> b.getExecutor()}\n            return build ? build.getExecutor().isActive() : false\n          GROOVY\n          JSON.parse parse_result(res)\n        end",
    "comment": "Is a given job running?  @param name [String] the name of the job @return [Boolean] is the job running?",
    "label": "",
    "id": "3501"
  },
  {
    "raw_code": "def number_of_jobs_running(name)\n          res = execute <<~GROOVY\n            project = Jenkins.instance.getProjects().find{p -> p.getName().equals('#{name}')}\n            builds = project.getBuilds().findAll{b -> b.getExecutor()}\n            return builds.size\n          GROOVY\n          JSON.parse parse_result(res)&.to_i\n        end",
    "comment": "Number of builds currently executing for a given job  @param name [String] the name of the job @return [Integer] the number of builds currently running",
    "label": "",
    "id": "3502"
  },
  {
    "raw_code": "def last_build_status(name)\n          res = execute <<~GROOVY\n            project = Jenkins.instance.getProjects().find{p -> p.getName().equals('#{name}')}\n            build = project.getBuilds()[-1]\n            return build.getResult()\n          GROOVY\n          parse_result(res)&.downcase&.to_sym\n        end",
    "comment": "Latest build status for a job  @param name [String] the name of the job @return [Symbol] the latest build status eg, (:success, :failure, etc)",
    "label": "",
    "id": "3503"
  },
  {
    "raw_code": "def last_build_id(job_name)\n          res = execute <<~GROOVY\n            project = Jenkins.instance.getProjects().find{p -> p.getName().equals('#{job_name}')}\n            build = project.getBuilds()[-1]\n            return build.getId()\n          GROOVY\n          parse_result(res)&.to_i\n        end",
    "comment": "Latest build id for a job Can be used to reference in other queries  @param job_name [String] the name of the job @return [Integer] the latest build id",
    "label": "",
    "id": "3504"
  },
  {
    "raw_code": "def last_build_log(job_name, start = 0)\n          get(\n            path: \"/job/#{job_name}/#{last_build_id(job_name)}/logText/progressiveText\",\n            params: { start: start }\n          ).body\n        end",
    "comment": "Latest build log for a job  @param job_name [String] the name of the job @param start [Integer] the log offset to return @return [String] the latest Jenkins log/output for this job",
    "label": "",
    "id": "3505"
  },
  {
    "raw_code": "def build(name, params: {})\n          post(params, path: \"/job/#{name}/build\")\n        end",
    "comment": "Triggers a build for a given job  @param name [String] the name of the job to trigger a build for @param [Hash] params the query parameters as a hash for the build endpoint",
    "label": "",
    "id": "3506"
  },
  {
    "raw_code": "def execute(script)\n          post(\"script=#{script}\", path: '/scriptText')\n        end",
    "comment": "Executes a Groovy script against the Jenkins instance  @param script [String] the Groovy script to execute",
    "label": "",
    "id": "3507"
  },
  {
    "raw_code": "def post_xml(xml, params: {}, path: '')\n          post(xml, params: params, path: path, headers: { 'Content-Type' => 'text/xml' })\n        end",
    "comment": "Sends XML to a given Jenkins endpoint This might be useful for filling in gaps in this lib  @param xml [String] the xml to post @param params [Hash] the query parameters as a hash @param path [String] the path to post to ex: /job/<name>/build @return [Typhoeus::Response]",
    "label": "",
    "id": "3508"
  },
  {
    "raw_code": "def post(data, params: {}, path: '', headers: {})\n          get_crumb\n          RestClient.post(\n            \"#{api_path}#{path}?#{params_to_s(params)}\",\n            data,\n            headers.merge(full_headers)\n          )\n        end",
    "comment": "Posts data to Jenkins This might be useful for filling in gaps in this lib  @param data [String | Hash] the xml to post @param params [Hash] the query parameters as a hash @param path [String] the path to post to ex: /job/<name>/build @param headers [Hash] additional headers to send @return [Typhoeus::Response]",
    "label": "",
    "id": "3509"
  },
  {
    "raw_code": "def get(path: '', params: {})\n          get_crumb\n          RestClient.get(\n            \"#{api_path}#{path}?#{params_to_s(params)}\",\n            full_headers\n          )\n        end",
    "comment": "Gets from a Jenkins endpoint This might be useful for filling in gaps in this lib  @param path [String] the path to get from ex: /job/<name>/builds/<build_id>/logText/progressiveText @param params [Hash] the query parameters as a hash @return [Typhoeus::Response]",
    "label": "",
    "id": "3510"
  },
  {
    "raw_code": "def configure_gitlab_plugin(url, access_token:, secret_id: SecureRandom.hex(4), **hargs)\n          configure_secret(access_token, secret_id)\n          configure_gitlab(url, secret_id, **hargs)\n        end",
    "comment": "configures the Jenkins GitLab plugin  @param url [String] the url for the GitLab instance @param access_token [String] an access token for the GitLab instance @param secret_id [String] an secret id used for the Jenkins GitLab credentials @param hargs [Hash] extra keyword arguments to provide @option hargs [String] :connection_name the name to use for the gitlab connection @option hargs [Integer] :read_timeout the read timeout for GitLab Jenkins @option hargs [Integer] :connection_timeout the connection timeout for GitLab Jenkins @option hargs [Boolean] :ignore_ssl_errors whether GitLab Jenkins should ignore SSL errors @return [String] the execute response from Jenkins",
    "label": "",
    "id": "3511"
  },
  {
    "raw_code": "def initialize(name, client)\n          @name = name\n          @client = client\n        end",
    "comment": "Prefer Jenkins::Client#jobs and Jenkins::Client.create_job over this constructor  @param name [String] the name of the job @param client [Jenkins::Client] the jenkins client",
    "label": "",
    "id": "3512"
  },
  {
    "raw_code": "def create\n          validate_required_fields!\n\n          response = @client.post_xml(build, path: '/createItem', params: { name: name })\n\n          check_network_error(response)\n          response.body\n        end",
    "comment": "Saves the Job in Jenkins",
    "label": "",
    "id": "3513"
  },
  {
    "raw_code": "def run\n          @client.build(@name)\n        end",
    "comment": "Triggers a build for the job",
    "label": "",
    "id": "3514"
  },
  {
    "raw_code": "def status\n          @client.last_build_status(@name)\n        end",
    "comment": "Returns the jobs last build status",
    "label": "",
    "id": "3515"
  },
  {
    "raw_code": "def log(start: 0)\n          @client.last_build_log(@name, start)\n        end",
    "comment": "Returns the jobs last log  @param start [Integer] the log offset to query",
    "label": "",
    "id": "3516"
  },
  {
    "raw_code": "def running?\n          @client.job_running?(@name)\n        end",
    "comment": "Returns whether the job is running  @return [Boolean]",
    "label": "",
    "id": "3517"
  },
  {
    "raw_code": "def active_runs\n          @client.number_of_jobs_running(@name)\n        end",
    "comment": "Returns the count of active builds  @return [Integer]",
    "label": "",
    "id": "3518"
  },
  {
    "raw_code": "def context\n          payload[:context]\n        end",
    "comment": "Smocker context including call counter",
    "label": "",
    "id": "3519"
  },
  {
    "raw_code": "def request\n          payload[:request]\n        end",
    "comment": "Smocker request data",
    "label": "",
    "id": "3520"
  },
  {
    "raw_code": "def received\n          date = request&.dig(:date)\n          Time.parse date if date\n        end",
    "comment": "@return [Time] Time request was recieved",
    "label": "",
    "id": "3521"
  },
  {
    "raw_code": "def elapsed(target)\n          (received.to_f - target.to_f).round if received\n        end",
    "comment": "Find time elapsed since <target>  @param target [Time] target time @return [Integer] seconds elapsed since <target>",
    "label": "",
    "id": "3522"
  },
  {
    "raw_code": "def base_url\n          @base_url ||= \"#{scheme}://#{host}:#{public_port}\"\n        end",
    "comment": "@return [String] Base url of mock endpoint",
    "label": "",
    "id": "3523"
  },
  {
    "raw_code": "def admin_url\n          @admin_url ||= \"#{scheme}://#{host}:#{admin_port}\"\n        end",
    "comment": "@return [String] Url of admin endpoint",
    "label": "",
    "id": "3524"
  },
  {
    "raw_code": "def url(endpoint = 'default')\n          \"#{base_url}/#{endpoint}\"\n        end",
    "comment": "@param endpoint [String] path for mock endpoint @return [String] url for mock endpoint",
    "label": "",
    "id": "3525"
  },
  {
    "raw_code": "def wait_for_ready(wait: 10)\n          Support::Waiter.wait_until(max_duration: wait, sleep_interval: 1, log: false) do\n            ready?\n          end",
    "comment": "Waits for the smocker server to be ready  @param wait [Integer] wait duration for smocker readiness",
    "label": "",
    "id": "3526"
  },
  {
    "raw_code": "def ready?\n          QA::Runtime::Logger.debug 'Checking Smocker readiness'\n          get(\"#{admin_url}/version\")\n          true\n          # rescuing StandardError because RestClient::ExceptionWithResponse isn't propagating\n        rescue StandardError => e\n          QA::Runtime::Logger.debug \"Smocker not ready yet \\n #{e}\"\n          false\n        end",
    "comment": "Is smocker server ready for interaction?  @return [Boolean]",
    "label": "",
    "id": "3527"
  },
  {
    "raw_code": "def reset(force: true)\n          response = post(\"#{admin_url}/reset?force=#{force}\", {}.to_json)\n          parse_body(response)['message'] == 'Reset successful'\n        end",
    "comment": "Clears mocks and history  @param force [Boolean] remove locked mocks? @return [Boolean] reset was successful?",
    "label": "",
    "id": "3528"
  },
  {
    "raw_code": "def get_session_id(name)\n          sessions = parse_body get(\"#{admin_url}/sessions/summary\")\n          current = sessions.find do |session|\n            session[:name] == name\n          end",
    "comment": "Fetches an active session id from a name  @param name [String] the name of the session @return [String] the unique session id",
    "label": "",
    "id": "3529"
  },
  {
    "raw_code": "def register(yaml = DEFAULT_MOCK, session: nil)\n          query_params = build_params(session: session)\n          url = \"#{admin_url}/mocks?#{query_params}\"\n          headers = { 'Content-Type' => 'application/x-yaml' }\n          response = post(url, yaml, headers: headers)\n          parse_body(response)\n        end",
    "comment": "Registers a mock to Smocker If a session name is provided, the mock will register to that session https://smocker.dev/technical-documentation/mock-definition.html  @param yaml [String] the yaml representing the mock @param session [String] the session name for the mock",
    "label": "",
    "id": "3530"
  },
  {
    "raw_code": "def history(session_name = nil)\n          query_params = session_name ? build_params(session: get_session_id(session_name)) : ''\n          response = get(\"#{admin_url}/history?#{query_params}\")\n          body = parse_body(response)\n\n          raise body[:message] unless body.is_a?(Array)\n\n          body.map do |entry|\n            HistoryResponse.new(entry)\n          end",
    "comment": "Fetches call history for a mock  @param session_name [String] the session name for the mock @return [Array<HistoryResponse>]",
    "label": "",
    "id": "3531"
  },
  {
    "raw_code": "def verify(session_name = nil)\n          payload = { session: session_name ? get_session_id(session_name) : nil }\n          response = post(\"#{admin_url}/sessions/verify\", payload)\n\n          VerifyResponse.new(parse_body(response))\n        end",
    "comment": "Fetch session verify response  @param [String] session_name @return [VerifyResponse]",
    "label": "",
    "id": "3532"
  },
  {
    "raw_code": "def stringified_history(session_name = nil)\n          history(session_name).map(&:payload).join(\"\\n\")\n        end",
    "comment": "Returns a stringfied version of the Smocker history  @param session_name [String] the session name for the mock @return [String] stringified event payloads",
    "label": "",
    "id": "3533"
  },
  {
    "raw_code": "def success?\n          payload.dig(:mocks, :verified) && payload.dig(:history, :verified)\n        end",
    "comment": "Check if session did not have any errors  @return [Boolean]",
    "label": "",
    "id": "3534"
  },
  {
    "raw_code": "def all_used?\n          payload.dig(:mocks, :all_used)\n        end",
    "comment": "Check if all mock definitions have been used  @return [Boolean]",
    "label": "",
    "id": "3535"
  },
  {
    "raw_code": "def failures\n          (payload.dig(:mocks, :failures) || []) + (payload.dig(:history, :failures) || [])\n        end",
    "comment": "Fetch failures  @return [Array]",
    "label": "",
    "id": "3536"
  },
  {
    "raw_code": "def unused\n          payload.dig(:mocks, :unused)\n        end",
    "comment": "Fetch unused mock definitions  @return [Array]",
    "label": "",
    "id": "3537"
  },
  {
    "raw_code": "def session_token\n          @session_token ||= `echo '#{@password}' | op account add --address #{@address} --email #{@email} --secret-key #{@secret} --signin --raw`\n        end",
    "comment": "OP session tokens are valid for 30 minutes. We are caching the session token here and this is fine currently as we just have one test that is not expected to go over 30 minutes. But note that if we add more tests that use this class, we might need to add a mechanism to invalidate the cache after 30 minutes or if the session_token is rejected by op CLI.",
    "label": "",
    "id": "3538"
  },
  {
    "raw_code": "def run_in_parallel?\n        return @parallel if defined?(@parallel)\n\n        # Do not use parallel with retry as parallel_tests is not capable to correctly detect\n        # groups of tests when `--only-failures` parameter is used\n        @parallel = (Runtime::Scenario.attributes[:parallel] || Runtime::Env.run_in_parallel?) && !rspec_retried?\n      end",
    "comment": "Trigger tests using parallel runner  @return [Boolean]",
    "label": "",
    "id": "3539"
  },
  {
    "raw_code": "def example_data\n        Support::ExampleData.fetch(rspec_tags).each_with_object({}) do |example, ids|\n          ids[example[:id]] = example[:status]\n        end",
    "comment": "Example ids with their execution status  @return [Hash<String, String>]",
    "label": "",
    "id": "3540"
  },
  {
    "raw_code": "def self.kwargs_warning\n        %r{warning: (?:Using the last argument (?:for `.+' )?as keyword parameters is deprecated; maybe \\*\\* should be added to the call|Passing the keyword argument (?:for `.+' )?as the last hash parameter is deprecated|Splitting the last argument (?:for `.+' )?into positional and keyword parameters is deprecated|The called method (?:`.+' )?is defined here)\\n\\z}\n      end",
    "comment": "Taken from https://github.com/jeremyevans/ruby-warning/blob/1.1.0/lib/warning.rb#L18 rubocop:disable Layout/LineLength",
    "label": "",
    "id": "3541"
  },
  {
    "raw_code": "def self.configure!\n        # Enable ruby deprecations for keywords, it's suppressed by default in Ruby 2.7\n        Warning[:deprecated] = true\n\n        DeprecationToolkit::Configuration.test_runner = :rspec\n        DeprecationToolkit::Configuration.deprecation_path = 'deprecations'\n        DeprecationToolkit::Configuration.warnings_treated_as_deprecation = [kwargs_warning]\n      end",
    "comment": "rubocop:enable Layout/LineLength",
    "label": "",
    "id": "3542"
  },
  {
    "raw_code": "def knapsack_pattern(spec_paths)\n          paths = spec_paths.map do |path|\n            relative_path = path.gsub(\"#{Runtime::Path.qa_root}/\", '')\n            File.directory?(relative_path) ? \"#{relative_path}/**/*_spec.rb\" : relative_path\n          end",
    "comment": "Create knapsack pattern from spec paths  @param spec_paths [Array<String>] @return [Array<String>]",
    "label": "",
    "id": "3543"
  },
  {
    "raw_code": "def run\n        Runtime::Logger.debug(\"Using parallel runner to trigger tests with arguments: '#{execution_args}'\")\n\n        set_environment!\n        perform_global_setup!\n        create_runtime_log!\n\n        ParallelTests::CLI.new.run(execution_args)\n      end",
    "comment": "Execute tests using parallel runner  @return [void]",
    "label": "",
    "id": "3544"
  },
  {
    "raw_code": "def default_paths?\n        paths == Runner::DEFAULT_TEST_PATH_ARGS\n      end",
    "comment": "Specific spec paths are default paths containing all specs  @return [Boolean]",
    "label": "",
    "id": "3545"
  },
  {
    "raw_code": "def executable_specs\n        @executable_specs ||= example_data.each_with_object([]) do |(id, status), paths|\n          paths << id.match(%r{\\./(\\S+)\\[\\S+\\]})[1] if status == \"passed\"\n        end.uniq\n      end",
    "comment": "Executable specs based on example data  @return [Array<String>]",
    "label": "",
    "id": "3546"
  },
  {
    "raw_code": "def parallel_processes\n        spec_files = path_options.select { |arg| arg.match?(/^.*_spec.rb$/) }\n        processes = Runtime::Env.parallel_processes\n        return spec_files.size if !spec_files.empty? && spec_files.size < processes\n\n        processes\n      end",
    "comment": "Parallel processes  If amount of explicitly passed spec files is smaller than configured processes, set it to spec files amount  @return [Integer]",
    "label": "",
    "id": "3547"
  },
  {
    "raw_code": "def path_options\n        @path_options ||= default_paths? ? executable_specs : paths\n      end",
    "comment": "Rspec path options  When default path is used, parallel runner will try to split tests based on the amount of spec files found within the path even though rspec tags would end up skipping most of these tests To avoid spawning processes that skip all tests, set spec paths based on example data which takes in to account which tags have been used  @return [Array]",
    "label": "",
    "id": "3548"
  },
  {
    "raw_code": "def execution_args\n        return @execution_args if @execution_args\n\n        @execution_args = [\n          \"--type\", \"rspec\",\n          \"-n\", parallel_processes.to_s,\n          \"--runtime-log\", RUNTIME_LOG_FILE,\n          \"--serialize-stdout\",\n          '--first-is-1',\n          \"--combine-stderr\"\n        ]\n        @execution_args.push(\"--\", *rspec_args) unless rspec_args.empty?\n        # specific spec paths need to be separated by additional \"--\"\n        @execution_args.push(\"--\", *path_options) unless path_options.empty?\n\n        @execution_args\n      end",
    "comment": "Execution arguments for parallel runner  @return [Array]",
    "label": "",
    "id": "3549"
  },
  {
    "raw_code": "def perform_global_setup!\n        Runtime::Browser.configure!\n        Runtime::Release.perform_before_hooks\n      end",
    "comment": "Perform global test setup once before starting parallel processes  @return [void]",
    "label": "",
    "id": "3550"
  },
  {
    "raw_code": "def set_environment!\n        ENV.store(\"NO_KNAPSACK\", \"true\")\n\n        return if ENV[\"QA_GITLAB_URL\"].present?\n\n        Support::GitlabAddress.define_gitlab_address_attribute!\n        ENV.store(\"QA_GITLAB_URL\", Support::GitlabAddress.address_with_port(with_default_port: false))\n      end",
    "comment": "Set necessary environment variables for parallel processes  @return [void]",
    "label": "",
    "id": "3551"
  },
  {
    "raw_code": "def create_runtime_log!\n        Runtime::Logger.debug(\"Creating runtime log file for parallel runner\")\n        knapsack_report = Support::KnapsackReport.knapsack_report(example_data)\n        File.write(RUNTIME_LOG_FILE, knapsack_report.map { |spec, runtime| \"#{spec}:#{runtime}\" }.join(\"\\n\"))\n      end",
    "comment": "Create test runtime log  @return [void]",
    "label": "",
    "id": "3552"
  },
  {
    "raw_code": "def set_mocks\n      return Runtime::Logger.warn(\"Mock host is not set, skipping github response setup\") unless smocker_host\n\n      mock_definition = ENV[\"QA_PROXY_GITHUB_REQUESTS\"] == \"true\" ? \"github_proxy.yml\" : \"github_import.yml\"\n\n      smocker.reset\n      smocker.register(File.read(File.join(mocks_path, mock_definition)))\n    end",
    "comment": "Setup github mocked responses if mock server host is present  @return [void]",
    "label": "",
    "id": "3553"
  },
  {
    "raw_code": "def verify_mocks\n      return Runtime::Logger.warn(\"Mock host is not set, skipping verify step\") unless smocker_host\n\n      verify_response = smocker.verify\n      return if verify_response.success?\n\n      raise \"Mock failures detected:\\n#{JSON.pretty_generate(verify_response.failures)}\"\n    end",
    "comment": "Verify mock session  @return [void]",
    "label": "",
    "id": "3554"
  },
  {
    "raw_code": "def file_name(index)\n        \"#{index.to_s.rjust(2, '0')} file name\"\n      end",
    "comment": "Currently the files are returned in alphabetical order and not in the order they are created. However, it might soon change - see https://gitlab.com/gitlab-org/gitlab/-/issues/250836. By using a leading \"0\" we make sure the test works with either implementation.",
    "label": "",
    "id": "3555"
  },
  {
    "raw_code": "def retry_on_fail(&block)\n        Support::Retrier.retry_on_exception(max_attempts: 6, reload_page: false, sleep_interval: 10, &block)\n      end",
    "comment": "Application settings are cached for up to a minute. So when we change the `receive_max_input_size` setting, the setting might not be applied for minute. This caused the tests to intermittently fail. See https://gitlab.com/gitlab-org/quality/nightly/issues/113  Instead of waiting a minute after changing the setting, we retry the attempt to push if it fails. Most of the time the setting is updated in under a minute, i.e., in fewer than 6 attempts with a 10 second sleep between attempts. See https://gitlab.com/gitlab-org/gitlab-foss/merge_requests/30233#note_188616863",
    "label": "",
    "id": "3556"
  },
  {
    "raw_code": "def retry_on_fail(&block)\n        Support::Retrier.retry_on_exception(max_attempts: 6, reload_page: false, sleep_interval: 10, &block)\n      end",
    "comment": "Application settings are cached for up to a minute. Instead of waiting, retry the attempt to push if it fails.",
    "label": "",
    "id": "3557"
  },
  {
    "raw_code": "def enforce_two_factor_authentication_on_group(group)\n        Flow::Login.while_signed_in(as: owner_user) do\n          group.visit!\n\n          Page::Group::Menu.perform(&:go_to_general_settings)\n          Page::Group::Settings::General.perform(&:set_require_2fa_enabled)\n\n          QA::Support::Retrier.retry_on_exception(reload_page: page) do\n            expect(page).to have_text(two_fa_expected_text)\n          end",
    "comment": "We are intentionally using the UI to enforce 2FA to exercise the flow with UI. Any future tests should use the API for this purpose.",
    "label": "",
    "id": "3558"
  },
  {
    "raw_code": "def save_gitlab_logs(name)\n      Service::DockerRun::Gitlab.new(name: name).extract_service_logs\n    end",
    "comment": "Copy GitLab logs from inside the named Docker container running the GitLab OAuth instance",
    "label": "",
    "id": "3559"
  },
  {
    "raw_code": "def comparable_release(release)\n          release&.except(:_links)&.merge(\n            {\n              author: release[:author].except(:web_url),\n              commit: release[:commit].except(:web_url),\n              commit_path: release[:commit_path].split(\"/-/\").last,\n              tag_path: release[:tag_path].split(\"/-/\").last,\n              assets: release[:assets].merge({\n                sources: release.dig(:assets, :sources).map do |source|\n                  source.merge({ url: source[:url].split(\"/-/\").last })\n                end",
    "comment": "Update release object to be comparable  Convert objects with project specific attributes like paths and urls to be comparable  @param [Hash] release @return [Hash]",
    "label": "",
    "id": "3560"
  },
  {
    "raw_code": "def fetch_source_gitlab_objects\n        logger.info(\"== Fetching source group objects ==\")\n\n        source_branches\n        source_commits\n        source_labels\n        source_milestones\n        source_pipelines\n        source_mrs\n        source_issues\n      end",
    "comment": "rubocop:enable RSpec/InstanceVariable Fetch source project objects for comparison  @return [void]",
    "label": "",
    "id": "3561"
  },
  {
    "raw_code": "def verify_repository_import\n        logger.info(\"== Verifying repository import ==\")\n        expect(imported_project.description).to eq(source_project.description)\n        expect(branches).to match_array(source_branches)\n        expect(commits).to match_array(source_commits)\n      end",
    "comment": "Verify repository imported correctly  @return [void]",
    "label": "",
    "id": "3562"
  },
  {
    "raw_code": "def verify_labels_import\n        logger.info(\"== Verifying label import ==\")\n        expect(labels).to include(*source_labels)\n      end",
    "comment": "Verify imported labels  @return [void]",
    "label": "",
    "id": "3563"
  },
  {
    "raw_code": "def verify_milestones_import\n        logger.info(\"== Verifying milestones import ==\")\n        expect(milestones).to match_array(source_milestones)\n      end",
    "comment": "Verify milestones import  @return [void]",
    "label": "",
    "id": "3564"
  },
  {
    "raw_code": "def verify_pipelines_import\n        logger.info(\"== Verifying pipelines import ==\")\n        expect(pipelines).to eq(source_pipelines)\n      end",
    "comment": "Verify pipelines import  @return [void]",
    "label": "",
    "id": "3565"
  },
  {
    "raw_code": "def verify_merge_requests_import\n        logger.info(\"== Verifying merge request import ==\")\n        @mr_diff = verify_mrs_or_issues('mr')\n      end",
    "comment": "Verify imported merge requests and mr issues  @return [void]",
    "label": "",
    "id": "3566"
  },
  {
    "raw_code": "def verify_issues_import\n        logger.info(\"== Verifying issue import ==\")\n        @issue_diff = verify_mrs_or_issues('issue')\n      end",
    "comment": "Verify imported issues and issue comments  @return [void]",
    "label": "",
    "id": "3567"
  },
  {
    "raw_code": "def verify_mrs_or_issues(type)\n        # Compare length to have easy to read overview how many objects are missing\n        #\n        expected = type == 'mr' ? source_mrs : source_issues\n        actual = type == 'mr' ? mrs : issues\n        count_msg = \"Expected to contain same amount of #{type}s. Source: #{expected.length}, Target: #{actual.length}\"\n        expect(actual.length).to eq(expected.length), count_msg\n\n        comment_diff = verify_comments(type, actual, expected)\n\n        {\n          \"missing_#{type}s\": (expected.keys - actual.keys).filter_map { |it| expected[it]&.slice(:title, :url) },\n          \"extra_#{type}s\": (actual.keys - expected.keys).filter_map { |it| actual[it]&.slice(:title, :url) },\n          \"#{type}_comments\": comment_diff\n        }\n      end",
    "comment": "Verify imported mrs or issues and return missing items  @param [String] type verification object, 'mr' or 'issue' @return [Hash]",
    "label": "",
    "id": "3568"
  },
  {
    "raw_code": "def verify_comments(type, actual, expected)\n        actual.each_with_object([]) do |(key, actual_item), diff|\n          expected_item = expected[key]\n          title = actual_item[:title]\n          msg = \"expected #{type} with title '#{title}' to have\"\n\n          # Print title in the error message to see which object is missing\n          #\n          expect(actual_item).to be_truthy, \"#{msg} been imported\"\n          next unless expected_item\n\n          # Print difference in the description\n          #\n          expected_body = remove_backticks(expected_item[:body])\n          actual_body = remove_backticks(actual_item[:body])\n          body_msg = \"#{msg} same description. diff:\\n#{differ.diff(expected_body, actual_body)}\"\n          expect(actual_body).to eq(expected_body), body_msg\n\n          # Print difference in state\n          #\n          expected_state = expected_item[:state]\n          actual_state = actual_item[:state]\n          state_msg = \"#{msg} same state. Source: #{expected_state}, Target: #{actual_state}\"\n          expect(actual_state).to eq(expected_state), state_msg\n\n          # Print amount difference first\n          #\n          expected_comments = expected_item[:comments].map { |comment| remove_backticks(comment) }\n          actual_comments = actual_item[:comments].map { |comment| remove_backticks(comment) }\n          comment_count_msg = <<~MSG\n            #{msg} same amount of comments. Source: #{expected_comments.length}, Target: #{actual_comments.length}\n          MSG\n          expect(actual_comments.length).to eq(expected_comments.length), comment_count_msg\n          expect(actual_comments).to match_array(expected_comments)\n\n          # Save comment diff\n          #\n          missing_comments = expected_comments - actual_comments\n          extra_comments = actual_comments - expected_comments\n          next if missing_comments.empty? && extra_comments.empty?\n\n          diff << {\n            title: title,\n            target_url: actual_item[:url],\n            source_url: expected_item[:url],\n            missing_comments: missing_comments,\n            extra_comments: extra_comments\n          }\n        end",
    "comment": "Verify imported comments  @param [String] type verification object, 'mrs' or 'issues' @param [Hash] actual @param [Hash] expected @return [Hash]",
    "label": "",
    "id": "3569"
  },
  {
    "raw_code": "def fetch_mrs(project, client, transform_urls: false)\n        imported_mrs = project.merge_requests(auto_paginate: true, attempts: 3)\n\n        Parallel.map(imported_mrs, in_threads: api_parallel_threads) do |mr|\n          resource = build(:merge_request, project: project, iid: mr[:iid], api_client: client)\n\n          [mr[:iid], {\n            url: mr[:web_url],\n            title: mr[:title],\n            body: sanitize_description(mr[:description], transform_urls) || '',\n            state: mr[:state],\n            comments: resource\n              .comments(auto_paginate: true, attempts: 3)\n              .map { |c| sanitize_comment(c[:body], transform_urls) }\n          }]\n        end.to_h\n      end",
    "comment": "Project merge requests with comments  @param [QA::Resource::Project] @param [Runtime::API::Client] client @param [Boolean] transform_urls @return [Hash]",
    "label": "",
    "id": "3570"
  },
  {
    "raw_code": "def fetch_issues(project, client, transform_urls: false)\n        imported_issues = project.issues(auto_paginate: true, attempts: 3)\n\n        Parallel.map(imported_issues, in_threads: api_parallel_threads) do |issue|\n          resource = build(:issue, project: project, iid: issue[:iid], api_client: client)\n\n          [issue[:iid], {\n            url: issue[:web_url],\n            title: issue[:title],\n            state: issue[:state],\n            body: sanitize_description(issue[:description], transform_urls) || '',\n            comments: resource\n              .comments(auto_paginate: true, attempts: 3)\n              .map { |c| sanitize_comment(c[:body], transform_urls) }\n          }]\n        end.to_h\n      end",
    "comment": "Project issues with comments  @param [QA::Resource::Project] @param [Runtime::API::Client] client @param [Boolean] transform_urls @return [Hash]",
    "label": "",
    "id": "3571"
  },
  {
    "raw_code": "def sanitize_comment(body, transform_urls)\n        comment = body&.gsub(created_by_pattern, \"\")\n        return comment unless transform_urls\n\n        comment&.gsub(source_project_url, imported_project_url)\n      end",
    "comment": "Remove added postfixes and transform urls  Source urls need to be replaced with target urls for comparison to work  @param [String] body @param [Boolean] transform_urls @return [String]",
    "label": "",
    "id": "3572"
  },
  {
    "raw_code": "def sanitize_description(body, transform_urls)\n        description = body&.gsub(created_by_pattern, \"\")\n        return description unless transform_urls\n\n        description&.gsub(source_project_url, imported_project_url)\n      end",
    "comment": "Remove added postfixes and transform urls  Source urls need to be replaced with target urls for comparison to work  @param [String] body @param [Boolean] transform_urls @return [String]",
    "label": "",
    "id": "3573"
  },
  {
    "raw_code": "def created_by_pattern\n        @created_by_pattern ||= /\\n\\n \\*By .+ on \\S+\\*/\n      end",
    "comment": "Following objects are memoized via instance variables due to Parallel having some type of issue calling helpers defined via rspec let method Importer user mention pattern  @return [Regex]",
    "label": "",
    "id": "3574"
  },
  {
    "raw_code": "def remove_backticks(text)\n        return unless text.present?\n\n        text.delete('`')\n      end",
    "comment": "Remove backticks from string  @param [String] text @return [String] modified text",
    "label": "",
    "id": "3575"
  },
  {
    "raw_code": "def source_project_url\n        @source_group_url ||= \"#{source_gitlab_address}/#{source_project.full_path}\"\n      end",
    "comment": "Source project url  @return [String]",
    "label": "",
    "id": "3576"
  },
  {
    "raw_code": "def imported_project_url\n        @imported_group_url ||= \"#{Runtime::Scenario.gitlab_address}/#{imported_group.full_path}/#{source_project.path}\"\n      end",
    "comment": "Imported project url  This needs to be constructed manually because it is called before project import finishes  @return [String]",
    "label": "",
    "id": "3577"
  },
  {
    "raw_code": "def save_json(json)\n        File.open(\"tmp/gitlab-import-data.json\", \"w\") { |file| file.write(JSON.pretty_generate(json)) }\n      end",
    "comment": "Save json as file  @param [Hash] json @return [void]",
    "label": "",
    "id": "3578"
  },
  {
    "raw_code": "def fetch_events_and_comments(issuable)\n          comments = issuable.comments.pluck(:body)\n          events = [\n            *issuable.label_events.map { |e| { name: \"#{e[:action]}_label\", label: e.dig(:label, :name) } },\n            *issuable.state_events.map { |e| { name: e[:state] } },\n            *issuable.milestone_events.map { |e| { name: \"#{e[:action]}_milestone\", label: e.dig(:milestone, :title) } }\n          ]\n\n          [comments, events]\n        end",
    "comment": "Fetch events and comments from issue or mr  @param [QA::Resource::Issuable] issuable @return [Array]",
    "label": "",
    "id": "3579"
  },
  {
    "raw_code": "def test_result_data(additional_data = {})\n        {\n          importer: :github,\n          source: {\n            name: \"GitHub\",\n            project_name: github_repo,\n            address: \"https://github.com\"\n          },\n          target: {\n            name: \"GitLab\",\n            address: gitlab_address,\n            project_name: imported_project.full_path\n          }\n        }.deep_merge(additional_data)\n      end",
    "comment": "Base test result data used for test result reporting  @param [Hash] additional_data @return [Hash]",
    "label": "",
    "id": "3580"
  },
  {
    "raw_code": "def fetch_github_objects\n        logger.info(\"== Fetching github repo objects ==\")\n\n        gh_repo\n        gh_branches\n        gh_commits\n        gh_labels\n        gh_milestones\n        gh_issues\n        gh_prs\n      end",
    "comment": "Persist all objects from repository being imported  @return [void]",
    "label": "",
    "id": "3581"
  },
  {
    "raw_code": "def verify_repository_import\n        logger.info(\"== Verifying repository import ==\")\n        expect(imported_project.description).to eq(gh_repo.description)\n        expect(gl_branches).to include(*gh_branches)\n\n        # When testing with very large repositories, comparing with include will raise 'stack level too deep' error\n        # Compare just the size in this case\n        if gh_commits.size > 10000\n          expect(gl_commits.size).to be >= gh_commits.size\n        else\n          expect(gl_commits).to include(*gh_commits)\n        end",
    "comment": "Verify repository imported correctly  @return [void]",
    "label": "",
    "id": "3582"
  },
  {
    "raw_code": "def verify_labels_import\n        logger.info(\"== Verifying label import ==\")\n        expect(gl_labels).to include(*gh_labels)\n      end",
    "comment": "Verify imported labels  @return [void]",
    "label": "",
    "id": "3583"
  },
  {
    "raw_code": "def verify_milestones_import\n        logger.info(\"== Verifying milestones import ==\")\n        expect(gl_milestones).to include(*gh_milestones)\n      end",
    "comment": "Verify milestones import  @return [void]",
    "label": "",
    "id": "3584"
  },
  {
    "raw_code": "def verify_merge_requests_import\n        logger.info(\"== Verifying merge request import ==\")\n        @mr_diff = verify_mrs_or_issues('mr')\n      end",
    "comment": "Verify imported merge requests and mr issues  @return [void]",
    "label": "",
    "id": "3585"
  },
  {
    "raw_code": "def verify_issues_import\n        logger.info(\"== Verifying issue import ==\")\n        @issue_diff = verify_mrs_or_issues('issue')\n      end",
    "comment": "Verify imported issues and issue comments  @return [void]",
    "label": "",
    "id": "3586"
  },
  {
    "raw_code": "def fetch_issuable_comments(id, type)\n        pr = type == \"pr\"\n        comments = []\n        # every pr is also an issue, so when fetching pr comments, issue endpoint has to be used as well\n        comments.push(*with_paginated_request { github_client.issue_comments(github_repo, id) })\n        comments.push(*with_paginated_request { github_client.pull_request_comments(github_repo, id) }) if pr\n        comments.map! { |comment| comment.body&.gsub(gh_link_pattern, dummy_url) }\n        return comments unless pr\n\n        # some suggestions can contain extra whitespaces which gitlab will remove\n        comments.map { |comment| comment.gsub(/suggestion\\s+\\r/, \"suggestion\\r\") }\n      end",
    "comment": "Fetch issuable object comments  @param [Integer] id @param [String] type @return [Array]",
    "label": "",
    "id": "3587"
  },
  {
    "raw_code": "def fetch_issuable_events(id)\n        with_paginated_request { github_client.issue_events(github_repo, id) }\n          .reject { |event| deleted_milestone_event?(event) }\n          .map { |event| event[:event] }\n          .reject { |event| unsupported_events.include?(event) }\n      end",
    "comment": "Fetch issuable object events  @param [Integer] id @return [Array]",
    "label": "",
    "id": "3588"
  },
  {
    "raw_code": "def verify_mrs_or_issues(type)\n        # Compare length to have easy to read overview how many objects are missing\n        #\n        expected = type == 'mr' ? gh_prs : gh_issues\n        actual = type == 'mr' ? mrs : gl_issues\n\n        missing_objects = (expected.keys - actual.keys).map { |it| expected[it].slice(:title, :url) }\n        extra_objects = (actual.keys - expected.keys).map { |it| actual[it].slice(:title, :url) }\n        count_msg = <<~MSG\n          Expected to contain all of GitHub's #{type}s. Gitlab: #{actual.length}, Github: #{expected.length}.\n          Missing: #{missing_objects.map { |it| it[:url] }}\n        MSG\n        expect(expected.length <= actual.length).to be_truthy, count_msg\n\n        content_diff = verify_comments_and_events(type, actual, expected)\n\n        {\n          \"extra_#{type}s\": extra_objects,\n          \"missing_#{type}s\": missing_objects,\n          \"#{type}_content_diff\": content_diff\n        }.compact_blank\n      end",
    "comment": "Verify imported mrs or issues and return content diff  @param [String] type verification object, 'mrs' or 'issues' @return [Hash]",
    "label": "",
    "id": "3589"
  },
  {
    "raw_code": "def verify_comments_and_events(type, actual, expected)\n        actual.each_with_object([]) do |(key, actual_item), content_diff|\n          expected_item = expected[key]\n          title = actual_item[:title]\n          msg = \"expected #{type} with iid '#{key}' to have\"\n\n          # Print title in the error message to see which object is missing\n          #\n          expect(expected_item).to be_truthy, \"#{msg} been imported\"\n          next unless expected_item\n\n          # Print difference in the description\n          #\n          expected_body = expected_item[:body]\n          actual_body = actual_item[:body]\n          body_msg = \"#{msg} same description\"\n          expect(expected_body).to eq(actual_body), body_msg\n\n          # Print amount difference first\n          #\n          expected_comments = expected_item[:comments]\n          actual_comments = actual_item[:comments]\n          comment_count_msg = <<~MSG.strip\n            #{msg} same comments. GitHub: #{expected_comments.length}, GitLab: #{actual_comments.length}\n          MSG\n          expect(actual_comments).to include(*expected_comments), comment_count_msg\n\n          expected_events = expected_item[:events]\n          actual_events = actual_item[:events]\n          event_count_msg = <<~MSG.strip\n            #{msg} same events. GitHub: #{expected_events.length}, GitLab: #{actual_events.length}.\n            Missing event: #{expected_events - actual_events}\n          MSG\n          expect(actual_events).to include(*expected_events), event_count_msg\n\n          # Save comment and event diff\n          #\n          missing_comments = expected_comments - actual_comments\n          extra_comments = actual_comments - expected_comments\n          missing_events = expected_events - actual_events\n          extra_events = actual_events - expected_events\n          next if [missing_comments, missing_events, extra_comments, extra_events].all?(&:empty?)\n\n          content_diff << {\n            title: title,\n            github_url: expected_item[:url],\n            gitlab_url: actual_item[:url],\n            missing_comments: missing_comments,\n            extra_comments: extra_comments,\n            missing_events: missing_events,\n            extra_events: extra_events\n          }.compact_blank\n        end",
    "comment": "Verify imported comments and events  @param [String] type verification object, 'mrs' or 'issues' @param [Hash] actual @param [Hash] expected @return [Hash]",
    "label": "",
    "id": "3590"
  },
  {
    "raw_code": "def gl_branches\n        @gl_branches ||= begin\n          logger.debug(\"= Fetching branches =\")\n          imported_project.repository_branches(auto_paginate: true, attempts: 3).map { |b| b[:name] }\n        end",
    "comment": "Imported project branches  @return [Array]",
    "label": "",
    "id": "3591"
  },
  {
    "raw_code": "def gl_commits\n        @gl_commits ||= begin\n          logger.debug(\"= Fetching commits =\")\n          imported_project.commits(auto_paginate: true, attempts: 3).map { |c| c[:id] }\n        end",
    "comment": "Imported project commits  @return [Array]",
    "label": "",
    "id": "3592"
  },
  {
    "raw_code": "def gl_labels\n        @gl_labels ||= begin\n          logger.debug(\"= Fetching labels =\")\n          imported_project.labels(auto_paginate: true, attempts: 3).map { |label| label.slice(:name, :color) }\n        end",
    "comment": "Imported project labels  @return [Array]",
    "label": "",
    "id": "3593"
  },
  {
    "raw_code": "def gl_milestones\n        @gl_milestones ||= begin\n          logger.debug(\"= Fetching milestones =\")\n          imported_project.milestones(auto_paginate: true, attempts: 3).map { |ms| ms.slice(:title, :description) }\n        end",
    "comment": "Imported project milestones  @return [<Type>] <description>",
    "label": "",
    "id": "3594"
  },
  {
    "raw_code": "def mrs\n        @mrs ||= begin\n          logger.debug(\"= Fetching merge requests =\")\n          imported_mrs = imported_project.merge_requests(**api_request_params)\n\n          logger.debug(\"- Fetching merge request comments #{api_parallel_threads} parallel threads -\")\n          Parallel.map(imported_mrs, in_threads: api_parallel_threads) do |mr|\n            resource = build(:merge_request, project: imported_project, iid: mr[:iid], api_client: api_client)\n\n            comments = resource.comments(**api_request_params)\n            label_events = resource.label_events(**api_request_params)\n            state_events = resource.state_events(**api_request_params)\n            milestone_events = resource.milestone_events(**api_request_params)\n\n            [mr[:iid], {\n              url: mr[:web_url],\n              title: mr[:title],\n              body: sanitize_description(mr[:description]) || '',\n              events: events(comments, label_events, state_events, milestone_events),\n              comments: non_event_comments(comments)\n            }]\n          end.to_h\n        end",
    "comment": "Imported project merge requests  @return [Hash]",
    "label": "",
    "id": "3595"
  },
  {
    "raw_code": "def gl_issues\n        @gl_issues ||= begin\n          logger.debug(\"= Fetching issues =\")\n          imported_issues = imported_project.issues(**api_request_params)\n\n          logger.debug(\"- Fetching issue comments #{api_parallel_threads} parallel threads -\")\n          Parallel.map(imported_issues, in_threads: api_parallel_threads) do |issue|\n            resource = build(:issue, project: imported_project, iid: issue[:iid], api_client: api_client)\n\n            comments = resource.comments(**api_request_params)\n            label_events = resource.label_events(**api_request_params)\n            state_events = resource.state_events(**api_request_params)\n            milestone_events = resource.milestone_events(**api_request_params)\n\n            [issue[:iid], {\n              url: issue[:web_url],\n              title: issue[:title],\n              body: sanitize_description(issue[:description]) || '',\n              events: events(comments, label_events, state_events, milestone_events),\n              comments: non_event_comments(comments)\n            }]\n          end.to_h\n        end",
    "comment": "Imported project issues  @return [Hash]",
    "label": "",
    "id": "3596"
  },
  {
    "raw_code": "def non_event_comments(comments)\n        comments\n          .reject { |c| c[:system] || c[:body].match?(event_pattern) }\n          .map { |c| sanitize_comment(c[:body]) }\n      end",
    "comment": "Filter out event comments  @param [Array] comments @return [Array]",
    "label": "",
    "id": "3597"
  },
  {
    "raw_code": "def events(comments, label_events, state_events, milestone_events)\n        mapped_label_events = label_events.map { |event| event_mapping[\"label_#{event[:action]}\"] }\n        mapped_milestone_events = milestone_events.map { |event| event_mapping[\"milestone_#{event[:action]}\"] }\n        # merged events are fetched through comments so duplicates need to be removed\n        mapped_state_event = state_events.map { |event| event[:state] }.reject { |state| state == \"merged\" }\n        mapped_comment_events = comments.map do |c|\n          event_mapping[c[:body].match(event_pattern)&.named_captures&.fetch(\"event\", nil)]\n        end",
    "comment": "Events  @param [Array] comments @param [Array] label_events @param [Array] state_events @param [Array] milestone_events @return [Array]",
    "label": "",
    "id": "3598"
  },
  {
    "raw_code": "def deleted_milestone_event?(event)\n        return false if %w[milestoned demilestoned].exclude?(event[:event])\n\n        gh_milestone_titles.exclude?(event[:milestone][:title])\n      end",
    "comment": "Check if a milestone event is from a deleted milestone  @param [Hash] event @return [Boolean]",
    "label": "",
    "id": "3599"
  },
  {
    "raw_code": "def sanitize_comment(body)\n        body\n          .gsub(created_by_pattern, \"\")\n          .gsub(suggestion_pattern, \"suggestion\\r\")\n          .gsub(gl_link_pattern, dummy_url)\n          .gsub(gh_link_pattern, dummy_url)\n      end",
    "comment": "Normalize comments and make them directly comparable  * remove created by prefixes * unify suggestion format * replace github and gitlab urls - some of the links to objects get transformed to gitlab entities, some don't, update all links to example.com for now  @param [String] body @return [String]",
    "label": "",
    "id": "3600"
  },
  {
    "raw_code": "def sanitize_description(body)\n        body&.gsub(created_by_pattern, \"\")\n      end",
    "comment": "Remove created by prefix from descripion  @param [String] body @return [String]",
    "label": "",
    "id": "3601"
  },
  {
    "raw_code": "def save_data_json(json)\n        File.open(\"tmp/github-import-data.json\", \"w\") { |file| file.write(JSON.pretty_generate(json)) }\n      end",
    "comment": "Save json as file  @param [Hash] json @return [void]",
    "label": "",
    "id": "3602"
  },
  {
    "raw_code": "def with_paginated_request(&block)\n        resources = with_rate_limit(&block)\n\n        loop do\n          next_link = github_client.last_response.rels[:next]&.href\n          break unless next_link\n\n          logger.debug(\"Fetching resources from next page: '#{next_link}'\")\n          resources.concat(with_rate_limit { github_client.get(next_link) })\n        end",
    "comment": "Custom pagination for github requests  Default autopagination doesn't work correctly with rate limit  @return [Array]",
    "label": "",
    "id": "3603"
  },
  {
    "raw_code": "def with_rate_limit\n        yield\n      rescue Faraday::ForbiddenError => e\n        raise e unless e.response[:status] == 403\n\n        wait = github_client.rate_limit.resets_in + 5\n        logger.warn(\"GitHub rate api rate limit reached, resuming in '#{wait}' seconds\")\n        logger.debug(JSON.parse(e.response[:body])['message'])\n        sleep(wait)\n\n        retry\n      end",
    "comment": "Handle rate limit  @return [Array]",
    "label": "",
    "id": "3604"
  },
  {
    "raw_code": "def current_thread\n        Thread.current.object_id\n      end",
    "comment": "Get current thread id for better logging  @return [Integer]",
    "label": "",
    "id": "3605"
  },
  {
    "raw_code": "def upstream_pipeline\n        create(:pipeline, project: upstream_project, id: upstream_project.latest_pipeline[:id])\n      end",
    "comment": "Fetch upstream project's parent pipeline",
    "label": "",
    "id": "3606"
  },
  {
    "raw_code": "def child_pipeline\n        create(:pipeline,\n          project: upstream_project,\n          id: upstream_pipeline.downstream_pipeline_id(bridge_name: 'trigger_child'))\n      end",
    "comment": "Fetch upstream project's child pipeline",
    "label": "",
    "id": "3607"
  },
  {
    "raw_code": "def downstream_project_pipeline\n        create(:pipeline,\n          project: downstream_project,\n          id: upstream_pipeline.downstream_pipeline_id(bridge_name: 'trigger_downstream_project'))\n      end",
    "comment": "Fetch downstream project's pipeline",
    "label": "",
    "id": "3608"
  },
  {
    "raw_code": "def production_domain(tld)\n          return 'gitlab' unless GitlabEdition.jh?\n          return 'gitlab' if tld == 'hk' || tld == 'cn'\n          return 'jihulab' if tld == 'com'\n        end",
    "comment": "Get production domain value based on GitLab edition and URI's top level domain  @param tld [String] top level domain, e.g. 'hk', 'com' @return [String] 'gitlab' or 'jihulab'",
    "label": "",
    "id": "3609"
  },
  {
    "raw_code": "def fetch_fq_file\n          download_fast_quarantine\n        end",
    "comment": "Fetch and save fast quarantine file  @return [void]",
    "label": "",
    "id": "3610"
  },
  {
    "raw_code": "def configure_rspec\n          # Shared tooling that adds relevant rspec configuration\n          require_relative '../../../../spec/support/fast_quarantine'\n        end",
    "comment": "Configure rspec  @return [void]",
    "label": "",
    "id": "3611"
  },
  {
    "raw_code": "def fq_path\n          @fq_path ||= ENV[\"RSPEC_FAST_QUARANTINE_PATH\"] = File.join(Runtime::Path.qa_root, \"tmp\", fq_filename)\n        end",
    "comment": "Force path to be relative to ruby process in order to avoid issues when dealing with different execution contexts of qa docker container and CI runner environment",
    "label": "",
    "id": "3612"
  },
  {
    "raw_code": "def describe_successfully(*args, &describe_body)\n          describe_run(*args, passed: true, &describe_body)\n        end",
    "comment": "We use an example group wrapper to prevent the state of internal tests expanding into the global state See: https://github.com/rspec/rspec-core/issues/2603",
    "label": "",
    "id": "3613"
  },
  {
    "raw_code": "def skip_or_run_quarantined_tests_or_contexts(example)\n          return if Runtime::Env.quarantine_disabled?\n\n          if filters.key?(:quarantine)\n            included_filters = filters_other_than_quarantine\n\n            # If :quarantine is focused, skip the test/context unless its metadata\n            # includes quarantine and any other filters\n            # E.g., Suppose a test is tagged :smoke and :quarantine, and another is tagged\n            # :ldap and :quarantine. If we wanted to run just quarantined smoke tests\n            # using `--tag quarantine --tag smoke`, without this check we'd end up\n            # running that ldap test as well because of the :quarantine metadata.\n            # We could use an exclusion filter, but this way the test report will list\n            # the quarantined tests when they're not run so that we're aware of them\n            if should_skip_when_focused?(example.metadata, included_filters)\n              example.metadata[:skip] = \"Only running tests tagged with :quarantine and any of #{included_filters.keys}\"\n            end",
    "comment": "Skip tests in quarantine unless we explicitly focus on them or quarantine disabled",
    "label": "",
    "id": "3614"
  },
  {
    "raw_code": "def should_skip_when_focused?(metadata, included_filters)\n          return true unless metadata.key?(:quarantine)\n          return false if included_filters.empty?\n\n          (metadata.keys & included_filters.keys).empty?\n        end",
    "comment": "Checks if a test or context should be skipped.  Returns true if - the metadata does not includes the :quarantine tag or if - the metadata includes the :quarantine tag - and the filter includes other tags that aren't in the metadata",
    "label": "",
    "id": "3615"
  },
  {
    "raw_code": "def configure!\n            return if Runtime::Env.dry_run\n\n            configure_rspec\n          end",
    "comment": "Set up feature flags  @return [void]",
    "label": "",
    "id": "3616"
  },
  {
    "raw_code": "def configure_rspec\n            setup = new\n\n            ::RSpec.configure do |config|\n              config.before(:suite) { setup.run_before }\n              config.after(:suite) { setup.run_after }\n            end",
    "comment": "Add global hooks to perform feature flag changes  @return [void]",
    "label": "",
    "id": "3617"
  },
  {
    "raw_code": "def run_before\n          set_feature_flags\n\n          enable_features\n          disable_features\n        end",
    "comment": "Run feature setup before suite  @return [void]",
    "label": "",
    "id": "3618"
  },
  {
    "raw_code": "def run_after\n          Runtime::Feature.disable(enable_feature) if enable_feature && !enabled\n          Runtime::Feature.enable(disable_feature) if disable_feature && !disabled\n        end",
    "comment": "Restore feature state after suite  @return [void]",
    "label": "",
    "id": "3619"
  },
  {
    "raw_code": "def feature_flags\n          return @feature_flags if defined?(@feature_flags)\n\n          @feature_flags ||= options[:set_feature_flags] || feature_flags_from_env\n        end",
    "comment": "Feature flags to set  @return [<String, nil>]",
    "label": "",
    "id": "3620"
  },
  {
    "raw_code": "def feature_flags_from_env\n          ff = ENV[\"QA_FEATURE_FLAGS\"]\n          return if ff.blank?\n\n          ff.split(\",\").each_with_object({}) do |flag, hash|\n            unless flag.match?(FF_PATTERN)\n              error_msg = \"'#{flag}' in QA_FEATURE_FLAGS environment variable doesn't match pattern '#{FF_PATTERN}'\"\n              next logger.error(error_msg)\n            end",
    "comment": "Fetch feature flags from environment variable  @return [<Hash, nil>]",
    "label": "",
    "id": "3621"
  },
  {
    "raw_code": "def set_feature_flags\n          return unless feature_flags\n\n          Runtime::Feature.set(feature_flags)\n        end",
    "comment": "Update group of feature flags  @return [void]",
    "label": "",
    "id": "3622"
  },
  {
    "raw_code": "def enable_features\n          return unless enable_feature\n\n          @enabled = Runtime::Feature.enabled?(enable_feature)\n          return if @enabled\n\n          Runtime::Feature.enable(enable_feature)\n        end",
    "comment": "Enable features  @return [void]",
    "label": "",
    "id": "3623"
  },
  {
    "raw_code": "def disable_features\n          return unless disable_feature\n\n          @disabled = !Runtime::Feature.enabled?(disable_feature)\n          return if @disabled\n\n          Runtime::Feature.disable(disable_feature)\n        end",
    "comment": "Disable features  @return [void]",
    "label": "",
    "id": "3624"
  },
  {
    "raw_code": "def group_name\n          Env.namespace_name || \"e2e-test-#{time.strftime('%Y-%m-%d-%H-%M-%S')}-#{SecureRandom.hex(8)}\"\n        end",
    "comment": "Random group name with specific pattern  @return [String]",
    "label": "",
    "id": "3625"
  },
  {
    "raw_code": "def sandbox_name\n          return \"gitlab-e2e-sandbox-group-#{sandbox_number}\" if live_env?\n\n          \"e2e-sandbox-#{SecureRandom.hex(6)}\"\n        end",
    "comment": "Top level group name  @return [String]",
    "label": "",
    "id": "3626"
  },
  {
    "raw_code": "def live_env?\n          return @live_env unless @live_env.nil?\n\n          # Memoize the result of this check so every call doesn't parse gitlab address and check hostname\n          # There is no case to change gitlab address in the middle of test process so it should be safe to do\n          @live_env = Runtime::Env.running_on_live_env?\n        end",
    "comment": "Test is running on live environment with limitations for top level group creation  @return [Boolean]",
    "label": "",
    "id": "3627"
  },
  {
    "raw_code": "def sandbox_number\n          ENV['CI_NODE_INDEX'] ? ((ENV['CI_NODE_INDEX'].to_i - 1) % 8) + 1 : @random_sandbox_id ||= rand(1..8)\n        end",
    "comment": "Determines the sandbox group number for live environments  Live environments (like .com) have exactly 8 pre-created sandbox groups (gitlab-e2e-sandbox-group-1 through gitlab-e2e-sandbox-group-8). This method maps CI_NODE_INDEX values to the available 1-8 range using modulo arithmetic to handle parallel jobs with more than 8 nodes. When CI_NODE_INDEX is not set, returns a memoized random number between 1-8.  @return [Integer] A number between 1 and 8 inclusive, corresponding to available sandbox groups @example ENV['CI_NODE_INDEX'] = '1'  # returns 1 (gitlab-e2e-sandbox-group-1) ENV['CI_NODE_INDEX'] = '9'  # returns 1 (wraps to gitlab-e2e-sandbox-group-1) ENV['CI_NODE_INDEX'] = nil  # returns random 1-8 (memoized)",
    "label": "",
    "id": "3628"
  },
  {
    "raw_code": "def self.apply_lookalike_policy\n        gitlab_host = URI.parse(Runtime::Scenario.attributes[:gitlab_address]).host\n        policy_dir = \"/etc/opt/chrome/policies/managed\"\n        policy_file = File.join(policy_dir, \"lookalike-policy.json\")\n\n        return if File.exist?(policy_file)\n\n        policy = {\n          \"LookalikeWarningAllowlistDomains\" => [gitlab_host]\n        }\n\n        FileUtils.mkdir_p(policy_dir)\n        File.write(policy_file, JSON.pretty_generate(policy))\n        QA::Runtime::Logger.info(\"Chrome LookalikeWarningAllowlistDomains policy created to allow: #{gitlab_host}\")\n      rescue StandardError => e\n        QA::Runtime::Logger.info(\"Chrome policy creation failed: #{e.message}\")\n      end",
    "comment": "Adds GitLab host to allow list to avoid lookalike warnings",
    "label": "",
    "id": "3629"
  },
  {
    "raw_code": "def visit(address, page_class, &)\n        Browser::Session.new(address, page_class).perform(&)\n      end",
    "comment": " Visit a page that belongs to a GitLab instance under given address.  Example:  visit(:gitlab, Page::Main::Login) visit('http://gitlab.example/users/sign_in')  In case of an address that is a symbol we will try to guess address based on `Runtime::Scenario#something_address`. ",
    "label": "",
    "id": "3630"
  },
  {
    "raw_code": "def self.target_canary(enable_canary)\n          if QA::Runtime::Env.qa_cookies.to_s.include?(\"gitlab_canary=true\")\n            QA::Runtime::Logger.warn(\"WARNING: Setting cookie through QA_COOKIES var is incompatible with this method.\")\n            return\n          end",
    "comment": "To redirect the browser to a canary or non-canary web node after loading a subject test page @param [Boolean] Send to canary true or false @example: Runtime::Browser::Session.target_canary(true)",
    "label": "",
    "id": "3631"
  },
  {
    "raw_code": "def clear!\n          visit(url)\n          reset_session!\n          @network_conditions_configured = false\n        end",
    "comment": " Selenium allows to reset session cookies for current domain only.  See gitlab-org/gitlab-qa#102 ",
    "label": "",
    "id": "3632"
  },
  {
    "raw_code": "def configure!\n          return if Env.dry_run\n          return unless Env.generate_allure_report?\n\n          configure_allure\n          configure_attachments\n          configure_rspec\n        end",
    "comment": "Configure allure reports  @return [void]",
    "label": "",
    "id": "3633"
  },
  {
    "raw_code": "def configure_allure\n          AllureRspec.configure do |config|\n            config.results_directory = ENV['QA_ALLURE_RESULTS_DIRECTORY'] || 'tmp/allure-results'\n            config.clean_results_directory = false\n\n            # automatically attach links to testcases and issues\n            config.tms_tag = :testcase\n            config.link_tms_pattern = '{}'\n            config.issue_tag = :issue\n            config.link_issue_pattern = '{}'\n\n            # custom grouping of failures, https://docs.qameta.io/allure-report/#_categories_2\n            config.categories = File.new(File.join(Runtime::Path.qa_root, \"allure\", \"categories.json\"))\n\n            if Env.running_in_ci?\n              config.environment_properties = environment_info\n              # Set custom environment name to separate same specs executed in different jobs\n              # Drop number postfixes from parallel jobs by only matching non whitespace characters\n              config.environment = Env.ci_job_name.match(/^\\S+/)[0]\n            end",
    "comment": "Configure allure reporter  @return [void]",
    "label": "",
    "id": "3634"
  },
  {
    "raw_code": "def configure_attachments\n          Capybara::Screenshot.after_save_screenshot do |path|\n            Allure.add_attachment(\n              name: 'screenshot',\n              source: File.open(path),\n              type: Allure::ContentType::PNG,\n              test_case: true\n            )\n          end",
    "comment": "Set up failure screenshot attachments  @return [void]",
    "label": "",
    "id": "3635"
  },
  {
    "raw_code": "def configure_rspec\n          RSpec.configure do |config|\n            config.add_formatter(QA::Support::Formatters::AllureMetadataFormatter)\n            config.add_formatter(AllureRspecFormatter)\n\n            config.append_after do\n              Allure.add_attachment(\n                name: 'browser.log',\n                source: Capybara.current_session.driver.browser.logs.get(:browser).map(&:to_s).join(\"\\n\\n\"),\n                type: Allure::ContentType::TXT,\n                test_case: true\n              )\n            end",
    "comment": "Configure rspec  @return [void]",
    "label": "",
    "id": "3636"
  },
  {
    "raw_code": "def environment_info\n          -> do\n            api_token = User::Data.admin_api_token || User::Data.test_user_api_token\n            return {} unless api_token\n\n            response = get(API::Request.new(API::Client.new(personal_access_token: api_token), '/metadata').url)\n            JSON.parse(response.body, symbolize_names: true).then do |metadata|\n              {\n                **metadata.slice(:version, :revision),\n                kas_version: metadata.dig(:kas, :version)\n              }.compact\n            end",
    "comment": "Gitlab version and revision information  @return [Hash]",
    "label": "",
    "id": "3637"
  },
  {
    "raw_code": "def canary_cookie\n        canary = ENV['QA_COOKIES']&.scan(/gitlab_canary=(true|false)/)&.dig(0, 0)\n\n        canary ? { gitlab_canary: canary } : {}\n      end",
    "comment": "Retrieves the value of the gitlab_canary cookie if set or returns an empty hash.  @return [Hash]",
    "label": "",
    "id": "3638"
  },
  {
    "raw_code": "def webdriver_headless?\n        if ENV.key?('CHROME_HEADLESS')\n          Rails.application.deprecators[:qa].warn(\"CHROME_HEADLESS is deprecated. Use WEBDRIVER_HEADLESS instead.\")\n        end",
    "comment": "set to 'false' to have the browser run visibly instead of headless",
    "label": "",
    "id": "3639"
  },
  {
    "raw_code": "def reuse_chrome_profile?\n        enabled?(ENV['CHROME_REUSE_PROFILE'], default: false)\n      end",
    "comment": "set to 'true' to have Chrome use a fixed profile directory",
    "label": "",
    "id": "3640"
  },
  {
    "raw_code": "def disable_dev_shm?\n        running_in_ci? || enabled?(ENV['CHROME_DISABLE_DEV_SHM'], default: false)\n      end",
    "comment": "Disable /dev/shm use in CI. See https://gitlab.com/gitlab-org/gitlab/issues/4252",
    "label": "",
    "id": "3641"
  },
  {
    "raw_code": "def running_on_release?\n        gitlab_host == 'release.gitlab.net'\n      end",
    "comment": "This method can be removed after decommissioning `release` instance https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/1571",
    "label": "",
    "id": "3642"
  },
  {
    "raw_code": "def personal_access_tokens_disabled?\n        enabled?(ENV['PERSONAL_ACCESS_TOKENS_DISABLED'], default: false)\n      end",
    "comment": "PATs are disabled for FedRamp",
    "label": "",
    "id": "3643"
  },
  {
    "raw_code": "def qa_hostname\n        ENV['QA_HOSTNAME']\n      end",
    "comment": "this is set by the integrations job which will allow bidirectional communication between the app and the specs container should the specs container spin up a server",
    "label": "",
    "id": "3644"
  },
  {
    "raw_code": "def workspaces_cluster_available?\n        enabled?(ENV['WORKSPACES_CLUSTER_AVAILABLE'], default: false)\n      end",
    "comment": "ENV variables for workspaces to run against existing cluster or creating new cluster",
    "label": "",
    "id": "3645"
  },
  {
    "raw_code": "def workspaces_oauth_app_id\n        ENV.fetch(\"WORKSPACES_OAUTH_APP_ID\")\n      end",
    "comment": "ENV variables for workspaces OAuth App and the domain",
    "label": "",
    "id": "3646"
  },
  {
    "raw_code": "def github_access_token\n        ENV['QA_GITHUB_ACCESS_TOKEN'].to_s.strip\n      end",
    "comment": "Specifies the token that can be used for the GitHub API",
    "label": "",
    "id": "3647"
  },
  {
    "raw_code": "def can_test?(feature)\n        raise ArgumentError, %(Unknown feature \"#{feature}\") unless SUPPORTED_FEATURES.include? feature\n\n        enabled?(ENV[SUPPORTED_FEATURES[feature]], default: true)\n      end",
    "comment": "Returns true if there is an environment variable that indicates that the feature is supported in the environment under test. All features are supported by default.",
    "label": "",
    "id": "3648"
  },
  {
    "raw_code": "def deploy_version\n        ENV['DEPLOY_VERSION']\n      end",
    "comment": "Get the version of GitLab currently being tested against @return String Version @example > Env.deploy_version #=> 13.3.4-ee.0",
    "label": "",
    "id": "3649"
  },
  {
    "raw_code": "def third_party_docker_registry\n        ENV['QA_THIRD_PARTY_DOCKER_REGISTRY']\n      end",
    "comment": "ENV variables for authenticating against a private container registry These need to be set if using the Service::DockerRun::Mixins::ThirdPartyDocker module",
    "label": "",
    "id": "3650"
  },
  {
    "raw_code": "def docker_network\n        ENV[\"QA_DOCKER_NETWORK\"]\n      end",
    "comment": "Docker network to use when starting sidecar containers  @return [String]",
    "label": "",
    "id": "3651"
  },
  {
    "raw_code": "def pa_configurator_url\n        ENV['PA_CONFIGURATOR_URL']\n      end",
    "comment": "Product analytics configurator string (e.g. https://usr:pass@gl-configurator.gitlab.com)  @return [String]",
    "label": "",
    "id": "3652"
  },
  {
    "raw_code": "def pa_collector_host\n        ENV['PA_COLLECTOR_HOST']\n      end",
    "comment": "Product analytics collector url (e.g. https://collector.gitlab.com)  @return [String]",
    "label": "",
    "id": "3653"
  },
  {
    "raw_code": "def pa_cube_api_url\n        ENV['PA_CUBE_API_URL']\n      end",
    "comment": "Product analytics cube api url (e.g. https://cube.gitlab.com)  @return [String]",
    "label": "",
    "id": "3654"
  },
  {
    "raw_code": "def pa_cube_api_key\n        ENV['PA_CUBE_API_KEY']\n      end",
    "comment": "Product analytics cube api key  @return [String]",
    "label": "",
    "id": "3655"
  },
  {
    "raw_code": "def rspec_retried?\n        enabled?(ENV['QA_RSPEC_RETRIED'], default: false)\n      end",
    "comment": "Test run is in rspec retried process  @return [Boolean]",
    "label": "",
    "id": "3656"
  },
  {
    "raw_code": "def parallel_run?\n        ENV[\"TEST_ENV_NUMBER\"].present?\n      end",
    "comment": "Execution was started by parallel runner  @return [Boolean]",
    "label": "",
    "id": "3657"
  },
  {
    "raw_code": "def run_in_parallel?\n        enabled?(ENV[\"QA_RUN_IN_PARALLEL\"], default: false)\n      end",
    "comment": "Execute tests in multiple parallel processes  @return [Boolean]",
    "label": "",
    "id": "3658"
  },
  {
    "raw_code": "def no_admin_environment?\n        enabled?(ENV[\"QA_NO_ADMIN_ENV\"], default: false) || gitlab_host == \"gitlab.com\"\n      end",
    "comment": "Environment has no support for admin operations  @return [Boolean]",
    "label": "",
    "id": "3659"
  },
  {
    "raw_code": "def run_type\n        ENV[\"QA_RUN_TYPE\"].presence\n      end",
    "comment": "Test run type  @return [String]",
    "label": "",
    "id": "3660"
  },
  {
    "raw_code": "def dry_run\n        enabled?(ENV[\"QA_RSPEC_DRY_RUN\"], default: false)\n      end",
    "comment": "Execution performed with --dry-run flag  @return [Boolean]",
    "label": "",
    "id": "3661"
  },
  {
    "raw_code": "def ignore_runtime_data?\n        enabled?(ENV[\"QA_IGNORE_RUNTIME_DATA\"], default: false)\n      end",
    "comment": "Ignore runtime data when generating knapsack reports  @return [Boolean]",
    "label": "",
    "id": "3662"
  },
  {
    "raw_code": "def create_unique_test_users?\n        enabled?(ENV[\"QA_CREATE_UNIQUE_TEST_USERS\"], default: true)\n      end",
    "comment": "Create uniq test users for each test  @return [Boolean]",
    "label": "",
    "id": "3663"
  },
  {
    "raw_code": "def project_studio_enabled?\n        enabled?(ENV[\"QA_PROJECT_STUDIO\"], default: false)\n      end",
    "comment": "Run tests against Project Studio UI  @return [Boolean]",
    "label": "",
    "id": "3664"
  },
  {
    "raw_code": "def gitlab_host\n        # gitlab address should be immutable so it's ok to memoize as global\n        @gitlab_host ||= URI.parse(Runtime::Scenario.gitlab_address).host\n      end",
    "comment": "Gitlab host tests are running against  @return [String]",
    "label": "",
    "id": "3665"
  },
  {
    "raw_code": "def initialize(gdk_folder: nil)\n        @gdk_folder = gdk_folder\n      end",
    "comment": "@param gdk_folder [string] path to the folder for the running GDK instance, used to determine if running locally",
    "label": "",
    "id": "3666"
  },
  {
    "raw_code": "def has_project?(project_name)\n        query_string = \"select name from projects where name LIKE '#{project_name}';\"\n\n        query(query_string).include?(project_name)\n      end",
    "comment": "checks if a project exists in the datastore @param project_name [string] name of project to be checked @return [boolean]",
    "label": "",
    "id": "3667"
  },
  {
    "raw_code": "def query(query_string)\n        raise 'query handler not defined for this instance' unless has_gdk_folder? # local GDK instance\n\n        query_gdk(query_string)\n      end",
    "comment": "runs a query against the datastore, filters based on what the datastores it's running against @param query_string [string] query to be run @return [string] result of the query",
    "label": "",
    "id": "3668"
  },
  {
    "raw_code": "def query_gdk(query_string)\n        raise \"GitLab Development Kit is not running on #{@gdk_folder}\" unless gdk_is_running?\n\n        command = \"./bin/gdk psql -t -c \\\"#{query_string}\\\"\"\n\n        `cd #{@gdk_folder}; #{command}`\n      end",
    "comment": "runs a query against a GDK database running locally @param query_string [string] SQL query to be run @return [string] result of the query",
    "label": "",
    "id": "3669"
  },
  {
    "raw_code": "def set(flags, **scopes)\n          flags.each_pair do |flag, state|\n            case state\n            when 'enabled', 'enable', 'true', 1, true\n              enable(flag, **scopes)\n            when 'disabled', 'disable', 'false', 0, false\n              disable(flag, **scopes)\n            when 'deleted'\n              QA::Runtime::Logger.info(\"Feature flag definition for '#{flag}' was deleted. The state of the feature flag has not been changed.\")\n            else\n              raise UnknownStateError, \"Unknown feature flag state: #{state}\"\n            end",
    "comment": "Set one or more flags to their specified state.  @param [Hash] flags The feature flags and desired values, e.g., { 'flag1' => 'enabled', 'flag2' => \"disabled\" } @param [Hash] scopes The scope (user, project, group) to apply the feature flag to.",
    "label": "",
    "id": "3670"
  },
  {
    "raw_code": "def set_and_verify(key, enable:, **scopes)\n          msg = \"#{enable ? 'En' : 'Dis'}abling feature: #{key}\"\n          msg += \" for scope \\\"#{scopes_to_s(**scopes)}\\\"\" if scopes.present?\n          QA::Runtime::Logger.info(msg)\n\n          Support::Retrier.retry_on_exception(sleep_interval: 2) do\n            set_feature(key, enable, **scopes)\n\n            is_enabled = nil\n\n            QA::Support::Waiter.wait_until(sleep_interval: 1) do\n              is_enabled = enabled?(key, **scopes)\n              is_enabled == enable || (!enable && scopes.present?)\n            end",
    "comment": "Change a feature flag and verify that the change was successful Arguments: key: The feature flag to set (as a string) enable: `true` to enable the flag, `false` to disable it scopes: Any scope (user, project, group) to restrict the change to",
    "label": "",
    "id": "3671"
  },
  {
    "raw_code": "def set_application_settings(api_client: admin_api_client, **application_settings)\n          @original_application_settings = get_application_settings(api_client: api_client)\n\n          QA::Runtime::Logger.info(\"Setting application settings: #{application_settings}\")\n          r = put(Runtime::API::Request.new(api_client, APPLICATION_SETTINGS_PATH).url, **application_settings)\n          return if r.code == QA::Support::API::HTTP_STATUS_OK\n\n          body = parse_body(r)\n          raise(\"Couldn't set application settings #{application_settings.inspect}, code: '#{r.code}', body: #{body}\")\n        end",
    "comment": "Set a GitLab application setting Example: #set({ allow_local_requests_from_web_hooks_and_services: true }) #set(allow_local_requests_from_web_hooks_and_services: true) https://docs.gitlab.com/ee/api/settings.html",
    "label": "",
    "id": "3672"
  },
  {
    "raw_code": "def get_application_setting(setting, api_client: admin_api_client)\n          get_application_settings(api_client: api_client).fetch(setting)\n        end",
    "comment": "Get a single application setting  @param setting [Symbol] the name of the setting to get @param api_client [Runtime::API::Client] the API client representing the admin user who will get the setting @return [String]",
    "label": "",
    "id": "3673"
  },
  {
    "raw_code": "def restore_application_settings(...)\n          set_application_settings(**@original_application_settings.slice(...))\n        end",
    "comment": "TODO: This class probably needs to be refactored because this method relies on original settings to have been populated sometime in the past and there is no guarantee original settings instance variable is still valid",
    "label": "",
    "id": "3674"
  },
  {
    "raw_code": "def enable_local_requests\n          set_application_settings(allow_local_requests_from_web_hooks_and_services: true)\n        end",
    "comment": "Enable the application setting that allows requests from local services to the GitLab instance  @return [Void]",
    "label": "",
    "id": "3675"
  },
  {
    "raw_code": "def disable_local_requests\n          set_application_settings(allow_local_requests_from_web_hooks_and_services: false)\n        end",
    "comment": "Disables the application setting that allows local requests  @return [Void]",
    "label": "",
    "id": "3676"
  },
  {
    "raw_code": "def logger\n          @logger ||= Gitlab::QA::TestLogger.logger(\n            level: Gitlab::QA::Runtime::Env.log_level,\n            source: logger_source,\n            path: log_path\n          )\n        end",
    "comment": "Global logger instance  @return [ActiveSupport::Logger]",
    "label": "",
    "id": "3677"
  },
  {
    "raw_code": "def admin_username\n          @admin_username ||= admin_variable_with_default(\n            \"username\",\n            ADMIN_USERNAME_VARIABLE_NAME,\n            DEFAULT_ADMIN_USERNAME\n          )\n        end",
    "comment": "Admin user username  @return [String]",
    "label": "",
    "id": "3678"
  },
  {
    "raw_code": "def admin_password\n          @admin_password ||= admin_variable_with_default(\n            \"password\",\n            ADMIN_PASSWORD_VARIABLE_NAME,\n            DEFAULT_ADMIN_PASSWORD\n          )\n        end",
    "comment": "Admin user password  @return [String]",
    "label": "",
    "id": "3679"
  },
  {
    "raw_code": "def admin_api_token\n          @admin_api_token ||= ENV[ADMIN_API_TOKEN_VARIABLE]\n        end",
    "comment": "Admin api token  @return [String]",
    "label": "",
    "id": "3680"
  },
  {
    "raw_code": "def test_user_username\n          ENV[TEST_USER_USERNAME_VARIABLE_NAME]\n        end",
    "comment": "Global test user username  @return [String]",
    "label": "",
    "id": "3681"
  },
  {
    "raw_code": "def test_user_password\n          ENV[TEST_USER_PASSWORD_VARIABLE_NAME]\n        end",
    "comment": "Global test user password  @return [String]",
    "label": "",
    "id": "3682"
  },
  {
    "raw_code": "def test_user_api_token\n          ENV[TEST_USER_API_TOKEN_VARIABLE_NAME]\n        end",
    "comment": "Global test user api token  @return [String]",
    "label": "",
    "id": "3683"
  },
  {
    "raw_code": "def ldap_username\n          ENV[\"GITLAB_LDAP_USERNAME\"]\n        end",
    "comment": "LDAP user username  @return [String]",
    "label": "",
    "id": "3684"
  },
  {
    "raw_code": "def ldap_password\n          ENV[\"GITLAB_LDAP_PASSWORD\"]\n        end",
    "comment": "LDAP user password  @return [String]",
    "label": "",
    "id": "3685"
  },
  {
    "raw_code": "def admin_variable_with_default(type, var_name, default)\n          ENV[var_name].then do |value|\n            next value unless value.blank?\n\n            Logger.warn(\"Admin #{type} variable '#{var_name}' not set, using default value!\")\n            default\n          end",
    "comment": "Admin user related variable with default value  @param type [String] @param var_name [String] @param default [String] @return [String]",
    "label": "",
    "id": "3686"
  },
  {
    "raw_code": "def default_api_client\n            user_api_client || admin_api_client\n          end",
    "comment": "Default api client depending on environment setup  @return [QA::Runtime::API::Client]",
    "label": "",
    "id": "3687"
  },
  {
    "raw_code": "def admin_api_client\n            return @admin_api_client if defined?(@admin_api_client)\n            return @admin_api_client = nil if Env.no_admin_environment? || Env.personal_access_tokens_disabled?\n\n            info(\"Creating admin api client for api fabrications\")\n            @admin_api_client = create_api_client(\n              token: admin_api_token,\n              default_token: Data::DEFAULT_ADMIN_API_TOKEN,\n              user_proc: -> { admin_user },\n              check_admin: true)\n\n            info(\"Global admin api client set up successfully\")\n            @admin_api_client\n          rescue InvalidCredentialsError => e\n            unless admin_username == Data::DEFAULT_ADMIN_USERNAME && admin_password == Data::DEFAULT_ADMIN_PASSWORD\n              # Only raise error when explicitly configured credentials are invalid\n              raise e\n            end",
    "comment": "Global admin client  @return [QA::Runtime::API::Client]",
    "label": "",
    "id": "3688"
  },
  {
    "raw_code": "def user_api_client\n            return @user_api_client if defined?(@user_api_client)\n            return @user_api_client = nil if Env.personal_access_tokens_disabled?\n\n            @user_api_client = if create_unique_test_user?\n                                 test_user.api_client\n                               else\n                                 info(\"Creating api client for global test user\")\n                                 create_api_client(token: test_user_api_token, user_proc: -> { test_user })\n                               end",
    "comment": "Global test user api client This api client is used as a primary one for resource fabrication that do not require admin privileges  @return [QA::Runtime::API::Client]",
    "label": "",
    "id": "3689"
  },
  {
    "raw_code": "def admin_user\n            return @admin_user if defined?(@admin_user)\n            return @admin_user = nil if Env.no_admin_environment?\n\n            info(\"Initializing admin user using predefined credentials\")\n            @admin_user = init_user(\n              username: admin_username,\n              password: admin_password,\n              api_client: @admin_api_client,\n              admin: true\n            )\n          end",
    "comment": "Global admin user  @return [QA::Resource::User]",
    "label": "",
    "id": "3690"
  },
  {
    "raw_code": "def test_user\n            return @test_user if defined?(@test_user)\n\n            if ldap_user_configured?\n              info(\"LDAP credentials configured, using LDAP user for test as main test user\")\n              return @test_user = ldap_user\n            elsif create_unique_test_user?\n              return @test_user = create_new_user\n            end",
    "comment": "Global test user used as a primary user for test execution  @return [QA::Resource::User]",
    "label": "",
    "id": "3691"
  },
  {
    "raw_code": "def ldap_user\n            raise MissingLdapCredentialsError, 'LDAP credentials not configured' unless ldap_user_configured?\n\n            Resource::User.init do |user|\n              user.username = ldap_username\n              user.password = ldap_password\n              user.ldap_user = true\n            end",
    "comment": "Instance of user with LDAP username and password  @return [QA::Resource::User]",
    "label": "",
    "id": "3692"
  },
  {
    "raw_code": "def additional_test_user\n            return create_new_user if admin_api_client\n\n            init_user(\n              username: extra_test_user_credential(Data::ADDITIONAL_TEST_USERNAME_VARIABLE_NAME),\n              password: extra_test_user_credential(Data::ADDITIONAL_TEST_PASSWORD_VARIABLE_NAME)\n            ).reload!\n          end",
    "comment": "Additional test user  @return [QA::Resource::User]",
    "label": "",
    "id": "3693"
  },
  {
    "raw_code": "def reset_test_user!\n            remove_instance_variable(:@test_user) if instance_variable_defined?(:@test_user)\n            remove_instance_variable(:@user_api_client) if instance_variable_defined?(:@user_api_client)\n          end",
    "comment": "Reset stored test user  @return [void]",
    "label": "",
    "id": "3694"
  },
  {
    "raw_code": "def create_unique_test_user?\n            return false unless Env.create_unique_test_users?\n\n            !Env.running_on_live_env? && !Env.personal_access_tokens_disabled? && admin_api_client\n          end",
    "comment": "Create unique test user when fetching test user instead of using predefined one  @return [Boolean]",
    "label": "",
    "id": "3695"
  },
  {
    "raw_code": "def create_api_client(token:, user_proc:, default_token: nil, check_admin: false)\n            if token\n              API::Client.new(personal_access_token: token).tap do |client|\n                validate_api_client!(client, check_admin: check_admin)\n              end",
    "comment": "Create api client with provided token with fallback to UI creation of token  @param [String] token @param [Proc] user_proc @param [String] default_token @return [QA::Runtime::API::Client]",
    "label": "",
    "id": "3696"
  },
  {
    "raw_code": "def init_user(username:, password:, api_client: nil, admin: false)\n            return if username.nil? || password.nil?\n\n            user = Resource::User.init do |user|\n              user.username = username\n              user.password = password\n              user.is_admin = admin\n            end",
    "comment": "Initialize new user with predefined username and password  @param [String] username @param [String] password @param [QA::Runtime::API::Client] api_client @param [Boolean] admin @return [QA::Resource::User]",
    "label": "",
    "id": "3697"
  },
  {
    "raw_code": "def create_new_user\n            info(\"Creating test user\")\n            Resource::User.fabricate! do |user|\n              user.with_personal_access_token = true\n              user.api_client = admin_api_client\n            end",
    "comment": "Create new user with personal access token  @return [QA::Resource::User]",
    "label": "",
    "id": "3698"
  },
  {
    "raw_code": "def token_valid?(token, check_admin:)\n            return unless token\n\n            debug(\"Validating if api token is valid\")\n            validate_api_client!(API::Client.new(personal_access_token: token), check_admin: check_admin)\n            debug(\"Api token is valid\")\n            true\n          rescue InvalidTokenError => e\n            debug(\"Api token is not valid, error: #{e}. Skipping...\")\n            false\n          end",
    "comment": "Check if provided token is valid?  @param [String] token @param [Boolean] check_admin @return [Boolean]",
    "label": "",
    "id": "3699"
  },
  {
    "raw_code": "def create_api_token_via_ui!(user)\n            pat = Resource::PersonalAccessToken.fabricate_via_browser_ui! do |resource|\n              resource.username = user.username\n              resource.password = user.password\n            end",
    "comment": "Create api token via UI for provided user Update user api_client to use fabricated token  @param [QA::Resource::User] user @return [String]",
    "label": "",
    "id": "3700"
  },
  {
    "raw_code": "def validate_api_client!(client, check_admin: true)\n            debug(\"Validating api client\")\n            resp = fetch_user_details(client)\n\n            if resp.code == 403 && resp.body.include?(\"Your password expired\")\n              raise ExpiredPasswordError, \"Password for client's user has expired and must be reset\"\n            elsif !status_ok?(resp)\n              raise InvalidTokenError, \"API client validation failed! Code: #{resp.code}, Err: '#{resp.body}'\"\n            end",
    "comment": "Validate if client belongs to an admin user  @param [QA::Runtime::API::Client] client @return [void]",
    "label": "",
    "id": "3701"
  },
  {
    "raw_code": "def client_belongs_to_user?(client, user)\n            resp = fetch_user_details(client)\n            unless status_ok?(resp)\n              raise InvalidTokenError, \"API client validation failed! Code: #{resp.code}, Err: '#{resp.body}'\"\n            end",
    "comment": "Check if token belongs to specific user  @param [QA::Runtime::API::Client] client @param [QA::Resource::User] user @return [Boolean]",
    "label": "",
    "id": "3702"
  },
  {
    "raw_code": "def fetch_user_details(client)\n            Support::API.get(API::Request.new(client, \"/user\").url)\n          end",
    "comment": "Fetch user details of given api client  @param [QA::Runtime::API::Client] client @return [RestClient::Response]",
    "label": "",
    "id": "3703"
  },
  {
    "raw_code": "def status_ok?(resp)\n            resp.code == Support::API::HTTP_STATUS_OK\n          end",
    "comment": "Validate 200 HTTP status code of response  @param [RestClient::Response] resp @return [Boolean]",
    "label": "",
    "id": "3704"
  },
  {
    "raw_code": "def ldap_user_configured?\n            ldap_username.present? && ldap_password.present?\n          end",
    "comment": "Check if environment has ldap user set  @return [Boolean]",
    "label": "",
    "id": "3705"
  },
  {
    "raw_code": "def extra_test_user_credential(var_name)\n            ENV[var_name].presence || raise(MissingUserCredentialError, \"Missing '#{var_name}' environment variable\")\n          end",
    "comment": "Additional test user credential  @param var_name [String] @return [String]",
    "label": "",
    "id": "3706"
  },
  {
    "raw_code": "def self.as_admin\n          User::Store.admin_api_client\n        end",
    "comment": "TODO: remove this method as currently it only serves as a delegator to User::Store",
    "label": "",
    "id": "3707"
  },
  {
    "raw_code": "def request_path(path, version: API_VERSION, **query_string)\n          full_path = if path == '/graphql'\n                        ::File.join('/api', path)\n                      else\n                        ::File.join('/api', version, path)\n                      end",
    "comment": "Prepend a request path with the path to the API  path - Path to append  Examples  >> request_path('/issues') => \"/api/v4/issues\"  >> request_path('/issues', private_token: 'sometoken) => \"/api/v4/issues?private_token=...\"  Returns the relative path to the requested API resource",
    "label": "",
    "id": "3708"
  },
  {
    "raw_code": "def self.fetch(\n        stage_name:,\n        access_token:,\n        project_id: ENV[\"CI_PROJECT_ID\"],\n        pipeline_id: ENV[\"CI_PIPELINE_ID\"]\n      )\n        new(\n          stage_name: stage_name,\n          project_id: project_id,\n          pipeline_id: pipeline_id,\n          access_token: access_token\n        ).parallel_jobs\n      end",
    "comment": "Fetch parallel job names in given stage  Default to arguments available on CI  @param [String] stage_name @param [Integer] project_id @param [Integer] pipeline_id @param [String] access_token @return [Array]",
    "label": "",
    "id": "3709"
  },
  {
    "raw_code": "def parallel_jobs\n        api_get(\"projects/#{project_id}/pipelines/#{pipeline_id}/jobs?per_page=100\")\n          .select { |job| job[:stage] == stage_name && job[:name].match?(PARALLEL_JOB_NAME_PATTERN) }\n          .map { |job| job[:name].gsub(%r{ \\d+/\\d+}, \"\") }\n          .uniq\n      end",
    "comment": "Parallel job list  @return [Array<String>]",
    "label": "",
    "id": "3710"
  },
  {
    "raw_code": "def api_get(path)\n        response = get(\"#{api_url}/#{path}\", { headers: { \"PRIVATE-TOKEN\" => access_token } })\n        raise \"Failed to fetch pipeline jobs: '#{response.body}'\" unless response.code == API::HTTP_STATUS_OK\n\n        parse_body(response)\n      end",
    "comment": "Api get request  @param [String] path @param [Hash] payload @return [Hash, Array]",
    "label": "",
    "id": "3711"
  },
  {
    "raw_code": "def api_url\n        @api_url ||= ENV['CI_API_V4_URL'] || \"https://gitlab.com/api/v4\"\n      end",
    "comment": "Gitlab api url  @return [String]",
    "label": "",
    "id": "3712"
  },
  {
    "raw_code": "def gcs_client\n        Fog::Google::Storage.new(\n          google_project: ENV['QA_METRICS_GCS_PROJECT_ID'] || raise('Missing QA_METRICS_GCS_PROJECT_ID env variable'),\n          **gcs_credentials)\n      end",
    "comment": "GCS Client  @return [Fog::Google::StorageJSON]",
    "label": "",
    "id": "3713"
  },
  {
    "raw_code": "def gcs_credentials\n        json_key = ENV['QA_METRICS_GCS_CREDS'] || raise(\n          'QA_METRICS_GCS_CREDS env variable is required!'\n        )\n        return { google_json_key_location: json_key } if File.exist?(json_key)\n\n        { google_json_key_string: json_key }\n      end",
    "comment": "GCS Credentials  @return [Hash]",
    "label": "",
    "id": "3714"
  },
  {
    "raw_code": "def log_completion(log, attempts)\n        return unless log && attempts > 0\n\n        QA::Runtime::Logger.debug('ended retry')\n      end",
    "comment": "Log completion if more than one attempt performed  @param [Boolean] log @param [Integer] attempts @return [void]",
    "label": "",
    "id": "3715"
  },
  {
    "raw_code": "def configure!\n        return unless QA::Runtime::Env.knapsack?\n\n        logger.debug(\"Configuring knapsack execution\")\n        setup_logger!\n        setup_environment!\n      end",
    "comment": "Configure knapsack report  * Setup variables * Fetch latest report  @return [void]",
    "label": "",
    "id": "3716"
  },
  {
    "raw_code": "def create_local_report!(example_data)\n        logger.info(\"Creating knapsack report from runtime data\")\n        report_path = File.join(BASE_PATH, report_name)\n        knapsack_report = knapsack_report(example_data)\n        File.write(report_path, knapsack_report.to_json)\n        ENV[\"KNAPSACK_REPORT_PATH\"] = report_path\n\n        knapsack_report\n      rescue StandardError => e\n        ENV[\"KNAPSACK_REPORT_PATH\"] = FALLBACK_REPORT\n        logger.warn(\"Failed to create knapsack report: #{e}\")\n        logger.warn(\"Falling back to '#{FALLBACK_REPORT}'\")\n      end",
    "comment": "Create local knapsack report based on example runtime data and configure it to be used by knapsack  Passing list of examples allows to craft a more precise report that will not have runtime data for examples that will actually be skipped due to dynamic metadata which can cause uneven test distribution  @param example_data [Hash<String, String>] example id list to be included in the report @return [Hash<String, Number>]",
    "label": "",
    "id": "3717"
  },
  {
    "raw_code": "def knapsack_report(example_data)\n        runtime_report = JSON.load_file(RUNTIME_REPORT)\n        report = example_data.each_with_object(Hash.new { |h, k| h[k] = 0 }) do |(id, status), report|\n          next report[example_file_path(id)] += runtime_report[id] || 0.01 if status == \"passed\"\n\n          # if example was not executed, add small runtime to the report\n          # this is needed for knapsack to not consider all specs that got skipped dynamically as leftover specs\n          # https://github.com/KnapsackPro/knapsack?tab=readme-ov-file#what-does-leftover-specs-mean\n          report[example_file_path(id)] += 0.01\n        end",
    "comment": "Knapsack report hash  @param example_data [Hash<String, String>] @return [Hash<String, Number>]",
    "label": "",
    "id": "3718"
  },
  {
    "raw_code": "def upload_example_runtimes(glob)\n        raise \"QA_RUN_TYPE must be set for custom report\" unless run_type\n\n        reports = Pathname.glob(glob).select { |file| file.extname == \".json\" }\n        raise \"Glob '#{glob}' did not contain any valid report files!\" if reports.empty?\n\n        logger.info(\"Processing '#{reports.size}' report files\")\n        report = example_runtimes(reports).sort.to_h\n\n        file = \"#{EXAMPLE_RUNTIMES_PATH}/#{run_type}.json\"\n        logger.info(\"Uploading example runtime report '#{file}'\")\n        client.put_object(BUCKET, file, JSON.pretty_generate(report))\n      end",
    "comment": "Create and upload custom report based on data from JsonFormatter report files  @param glob [String] @return [void]",
    "label": "",
    "id": "3719"
  },
  {
    "raw_code": "def create_merged_runtime_report\n        logger.info(\"Fetching all example runtime data from GCS '#{BUCKET}' bucket\")\n        items = client.list_objects(BUCKET, prefix: EXAMPLE_RUNTIMES_PATH).items\n\n        logger.info(\"Fetched example runtime files #{items.map(&:name)}, creating merged knapsack report\")\n        items.each_with_object({}) do |report, runtimes|\n          json = JSON.parse(client.get_object(BUCKET, report.name)[:body])\n\n          # merge report and keep only the longest runtime\n          json.each { |id, runtime| runtimes[id] = runtime unless (runtimes[id] || 0) > runtime }\n        end",
    "comment": "Merged example runtime data report from all report files  @return [Hash<String, Number>]",
    "label": "",
    "id": "3720"
  },
  {
    "raw_code": "def create_knapsack_report(runtime_report)\n        runtime_report.each_with_object(Hash.new { |hsh, key| hsh[key] = 0 }) do |(id, runtime), spec_runtimes|\n          spec_runtimes[example_file_path(id)] += runtime\n        end",
    "comment": "Create knapsack report from example runtime data  @param runtime_report [Hash<String, Number>] @return [Hash<String, Number>]",
    "label": "",
    "id": "3721"
  },
  {
    "raw_code": "def setup_logger!\n        Knapsack.logger = logger\n      end",
    "comment": "Setup knapsack logger  @return [void]",
    "label": "",
    "id": "3722"
  },
  {
    "raw_code": "def setup_environment!\n        ENV[\"KNAPSACK_TEST_DIR\"] = \"qa/specs/features\"\n        ENV[\"KNAPSACK_REPORT_PATH\"] = FALLBACK_REPORT\n\n        if ENV[\"KNAPSACK_TEST_FILE_PATTERN\"]\n          logger.warn(<<~MSG)\n            KNAPSACK_TEST_FILE_PATTERN variable is set and is overriding automatically generated test pattern '#{test_pattern}'!\n            To ensure correct test distribution, please remove KNAPSACK_TEST_FILE_PATTERN variable and pass specific test folders as command line arguments instead.\n          MSG\n        else\n          logger.debug(\"Setting knapsack test pattern to '#{test_pattern}'\")\n          ENV[\"KNAPSACK_TEST_FILE_PATTERN\"] = test_pattern\n        end",
    "comment": "Set knapsack environment variables  @return [void]",
    "label": "",
    "id": "3723"
  },
  {
    "raw_code": "def client\n        @client ||= Fog::Google::Storage.new(google_project: PROJECT, **gcs_credentials)\n      end",
    "comment": "GCS client  @return [Fog::Google::Storage]",
    "label": "",
    "id": "3724"
  },
  {
    "raw_code": "def report_base_path\n        @report_base_path ||= \"knapsack\"\n      end",
    "comment": "Base path of knapsack report  @return [String]",
    "label": "",
    "id": "3725"
  },
  {
    "raw_code": "def gcs_credentials\n        json_key = ENV[\"QA_KNAPSACK_REPORT_GCS_CREDENTIALS\"] || raise(\n          \"QA_KNAPSACK_REPORT_GCS_CREDENTIALS env variable is required!\"\n        )\n        return { google_json_key_location: json_key } if File.exist?(json_key)\n\n        { google_json_key_string: json_key }\n      end",
    "comment": "GCS credentials json  @return [Hash]",
    "label": "",
    "id": "3726"
  },
  {
    "raw_code": "def example_runtimes(reports)\n        reports\n          .flat_map { |report| JSON.load_file(report, symbolize_names: true) }\n          .each_with_object({}) do |json, runtimes|\n            json[:examples].each do |ex|\n              next if ex[:ignore_runtime_data] || ex[:status] != \"passed\"\n\n              # keep the longest running example\n              runtimes[ex[:id]] = ex[:run_time] unless (runtimes[:id] || 0) > ex[:run_time]\n            end",
    "comment": "Get example runtimes from JsonFormatter report files  @param reports [Array<Pathname>] @return [Hash<Number>]",
    "label": "",
    "id": "3727"
  },
  {
    "raw_code": "def report_name\n        \"#{ENV['CI_JOB_NAME_SLUG'] || 'local'}-knapsack-report.json\"\n      end",
    "comment": "Knapsack report file name  @return [String]",
    "label": "",
    "id": "3728"
  },
  {
    "raw_code": "def example_file_path(example_id)\n        example_id.match(/(\\S+)\\[\\S+\\]/)[1].gsub(\"./\", \"\")\n      end",
    "comment": "Extract file path from example id  @param example_id [String] @return [String]",
    "label": "",
    "id": "3729"
  },
  {
    "raw_code": "def get\n          @options ||= {}\n        end",
    "comment": "Get global cli options  @return [Hash]",
    "label": "",
    "id": "3730"
  },
  {
    "raw_code": "def set(options)\n          @options = options\n        end",
    "comment": "Set global cli options  @param [Hash] options @return [Hash]",
    "label": "",
    "id": "3731"
  },
  {
    "raw_code": "def record_method_call(name:, runtime:, filename:, call_arg: nil)\n          method_call_data[name] << { runtime: runtime, filename: filename, call_arg: call_arg }\n        end",
    "comment": "Record data for a method call  @param [String] name method name @param [Number] runtime method execution runtime @param [String] call_arg method call argument @return [void]",
    "label": "",
    "id": "3732"
  },
  {
    "raw_code": "def method_call_data\n          @method_calls ||= Hash.new { |hsh, key| hsh[key] = [] }\n        end",
    "comment": "Recorded method calls  @return [Hash]",
    "label": "",
    "id": "3733"
  },
  {
    "raw_code": "def container_logs\n        smocker_container.logs\n      end",
    "comment": "Fetch smocker container logs  @return [String]",
    "label": "",
    "id": "3734"
  },
  {
    "raw_code": "def reset!\n        api.reset\n        api.register(mocks)\n      end",
    "comment": "Reset mock definitions  @return [void]",
    "label": "",
    "id": "3735"
  },
  {
    "raw_code": "def teardown!\n        smocker_container&.remove!\n      end",
    "comment": "Remove the Smocker Docker container  @return [void]",
    "label": "",
    "id": "3736"
  },
  {
    "raw_code": "def destination_url\n        @logs_endpoint ||= api.url('logs')\n      end",
    "comment": "Stream destination url  @return [String]",
    "label": "",
    "id": "3737"
  },
  {
    "raw_code": "def wait_for_event(event_type, entity_type, entity_path = nil, wait: 10, raise_on_failure: true)\n        event = Waiter.wait_until(max_duration: wait, sleep_interval: 1, raise_on_failure: false) do\n          api.history.find do |record|\n            body = record.request[:body]\n            next if body.blank?\n\n            body&.dig(:event_type) == event_type.to_s &&\n              body&.dig(:entity_type) == entity_type &&\n              (!entity_path || body&.dig(:entity_path) == entity_path)\n          end&.request\n        end",
    "comment": "Wait for the mock service to receive a request with the specified event type  @param [Symbol] event_type the event to wait for @param [String] entity_type the entity type of the event @param [String] entity_path the event entity identifier @param [Integer] wait the amount of time to wait for the event to be received @param [Boolean] raise_on_failure raise an error if the event is not received @return [Hash] the request",
    "label": "",
    "id": "3738"
  },
  {
    "raw_code": "def wait_for_streaming_to_start(event_type:, entity_type:)\n        # Create and then remove an SSH key and confirm that the mock streaming server received the event\n        Waiter.wait_until(max_duration: 60, sleep_interval: 5, message: 'Waiting for streaming to start') do\n          yield\n\n          wait_for_event(event_type, entity_type, wait: 2, raise_on_failure: false)\n        end",
    "comment": "Wait for GitLab to start streaming audit events and for the Smocker server to be ready to receive them.  When we start the mock streaming server it sometimes doesn't start receiving traffic immediately, even when the smocker API reports that it's ready. In addition, there can be a brief delay after a new streaming destination is configured before it sends events.",
    "label": "",
    "id": "3739"
  },
  {
    "raw_code": "def check_page_for_error_code(page)\n          QA::Runtime::Logger.debug \"Performing page error check!\"\n\n          # Test for 404 img alt\n          error_code = page_html(page).xpath(\"//img\").map { |t| t[:alt] }.first\n          return report!(page, 404) if error_code && error_code.include?('404')\n\n          # 500 error page in header surrounded by newlines, try to match\n          five_hundred_test = page_html(page).xpath(\"//h1/text()\").map.first\n          five_hundred_title = page_html(page).xpath(\"//head/title/text()\").map.first\n          if five_hundred_test&.text&.include?('500') && five_hundred_title&.text.eql?('Something went wrong (500)')\n            return report!(page, 500)\n          end",
    "comment": "rubocop:disable Rails/Pluck",
    "label": "",
    "id": "3740"
  },
  {
    "raw_code": "def log_request_errors(page)\n          return if !QA::Runtime::Env.can_intercept? || QA::Runtime::Browser.blank_page?\n\n          url = page.driver.browser.current_url\n          QA::Runtime::Logger.debug \"Fetching API error cache for #{url}\"\n\n          cache = page.execute_script <<~JS\n            return !(typeof(Interceptor)===\"undefined\") ? Interceptor.getCache() : null;\n          JS\n\n          return unless cache&.dig('errors')\n\n          grouped_errors = group_errors(cache['errors'])\n\n          errors = grouped_errors.map do |error_metadata, error_body|\n            \"#{error_metadata} -- #{error_body[:request_id_string]}\\n#{error_body[:error_body]}\"\n          end",
    "comment": "rubocop:enable Rails/Pluck Log request errors triggered from async api calls from the browser  If any errors are found in the session, log them using QA::Runtime::Logger @param [Capybara::Session] page",
    "label": "",
    "id": "3741"
  },
  {
    "raw_code": "def masked_parsed_response(response, mask_by_key:)\n        Helpers::Masker.mask(parse_body(response), by_key: Array(mask_by_key))\n      end",
    "comment": "Returns the response body with secrets masked.  @param [String] response the response body as the string value of a JSON hash @param [Array<Symbol>] mask_by_key the keys of the JSON parsed response body whose values will be masked @return [Hash] the response body with the specified secrets values replaced with `****`",
    "label": "",
    "id": "3742"
  },
  {
    "raw_code": "def with_canary(args)\n        canary_cookie = QA::Runtime::Env.canary_cookie\n        return args if canary_cookie.empty?\n\n        args.deep_merge(cookies: QA::Runtime::Env.canary_cookie)\n      end",
    "comment": "Merges the gitlab_canary cookie into existing cookies for mixed environment testing.  @param [Hash] args the existing args passed to method @return [Hash] args or args with merged canary cookie if it exists",
    "label": "",
    "id": "3743"
  },
  {
    "raw_code": "def start_fabrication\n          Thread.current[:fabrications_ongoing] = 0 unless Thread.current.key?(:fabrications_ongoing)\n\n          Thread.current[:fabrications_ongoing] += 1\n        end",
    "comment": "Start fabrication and increment ongoing fabrication count  @return [void]",
    "label": "",
    "id": "3744"
  },
  {
    "raw_code": "def finish_fabrication\n          Thread.current[:fabrications_ongoing] -= 1\n        end",
    "comment": "Finish fabrication and decrement ongoing fabrication count  @return [void]",
    "label": "",
    "id": "3745"
  },
  {
    "raw_code": "def save_fabrication(type, time)\n          return unless Thread.current.key?(type)\n          return unless top_level_fabrication?\n\n          Thread.current[type] += time\n        end",
    "comment": "Save fabrication time if it's first in fabrication stack  @param [Symbol] type @param [Symbol] time @return [void]",
    "label": "",
    "id": "3746"
  },
  {
    "raw_code": "def top_level_fabrication?\n          Thread.current[:fabrications_ongoing] == 1\n        end",
    "comment": "Check if current fabrication is the only one in the stack  @return [Boolean]",
    "label": "",
    "id": "3747"
  },
  {
    "raw_code": "def use_typical_params?\n        [443, 80].include?(uri.port)\n      end",
    "comment": "Checks if typical parameters should be used. That means the SSH port will not be needed because it's port 22, and the git user is named 'git'. We assume that typical parameters should be used if the host URI includes a typical HTTP(S) port (80 or 443)  @return [Boolean] whether typical SSH port and git user parameters should be used",
    "label": "",
    "id": "3748"
  },
  {
    "raw_code": "def query_api\n        @query_api ||= influx_client.create_query_api\n      end",
    "comment": "Query client  @return [QueryApi]",
    "label": "",
    "id": "3749"
  },
  {
    "raw_code": "def write_api\n        @write_api ||= influx_client.create_write_api\n      end",
    "comment": "Write client  @return [WriteApi]",
    "label": "",
    "id": "3750"
  },
  {
    "raw_code": "def influx_client\n        @influx_client ||= InfluxDB2::Client.new(\n          ENV[\"QA_INFLUXDB_URL\"] || raise(\"Missing QA_INFLUXDB_URL env variable\"),\n          ENV[\"QA_INFLUXDB_TOKEN\"] || raise(\"Missing QA_INFLUXDB_TOKEN env variable\"),\n          bucket: INFLUX_TEST_METRICS_BUCKET,\n          org: \"gitlab-qa\",\n          precision: InfluxDB2::WritePrecision::NANOSECOND,\n          read_timeout: ENV[\"QA_INFLUXDB_TIMEOUT\"]&.to_i || 60,\n          open_timeout: ENV[\"QA_INFLUXDB_TIMEOUT\"]&.to_i || 60\n        )\n      end",
    "comment": "InfluxDb client  @return [InfluxDB2::Client]",
    "label": "",
    "id": "3751"
  },
  {
    "raw_code": "def run_type\n        @run_type ||= if env('QA_RUN_TYPE')\n                        env('QA_RUN_TYPE')\n                      elsif LIVE_ENVS.exclude?(ci_project_name)\n                        nil\n                      else\n                        test_subset = if env('SMOKE_ONLY') == 'true'\n                                        'sanity'\n                                      else\n                                        'full'\n                                      end",
    "comment": "Test run type Automatically infer for staging (`gstg`, `gstg-cny`, `gstg-ref`), canary, preprod or production env  @return [String, nil]",
    "label": "",
    "id": "3752"
  },
  {
    "raw_code": "def merge_request_iid\n        env('CI_MERGE_REQUEST_IID') || env('TOP_UPSTREAM_MERGE_REQUEST_IID')\n      end",
    "comment": "Merge request iid  @return [String]",
    "label": "",
    "id": "3753"
  },
  {
    "raw_code": "def env(name)\n        return unless ENV[name] && !ENV[name].empty?\n\n        ENV[name]\n      end",
    "comment": "Return non empty environment variable value  @param [String] name @return [String, nil]",
    "label": "",
    "id": "3754"
  },
  {
    "raw_code": "def fetch(tags, specs = nil, logger: Runtime::Logger.logger)\n          logger.debug(\"Fetching example data for tags '#{tags}' and specs '#{specs}'\")\n\n          Tempfile.open(\"test-metadata.json\") do |file|\n            tags = tags.presence || Specs::Runner::DEFAULT_SKIPPED_TAGS\n            args = [\n              \"--dry-run\",\n              \"--no-color\",\n              \"--format\", QA::Support::JsonFormatter.to_s, \"--out\", file.path,\n              *tags.flat_map { |tag| [\"--tag\", tag.to_s] }\n            ]\n            args.push(\"--\", *(specs.presence || Specs::Runner::DEFAULT_TEST_PATH_ARGS))\n\n            logger.debug(\"Executing rspec in subprocess with args: #{args.join(' ')}\")\n            status, output = run_rspec_subprocess(args)\n            unless status.success?\n              logger.error(\"Failed to fetch example data, subprocess output:\")\n              logger.error(\"====== BEGIN OUTPUT ======\\n#{output}\\n====== END OUTPUT ======\")\n              raise \"Failed to fetch example data for tags '#{tags}' and specs '#{specs}'\"\n            end",
    "comment": "Fetch example data for particular tag and spec combination  @param tags [Array<String>] @param specs [Array<String>] @param logger [Logger] @return [Array<Hash>]",
    "label": "",
    "id": "3755"
  },
  {
    "raw_code": "def run_rspec_subprocess(args)\n          Tempfile.open(\"output.log\") do |output|\n            Process.fork do\n              ENV.store(\"QA_RSPEC_DRY_RUN\", \"true\")\n\n              status = RSpec::Core::Runner.run([\"--out\", output.path, *args])\n              Kernel.exit(status)\n            end",
    "comment": "Execute rspec in a forked subprocess with dry run enabled  @param args [Array<String>] @return [Process::Status]",
    "label": "",
    "id": "3756"
  },
  {
    "raw_code": "def define_gitlab_address_attribute!(address = Runtime::Env.gitlab_url)\n          return if initialized?\n\n          validate_address(address)\n\n          Runtime::Scenario.define(:gitlab_address, address_with_port(address, with_default_port: false))\n          # Define the \"About\" page as an `about` subdomain.\n          # @example\n          #   Given *gitlab_address* = 'https://gitlab.com/' #=> https://about.gitlab.com/\n          #   Given *gitlab_address* = 'https://staging.gitlab.com/' #=> https://about.staging.gitlab.com/\n          #   Given *gitlab_address* = 'http://gitlab-abc123.test/' #=> http://about.gitlab-abc123.test/\n          Runtime::Scenario.define(\n            :about_address,\n            URI(address).then { |uri| \"#{uri.scheme}://about.#{host_with_port(address, with_default_port: false)}\" }\n          )\n\n          @initialized = true\n        end",
    "comment": "Define gitlab address  @param [String] address @return [void]",
    "label": "",
    "id": "3757"
  },
  {
    "raw_code": "def address_with_port(address = Runtime::Scenario.gitlab_address, with_default_port: true)\n          uri = URI.parse(address)\n\n          \"#{uri.scheme}://#{host_with_port(uri, with_default_port: with_default_port)}\"\n        end",
    "comment": "Get gitlab address with port and path  @param [String] address @param [Boolean] with_default_port keep default port 80 or 443 @return [String]",
    "label": "",
    "id": "3758"
  },
  {
    "raw_code": "def host_with_port(address = Runtime::Scenario.gitlab_address, with_default_port: true)\n          uri = address.is_a?(URI) ? address : URI.parse(address)\n          port = !with_default_port && [80, 443].include?(uri.port) ? \"\" : \":#{uri.port}\"\n\n          \"#{uri.host}#{port}#{uri.path}\"\n        end",
    "comment": "Get gitlab host with port and path  @param [<String, URI>] address @param [Boolean] with_default_port keep default port 80 or 443 @return [String]",
    "label": "",
    "id": "3759"
  },
  {
    "raw_code": "def initialized?\n          @initialized\n        end",
    "comment": "Gitlab address already set up  @return [Boolean]",
    "label": "",
    "id": "3760"
  },
  {
    "raw_code": "def validate_address(address)\n          Runtime::Address.valid?(address) || raise(\n            ::ArgumentError, \"Configured gitlab address is not a valid url: #{address}\"\n          )\n        end",
    "comment": "Validate if address is a valid url  @param [String] address @return [void]",
    "label": "",
    "id": "3761"
  },
  {
    "raw_code": "def click_element(name, page = nil, **kwargs)\n          msg = [\"clicking :#{highlight_element(name)}\"]\n          msg << \"and ensuring #{page} is present\" if page\n\n          log(msg.join(' '), :info)\n          log(\"with args #{kwargs}\")\n          log_slow_code(name, **kwargs) { super }\n        end",
    "comment": "@param name [Symbol, String] name of the data_qa_selector or data-testid element @param page [Class] a target page class to check existence of (class must inherit from QA::Page::Base) @param kwargs [Hash] keyword arguments to pass to Capybara finder",
    "label": "",
    "id": "3762"
  },
  {
    "raw_code": "def log(msg, level = :debug)\n          QA::Runtime::Logger.public_send(level, msg)\n        end",
    "comment": "Log message  @param [String] msg @param [Symbol] level @return [void]",
    "label": "",
    "id": "3763"
  },
  {
    "raw_code": "def highlight_element(element)\n          element.to_s.underline.bright\n        end",
    "comment": "Highlight element for enhanced logging  @param [String] element @return [String]",
    "label": "",
    "id": "3764"
  },
  {
    "raw_code": "def log_has_element_or_not(method, name, found, **kwargs)\n          name = name.name if name.is_a? QA::Page::Element\n\n          msg = [\"#{method} :#{name}\"]\n          msg << %(with text \"#{kwargs[:text]}\") if kwargs[:text]\n          msg << \"class: #{kwargs[:class]}\" if kwargs[:class]\n          msg << \"(wait: #{kwargs[:wait] || Capybara.default_max_wait_time})\"\n          msg << \"returned: #{found}\"\n\n          log(msg.compact.join(' '))\n        end",
    "comment": "Log message for has_element? and has_no_element? methods  @param [String] method the method name @param [Symbol, String, QA::Page::Element] name the name of the element @param [Boolean] found the result of the method @param [Hash] kwargs",
    "label": "",
    "id": "3765"
  },
  {
    "raw_code": "def log_slow_code(param_info = '', **kwargs)\n          starting = kwargs.fetch(:starting_time, Time.now)\n          result = yield\n          ending = kwargs.fetch(:ending_time, Time.now)\n          duration = (ending - starting).round(3)\n          method_name = caller_locations(1, 1).first.label\n          called_from = caller_locations(2, 1).first.path\n          CodeRuntimeTracker.record_method_call(\n            name: method_name,\n            runtime: duration,\n            filename: called_from.gsub(\"#{Runtime::Path.qa_root}/\", ''),\n            call_arg: param_info.is_a?(QA::Page::Element) ? param_info.name : param_info\n          )\n\n          if duration > kwargs.fetch(:log_slow_threshold, 1)\n            Runtime::Logger.warn(\"Potentially Slow Code '#{method_name} #{param_info}' took #{duration}s\")\n          end",
    "comment": "Prints warning log if code duration is slower than threshold @param [String (frozen)] paramInfo is info relating to the slow element",
    "label": "",
    "id": "3766"
  },
  {
    "raw_code": "def wrap(actual)\n          actual = actual.to_capybara_node if actual.respond_to?(:to_capybara_node)\n          @context_el = if actual.respond_to?(:has_selector?)\n                          actual\n                        else\n                          Capybara.string(actual.to_s)\n                        end",
    "comment": "From https://github.com/teamcapybara/capybara/blob/fe5940c6afbfe32152df936ce03ad1371ae05354/lib/capybara/rspec/matchers/base.rb#L66",
    "label": "",
    "id": "3767"
  },
  {
    "raw_code": "def wait_and_check(actual, expectation_name)\n              attempt = 0\n\n              QA::Runtime::Logger.info(\n                \"Running eventually matcher with '#{operator_msg}' operator with: '#{retry_args}' arguments\"\n              )\n              QA::Support::Retrier.retry_until(**retry_args, log: false) do\n                QA::Runtime::Logger.debug(\"evaluating expectation, attempt: #{attempt += 1}\")\n\n                public_send(expectation_name, actual)\n              rescue RSpec::Expectations::ExpectationNotMetError, QA::Resource::ApiFabricator::ResourceNotFoundError\n                false\n              end",
    "comment": "Execute rspec expectation within retrier  @param [Proc] actual @param [Symbol] expectation_name @return [Boolean]",
    "label": "",
    "id": "3768"
  },
  {
    "raw_code": "def default_expectation(actual)\n              expect(result(&actual)).to public_send(*expectation_args)\n            end",
    "comment": "Execute rspec expectation  @param [Proc] actual @return [void]",
    "label": "",
    "id": "3769"
  },
  {
    "raw_code": "def when_negated_expectation(actual)\n              expect(result(&actual)).not_to public_send(*expectation_args)\n            end",
    "comment": "Execute negated rspec expectation  @param [Proc] actual @return [void]",
    "label": "",
    "id": "3770"
  },
  {
    "raw_code": "def result\n              @result = yield\n            end",
    "comment": "Result of actual block  @return [Object]",
    "label": "",
    "id": "3771"
  },
  {
    "raw_code": "def e\n              @e ||= 'Waiter did not fail!'\n            end",
    "comment": "Error message placeholder to indicate waiter did not fail properly This message should not appear under normal circumstances since it should always be assigned from repeater  @return [String]",
    "label": "",
    "id": "3772"
  },
  {
    "raw_code": "def operator_msg\n              operator == 'eq' ? 'equal' : operator\n            end",
    "comment": "Operator message  @return [String]",
    "label": "",
    "id": "3773"
  },
  {
    "raw_code": "def operator\n              @operator ||= name.to_s.match(/eventually_(.+?)$/).to_a[1].to_s\n            end",
    "comment": "Expect operator  @return [String]",
    "label": "",
    "id": "3774"
  },
  {
    "raw_code": "def expectation_args\n              if operator.include?('truthy') || operator.include?('falsey') || operator.include?('empty')\n                operator\n              elsif operator == 'include' && expected.is_a?(Array)\n                [operator, *expected]\n              else\n                [operator, expected]\n              end",
    "comment": "Expectation args  @return [String, Array]",
    "label": "",
    "id": "3775"
  },
  {
    "raw_code": "def retry_args\n              @retry_args ||= { sleep_interval: 0.5 }\n            end",
    "comment": "Custom retry arguments  @return [Hash]",
    "label": "",
    "id": "3776"
  },
  {
    "raw_code": "def fail_message(negate: false)\n              \"#{e}:\\n\\nexpected #{negate ? 'not ' : ''}to #{description}\\n\\n\" \\\n                \"last attempt was: #{@result.nil? ? 'nil' : actual_formatted}\\n\\n\" \\\n                \"Diff:#{diff}\"\n            end",
    "comment": "Custom failure message  @param [Boolean] negate @return [String]",
    "label": "",
    "id": "3777"
  },
  {
    "raw_code": "def expected_formatted\n              RSpec::Support::ObjectFormatter.format(expected)\n            end",
    "comment": "Formatted expect  @return [String]",
    "label": "",
    "id": "3778"
  },
  {
    "raw_code": "def actual_formatted\n              RSpec::Support::ObjectFormatter.format(@result)\n            end",
    "comment": "Formatted actual result  @return [String]",
    "label": "",
    "id": "3779"
  },
  {
    "raw_code": "def diff\n              RSpec::Support::Differ.new(color: true).diff(@result, expected)\n            end",
    "comment": "Object diff  @return [String]",
    "label": "",
    "id": "3780"
  },
  {
    "raw_code": "def example_finished(example_notification)\n          example = example_notification.example\n\n          add_quarantine_issue_link(example)\n          add_failure_issues_link(example, example_notification)\n          add_ci_job_link(example)\n          set_behavior_categories(example)\n        end",
    "comment": "Finished example Add additional metadata to report  @param [RSpec::Core::Notifications::ExampleNotification] example_notification @return [void]",
    "label": "",
    "id": "3781"
  },
  {
    "raw_code": "def add_quarantine_issue_link(example)\n          issue_link = example.metadata.dig(:quarantine, :issue)\n\n          return unless issue_link\n          return example.issue('Quarantine issue', issue_link) if issue_link.is_a?(String)\n\n          issue_link.each { |link| example.issue('Quarantine issue', link) } if issue_link.is_a?(Array)\n        rescue StandardError => e\n          log(:error, \"Failed to add quarantine issue link for example '#{example.description}', error: #{e}\")\n        end",
    "comment": "Add quarantine issue links  @param [RSpec::Core::Example] example @return [void]",
    "label": "",
    "id": "3782"
  },
  {
    "raw_code": "def add_failure_issues_link(example, example_notification)\n          return unless example.execution_result.status == :failed\n\n          search_parameters = {\n            sort: 'updated_desc',\n            scope: 'all',\n            state: 'opened'\n          }.map { |key, value| \"#{key}=#{value}\" }.join('&')\n\n          exception_message = example.exception.message || \"\"\n          message_lines = strip_ansi_codes(example_notification.message_lines) || []\n          exception_message_lines = message_lines.first(20)\n          search_terms = {\n            test_file_path: example.file_path.gsub('./qa/specs/features/', '').to_s,\n            exception_message: exception_message_lines.empty? ? exception_message : exception_message_lines.join(\"\\n\")\n          }.map { |_, value| \"search=#{ERB::Util.url_encode(value)}\" }.join('&')\n\n          search_url = \"https://gitlab.com/#{ISSUE_PROJECT}/-/issues?#{search_parameters}&#{search_terms}\"\n          example.issue('Failure issues', search_url)\n        rescue StandardError => e\n          log(:error, \"Failed to add failure issue link for example '#{example.description}', error: #{e}\")\n        end",
    "comment": "Add failure issues link  @param [RSpec::Core::Example] example @return [void]",
    "label": "",
    "id": "3783"
  },
  {
    "raw_code": "def add_ci_job_link(example)\n          return unless Runtime::Env.running_in_ci?\n\n          example.add_link(name: \"Job(#{Runtime::Env.ci_job_name})\", url: Runtime::Env.ci_job_url)\n        rescue StandardError => e\n          log(:error, \"Failed to add ci job link for example '#{example.description}', error: #{e}\")\n        end",
    "comment": "Add ci job link  @param [RSpec::Core::Example] example @return [void]",
    "label": "",
    "id": "3784"
  },
  {
    "raw_code": "def set_behavior_categories(example)\n          file_path = example.file_path.gsub('./qa/specs/features', '')\n          devops_stage = file_path.match(%r{\\d{1,2}_(\\w+)/})&.captures&.first\n\n          feature_category = example.metadata[:feature_category]\n          product_group = example.metadata[:product_group]\n\n          example.epic(devops_stage) if devops_stage\n          example.feature(feature_category) if feature_category\n          example.feature(product_group) if product_group && feature_category.nil?\n        end",
    "comment": "Add behavior categories to report  @param [RSpec::Core::Example] example @return [void]",
    "label": "",
    "id": "3785"
  },
  {
    "raw_code": "def log(level, message)\n          QA::Runtime::Logger.public_send(level, \"[Allure]: #{message}\")\n        end",
    "comment": "Print log message  @param [Symbol] level @param [String] message @return [void]",
    "label": "",
    "id": "3786"
  },
  {
    "raw_code": "def example_group_started(example_group_notification)\n          group = example_group_notification.group\n\n          skip_or_run_feature_flag_tests_or_contexts(group)\n        end",
    "comment": "Starts example group @param [RSpec::Core::Notifications::GroupNotification] example_group_notification @return [void]",
    "label": "",
    "id": "3787"
  },
  {
    "raw_code": "def example_started(example_notification)\n          example = example_notification.example\n\n          # if skip propagated from example_group, do not reset skip metadata\n          skip_or_run_feature_flag_tests_or_contexts(example) unless example.metadata[:skip]\n        end",
    "comment": "Starts example @param [RSpec::Core::Notifications::ExampleNotification] example_notification @return [void]",
    "label": "",
    "id": "3788"
  },
  {
    "raw_code": "def example_group_started(example_group_notification)\n          group = example_group_notification.group\n\n          skip_or_run_quarantined_tests_or_contexts(group)\n        end",
    "comment": "Starts example group @param [RSpec::Core::Notifications::GroupNotification] example_group_notification @return [void]",
    "label": "",
    "id": "3789"
  },
  {
    "raw_code": "def example_started(example_notification)\n          example = example_notification.example\n\n          # if skip propagated from example_group, do not reset skip metadata\n          skip_or_run_quarantined_tests_or_contexts(example) unless example.metadata[:skip]\n        end",
    "comment": "Starts example @param [RSpec::Core::Notifications::ExampleNotification] example_notification @return [void]",
    "label": "",
    "id": "3790"
  },
  {
    "raw_code": "def example_group_started(example_group_notification)\n          set_skip_metadata(example_group_notification.group)\n        end",
    "comment": "Starts example group @param [RSpec::Core::Notifications::GroupNotification] example_group_notification @return [void]",
    "label": "",
    "id": "3791"
  },
  {
    "raw_code": "def example_started(example_notification)\n          example = example_notification.example\n\n          # if skip propagated from example_group, do not reset skip metadata\n          set_skip_metadata(example_notification.example) unless example.metadata[:skip]\n        end",
    "comment": "Starts example @param [RSpec::Core::Notifications::ExampleNotification] example_notification @return [void]",
    "label": "",
    "id": "3792"
  },
  {
    "raw_code": "def set_skip_metadata(example)\n          return if Runtime::Scenario.attributes[:test_metadata_only]\n          return skip_only(example.metadata) if example.metadata.key?(:only)\n          return skip_except(example.metadata) if example.metadata.key?(:except)\n        end",
    "comment": "Skip example_group or example  @param [<RSpec::Core::ExampleGroup, RSpec::Core::Example>] example @return [void]",
    "label": "",
    "id": "3793"
  },
  {
    "raw_code": "def skip_only(metadata)\n          return if context_matches?(metadata[:only])\n\n          metadata[:skip] = 'Test is not compatible with this environment or pipeline'\n        end",
    "comment": "Skip based on 'only' condition  @param [Hash] metadata @return [void]",
    "label": "",
    "id": "3794"
  },
  {
    "raw_code": "def skip_except(metadata)\n          return unless except?(metadata[:except])\n\n          metadata[:skip] = 'Test is excluded in this job'\n        end",
    "comment": "Skip based on 'except' condition  @param [Hash] metadata @return [void]",
    "label": "",
    "id": "3795"
  },
  {
    "raw_code": "def stop(_notification)\n          save_test_mapping\n        end",
    "comment": "Runs at the end of suite  @param [RSpec::Core::Notifications::ExamplesNotification] notification @return [void]",
    "label": "",
    "id": "3796"
  },
  {
    "raw_code": "def example_started(example_notification)\n          return if example_notification.example.metadata[:skip]\n\n          response = nil\n          QA::Support::Retrier.retry_until(max_attempts: 5, sleep_interval: 1) do\n            response = delete(cov_api_endpoint, headers: headers_access_token)\n            next true if response.code == 200\n\n            logger.debug(\"Failed to clear coverage, code: #{response.code}, body: #{response.body}\")\n            false\n          end",
    "comment": "Example start event",
    "label": "",
    "id": "3797"
  },
  {
    "raw_code": "def example_finished(example_notification)\n          return if example_notification.example.metadata[:skip] || example_failed?(example_notification)\n\n          response = nil\n          QA::Support::Retrier.retry_until(max_attempts: 1, sleep_interval: 2) do\n            response = get(cov_api_endpoint, headers: headers_access_token)\n            coverage = JSON.parse(response.body)\n            next true if response.code == 200 && coverage.any?\n\n            if response.code != 200\n              logger.debug(\"Fetching coverage data failed, code: #{response.code}, body: #{response.body}\")\n            end",
    "comment": "Example finish event",
    "label": "",
    "id": "3798"
  },
  {
    "raw_code": "def save_test_mapping\n          file = \"tmp/test-code-paths-mapping-#{ENV['CI_JOB_NAME_SLUG'] || 'local'}-#{SecureRandom.hex(6)}.json\"\n          # To write two different files in case of failed specs being retried\n\n          File.write(file, test_mapping.to_json)\n          logger.info(\"Saved test coverage mapping data to #{file}\")\n        rescue StandardError => e\n          logger.error(\"Failed to save test coverage mapping data, error: #{e}\")\n        end",
    "comment": "Save coverage test mapping file  @return [void]",
    "label": "",
    "id": "3799"
  },
  {
    "raw_code": "def stop(notification)\n          return log(:warn, \"Missing run_type, skipping metrics export!\") unless run_type\n\n          parse_execution_data(notification.examples)\n\n          export_test_metrics\n          save_test_metrics\n        end",
    "comment": "Finish test execution  @param [RSpec::Core::Notifications::ExamplesNotification] notification @return [void]",
    "label": "",
    "id": "3800"
  },
  {
    "raw_code": "def execution_data(examples = nil)\n          @execution_metrics ||= examples.filter_map { |example| test_stats(example) }\n        end",
    "comment": "Save execution data for the run  @param [Array<RSpec::Core::Example>] examples @return [Array<Hash>]",
    "label": "",
    "id": "3801"
  },
  {
    "raw_code": "def export_test_metrics\n          return log(:info, \"Exporting test metrics to not enabled, skipping \") unless export_metrics?\n\n          push_test_metrics\n          push_fabrication_metrics\n          push_code_runtime_metrics\n        end",
    "comment": "Export metrics directly to InfluxDb or GCS bucket  @return [void]",
    "label": "",
    "id": "3802"
  },
  {
    "raw_code": "def save_test_metrics\n          return log(:info, \"Saving test metrics json not enabled, skipping\") unless save_metrics_json?\n\n          file = File.join('tmp', metrics_file_name(prefix: 'test', with_pipeline_id_postfix: false))\n\n          File.write(file, execution_data.to_json) && log(:debug, \"Saved test metrics to #{file}\")\n        rescue StandardError => e\n          log(:error, \"Failed to save test execution metrics, error: #{e}\")\n        end",
    "comment": "Save metrics in json file  @return [void]",
    "label": "",
    "id": "3803"
  },
  {
    "raw_code": "def push_test_metrics\n          push_test_metrics_to_influxdb\n          push_test_metrics_to_gcs\n        end",
    "comment": "Upload test execution metrics  @return [void]",
    "label": "",
    "id": "3804"
  },
  {
    "raw_code": "def push_test_metrics_to_influxdb\n          write_api.write(data: execution_data)\n          log(:info, \"Pushed #{execution_data.length} test execution entries to influxdb\")\n        rescue StandardError => e\n          log(:error, \"Failed to push test execution metrics to influxdb, error: #{e}\")\n        end",
    "comment": "Push test execution metrics to InfluxDB  @return [void]",
    "label": "",
    "id": "3805"
  },
  {
    "raw_code": "def push_test_metrics_to_gcs\n          init_gcs_client! # init client and exit early if mandatory configuration is missing\n          retry_on_exception(sleep_interval: 30, message: 'Failed to push test metrics to GCS') do\n            gcs_client.put_object(\n              gcs_bucket,\n              metrics_file_name(prefix: 'test'),\n              execution_data.to_json,\n              force: true, content_type: 'application/json'\n            )\n\n            log(:info, \"Pushed #{execution_data.length} test execution entries to GCS\")\n          end",
    "comment": "Push test execution metrics to GCS  @return [void]",
    "label": "",
    "id": "3806"
  },
  {
    "raw_code": "def push_fabrication_metrics\n          data = Tools::TestResourceDataProcessor.resources.flat_map do |resource, values|\n            values.map { |v| fabrication_stats(resource: resource, **v) }\n          end",
    "comment": "Push resource fabrication metrics to influxdb  @return [void]",
    "label": "",
    "id": "3807"
  },
  {
    "raw_code": "def push_code_runtime_metrics\n          return if method_call_data.empty?\n\n          write_api.write(data: method_call_data)\n          log(:info, \"Pushed #{method_call_data.length} code runtime entries to influxdb\")\n        rescue StandardError => e\n          log(:error, \"Failed to push code runtime metrics to influxdb, error: #{e}\")\n        end",
    "comment": "Push code runtime metrics to influxdb  @return [void]",
    "label": "",
    "id": "3808"
  },
  {
    "raw_code": "def push_fabrication_metrics_gcs(data)\n          init_gcs_client! # init client and exit early if mandatory configuration is missing\n          retry_on_exception(sleep_interval: 30, message: 'Failed to push resource fabrication metrics to GCS') do\n            gcs_client.put_object(\n              gcs_bucket,\n              metrics_file_name(prefix: 'fabrication'),\n              data.to_json, force: true,\n              content_type: 'application/json'\n            )\n\n            log(:info, \"Pushed #{data.length} resource fabrication entries to GCS\")\n          end",
    "comment": "Push resource fabrication metrics to GCS  @param [Hash] data fabrication data hash @return [void]",
    "label": "",
    "id": "3809"
  },
  {
    "raw_code": "def push_fabrication_metrics_influxdb(data)\n          write_api.write(data: data)\n          log(:info, \"Pushed #{data.length} resource fabrication entries to influxdb\")\n        rescue StandardError => e\n          log(:error, \"Failed to push fabrication metrics to influxdb, error: #{e}\")\n        end",
    "comment": "Push resource fabrication metrics to InfluxDB  @param [Hash] data fabrication data hash @return [void]",
    "label": "",
    "id": "3810"
  },
  {
    "raw_code": "def init_gcs_client!\n          gcs_client || gcs_bucket\n        end",
    "comment": "Init client raising error if configuration variables are missing  @return [void]",
    "label": "",
    "id": "3811"
  },
  {
    "raw_code": "def gcs_bucket\n          @gcs_bucket ||= ENV['QA_METRICS_GCS_BUCKET_NAME'] ||\n            raise('Missing QA_METRICS_GCS_BUCKET_NAME env variable')\n        end",
    "comment": "Get GCS Bucket Name or raise error if missing  @return [String]",
    "label": "",
    "id": "3812"
  },
  {
    "raw_code": "def metrics_file_name(prefix:, with_pipeline_id_postfix: true)\n          name = [\"#{prefix}-metrics-#{env('CI_JOB_NAME_SLUG') || 'local'}\"]\n          name << \"-env-#{env('TEST_ENV_NUMBER') || 1}\" if parallel_run?\n          name << \"-retry-#{rspec_retried?}\" if retry_failed_specs?\n          name << \"-#{env('CI_PIPELINE_ID') || 'local'}\" if with_pipeline_id_postfix\n          name << \".json\"\n\n          name.join\n        end",
    "comment": "Construct file name for metrics  @param [String] prefix @param [Boolean] with_pipeline_id_postfix @return [String]",
    "label": "",
    "id": "3813"
  },
  {
    "raw_code": "def test_stats(example)\n          # do not save failures from initial non retry run, as they will be retried and become flaky or failed\n          return if retry_failed_specs? && (!rspec_retried? && example.execution_result.status == :failed)\n\n          {\n            name: 'test-stats',\n            time: time,\n            tags: tags(example),\n            fields: fields(example)\n          }\n        rescue StandardError => e\n          log(:error, \"Failed to transform example '#{example.id}', error: #{e}\")\n          nil\n        end",
    "comment": "Transform example to influxdb compatible metrics data https://github.com/influxdata/influxdb-client-ruby#data-format  @param [RSpec::Core::Example] example @return [Hash]",
    "label": "",
    "id": "3814"
  },
  {
    "raw_code": "def tags(example)\n          # use rerun_file_path so shared_examples have the correct file path\n          file_path = example.metadata[:rerun_file_path].gsub('./qa/specs/features', '')\n\n          {\n            name: example.full_description,\n            file_path: file_path,\n            status: status(example),\n            smoke: example.metadata.key?(:smoke).to_s,\n            quarantined: quarantined(example),\n            job_name: job_name,\n            merge_request: merge_request,\n            run_type: run_type,\n            stage: devops_stage(file_path),\n            product_group: example.metadata[:product_group],\n            feature_category: example.metadata[:feature_category],\n            testcase: example.metadata[:testcase],\n            exception_class: example.execution_result.exception&.class&.to_s,\n            branch: branch,\n            **custom_metrics_tags(example.metadata)\n          }.compact\n        end",
    "comment": "Metrics tags  @param [RSpec::Core::Example] example @return [Hash]",
    "label": "",
    "id": "3815"
  },
  {
    "raw_code": "def fields(example)\n          api_fabrication = ((example.metadata[:api_fabrication] || 0) * 1000).round\n          ui_fabrication = ((example.metadata[:browser_ui_fabrication] || 0) * 1000).round\n\n          {\n            id: example.id,\n            run_time: (example.execution_result.run_time * 1000).round,\n            api_fabrication: api_fabrication,\n            ui_fabrication: ui_fabrication,\n            total_fabrication: api_fabrication + ui_fabrication,\n            job_url: ci_job_url,\n            job_status: env('CI_JOB_STATUS'),\n            pipeline_url: ci_pipeline_url,\n            pipeline_id: env('CI_PIPELINE_ID'),\n            job_id: env('CI_JOB_ID'),\n            merge_request_iid: merge_request_iid,\n            failure_exception: example.execution_result.exception.to_s.delete(\"\\n\"),\n            location: example_location(example),\n            failure_issue: example.metadata.dig(:quarantine, :issue),\n            **custom_metrics_fields(example.metadata)\n          }.compact\n        end",
    "comment": "Metrics fields  @param [RSpec::Core::Example] example @return [Hash]",
    "label": "",
    "id": "3816"
  },
  {
    "raw_code": "def fabrication_stats(resource:, info:, fabrication_method:, http_method:, fabrication_time:, timestamp:, **)\n          {\n            name: 'fabrication-stats',\n            time: time,\n            tags: {\n              resource: resource,\n              fabrication_method: fabrication_method,\n              http_method: http_method,\n              run_type: run_type,\n              merge_request: merge_request,\n              branch: branch\n            }.compact,\n            fields: {\n              fabrication_time: fabrication_time,\n              info: info,\n              job_url: ci_job_url,\n              pipeline_url: ci_pipeline_url,\n              timestamp: timestamp\n            }.compact\n          }\n        end",
    "comment": "Resource fabrication data point  @param [String] resource @param [String] info @param [Symbol] fabrication_method @param [Symbol] http_method @param [Integer] fabrication_time @param [String] timestamp @return [Hash]",
    "label": "",
    "id": "3817"
  },
  {
    "raw_code": "def method_call_data\n          @method_call_data ||= CodeRuntimeTracker.method_call_data.flat_map do |name, params|\n            params.map do |p|\n              {\n                name: 'method-call-stats',\n                time: time,\n                tags: {\n                  method: name,\n                  call_arg: p[:call_arg],\n                  run_type: run_type,\n                  merge_request: merge_request,\n                  branch: branch\n                }.compact,\n                fields: {\n                  runtime: (p[:runtime] * 1000).round,\n                  job_url: ci_job_url,\n                  pipeline_url: ci_pipeline_url,\n                  filename: p[:filename]\n                }.compact\n              }\n            end",
    "comment": "Data on method call and it's runtimes  @return [Array]",
    "label": "",
    "id": "3818"
  },
  {
    "raw_code": "def job_name\n          @job_name ||= ci_job_name&.gsub(%r{ \\d{1,2}/\\d{1,2}}, '')\n        end",
    "comment": "Base ci job name  @return [String]",
    "label": "",
    "id": "3819"
  },
  {
    "raw_code": "def time\n          @time ||= env('CI_PIPELINE_CREATED_AT')&.to_time || Time.now\n        end",
    "comment": "Single common timestamp for all exported example metrics to keep data points consistently grouped  @return [Time]",
    "label": "",
    "id": "3820"
  },
  {
    "raw_code": "def merge_request\n          (!!merge_request_iid).to_s\n        end",
    "comment": "Is a merge request execution  @return [String]",
    "label": "",
    "id": "3821"
  },
  {
    "raw_code": "def ci_pipeline_url\n          @ci_pipeline_url ||= env('CI_PIPELINE_URL')\n        end",
    "comment": "Pipeline url  @return [String]",
    "label": "",
    "id": "3822"
  },
  {
    "raw_code": "def branch\n          @branch ||= env('CI_COMMIT_REF_NAME')\n        end",
    "comment": "Branch name  @return [String]",
    "label": "",
    "id": "3823"
  },
  {
    "raw_code": "def quarantined(example)\n          return \"false\" unless example.metadata.key?(:quarantine)\n\n          # if quarantine key is present and status is pending, consider it quarantined\n          (example.execution_result.status == :pending).to_s\n        end",
    "comment": "Is spec quarantined  @param [RSpec::Core::Example] example @return [String]",
    "label": "",
    "id": "3824"
  },
  {
    "raw_code": "def status(example)\n          rspec_status = example.execution_result.status\n          return rspec_status if [:pending, :failed].include?(rspec_status)\n\n          rspec_retried? && rspec_status == :passed ? :flaky : :passed\n        end",
    "comment": "Return a more detailed status  - if test is failed or pending, return rspec status - if test passed but had more than 1 attempt, consider test flaky  @param [RSpec::Core::Example] example @return [Symbol]",
    "label": "",
    "id": "3825"
  },
  {
    "raw_code": "def custom_metrics_tags(metadata)\n          custom_metrics(metadata, :tags)\n        end",
    "comment": "Additional custom metrics tags  @param [Hash] metadata @return [Hash]",
    "label": "",
    "id": "3826"
  },
  {
    "raw_code": "def custom_metrics_fields(metadata)\n          custom_metrics(metadata, :fields)\n        end",
    "comment": "Additional custom metrics fields  @param [Hash] metadata @return [Hash]",
    "label": "",
    "id": "3827"
  },
  {
    "raw_code": "def custom_metrics(metadata, type)\n          custom_metrics = metadata[CUSTOM_METRICS_KEY]\n          return {} unless custom_metrics\n          return {} unless custom_metrics.is_a?(Hash) && custom_metrics[type].is_a?(Hash)\n\n          custom_metrics[type].to_h do |key, value|\n            k = key.to_sym\n            v = value.is_a?(Numeric) || value.nil? ? value : value.to_s\n\n            [k, v]\n          end",
    "comment": "Custom test metrics  @param [Hash] metadata @param [Symbol] type type of metric, :fields or :tags @return [Hash]",
    "label": "",
    "id": "3828"
  },
  {
    "raw_code": "def devops_stage(file_path)\n          file_path.match(%r{\\d{1,2}_(\\w+)/})&.captures&.first\n        end",
    "comment": "Get spec devops stage  @param [String] location @return [String, nil]",
    "label": "",
    "id": "3829"
  },
  {
    "raw_code": "def log(level, message)\n          QA::Runtime::Logger.public_send(level, \"[influxdb exporter]: #{message}\")\n        end",
    "comment": "Print log message  @param [Symbol] level @param [String] message @return [void]",
    "label": "",
    "id": "3830"
  },
  {
    "raw_code": "def example_location(example)\n          # ensures that location will be correct even in case of shared examples\n          file = example\n                 .metadata\n                 .fetch(:shared_group_inclusion_backtrace)\n                 .last\n                 &.formatted_inclusion_location\n\n          return example.location unless file\n\n          file\n        end",
    "comment": "Example location  @param [RSpec::Core::Example] example @return [String]",
    "label": "",
    "id": "3831"
  },
  {
    "raw_code": "def self.mask(content, by_key: [], by_value: [], mask: '****')\n          new(by_key: by_key, by_value: by_value, mask: mask).mask(content)\n        end",
    "comment": "Returns the content with secrets masked.  @param [Object] content the content to mask @param [Array<Symbol>] by_key the keys of the content whose values will be masked @param [Array<String>] by_value the content to be masked. Masks whole- or sub-strings @param [String] mask the string used to replace secrets (default '****') @return [Object] the content with the specified secrets replaced with the mask",
    "label": "",
    "id": "3832"
  },
  {
    "raw_code": "def initialize(by_key: [], by_value: [], mask: '****')\n          by_key.present? || by_value.present? ||\n            raise(ArgumentError, 'Please specify `by_key` or `by_value`')\n\n          @by_key = Array(by_key)\n          @by_value = Array(by_value)\n          @mask = mask\n        end",
    "comment": "@param [Array<Symbol>] by_key the keys of the content whose values will be masked @param [Array<String>] by_value the content to be masked. Masks whole- or sub-strings @param [String] mask the string used to replace secrets (default '****')",
    "label": "",
    "id": "3833"
  },
  {
    "raw_code": "def mask(content)\n          return content if content.blank? || [true, false].include?(content)\n\n          @content = content\n          @content = mask_by_key(@content) if @by_key.present?\n          @content = mask_by_value(@content) if @by_value.present?\n          @content\n        end",
    "comment": "@param [Object] content the content to mask @return [Object] the content with the specified secrets replaced with the mask",
    "label": "",
    "id": "3834"
  },
  {
    "raw_code": "def mask_by_key(content)\n          case content\n          when Hash\n            ActiveSupport::ParameterFilter.new(by_key, mask: @mask).filter(content)\n          when Array\n            content.map { |item| mask_by_key(item) }\n          else\n            content\n          end",
    "comment": "Masks by using the given secrets as hash keys. If the key exists, the corresponding value is replaced with the mask. Recursively masks nested hashes and arrays.",
    "label": "",
    "id": "3835"
  },
  {
    "raw_code": "def mask_by_value(content)\n          case content\n          when Hash\n            content.each { |k, v| content[k] = mask_by_value(v) }\n          when Array\n            content.map { |item| mask_by_value(item) }\n          when String\n            by_value.reduce(content) { |s, secret| s.gsub(secret.to_s, @mask) }\n          else\n            by_value.include?(content) ? @mask : content\n          end",
    "comment": "Masks by substituting the given secrets found in the content. If a secret exists as substrings, the substrings are replaced with the mask. Recursively masks nested hashes and arrays, and each element of arrays.",
    "label": "",
    "id": "3836"
  },
  {
    "raw_code": "def initialize(dry_run: false)\n        super\n\n        @type = 'group'\n      end",
    "comment": "@example mark subgroups for deletion that are older than 24 hours under all gitlab-e2e-sandbox-group-<#1-8> groups GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> bundle exec rake delete_subgroups  @example permanently delete subgroups older than 24 hours under all gitlab-e2e-sandbox-group-<#1-8> groups GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ PERMANENTLY_DELETE=true bundle exec rake delete_subgroups  @example mark subgroups for deletion under 'gitlab-e2e-sandbox-group-2' created before 2023-01-01 GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ TOP_LEVEL_GROUP_NAME=<gitlab-e2e-sandbox-group-2> \\ DELETE_BEFORE=2023-01-01 bundle exec rake delete_subgroups  @example - dry run GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> bundle exec rake \"delete_subgroups[true]\"",
    "label": "",
    "id": "3837"
  },
  {
    "raw_code": "def migrate_data\n        create_tmp_dir\n\n        INFLUX_BUCKETS.each do |bucket|\n          INFLUX_STATS_TYPE.each do |stats_type|\n            if bucket == Support::InfluxdbTools::INFLUX_MAIN_TEST_METRICS_BUCKET && stats_type == \"fabrication-stats\"\n              break\n            end",
    "comment": "Fetch data from Influx DB, store as JSON and upload to GCS  @return [void]",
    "label": "",
    "id": "3838"
  },
  {
    "raw_code": "def influx_to_json(influx_bucket, stats_type, data_file_name, range)\n        QA::Runtime::Logger.info(\"Fetching Influx data for stats: '#{stats_type}', \" \\\n          \"bucket: '#{influx_bucket}' in range #{range}...\")\n        all_runs = []\n\n        retry_on_exception(sleep_interval: 30) do\n          all_runs = query_api.query(query: query(influx_bucket, stats_type, range))\n        end",
    "comment": "Query InfluxDB and store in JSON  @param [String] influx_bucket bucket to fetch data @param [String] stats_type of data to fetch @param [String] data_file_name to store data @param [String] range for influxdb query @return [void]",
    "label": "",
    "id": "3839"
  },
  {
    "raw_code": "def test_stats(stats_type, record)\n        {\n          name: stats_type,\n          time: record.values['_time'],\n          tags: tags(record.values),\n          fields: fields(record.values)\n        }\n      end",
    "comment": "Produces a test_stats Hash  @param [String] stats_type of data @param [String] record to get the data from @return [Hash]",
    "label": "",
    "id": "3840"
  },
  {
    "raw_code": "def fabrication_stats(stats_type, record)\n        {\n          name: stats_type,\n          time: record.values['_time'],\n          tags: {\n            resource: record.values['resource'],\n            fabrication_method: record.values['fabrication_method'],\n            http_method: record.values['http_method'],\n            run_type: record.values['run_type'],\n            merge_request: record.values['merge_request']\n          },\n          fields: {\n            fabrication_time: record.values['fabrication_time'],\n            info: record.values['info']&.force_encoding('UTF-8'),\n            job_url: record.values['job_url'],\n            timestamp: record.values['timestamp']\n          }\n        }\n      end",
    "comment": "Produces a fabrication_stats Hash  @param [String] stats_type of data @param [String] record to get the data from @return [Hash]",
    "label": "",
    "id": "3841"
  },
  {
    "raw_code": "def tags(values)\n        tags = values.slice('name', 'file_path', 'status', 'smoke',\n          'quarantined', 'job_name', 'merge_request', 'run_type', 'stage',\n          'product_group', 'feature_category', 'testcase', 'exception_class')\n\n        # custom_test_metrics\n        tags['import_repo'] = values['import_repo']\n        tags['import_type'] = values['import_type']\n\n        tags\n      end",
    "comment": "Produces a tags Hash  @param [String] values record's values to get the data from @return [Hash]",
    "label": "",
    "id": "3842"
  },
  {
    "raw_code": "def fields(values)\n        fields = values.slice('id', 'run_time', 'api_fabrication', 'ui_fabrication',\n          'total_fabrication', 'job_url', 'pipeline_url', 'pipeline_id',\n          'job_id', 'merge_request_iid', 'failure_issue')\n\n        fields['failure_exception'] = values['failure_exception']&.force_encoding('UTF-8')\n        fields['import_time'] = values['import_time'] # custom_test_metrics\n\n        fields\n      end",
    "comment": "Produces a fields Hash  @param [String] values record's values to get the data from @return [Hash]",
    "label": "",
    "id": "3843"
  },
  {
    "raw_code": "def create_tmp_dir\n        FileUtils.mkdir_p('tmp/')\n      end",
    "comment": "Create a 'tmp' directory  @return [String]",
    "label": "",
    "id": "3844"
  },
  {
    "raw_code": "def upload_to_gcs(bucket, file_path, file_name)\n        retry_on_exception(sleep_interval: 30) do\n          file = gcs_client.put_object(bucket, file_name, File.new(file_path, \"r\"), force: true)\n          QA::Runtime::Logger.info(\"Uploaded file #{file_path} to #{gcs_url(bucket, file)}\")\n        end",
    "comment": "Upload file to GCS  @param [String] bucket to be uploaded to @param [String] file_path of file to be uploaded @param [String] file_name of file to be uploaded @return [void]",
    "label": "",
    "id": "3845"
  },
  {
    "raw_code": "def gcs_url(bucket, file)\n        \"https://storage.cloud.google.com/#{bucket}/#{file.name}\"\n      end",
    "comment": "Construct the url of the uploaded file in GCS @param [String] bucket name the file is uploaded to @param [String] file uploaded to gcs  @return [String]",
    "label": "",
    "id": "3846"
  },
  {
    "raw_code": "def upload(ci_project_name)\n        if files.empty?\n          logger.info(\"\\nNothing to upload!\")\n          return\n        end",
    "comment": "Upload resources from failed test suites to GCS bucket Files are organized by environment in which tests were executed  E.g: staging/failed-test-resources-<randomhex>.json",
    "label": "",
    "id": "3847"
  },
  {
    "raw_code": "def download(ci_project_name)\n        logger.info(\"Downloading resource files from GCS for #{ci_project_name}...\")\n        bucket_items = gcs_storage.list_objects(BUCKET, prefix: \"#{ci_project_name}/\").items\n\n        if bucket_items.blank?\n          logger.info(\"\\nNothing to download!\")\n          return\n        end",
    "comment": "Download files from GCS bucket by environment name Delete the files afterward",
    "label": "",
    "id": "3848"
  },
  {
    "raw_code": "def api_client\n        abort(\"\\nPlease provide GITLAB_ADDRESS\") unless ENV['GITLAB_ADDRESS']\n\n        @api_client ||= Runtime::API::Client.new(\n          ENV['GITLAB_ADDRESS'],\n          personal_access_token: personal_access_token\n        )\n      end",
    "comment": "Returns a memoized GitLab API client instance  Creates and caches a Runtime::API::Client configured with the GitLab instance address and authentication token. The client is used for all API operations including resource deletion, fetching, and GraphQL requests.  @return [Runtime::API::Client] Configured API client instance @raise [SystemExit] If GITLAB_ADDRESS environment variable is not set",
    "label": "",
    "id": "3849"
  },
  {
    "raw_code": "def delete_personal_resource(resource, delayed_verification, permanent, skip_verification)\n        username = resource[:author][:username]\n        user_client = set_api_client_by_username(username)\n\n        with_api_client(user_client) do\n          delete_resource(resource, delayed_verification, permanent, skip_verification)\n        end",
    "comment": "Deletes a personal resource using the original author's credentials  @param [Hash] resource The resource to delete, must contain [:author][:username] @param [Boolean] delayed_verification Wait until the end of the script to verify deletion @param [Boolean] permanent Permanently delete the resource instead of marking for deletion @param [Boolean] skip_verification Skip verification of deletion for time constraint purposes @return [Array<String, Hash>, Hash] Deletion result or resource for delayed verification",
    "label": "",
    "id": "3850"
  },
  {
    "raw_code": "def delete_resources( # rubocop:disable Metrics/CyclomaticComplexity, Metrics/PerceivedComplexity -- TODO: Break up this method\n        resources_hash,\n        delayed_verification = false,\n        permanent = @permanently_delete,\n        skip_verification = false)\n        unverified_deletions = []\n        results = []\n\n        resources_hash.each do |(key, value)|\n          type = key.split('::').last.downcase\n\n          value.each do |resource_hash|\n            next if resource_not_found?(resource_hash['api_path'])\n\n            resource = get_resource(resource_hash['api_path'])\n            next unless resource\n\n            resource_info = resource_info(resource_hash, key)\n            logger.info(\"Processing #{resource_info}...\")\n\n            resource[:api_path] = resource_hash['api_path']\n            resource[:type] = type\n\n            # When deleting immediately, only skip resources whose parents are already marked for deletion\n            next if permanent && group_or_project?(resource) && parent_marked_for_deletion?(resource)\n            # When soft-deleting, skip resources already marked for deletion either by parent or self\n            next if !permanent && group_or_project?(resource) && already_marked_for_deletion?(resource)\n            # Skip sandboxes already marked for deletion as these cannot be immediately deleted\n            next if resource[:type] == 'sandbox' && self_deletion_scheduled?(resource)\n\n            result = if personal_resource?(key)\n                       delete_personal_resource(resource, delayed_verification, permanent, skip_verification)\n                     elsif type == 'user'\n                       delete_resource(resource, true, permanent, skip_verification)\n                     elsif permanent && group_or_project?(resource) && self_deletion_scheduled?(resource)\n                       delete_permanently(resource)\n                     else\n                       delete_resource(resource, delayed_verification, permanent, skip_verification)\n                     end",
    "comment": "Deletes resources from a structured hash organized by resource type  @param [Hash<String, Array<Hash>>] resources_hash Hash where keys are resource class names @param [Boolean] delayed_verification Wait until the end of the script to verify deletions. Defaults to false @param [Boolean] permanent Permanently delete resources instead of marking for deletion. Defaults to true @param [Boolean] skip_verification Skip verification of deletion for time constraint purposes. Defaults to false @return [Array<Array<String, Hash>>] Array of deletion results",
    "label": "",
    "id": "3851"
  },
  {
    "raw_code": "def files\n        logger.info(\"Gathering JSON files using pattern #{@file_pattern}...\")\n        files = Dir.glob(@file_pattern)\n\n        if files.empty?\n          logger.info(\"There is no file with this pattern\")\n          exit 0\n        else\n          logger.info(\"Found #{files.size} JSON file(s) to process\")\n        end",
    "comment": "Gathers and validates JSON files matching the configured @file_pattern  @return [Array<String>] Array of file paths that match the pattern and contain data @raise [SystemExit] Exits with status 0 if no files match the pattern or all files are empty",
    "label": "",
    "id": "3852"
  },
  {
    "raw_code": "def filter_resources(resources) # rubocop:disable Metrics/CyclomaticComplexity, Metrics/PerceivedComplexity -- TODO: Break up this method\n        logger.info('Filtering resources - Only keep deletable resources...')\n\n        transformed_values = resources.transform_values! do |v|\n          v.reject do |attributes|\n            # We don't want to delete sandbox groups\n            attributes['info']&.match(/with full_path 'gitlab-e2e-sandbox-group(-\\d)?'/) ||\n              (attributes['http_method'] == 'get' && !attributes['info']&.include?(\"with username 'qa-\")) ||\n              attributes['api_path'] == 'Cannot find resource API path' ||\n              attributes['api_path'] == '/graphql' ||\n              # Group resources are deleted when their group is deleted, so we don't need to delete them manually\n              attributes['api_path'].match?(%r{\\A/groups/\\d+/.+\\z}) ||\n              # Project resources are deleted when their project is deleted, so we don't need to delete them manually\n              attributes['api_path'].match?(%r{\\A/projects/\\d+/.+\\z}) ||\n              # Only delete projects whose direct parent is a gitlab-e2e-sandbox-group\n              (attributes['api_path']&.start_with?('/projects/') &&\n                attributes['info']&.match(%r{\\Awith full_path 'gitlab-e2e-sandbox-group-\\d/[a-zA-Z0-9-]+/.*'\\z})) ||\n              (attributes['api_path']&.start_with?('/projects/') &&\n                attributes['info']&.exclude?('gitlab-e2e-sandbox-group-'))\n          end",
    "comment": "Filters resources to keep only those that are safe and appropriate for deletion  Removes resources that match exclusion criteria to prevent accidental deletion of protected or system resources. Filters out sandbox groups, non-deletable GET requests, invalid API paths, GraphQL endpoints, and resource types listed in IGNORED_RESOURCES constant.  @param [Hash<String, Array<Hash>>] resources Hash where keys are resource class names and values are arrays of resource attribute hashes @return [Hash<String, Array<Hash>>, nil] Filtered resources hash with unsafe resources removed, or nil if no deletable resources remain",
    "label": "",
    "id": "3853"
  },
  {
    "raw_code": "def gcs_storage\n        @gcs_storage ||= Fog::Google::Storage.new(\n          google_project: PROJECT,\n          **(File.exist?(json_key) ? { google_json_key_location: json_key } : { google_json_key_string: json_key })\n        )\n      rescue StandardError => e\n        abort(\"\\nThere might be something wrong with the JSON key file - [ERROR] #{e}\")\n      end",
    "comment": "Returns a memoized Google Cloud Storage client instance  Creates and caches a Fog::Google::Storage client configured with the project and authentication credentials. Automatically detects whether the json_key is a file path (uses google_json_key_location) or a JSON string (uses google_json_key_string) for authentication.  @return [Fog::Google::Storage] Configured GCS client instance @raise [SystemExit] Aborts program execution if JSON key file/string is invalid",
    "label": "",
    "id": "3854"
  },
  {
    "raw_code": "def json_key\n        unless ENV['QA_FAILED_TEST_RESOURCES_GCS_CREDENTIALS']\n          abort(\"\\nPlease provide QA_FAILED_TEST_RESOURCES_GCS_CREDENTIALS\")\n        end",
    "comment": "Returns the GCS service account JSON key for authentication  Retrieves and memoizes the Google Cloud Storage authentication credentials from the QA_FAILED_TEST_RESOURCES_GCS_CREDENTIALS environment variable. The value can be either a file path to a JSON key file or the JSON key content as a string.  @return [String] Either the file path to the JSON key file or the JSON key content as a string @raise [SystemExit] Aborts program execution if QA_FAILED_TEST_RESOURCES_GCS_CREDENTIALS is not set",
    "label": "",
    "id": "3855"
  },
  {
    "raw_code": "def organize_resources(filtered_resources)\n        organized_resources = {}\n\n        sandboxes = filtered_resources.delete('QA::Resource::Sandbox')\n        groups = filtered_resources.delete('QA::Resource::Group')\n        projects = filtered_resources.delete('QA::Resource::Project')\n        users = filtered_resources.delete('QA::Resource::User')\n\n        organized_resources['QA::Resource::Sandbox'] = sandboxes if sandboxes\n        organized_resources['QA::Resource::Group'] = groups if groups\n        organized_resources['QA::Resource::Project'] = projects if projects\n        organized_resources.merge!(filtered_resources) unless filtered_resources.empty?\n        organized_resources['QA::Resource::User'] = users if users\n\n        organized_resources\n      end",
    "comment": "Organizes resources in hierarchical deletion order for efficient cleanup  Reorders resources to respect dependency relationships during deletion. The deletion order prevents dependency conflicts by removing parent resources before child resources, and system resources before user resources.  @param [Hash<String, Array<Hash>>] filtered_resources Hash where keys are resource class names and values are arrays of resource data hashes @return [Hash<String, Array<Hash>>] Reorganized resources hash in deletion order: 1. Sandboxes, 2. Groups, 3. Projects, 4. Other resources, 5. Users",
    "label": "",
    "id": "3856"
  },
  {
    "raw_code": "def personal_resource?(key)\n        PERSONAL_RESOURCES.include?(key)\n      end",
    "comment": "Checks if a resource key is included in the PERSONAL_RESOURCES constant  @param [String] key The resource key to check @return [Boolean] true if the key is in PERSONAL_RESOURCES, false otherwise",
    "label": "",
    "id": "3857"
  },
  {
    "raw_code": "def read_file(file)\n        logger.info(\"Reading and processing #{file}...\")\n        JSON.parse(File.read(file))\n      rescue JSON::ParserError\n        logger.error(\"Failed to read #{file} - Invalid format\")\n        nil\n      end",
    "comment": "Reads and parses a JSON file  @param [String] file Path to the file to read @return [Hash, Array, nil] Parsed JSON content, or nil if parsing fails",
    "label": "",
    "id": "3858"
  },
  {
    "raw_code": "def resource_info(resource, key)\n        resource['info'] ? \"#{key} - #{resource['info']}\" : \"#{key} at #{resource['api_path']}\"\n      end",
    "comment": "Generates a descriptive string for a resource  @param [Hash] resource The resource hash containing resource data @param [String] key The resource type key (e.g., 'QA::Resource::Project') @return [String] Formatted resource description using 'info' if available, otherwise 'api_path'",
    "label": "",
    "id": "3859"
  },
  {
    "raw_code": "def resource_not_found?(api_path)\n        # if api path contains param \"?hard_delete=<boolean>\", remove it\n        get(Runtime::API::Request.new(api_client, api_path.split('?').first).url).code.eql? 404\n      end",
    "comment": "Checks if a resource exists by making a GET request to its API path  @param [String] api_path The API path to check, may include query parameters @return [Boolean] true if resource returns 404 (not found), false otherwise",
    "label": "",
    "id": "3860"
  },
  {
    "raw_code": "def resource_request(resource_or_path, **options)\n        api_path = resource_or_path.is_a?(Hash) ? resource_or_path[:api_path] : resource_or_path\n\n        Runtime::API::Request.new(api_client, api_path, **options).url\n      end",
    "comment": "Creates a GitLab API request URL from a resource or path  @param [Hash, String] resource_or_path Either a resource hash containing :api_path or a direct API path string @param [Hash] options Additional options to pass to the API request @return [String] The complete API request URL",
    "label": "",
    "id": "3861"
  },
  {
    "raw_code": "def set_api_client_by_username(username)\n        user_pat = if username == \"gitlab-qa\" && ENV['GITLAB_QA_ACCESS_TOKEN']\n                     ENV['GITLAB_QA_ACCESS_TOKEN']\n                   elsif username == \"gitlab-qa-user1\" && ENV['GITLAB_QA_USER1_ACCESS_TOKEN']\n                     ENV['GITLAB_QA_USER1_ACCESS_TOKEN']\n                   elsif username == \"gitlab-qa-user2\" && ENV['GITLAB_QA_USER2_ACCESS_TOKEN']\n                     ENV['GITLAB_QA_USER2_ACCESS_TOKEN']\n                   else\n                     personal_access_token\n                   end",
    "comment": "Creates an API client using the appropriate personal access token for a given username  @param [String] username The username to create an API client for @return [Runtime::API::Client] API client configured with the user's personal access token",
    "label": "",
    "id": "3862"
  },
  {
    "raw_code": "def with_api_client(client)\n        original_client = @api_client\n        @api_client = client\n        yield\n      ensure\n        @api_client = original_client\n      end",
    "comment": "Temporarily switches the API client context for the duration of the block  This method allows for temporary client switching, which is useful for operations that need to be performed with different authentication credentials (e.g., personal resource deletion with user-specific tokens).  @param [Runtime::API::Client] client The API client to use temporarily @yield [] The block to execute with the temporary client @return [Object] The return value of the yielded block @example user_client = set_api_client_by_username('gitlab-qa-user1') with_api_client(user_client) do delete_resource(personal_resource) end",
    "label": "",
    "id": "3863"
  },
  {
    "raw_code": "def execute\n        return puts(\"No long running specs detected, all good!\") if long_running_specs.empty?\n\n        specs = long_running_specs.map { |k, v| \"#{k}: #{(v / 60).round(2)} minutes\" }.join(\"\\n\")\n        average = mean_runtime < 60 ? \"#{mean_runtime.round(0)} seconds\" : \"#{(mean_runtime / 60).round(2)} minutes\"\n        msg = <<~MSG\n          Following spec files are exceeding #{RUNTIME_THRESHOLD / 60} minute runtime threshold!\n          Current average spec runtime: #{average}.\n        MSG\n\n        puts(\"#{msg}\\n#{specs}\")\n        notifier.post(icon_emoji: \":time-out:\", text: \"#{msg}\\n```#{specs}```\")\n      end",
    "comment": "Find and report specs exceeding runtime threshold  @return [void]",
    "label": "",
    "id": "3864"
  },
  {
    "raw_code": "def mean_runtime\n        @mean_runtime ||= knapsack_report.values\n          .select { |v| v < RUNTIME_THRESHOLD }\n          .then { |runtimes| runtimes.sum(0.0) / runtimes.length }\n      end",
    "comment": "Average runtime of spec files  @return [Number]",
    "label": "",
    "id": "3865"
  },
  {
    "raw_code": "def long_running_specs\n        @long_running_specs ||= knapsack_report.select { |_k, v| v > RUNTIME_THRESHOLD }\n      end",
    "comment": "Spec files exceeding runtime threshold  @return [Hash]",
    "label": "",
    "id": "3866"
  },
  {
    "raw_code": "def knapsack_report\n        @latest_report ||= JSON.parse(File.read(Support::KnapsackReport::FALLBACK_REPORT))\n      end",
    "comment": "Latest knapsack report  @return [Hash]",
    "label": "",
    "id": "3867"
  },
  {
    "raw_code": "def notifier\n        @notifier ||= Slack::Notifier.new(\n          slack_webhook_url,\n          channel: SLACK_CHANNEL,\n          username: \"Spec Runtime Report\"\n        )\n      end",
    "comment": "Slack notifier  @return [Slack::Notifier]",
    "label": "",
    "id": "3868"
  },
  {
    "raw_code": "def slack_webhook_url\n        @slack_webhook_url ||= ENV[\"SLACK_WEBHOOK\"] || raise(\"Missing SLACK_WEBHOOK env variable\")\n      end",
    "comment": "Slack webhook url  @return [String]",
    "label": "",
    "id": "3869"
  },
  {
    "raw_code": "def initialize(\n        project_tar_paths: Runtime::Path.fixture('export.tar.gz'),\n        group_path: \"import-test\",\n        project_copies: 10\n      )\n        @project_tar_paths = project_tar_paths\n        @group_path = group_path\n        @project_copies = project_copies\n        @logger = Runtime::Logger.logger\n      end",
    "comment": "Generate test group  @param [String] project_tar_paths exported project tar.gz file path, optionally several separated by ';' @param [String] group_path path of group where projects will be generated @param [Integer] project_copies number of projects to create in a group",
    "label": "",
    "id": "3870"
  },
  {
    "raw_code": "def generate\n        check_access_token\n        raise(\"Project pool has no valid archive files\") if project_pool.empty?\n\n        logger.info(\"Creating '#{group_path}' group with #{project_copies} copies of exported projects\")\n        create_group\n\n        (1..project_copies).each do\n          name = \"imported-project-#{SecureRandom.hex(8)}\"\n          tar = project_pool[rand(0..project_pool.size - 1)]\n\n          logger.info(\"Fabricating copy of '#{tar.basename}' with name '#{name}'\")\n          Resource::ImportProject.fabricate_via_api! do |project|\n            project.file_path = tar.to_s\n            project.api_client = api_client\n            project.name = name\n            project.group = group\n            # we mark project as not import so it doesn't wait for import to finish\n            # when generating large projects, it can take a long time\n            project.import = false\n          end",
    "comment": "Generate group with projects  @return [void]",
    "label": "",
    "id": "3871"
  },
  {
    "raw_code": "def access_token\n        @access_token ||= ENV['GITLAB_QA_ACCESS_TOKEN'] || raise(\"GITLAB_QA_ACCESS_TOKEN required\")\n      end",
    "comment": "Gitlab access token  @return [String]",
    "label": "",
    "id": "3872"
  },
  {
    "raw_code": "def api_client\n        @api_client ||= Runtime::API::Client.new(:gitlab, personal_access_token: access_token)\n      end",
    "comment": "API client  @return [Runtime::API::Client]",
    "label": "",
    "id": "3873"
  },
  {
    "raw_code": "def project_pool\n        @project_pool ||= project_tar_paths.split(\";\").filter_map do |f|\n          path = Pathname.new(f)\n          next logger.warn(\"#{f} is not a valid path!\") && nil unless path.exist?\n\n          path\n        end",
    "comment": "Pool of project tar files  @return [Array<Pathname>]",
    "label": "",
    "id": "3874"
  },
  {
    "raw_code": "def group\n        return @group if defined?(@group)\n\n        paths = group_path.split(\"/\")\n        sandbox = create(:sandbox, path: paths.first)\n        return @group = sandbox if paths.size == 1\n\n        @group = paths[1..].each_with_object([sandbox]) do |path, arr|\n          arr << create(:group, parent: arr.last, path: path)\n        end.last\n      end",
    "comment": "Create group with all subgroups  @return [<Resource::Sandbox, Resource::Group>]",
    "label": "",
    "id": "3875"
  },
  {
    "raw_code": "def create(type, path:, parent: nil)\n        resource_class = type == :sandbox ? Resource::Sandbox : Resource::Group\n\n        resource_class.fabricate_via_api! do |resource|\n          resource.api_client = api_client\n          resource.sandbox = parent unless type == :sandbox\n          resource.path = path\n        end",
    "comment": "Create group resource  @param [Symbol] type @param [String] path @param [<Resource::Sandbox, Resource::Group>] sandbox @return [<Resource::Sandbox, Resource::Group>]",
    "label": "",
    "id": "3876"
  },
  {
    "raw_code": "def perform\n        info(\"Waiting for Gitlab to become ready!\")\n        debug(\"Checking required element presence on sign-in page\")\n\n        wait_for_login_page_to_load\n\n        info(\"Gitlab is ready!\")\n      rescue StandardError => e\n        raise ReadinessCheckError, \"#{error_base} Reason: #{e}\"\n      end",
    "comment": "Validate gitlab readiness via check for presence of sign-in-form element  @return [void]",
    "label": "",
    "id": "3877"
  },
  {
    "raw_code": "def sign_in_url\n        @sign_in_url ||= \"#{Support::GitlabAddress.address_with_port(with_default_port: false)}/users/sign_in\"\n      end",
    "comment": "Sign in page url  @return [String]",
    "label": "",
    "id": "3878"
  },
  {
    "raw_code": "def error_base\n        @error_base ||= \"Gitlab readiness check failed, valid sign_in page did not appear within #{wait} seconds!\"\n      end",
    "comment": "Error message base  @return [String]",
    "label": "",
    "id": "3879"
  },
  {
    "raw_code": "def elements_css\n        @element_css ||= QA::Page::Main::Login.elements.select(&:required?).map(&:selector_css)\n      end",
    "comment": "Required elements css selectors  @return [Array<String>]",
    "label": "",
    "id": "3880"
  },
  {
    "raw_code": "def wait_for_login_page_to_load\n        return validate_readiness_via_ui! if Runtime::Env.running_on_live_env?\n\n        response = fetch_sign_in_page\n        return validate_readiness_via_ui! if cloudflare_response?(response)\n\n        debug(\"Checking for required elements via api\")\n        Support::Retrier.retry_on_exception(max_attempts: wait, sleep_interval: 1, log: false) do\n          # re-use initial response from cloudflare check\n          response ||= fetch_sign_in_page\n          validate_readiness_via_api!(response)\n        ensure\n          response = nil\n        end",
    "comment": "Check if sign_in page loads with all required elements  @return [void]",
    "label": "",
    "id": "3881"
  },
  {
    "raw_code": "def cloudflare_response?(response)\n        return false unless response\n\n        response.headers[:server] == \"cloudflare\" || response.code == 403\n      end",
    "comment": "Check if headless request got blocked by cloudflare  @param response [RestClient::Response] @return [Boolean]",
    "label": "",
    "id": "3882"
  },
  {
    "raw_code": "def validate_readiness_via_ui!\n        debug(\"Checking for required elements via web browser\")\n        Capybara.current_session.using_wait_time(wait) { Runtime::Browser.visit(:gitlab, Page::Main::Login) }\n        debug(\"Required elements are present!\")\n      end",
    "comment": "Check presence of required elements on sign_in page via UI  @return [void]",
    "label": "",
    "id": "3883"
  },
  {
    "raw_code": "def validate_readiness_via_api!(response)\n        raise \"Failed to obtain valid http response from #{sign_in_url}\" unless response\n        raise \"Got unsucessfull response code from #{sign_in_url}: #{response.code}\" unless ok_response?(response)\n        raise \"Sign in page missing required elements: '#{elements_css}'\" unless required_elements_present?(response)\n      end",
    "comment": "Check presence of required elements from headless sign in page request response  @param response [RestClient::Response] @return [void]",
    "label": "",
    "id": "3884"
  },
  {
    "raw_code": "def fetch_sign_in_page\n        get(sign_in_url)\n      rescue StandardError => e\n        debug(\"Error fetching sign-in page: #{e}\")\n        nil\n      end",
    "comment": "Response from sign-in page  @return [RestClient::Response]",
    "label": "",
    "id": "3885"
  },
  {
    "raw_code": "def ok_response?(response)\n        response.code == Support::API::HTTP_STATUS_OK\n      end",
    "comment": "Validate response code is 200  @param [RestClient::Response] response @return [Boolean]",
    "label": "",
    "id": "3886"
  },
  {
    "raw_code": "def required_elements_present?(response)\n        doc = Nokogiri::HTML.parse(response.body)\n\n        elements_css.all? { |sel| doc.css(sel).any? }\n      end",
    "comment": "Check required elements are present on sign-in page  @param [RestClient::Response] response @return [Boolean]",
    "label": "",
    "id": "3887"
  },
  {
    "raw_code": "def update_master_report\n        create_branch\n        create_commit\n        create_mr\n        return unless auto_merge?\n\n        logger.info(\"Performing auto merge\")\n        approve_mr\n        add_mr_to_merge_train\n      end",
    "comment": "Create master_report.json merge request  @return [void]",
    "label": "",
    "id": "3888"
  },
  {
    "raw_code": "def knapsack_reporter\n        @knapsack_reporter = Support::KnapsackReport.new(logger: logger)\n      end",
    "comment": "Knapsack report generator  @return [QA::Support::KnapsackReport]",
    "label": "",
    "id": "3889"
  },
  {
    "raw_code": "def gitlab_api_url\n        @gitlab_api_url ||= ENV[\"CI_API_V4_URL\"] || raise(\"Missing CI_API_V4_URL env variable\")\n      end",
    "comment": "Gitlab api url  @return [String]",
    "label": "",
    "id": "3890"
  },
  {
    "raw_code": "def gitlab_access_token\n        @gitlab_access_token ||= ENV[\"GITLAB_ACCESS_TOKEN\"] || raise(\"Missing GITLAB_ACCESS_TOKEN env variable\")\n      end",
    "comment": "Gitlab access token  @return [String]",
    "label": "",
    "id": "3891"
  },
  {
    "raw_code": "def approver_access_token\n        @approver_access_token ||= ENV[\"QA_KNAPSACK_REPORT_APPROVER_TOKEN\"].tap do |token|\n          logger.warn(\"QA_KNAPSACK_REPORT_APPROVER_TOKEN is not set\") unless token\n        end",
    "comment": "Knapsack report approver token  @return [String]",
    "label": "",
    "id": "3892"
  },
  {
    "raw_code": "def approver_user_id\n        @approver_user_id ||= approver_access_token.then do |token|\n          next 0 unless token\n\n          resp = get(\"#{gitlab_api_url}/user\", token_header(token))\n          next parse_body(resp)[:id] if success?(resp.code)\n\n          logger.error(\"Failed to fetch approver user id! Response: #{resp.body}\")\n          0\n        end",
    "comment": "Update mr approver user id  @return [Integer]",
    "label": "",
    "id": "3893"
  },
  {
    "raw_code": "def approver_user_valid?\n        approver_user_id != 0\n      end",
    "comment": "Valid approver user is set  @return [Boolean]",
    "label": "",
    "id": "3894"
  },
  {
    "raw_code": "def token_header(token = gitlab_access_token)\n        { headers: { \"PRIVATE-TOKEN\" => token } }\n      end",
    "comment": "Api request private token header  @return [Hash]",
    "label": "",
    "id": "3895"
  },
  {
    "raw_code": "def create_branch\n        logger.info(\"Creating branch '#{UPDATE_BRANCH_NAME}' branch\")\n        retry_attempts = 0\n\n        begin\n          api_request(:post, \"repository/branches\", {\n            branch: UPDATE_BRANCH_NAME,\n            ref: \"master\"\n          })\n        rescue StandardError => e\n          raise e if retry_attempts > 2\n\n          if e.message.include?(\"Branch already exists\")\n            logger.warn(\"Branch '#{UPDATE_BRANCH_NAME}' already exists, recreating it.\")\n            api_request(:delete, \"repository/branches/#{UPDATE_BRANCH_NAME}\")\n          end",
    "comment": "Create branch for knapsack report update  @return [void]",
    "label": "",
    "id": "3896"
  },
  {
    "raw_code": "def create_commit\n        logger.info(\"Creating master_report.json update commit\")\n        runtime_report = knapsack_reporter.create_merged_runtime_report.sort.to_h\n\n        api_request(:post, \"repository/commits\", {\n          branch: UPDATE_BRANCH_NAME,\n          commit_message: \"Update master_report.json for E2E tests\",\n          actions: [\n            {\n              action: \"update\",\n              file_path: File.join(\"qa\", Support::KnapsackReport::RUNTIME_REPORT),\n              content: \"#{JSON.pretty_generate(runtime_report)}\\n\"\n            },\n            {\n              action: \"update\",\n              file_path: File.join(\"qa\", Support::KnapsackReport::FALLBACK_REPORT),\n              content: \"#{JSON.pretty_generate(knapsack_reporter.create_knapsack_report(runtime_report).sort.to_h)}\\n\"\n            }\n          ]\n        })\n      end",
    "comment": "Create update commit for knapsack report  @return [void]",
    "label": "",
    "id": "3897"
  },
  {
    "raw_code": "def create_mr\n        logger.info(\"Creating merge request\")\n        resp = api_request(:post, \"merge_requests\", {\n          source_branch: UPDATE_BRANCH_NAME,\n          target_branch: \"master\",\n          title: \"Update knapsack runtime data for E2E tests\",\n          remove_source_branch: true,\n          squash: true,\n          reviewer_ids: approver_user_valid? ? [approver_user_id] : nil,\n          labels: MR_LABELS.join(\",\"),\n          description: \"Update fallback knapsack report and example runtime data report.\".then do |description|\n            next description if approver_user_valid?\n\n            \"#{description}\\n\\ncc: @gl-dx/maintainers\"\n          end",
    "comment": "Create merge request with updated knapsack master report  @return [void]",
    "label": "",
    "id": "3898"
  },
  {
    "raw_code": "def approve_mr\n        logger.info(\"  approving merge request\")\n        # due to async nature of mr creation, approval is being reset because it happens before commit creation\n        sleep(wait_before_approve)\n        api_request(:post, \"merge_requests/#{mr_iid}/approve\", {}, token_header(approver_access_token))\n      end",
    "comment": "Approve created merge request  @return [void]",
    "label": "",
    "id": "3899"
  },
  {
    "raw_code": "def add_mr_to_merge_train\n        logger.info(\"  adding merge request to merge train\")\n        sleep(wait_before_merge) # gitlab-org/gitlab takes a long time to create pipeline after approval\n        retry_attempts = 0\n        approver_header = token_header(approver_access_token)\n\n        begin\n          api_request(:post, \"merge_trains/merge_requests/#{mr_iid}\", { when_pipeline_succeeds: true }, approver_header)\n        rescue StandardError => e\n          raise e if retry_attempts > 2\n\n          logger.warn(\"  failed to add merge request to merge train, retrying...\")\n          logger.warn(e.message)\n          retry_attempts += 1\n          sleep(10)\n          retry\n        end",
    "comment": "Add merge request to merge train  @return [void]",
    "label": "",
    "id": "3900"
  },
  {
    "raw_code": "def auto_merge?\n        (mr_iid && approver_user_valid?).tap do |auto_merge|\n          logger.warn(\"Auto merge will not be performed!\") unless auto_merge\n        end",
    "comment": "Attempt to automatically merge created mr  @return [Boolean]",
    "label": "",
    "id": "3901"
  },
  {
    "raw_code": "def api_request(verb, path, payload = nil, headers = token_header)\n        args = [verb, \"#{gitlab_api_url}/projects/#{GITLAB_PROJECT_ID}/#{path}\", payload, headers].compact\n        response = public_send(*args)\n        raise \"Api request to #{path} failed! Body: #{response.body}\" unless success?(response.code)\n        return {} if response.body.empty?\n\n        parse_body(response)\n      end",
    "comment": "Api update request  @param [String] verb @param [String] path @param [Hash] payload @return [Hash, Array]",
    "label": "",
    "id": "3902"
  },
  {
    "raw_code": "def collect(resource:, info:, fabrication_method:, fabrication_time:)\n        # http_method is empty when we use the browser_ui to create a resource,\n        # so assign it a default :post value if it is empty otherwise it will be set to :get in resource_api_path\n        http_method = resource.api_fabrication_http_method || :post\n        api_path = resource_api_path(resource)\n        type = resource.class.name\n\n        resources[type] << {\n          info: info,\n          api_path: api_path,\n          fabrication_method: fabrication_method,\n          fabrication_time: fabrication_time,\n          http_method: http_method,\n          timestamp: Time.now.to_s\n        }\n      end",
    "comment": "Collecting resources created in E2E tests Data is a Hash of resources with keys as resource type (group, project, issue, etc.) Each type contains an array of resource object (hash) of the same type E.g: { \"QA::Resource::Project\": [ { info: 'foo', api_path: '/foo'}, {...} ] }  @param [QA::Resource::Base] resource fabricated resource @param [String] info resource info @param [Symbol] method fabrication method, api or browser_ui @param [Integer] time fabrication time @return [Hash]",
    "label": "",
    "id": "3903"
  },
  {
    "raw_code": "def write_to_file(suite_failed)\n        return if resources.empty?\n\n        start_str = mark_as_failed?(suite_failed) ? 'failed-test-resources' : 'test-resources'\n        file_name = Runtime::Env.running_in_ci? ? \"#{start_str}-#{SecureRandom.hex(3)}.json\" : \"#{start_str}.json\"\n        file = Pathname.new(File.join(Runtime::Path.qa_root, 'tmp', file_name))\n        FileUtils.mkdir_p(file.dirname)\n\n        data = resources.deep_dup\n        # merge existing json if present\n        JSON.parse(File.read(file)).deep_merge!(data) { |_, val, other_val| val + other_val } if file.exist?\n\n        File.write(file, JSON.pretty_generate(data))\n      end",
    "comment": "If JSON file exists and not empty, read and load file content Merge what is saved in @resources into the content from file Overwrite file content with the new data hash Otherwise create file and write data hash to file for the first time  @return [void]",
    "label": "",
    "id": "3904"
  },
  {
    "raw_code": "def mark_as_failed?(suite_failed)\n        return suite_failed unless ::Gitlab::QA::Runtime::Env.retry_failed_specs?\n\n        # if suite ran in the initial run, do not mark resource as failed even if suite failed\n        Runtime::Env.rspec_retried? ? suite_failed : false\n      end",
    "comment": "Check if resource file should be marked as failed  @param [Boolean] suite_failed @return [Boolean]",
    "label": "",
    "id": "3905"
  },
  {
    "raw_code": "def resource_api_path(resource)\n        default = 'Cannot find resource API path'\n\n        if resource.respond_to?(:api_delete_path)\n          resource.api_delete_path.gsub('%2F', '/')\n        elsif resource.respond_to?(:api_get_path)\n          resource.api_get_path.gsub('%2F', '/')\n        else\n          default\n        end",
    "comment": "Determine resource api path or return default value Some resources fabricated via UI can raise no attribute error  @param [QA::Resource::Base] resource @return [String]",
    "label": "",
    "id": "3906"
  },
  {
    "raw_code": "def initialize(dry_run: false, exclude_groups: nil)\n        super(dry_run: dry_run)\n\n        @type = 'group'\n        @exclude_groups = Array(exclude_groups.to_s.split(',')) + EXCLUDE_GROUPS\n        @permanently_delete = false # this option is only available for subgroups\n      end",
    "comment": "@example - delete user groups older than 24 hours GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ bundle exec rake delete_user_groups  @example - delete all user groups older than 2019-01-01 GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ DELETE_BEFORE=2019-01-01 \\ bundle exec rake delete_user_groups  @example - dry run GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ bundle exec rake \"delete_user_groups[true]\"",
    "label": "",
    "id": "3907"
  },
  {
    "raw_code": "def self.run(args)\n        migrator = new(args)\n\n        migrator.migrate_data\n      end",
    "comment": "Run Influx Migrator  @param [Hash] the arguments hash @return [void]",
    "label": "",
    "id": "3908"
  },
  {
    "raw_code": "def query(influx_bucket, stats_type, range)\n        <<~QUERY\n          from(bucket: \"#{influx_bucket}\")\n          |> range(#{range})\n          |> filter(fn: (r) => r[\"_measurement\"] == \"#{stats_type}\")\n          |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n        QUERY\n      end",
    "comment": "FluxQL query used to fetch data  @param [String] influx_bucket bucket to fetch data @param [String] stats_type of data to fetch @param [String] range for influxdb query @return [void]",
    "label": "",
    "id": "3909"
  },
  {
    "raw_code": "def create_a_discussion_on_issue_api_req(project_path_or_id, issue_id, body)\n        call_api(expected_response_code: 201) do\n          post Runtime::API::Request.new(@api_client, \"/projects/#{project_path_or_id}/issues/#{issue_id}/discussions\").url, \"body=\\\"#{body}\\\"\"\n        end",
    "comment": "API Requests",
    "label": "",
    "id": "3910"
  },
  {
    "raw_code": "def initialize(dry_run: false)\n        unless ENV['USER_ID'] || ENV['CLEANUP_ALL_QA_USER_PROJECTS']\n          raise ArgumentError, \"Please provide USER_ID or CLEANUP_ALL_QA_USER_PROJECTS environment variable\"\n        end",
    "comment": "@example - delete the given users projects older than 24 hours GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ USER_ID=<id> bundle exec rake delete_user_projects  @example - delete all users projects older than 2019-01-01 GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ DELETE_BEFORE=2019-01-01 \\ CLEANUP_ALL_QA_USER_PROJECTS=true bundle exec rake delete_user_projects  @example - dry run GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ USER_ID=<id> bundle exec rake \"delete_user_projects[true]\"",
    "label": "",
    "id": "3911"
  },
  {
    "raw_code": "def initialize(dry_run: false)\n        super\n\n        @type = 'project'\n      end",
    "comment": "@example mark projects for deletion that are older than 24 hours under gitlab-e2e-sandbox-group-<#1-8> groups GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> bundle exec rake delete_projects  @example permanently delete projects older than 24 hours under all gitlab-e2e-sandbox-group-<#1-8> groups GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ PERMANENTLY_DELETE=true bundle exec rake delete_projects  @example mark projects for deletion under 'gitlab-e2e-sandbox-group-2' created before 2023-01-01 GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> \\ TOP_LEVEL_GROUP_NAME=<gitlab-e2e-sandbox-group-2> \\ DELETE_BEFORE=2023-01-01 bundle exec rake delete_projects  @example - dry run GITLAB_ADDRESS=<address> \\ GITLAB_QA_ACCESS_TOKEN=<token> bundle exec rake \"delete_projects[true]\"",
    "label": "",
    "id": "3912"
  },
  {
    "raw_code": "def migrate_data\n        INFLUX_BUCKETS.each do |bucket|\n          INFLUX_STATS_TYPE.each do |stats_type|\n            if bucket == Support::InfluxdbTools::INFLUX_MAIN_TEST_METRICS_BUCKET && stats_type == \"fabrication-stats\"\n              break\n            end",
    "comment": "Fetch data from Influx DB, store as CSV and upload to GCS  @return [void]",
    "label": "",
    "id": "3913"
  },
  {
    "raw_code": "def influx_to_csv(influx_bucket, stats_type, data_file_name)\n        QA::Runtime::Logger.info(\"Fetching Influx data for the last #{@hours} hours\")\n        all_runs = query_api.query(query: query(influx_bucket, stats_type, \"start: -#{@hours}h\"))\n        CSV.open(data_file_name, \"wb\", col_sep: '|') do |csv|\n          stats_array = stats_type == \"test-stats\" ? TEST_STATS_FIELDS : FABRICATION_STATS_FIELDS\n          all_runs.each do |table|\n            table.records.each do |record|\n              csv << stats_array.map { |key| record.values[key] }\n            end",
    "comment": "Query InfluxDB and store in JSON  @param [String] influx_bucket bucket to fetch data @param [String] stats_type of data to fetch @param [String] data_file_name to store data @return void",
    "label": "",
    "id": "3914"
  },
  {
    "raw_code": "def upload_to_gcs(bucket, backup_file_path)\n        file_path = backup_file_path.tr('_0-9', '')\n\n        # Backup older file\n        begin\n          QA::Runtime::Logger.info(\"Backing up older file to #{backup_file_path}\")\n          gcs_client.copy_object(bucket, file_path, bucket, backup_file_path)\n        rescue Google::Apis::ClientError\n          QA::Runtime::Logger.warn(\"File #{file_path} is not found in GCS bucket, continuing with upload...\")\n        end",
    "comment": "Upload file to GCS  @param [String] bucket to be uploaded to @param [String] backup_file_path of file to be uploaded @return [void]",
    "label": "",
    "id": "3915"
  },
  {
    "raw_code": "def delete_permanently(resource, delayed_verification = false, skip_verification = false)\n        # Get the path of the project or group again since marking it for deletion changes the name\n        updated_path = get_resource(resource)\n        return unless updated_path\n\n        resource[:full_path] = updated_path[:full_path] if resource[:type].include?('group')\n        resource[:path_with_namespace] = updated_path[:path_with_namespace] if resource[:type].include?('project')\n\n        path = resource_path(resource)\n        response = delete(resource_request(resource, permanently_remove: true, full_path: path))\n\n        return log_failure(resource, response) unless success?(response&.code)\n\n        return resource if delayed_verification\n\n        return log_permanent_deletion(resource) if skip_verification\n\n        wait_for_resource_deletion(resource, true)\n\n        permanently_deleted?(resource) ? log_permanent_deletion(resource) : log_failure(resource, response)\n      end",
    "comment": "Permanently deletes a given resource  @param [Hash] resource @param [Boolean] delayed_verification Wait until the end of the script to verify deletion @param [Boolean] skip_verification Skip verification of deletion for time constraint purposes @return [Array<String, Hash>] results",
    "label": "",
    "id": "3916"
  },
  {
    "raw_code": "def delete_resource(\n        resource, delayed_verification = false, permanent = false, skip_verification = false,\n        **options)\n        max_retries = 6\n        retry_count = 0\n\n        while retry_count <= max_retries\n          response = delete(resource_request(resource, **options))\n\n          case\n          when deletion_successful?(response)\n            return handle_successful_deletion(resource, response, delayed_verification, permanent, skip_verification)\n          when response&.code == HTTP_STATUS_NOT_FOUND\n            return log_permanent_deletion(resource)\n          when should_remove_security_policy?(response, retry_count, max_retries)\n            find_and_unassign_security_policy_project(resource)\n            retry_count += 1\n          when resource[:type] == 'project' && should_remove_registry_tags?(response, retry_count, max_retries)\n            remove_registry_tags(resource)\n            retry_count += 1\n          else\n            return log_failure(resource, response)\n          end",
    "comment": "Deletes a given resource  @param [<Hash>] resource @param [Boolean] delayed_verification Wait until the end of the script to verify deletion @param [Boolean] permanent Permanently delete resources instead of marking for deletion @param [Boolean] skip_verification Skip verification of deletion for time constraint purposes @param [Hash] API call options @return [Array<String, Hash>] results",
    "label": "",
    "id": "3917"
  },
  {
    "raw_code": "def get_resource(resource)\n        response = get(resource_request(resource))\n\n        if success?(response&.code)\n          parse_body(response)\n        else\n          path = resource.is_a?(String) ? resource : resource_path(resource)\n          logger.warn(\"Get #{path}, returned #{response.code}\")\n          nil\n        end",
    "comment": "Fetches the given resource again and parses its response  @param [Hash] resource @return [Hash] resource",
    "label": "",
    "id": "3918"
  },
  {
    "raw_code": "def group_or_project?(resource)\n        %w[group project].include?(resource[:type])\n      end",
    "comment": "Checks if resource type is a group or project  @param [Hash] resource @return [Boolean]",
    "label": "",
    "id": "3919"
  },
  {
    "raw_code": "def log_dry_run_output(resources)\n        return logger.info(\"No resources would be deleted\") if resources.empty?\n\n        logger.info(\"The following #{resources.length} resources would be deleted:\")\n\n        resources.each do |resource|\n          created_at = resource[:created_at]\n          path = resource_path(resource)\n          logger.info(\"#{path} - created at: #{created_at}\")\n        end",
    "comment": "Print results of dry run  @param [Array<Hash>] resources List of resource hashes @return [void]",
    "label": "",
    "id": "3920"
  },
  {
    "raw_code": "def log_failure(resource, response)\n        path = resource_path(resource)\n        logger.error(\"\\e[31mFAILED\\e[0m to delete #{resource[:type]} #{path} with #{response.code}.\\n\")\n        [\"failed_deletions\", { path: path, response: response }]\n      end",
    "comment": "Print failure message for a given resource  @param [<Hash>] resource @param [<Hash>] response @return [Array<String, Hash>] results",
    "label": "",
    "id": "3921"
  },
  {
    "raw_code": "def log_marked_for_deletion(resource)\n        path = resource_path(resource)\n        logger.info(\"\\e[32mSUCCESS\\e[0m: Marked #{resource[:type]} #{path} for deletion\\n\")\n        [\"marked_deletions\", resource]\n      end",
    "comment": "Print marked for deletion message for a given resource  @param [<Hash>] resource @return [Array<String, Hash>] results",
    "label": "",
    "id": "3922"
  },
  {
    "raw_code": "def log_permanent_deletion(resource)\n        path = resource_path(resource)\n        logger.info(\"\\e[32mSUCCESS\\e[0m: Permanently deleted #{resource[:type]} #{path}\\n\")\n        [\"permanent_deletions\", resource]\n      end",
    "comment": "Print permanent deletion message for a given resource  @param [<Hash>] resource @return [Array<String, Hash>] results",
    "label": "",
    "id": "3923"
  },
  {
    "raw_code": "def log_results(results, dry_run = false)\n        return logger.info(\"Dry run complete\") if dry_run\n\n        return logger.info(\"No results to report\") if results.blank?\n\n        processed_results = results.group_by(&:shift).transform_values(&:flatten)\n\n        marked_deletions = processed_results[\"marked_deletions\"]\n        permanent_deletions = processed_results[\"permanent_deletions\"]\n        failed_deletions = processed_results[\"failed_deletions\"]\n\n        logger.info(\"Marked #{marked_deletions.length} resource(s) for deletion\") unless marked_deletions.blank?\n        logger.info(\"Deleted #{permanent_deletions.length} resource(s)\") unless permanent_deletions.blank?\n\n        print_failed_deletion_attempts(failed_deletions)\n\n        logger.info('Done')\n\n        exit 1 unless failed_deletions.blank?\n      end",
    "comment": "Print results of entire script run  @param [Array<String, Hash>] results @param [Boolean] dry_run Defaults to false @return [void]",
    "label": "",
    "id": "3924"
  },
  {
    "raw_code": "def mark_for_deletion_possible?(resource)\n        return false unless group_or_project?(resource) || resource[:type] == 'sandbox'\n\n        resource.key?(:marked_for_deletion_on)\n      end",
    "comment": "Check if a resource can be marked for deletion  @param [Hash] resource Resource to check @return [Boolean]",
    "label": "",
    "id": "3925"
  },
  {
    "raw_code": "def self_deletion_scheduled?(resource, fetch_again: false)\n        if fetch_again\n          resource = get_resource(resource)\n          return false unless resource\n        end",
    "comment": "Check if resource is marked for deletion  @param [Hash]resource Resource to check @param[Boolean] fetch_again Whether to fetch the resource again before checking. Default: false @return [Boolean]",
    "label": "",
    "id": "3926"
  },
  {
    "raw_code": "def permanently_deleted?(resource)\n        response = get(resource_request(resource))\n        response.code == HTTP_STATUS_NOT_FOUND\n      end",
    "comment": "Check if resource is permanently deleted  @param [Hash] resource Resource to check @return [Boolean]",
    "label": "",
    "id": "3927"
  },
  {
    "raw_code": "def personal_access_token\n        @personal_access_token ||= begin\n          admin_token = ENV['GITLAB_QA_ADMIN_ACCESS_TOKEN']\n          user_token = ENV['GITLAB_QA_ACCESS_TOKEN']\n\n          if admin_token.blank? && user_token.blank?\n            abort(\"\\nPlease provide either GITLAB_QA_ADMIN_ACCESS_TOKEN or GITLAB_QA_ACCESS_TOKEN\")\n          end",
    "comment": "Returns the appropriate personal access token for API authentication  Retrieves and memoizes a GitLab personal access token, prioritizing admin tokens when available. Admin tokens (GITLAB_QA_ADMIN_ACCESS_TOKEN) are preferred for environments that support admin scope operations, as they're required for cleaning up User resources and other admin-level operations.  Admin tokens will soon be required for resource cleanup  @return [String] Personal access token for GitLab API authentication @raise [SystemExit] Aborts program execution if neither token environment variable is set",
    "label": "",
    "id": "3928"
  },
  {
    "raw_code": "def print_failed_deletion_attempts(failed_deletions)\n        return logger.info('No failed deletion attempts to report!') if failed_deletions.blank?\n\n        logger.info(\"\\e[31mThere were #{failed_deletions.length} failed deletion attempts:\\e[0m\\n\")\n\n        failed_deletions.each do |attempt|\n          logger.info(\"Resource: #{attempt[:path]}\")\n          logger.error(\"Response: #{attempt[:response]}\\n\")\n        end",
    "comment": "Prints failed deletion attempts  @param [Array<Hash{path=>String, response=>Hash}>] failed_deletions List of hashes of failed deletion attempts @return [void]",
    "label": "",
    "id": "3929"
  },
  {
    "raw_code": "def resource_path(resource)\n        resource[:full_path] || resource[:path_with_namespace] || resource[:web_url]\n      end",
    "comment": "Resource path of a given resource  @param [Hash] resource Resource @return [String] Resource path",
    "label": "",
    "id": "3930"
  },
  {
    "raw_code": "def user_api_client(token)\n        Runtime::API::Client.new(ENV['GITLAB_ADDRESS'],\n          personal_access_token: token)\n      end",
    "comment": "Create a new api client for the specified token - used for deleting test user personal resources  @param [String] token Personal access token @return [Runtime::API::Client] API client",
    "label": "",
    "id": "3931"
  },
  {
    "raw_code": "def verify_deletions(unverified_deletions, permanent)\n        logger.info('Verifying deletions...')\n\n        unverified_deletions.filter_map do |resource|\n          wait_for_resource_deletion(resource, permanent: permanent)\n          response = get(resource_request(resource))\n\n          if response&.code == HTTP_STATUS_NOT_FOUND\n            log_permanent_deletion(resource)\n          else\n            log_failure(resource, response)\n          end",
    "comment": "Verifies deletions of given resources by attempting to find them. If the resource is found, logs a failure. Used with delayed_verification.  @param [Array<Hash>] unverified_deletions List of resources that were not verified @param [Boolean] permanent If resource is permanently deleted or only marked for deletion @return [void]",
    "label": "",
    "id": "3932"
  },
  {
    "raw_code": "def wait_for_resource_deletion(resource, permanent = false)\n        wait_until(max_duration: 160, sleep_interval: 2, raise_on_failure: false) do\n          response = get(resource_request(resource))\n          deleted = response&.code == HTTP_STATUS_NOT_FOUND\n\n          if permanent && resource[:type] != 'sandbox'\n            deleted\n          else\n            deleted || (success?(response&.code) && self_deletion_scheduled?(parse_body(response)))\n          end",
    "comment": "Wait for resource to be deleted (resource cannot be found or resource has been marked for deletion)  @param [Hash] resource Resource to wait for deletion for @return [Boolean] Whether the resource was deleted",
    "label": "",
    "id": "3933"
  },
  {
    "raw_code": "def find_and_unassign_security_policy_project(resource)\n        if has_security_policy_project?(resource)\n          unassign_security_policy_project(resource[:full_path])\n        elsif projects_with_security_policy_projects(resource).present?\n          projects_with_security_policy_projects(resource).each do |project|\n            unassign_security_policy_project(project[:fullPath])\n          end",
    "comment": "Finds and unassigns a security policy project from a resource Note: full_path is used for REST API resources and fullPath is used for GraphQL resources We can only unassign a security policy project through GraphQL and not the REST API.  @param [Hash] resource Resource to remove security policy project from @return [void]",
    "label": "",
    "id": "3934"
  },
  {
    "raw_code": "def unassign_security_policy_project(path)\n        logger.info(\"Unassigning security policy project for #{path}\")\n\n        mutation = <<~GQL\n          mutation {\n            securityPolicyProjectUnassign(input: { fullPath: \"#{path}\" }) {\n              errors\n            }\n          }\n        GQL\n\n        graphql_request(mutation)\n      end",
    "comment": "Unassigns security policy project from resource  @param [String] path Full path of the resource @return [response]",
    "label": "",
    "id": "3935"
  },
  {
    "raw_code": "def graphql_request(query)\n        response = post(Runtime::API::Request.new(api_client, '/graphql').url, { query: query })\n        parse_body(response)\n      end",
    "comment": "Posts GraphQL request  @param [query] query GraphQL query @return [response]",
    "label": "",
    "id": "3936"
  },
  {
    "raw_code": "def deletion_successful?(response)\n        success?(response&.code) || response.include?(\"already been marked for deletion\")\n      end",
    "comment": "Checks if response was successful  @param [Hash] response @return [Boolean]",
    "label": "",
    "id": "3937"
  },
  {
    "raw_code": "def handle_successful_deletion(resource, response, delayed_verification, permanent, skip_verification)\n        return resource if delayed_verification && !group_or_project?(resource)\n\n        if skip_verification\n          # If skip_verification is true and it's a group or project, delete permanently if permanent set\n          if group_or_project?(resource) && permanent\n            return delete_permanently(resource, delayed_verification,\n              skip_verification)\n          end",
    "comment": "Handles successful deletion  @param [Hash] resource @param [Hash] response @param [Boolean] delayed_verification Wait until the end of the script to verify deletion @param [Boolean] permanent @param [Boolean] skip_verification @return [Array<String, Hash>] results",
    "label": "",
    "id": "3938"
  },
  {
    "raw_code": "def should_remove_security_policy?(response, retry_count, max_retries)\n        response&.code == HTTP_STATUS_BAD_REQUEST &&\n          response&.include?(\"security policy project\") &&\n          retry_count < max_retries\n      end",
    "comment": "Checks if response indicates that the request failed because of a security policy project association  @param [Hash] response @param [Integer] retry_count @param [Integer] max_retries @return [Boolean]",
    "label": "",
    "id": "3939"
  },
  {
    "raw_code": "def should_remove_registry_tags?(response, retry_count, max_retries)\n        response&.code == HTTP_STATUS_BAD_REQUEST &&\n          response&.include?(\"Cannot rename project, the container registry path rename validation failed\") &&\n          retry_count < max_retries\n      end",
    "comment": "Checks if response indicates that the request failed because of container registry tags  @param [Hash] response @param [Integer] retry_count @param [Integer] max_retries @return [Boolean]",
    "label": "",
    "id": "3940"
  },
  {
    "raw_code": "def remove_registry_tags(resource)\n        project_id = extract_project_id(resource)\n        return unless project_id\n\n        logger.info(\"Removing registry tags for project #{project_id}...\")\n\n        repositories = fetch_registry_repositories(project_id)\n        return if repositories.empty?\n\n        repositories.each do |repository|\n          remove_repository_tags(project_id, repository)\n          delete_registry_repository(project_id, repository[:id])\n        end",
    "comment": "Removes all registry tags and repositories for a given project resource  @param [Hash] resource Project resource containing project ID @return [Boolean] Whether the operation was successful",
    "label": "",
    "id": "3941"
  },
  {
    "raw_code": "def delete_resources(\n        resources, delayed_verification = false, permanent = @permanently_delete,\n        skip_verification = @skip_verification, **options\n      )\n        logger.info(\"Deleting #{resources.length} #{@type}s...\\n\")\n\n        unverified_deletions = []\n        results = []\n\n        resources.each do |resource|\n          path = resource_path(resource)\n          resource[:type] = @type\n\n          logger.info(\"Deleting #{@type} #{path}...\")\n\n          result = delete_resource(resource, delayed_verification, permanent, skip_verification, **options)\n\n          if result.is_a?(Array)\n            results.append(result)\n          else\n            unverified_deletions << result\n          end",
    "comment": "Deletes a list of resources  @param [Array<Hash>] resources List of resources to delete @param [Boolean] delayed_verifications Wait until the end of the script to verify deletions @param [Boolean] permanent Permanently delete resources instead of marking for deletion @param [Boolean] skip_verification Skip verification of deletion for time constraint purposes @param [Hash] API call options @return [Array<String, Hash>] results",
    "label": "",
    "id": "3942"
  },
  {
    "raw_code": "def fetch_qa_user_id(qa_username)\n        user_response = get Runtime::API::Request.new(api_client, \"/users\", username: qa_username).url\n\n        unless user_response.code == HTTP_STATUS_OK\n          logger.error(\"Request for #{qa_username} returned (#{user_response.code}): `#{user_response}` \")\n          exit 1 if fatal_response?(user_response.code)\n          return\n        end",
    "comment": "Fetches the user ID of the given username  @param [String] qa_username @return [Integer]",
    "label": "",
    "id": "3943"
  },
  {
    "raw_code": "def fetch_resources(api_path, client = api_client)\n        logger.info(\"Fetching #{@type}s created before #{@delete_before} on #{ENV['GITLAB_ADDRESS']}...\")\n\n        page_no = '1'\n        resources = []\n\n        while page_no.present?\n          response = get Runtime::API::Request.new(\n            client,\n            api_path,\n            page: page_no,\n            per_page: ITEMS_PER_PAGE\n          ).url\n\n          if response.code == HTTP_STATUS_OK\n            resources.concat(parse_body(response).select { |r| Time.parse(r[:created_at]) < @delete_before })\n          else\n            logger.error(\"Request for #{@type} returned (#{response.code}): `#{response}` \")\n            exit 1 if fatal_response?(response.code)\n          end",
    "comment": "Fetches resources by api path that were created before the @delete_before date  @param [String] api_path Api path to fetch resources from @return [Array<Hash>] list of parsed resource hashes",
    "label": "",
    "id": "3944"
  },
  {
    "raw_code": "def fetch_token_user(token_name, client = api_client)\n        logger.info(\"Fetching #{token_name} user ...\")\n\n        user_response = get Runtime::API::Request.new(client, \"/user\").url\n\n        unless user_response.code == HTTP_STATUS_OK\n          logger.error(\"Request for user returned (#{user_response.code}): `#{user_response}` \")\n          exit 1 if [HTTP_STATUS_UNAUTHORIZED, HTTP_STATUS_BAD_REQUEST].include?(user_response.code)\n        end",
    "comment": "Fetches the user who owns the token with the given token name by using the given user_api_client  @param [String] token_name Token name, not the token itself @param [Runtime::API::Client] client, api client for the token @return [Hash] User",
    "label": "",
    "id": "3945"
  },
  {
    "raw_code": "def extract_project_id(resource)\n          # Try different ways to get project ID\n          project_id = resource[:id] ||\n            resource['id'] ||\n            resource[:api_path]&.match(%r{/projects/(\\d+)})&.[](1) ||\n            resource['api_path']&.match(%r{/projects/(\\d+)})&.[](1)\n\n          unless project_id\n            logger.error(\"Could not extract project ID from resource: #{resource}\")\n            return\n          end",
    "comment": "Extracts project ID from resource  @param [Hash] resource Project resource @return [String, nil] Project ID or nil if not found",
    "label": "",
    "id": "3946"
  },
  {
    "raw_code": "def fetch_registry_repositories(project_id)\n          response = get(Runtime::API::Request.new(api_client,\n            \"/projects/#{project_id}/registry/repositories\").url)\n\n          unless success?(response&.code)\n            logger.warn(\"Failed to fetch registry repositories for project #{project_id}: #{response&.code}\")\n            return []\n          end",
    "comment": "Fetches all registry repositories for a project  @param [String] project_id Project ID @return [Array<Hash>] Array of repository objects",
    "label": "",
    "id": "3947"
  },
  {
    "raw_code": "def remove_repository_tags(project_id, repository)\n          repository_id = repository[:id]\n          logger.info(\"Removing tags from repository #{repository_id}...\")\n\n          tags = fetch_repository_tags(project_id, repository_id)\n          return if tags.empty?\n\n          tags.each do |tag|\n            delete_registry_tag(project_id, repository_id, tag[:name])\n          end",
    "comment": "Removes all tags from a specific repository  @param [String] project_id Project ID @param [Hash] repository Repository object with :id @return [void]",
    "label": "",
    "id": "3948"
  },
  {
    "raw_code": "def fetch_repository_tags(project_id, repository_id)\n          response = get(Runtime::API::Request.new(api_client,\n            \"/projects/#{project_id}/registry/repositories/#{repository_id}/tags\").url)\n\n          unless success?(response&.code)\n            logger.warn(\"Failed to fetch tags for repository #{repository_id}: #{response&.code}\")\n            return []\n          end",
    "comment": "Fetches all tags for a specific repository  @param [String] project_id Project ID @param [Integer] repository_id Repository ID @return [Array<Hash>] Array of tag objects",
    "label": "",
    "id": "3949"
  },
  {
    "raw_code": "def delete_registry_tag(project_id, repository_id, tag_name)\n          response = delete(Runtime::API::Request.new(api_client,\n            \"/projects/#{project_id}/registry/repositories/#{repository_id}/tags/#{tag_name}\").url)\n\n          if success?(response&.code)\n            logger.info(\"Deleted tag '#{tag_name}' from repository #{repository_id}\")\n          else\n            logger.warn(\"Failed to delete tag '#{tag_name}' from repository #{repository_id}: #{response&.code}\")\n          end",
    "comment": "Deletes a specific registry tag  @param [String] project_id Project ID @param [Integer] repository_id Repository ID @param [String] tag_name Tag name to delete @return [void]",
    "label": "",
    "id": "3950"
  },
  {
    "raw_code": "def delete_registry_repository(project_id, repository_id)\n          response = delete(Runtime::API::Request.new(api_client,\n            \"/projects/#{project_id}/registry/repositories/#{repository_id}\").url)\n\n          if success?(response&.code)\n            logger.info(\"Deleted registry repository #{repository_id}\")\n          else\n            logger.warn(\"Failed to delete registry repository #{repository_id}: #{response&.code}\")\n          end",
    "comment": "Deletes a registry repository  @param [String] project_id Project ID @param [Integer] repository_id Repository ID @return [void]",
    "label": "",
    "id": "3951"
  },
  {
    "raw_code": "def self.create_noop(pipeline_path: \"tmp\", logger: Runtime::Logger.logger, reason: nil)\n          new([], pipeline_path: pipeline_path, logger: logger).create_noop(reason: reason)\n        end",
    "comment": "Generate noop pipeline file definitions for all supported pipelines  @param pipeline_path [String] @param logger [Logger] @return [void]",
    "label": "",
    "id": "3952"
  },
  {
    "raw_code": "def initialize(tests, pipeline_path: \"tmp\", env: {}, logger: Runtime::Logger.logger)\n          @tests = tests\n          @pipeline_path = pipeline_path\n          @env = env\n          @logger = logger\n        end",
    "comment": "@param tests [Array] specific tests to run @param pipeline_path [String] path for generated pipeline files @param env [Hash] environment configuration for generated pipelines @param logger [Logger] logger instance @return [void]",
    "label": "",
    "id": "3953"
  },
  {
    "raw_code": "def create(pipeline_types = FUNCTIONAL_E2E_PIPELINE_TYPES)\n          unless (pipeline_types - FUNCTIONAL_E2E_PIPELINE_TYPES).empty?\n            raise(ArgumentError, \"Unsupported pipeline type filter set!\")\n          end",
    "comment": "Generate functional E2E test pipelines yaml files  @param pipeline_types [Array] pipeline types to generate @return [void]",
    "label": "",
    "id": "3954"
  },
  {
    "raw_code": "def create_non_functional\n          base_variables = base_pipeline_variables.map { |k, v| \"  #{k}: \\\"#{v}\\\"\" }.join(\"\\n\")\n          definitions = non_functional_test_pipeline_definitions.transform_values do |yml|\n            \"#{yml}\\nvariables:\\n#{base_variables}\"\n          end",
    "comment": "Generate non functional E2E test pipeline yaml files  @return [void]",
    "label": "",
    "id": "3955"
  },
  {
    "raw_code": "def create_noop(reason: nil)\n          noop_yml = noop_pipeline_yml(reason || \"no-op run, nothing will be executed!\")\n\n          (FUNCTIONAL_E2E_PIPELINE_TYPES + NON_FUNCTIONAL_PIPELINE_TYPES).each do |type|\n            # omnibus pipeline trigger defined an input and if it isn't present in no-op pipeline, it will fail\n            # see trigger job definition 'e2e:test-on-omnibus-ee' in '.gitlab/ci/qa.gitlab-ci.yml'\n            yml = if type == :test_on_omnibus\n                    <<~YML\n                      spec:\n                        inputs:\n                          pipeline-type:\n                            type: string\n                            default: external\n                      ---\n                      #{noop_yml}\n                    YML\n                  else\n                    noop_yml\n                  end",
    "comment": "Create noop pipeline definitions for each supported pipeline type  @return [void]",
    "label": "",
    "id": "3956"
  },
  {
    "raw_code": "def project_root\n          @project_root ||= File.expand_path(\"../\", Runtime::Path.qa_root)\n        end",
    "comment": "Project root path  @return [String]",
    "label": "",
    "id": "3957"
  },
  {
    "raw_code": "def noop_pipeline\n          @noop_pipeline ||= File.read(File.join(project_root, \".gitlab\", \"ci\", \"overrides\", \"skip.yml\"))\n        end",
    "comment": "Content of noop pipeline definition file  @return [String]",
    "label": "",
    "id": "3958"
  },
  {
    "raw_code": "def ci_files_path\n          @ci_files_path ||= if ENV[\"CI_PROJECT_NAMESPACE\"] == \"gitlab-cn\"\n                               File.join(project_root, \"jh\", \".gitlab\", \"ci\")\n                             else\n                               File.join(project_root, \".gitlab\", \"ci\")\n                             end",
    "comment": "Path for ci configuration files  @return [String]",
    "label": "",
    "id": "3959"
  },
  {
    "raw_code": "def functional_pipeline_definitions\n          @functional_pipeline_definitions ||= FUNCTIONAL_E2E_PIPELINE_TYPES.index_with do |pipeline_type|\n            File.read(File.join(ci_files_path, pipeline_type.to_s.tr(\"_\", \"-\"), \"main.gitlab-ci.yml\"))\n          end",
    "comment": "Functional test pipeline definitions  @return [Hash<Symbol, String>]",
    "label": "",
    "id": "3960"
  },
  {
    "raw_code": "def non_functional_test_pipeline_definitions\n          @non_functional_test_pipeline_definitions ||= NON_FUNCTIONAL_PIPELINE_TYPES.index_with do |pipeline_type|\n            File.read(File.join(ci_files_path, pipeline_type.to_s.tr(\"_\", \"-\"), \"main.gitlab-ci.yml\"))\n          end",
    "comment": "Non functional test pipeline definitions  @return [Hash<Symbol, String>]",
    "label": "",
    "id": "3961"
  },
  {
    "raw_code": "def example_runtimes\n          @example_runtimes ||= JSON.load_file(Support::KnapsackReport::RUNTIME_REPORT)\n        end",
    "comment": "Example runtimes of all executed tests  @return [Hash<String, Number>]",
    "label": "",
    "id": "3962"
  },
  {
    "raw_code": "def generated_yml_file_name(pipeline_type)\n          File.join(pipeline_path, \"#{pipeline_type.to_s.tr('_', '-')}-pipeline.yml\")\n        end",
    "comment": "File name for generated pipeline definition file  @param pipeline_type [Symbol] @return [String]",
    "label": "",
    "id": "3963"
  },
  {
    "raw_code": "def scenario_examples\n          @scenario_examples ||= ScenarioExamples.fetch(tests)\n        end",
    "comment": "Specific examples to be executed  @return [Hash<Class, Array<String>]",
    "label": "",
    "id": "3964"
  },
  {
    "raw_code": "def base_pipeline_variables\n          @base_pipeline_variables ||= begin\n            ruby_version = File.read(File.join(project_root, \".ruby-version\")).strip\n\n            {\n              \"RUBY_VERSION\" => ENV[\"RUBY_VERSION\"] || ruby_version,\n              \"GITLAB_SEMVER_VERSION\" => File.read(File.join(project_root, \"VERSION\")),\n              \"FEATURE_FLAGS\" => env[\"QA_FEATURE_FLAGS\"]\n            }.compact\n          end",
    "comment": "Base variables included in all pipeline types  @return [Hash]",
    "label": "",
    "id": "3965"
  },
  {
    "raw_code": "def variables_section\n          @pipeline_variables ||= \"variables:\\n\".then do |variables|\n            vars = {\n              **base_pipeline_variables,\n              # QA_SUITES is only used by test-on-omnibus due to pipeline being reusable in external projects\n              \"QA_SUITES\" => executable_qa_suites,\n              \"QA_TESTS\" => tests&.join(\" \")\n            }.filter_map { |k, v| \"  #{k}: \\\"#{v}\\\"\" unless v.blank? }.join(\"\\n\")\n\n            \"#{variables}#{vars}\"\n          end",
    "comment": "Additional variables section for generated pipeline  @return [String]",
    "label": "",
    "id": "3966"
  },
  {
    "raw_code": "def executable_qa_suites\n          @executable_qa_suites ||= scenario_runtimes\n            # shorten suite klass name if it matches pattern\n            # fallback to klass.to_s simplifies testing with anonymous classes\n            .filter_map { |klass, runtime| klass.to_s.match(/^QA::.*(Test\\S+)$/)&.[](1) || klass.to_s if runtime > 0 }\n            .join(\",\")\n        end",
    "comment": "List of test suites that have executable tests  @return [String]",
    "label": "",
    "id": "3967"
  },
  {
    "raw_code": "def scenario_runtimes\n          @scenario_runtimes ||= scenario_examples\n            .each_with_object(Hash.new { |hsh, key| hsh[key] = 0 }) do |(scenario, examples), runtimes|\n              next unless scenario.pipeline_mapping\n\n              executable_examples = examples.reject { |example| example[:status] == \"pending\" }\n              # set runtime to 0 if particular scenario would skip all tests\n              next runtimes[scenario] = 0 if executable_examples.empty?\n\n              # Sum total runtime for all examples in scenario\n              # Default to small value if runtimes report has no value for particular example\n              # in order to not skip scenario entirely if report simply hasn't runtime data yet\n              executable_examples.each { |example| runtimes[scenario] += example_runtimes[example[:id]] || 0.01 }\n            end",
    "comment": "Total runtime value for each scenario that has pipeline mapping defined  @return [Hash<Class, Number>]",
    "label": "",
    "id": "3968"
  },
  {
    "raw_code": "def create_pipeline_definition_files(definitions)\n          definitions.each do |type, yaml|\n            file_name = generated_yml_file_name(type)\n            File.write(file_name, yaml)\n            logger.info(\"Pipeline definition file created: '#{file_name}'\")\n          end",
    "comment": "Create pipeline definition files  @param definitions [Hash<Symbol, String>] @return [void]",
    "label": "",
    "id": "3969"
  },
  {
    "raw_code": "def pipeline_job_runtimes\n          scenario_runtimes.each_with_object(Hash.new { |hsh, key| hsh[key] = [] }) do |(scenario, runtime), runtimes|\n            scenario.pipeline_mapping.each do |pipeline_type, jobs|\n              unless FUNCTIONAL_E2E_PIPELINE_TYPES.include?(pipeline_type)\n                raise \"Scenario class '#{scenario}' contains unsupported pipeline type '#{pipeline_type}'\"\n              end",
    "comment": "Pipeline job runtimes  Hash with pipeline type as key and array of runtimes for each job running within that pipeline  @return [Hash<Symbol, Array<Hash>]",
    "label": "",
    "id": "3970"
  },
  {
    "raw_code": "def updated_pipeline_definitions(pipeline_types)\n          pipeline_job_runtimes.each_with_object({}) do |(pipeline_type, jobs), definitions|\n            next unless pipeline_types.include?(pipeline_type)\n\n            logger.info(\"Processing pipeline '#{pipeline_type}'\")\n            zero_runtime = jobs.all? { |job| job[:runtime] == 0 }\n            if zero_runtime\n              logger.info(\"  All jobs have zero runtime, creating 'no-op' pipeline\")\n              next definitions[pipeline_type] = noop_pipeline_yml(\"no-op run, pipeline has no executable tests\")\n            end",
    "comment": "Updated pipeline yml files  @param pipeline_types [Array<Symbol>] @return [Hash<Symbol, String>]",
    "label": "",
    "id": "3971"
  },
  {
    "raw_code": "def updated_job(job_name, job_runtime, pipeline_yml, pipeline_type)\n          job_definition = job_definition(job_name, pipeline_yml)\n          raise \"Job definition not found for job '#{job_name}' in pipeline: #{pipeline_type}\" unless job_definition\n          return pipeline_yml.sub(job_definition, set_job_never_rule(job_definition)) if job_runtime == 0\n\n          parallel_count = calculate_parallel_jobs_count(job_runtime, pipeline_type)\n          pipeline_yml.sub(job_definition, update_job_parallel_count(job_definition, parallel_count))\n        end",
    "comment": "Update job definition in pipeline yml Correctly set: * job parallel count * never rule if no tests are to be executed * specific tests variables depending on job parallelization  @param job_name [String] @param job_runtime [Number] @param pipeline_yml [String] @param pipeline_type [Symbol] @return [String]",
    "label": "",
    "id": "3972"
  },
  {
    "raw_code": "def job_definition(job_name, pipeline_yml)\n          pipeline_yml.match(/^#{job_name}:\\n(?:\\s{2}.*\\n)+/)&.[](0)\n        end",
    "comment": "Get job definition from pipeline yaml  @param job_name [String] @param pipeline_yml [String] @return [String, nil]",
    "label": "",
    "id": "3973"
  },
  {
    "raw_code": "def set_job_never_rule(job_definition)\n          logger.info(\"   setting rule definition to 'never'\")\n          existing_rule = job_definition.match(/rules:\\n(?:\\s+-.*\\n)+/)&.[](0)\n          return \"#{job_definition}  #{RULE_NEVER}\" unless existing_rule\n\n          job_definition.sub(existing_rule, RULE_NEVER)\n        end",
    "comment": "Set job rule to never in order to skip it's execution  @param job_definition [String] @param rule [String] @return [String]",
    "label": "",
    "id": "3974"
  },
  {
    "raw_code": "def update_job_parallel_count(job_definition, parallel_count)\n          pattern = /^(\\s*parallel:) \\d+$/\n\n          logger.info(\"   setting parallel count to '#{parallel_count}'\")\n          return job_definition.sub(pattern, \"\\\\1 #{parallel_count}\") if job_definition.match?(pattern)\n\n          \"#{job_definition}  parallel: #{parallel_count}\\n\"\n        end",
    "comment": "Update job parallel count  @param job_definition [String] @param parallel_count [Integer] @return [String]",
    "label": "",
    "id": "3975"
  },
  {
    "raw_code": "def calculate_parallel_jobs_count(job_runtime, pipeline_type)\n          (job_runtime / TEST_RUNTIME_TARGET * RUNTIME_COEFFICIENT.fetch(pipeline_type, 1.0)).ceil\n        end",
    "comment": "Calculate needed parallel job count  @param job_runtime [Number] @param pipeline_type [Symbol] @return [Integer]",
    "label": "",
    "id": "3976"
  },
  {
    "raw_code": "def noop_pipeline_yml(reason)\n          <<~YML\n            variables:\n              SKIP_MESSAGE: \"#{reason}\"\n\n            #{noop_pipeline}\n          YML\n        end",
    "comment": "No-op pipeline yml with skip reason message  @param reason [String] @return [String]",
    "label": "",
    "id": "3977"
  },
  {
    "raw_code": "def export\n          return logger.warn(\"No files matched pattern '#{metrics_file_glob}'\") if metrics_files.empty?\n\n          push_test_metrics_to_influxdb\n          push_test_metrics_to_gcs\n        end",
    "comment": "Export metrics to main bucket  @return [void]",
    "label": "",
    "id": "3978"
  },
  {
    "raw_code": "def write_options\n          InfluxDB2::WriteOptions.new(\n            write_type: InfluxDB2::WriteType::BATCHING,\n            batch_size: 100,\n            max_retries: 3\n          )\n        end",
    "comment": "Write options for influxdb  @return [InfluxDB::WriteOptions]",
    "label": "",
    "id": "3979"
  },
  {
    "raw_code": "def metrics_files\n          @metrics_files ||= Dir.glob(metrics_file_glob)\n        end",
    "comment": "Metrics data files  @return [Array]",
    "label": "",
    "id": "3980"
  },
  {
    "raw_code": "def metrics_data\n          @metrics_data ||= metrics_files\n            .flat_map { |file| JSON.parse(File.read(file), symbolize_names: true) }\n            .map { |entry| entry.merge(time: entry[:time].to_time) }\n        end",
    "comment": "Test metrics data  @return [Array<Hash>]",
    "label": "",
    "id": "3981"
  },
  {
    "raw_code": "def gcs_bucket\n          @gcs_bucket ||= ENV['QA_METRICS_GCS_BUCKET_NAME'] ||\n            raise('Missing QA_METRICS_GCS_BUCKET_NAME env variable')\n        end",
    "comment": "Get GCS Bucket Name or raise error if missing  @return [String]",
    "label": "",
    "id": "3982"
  },
  {
    "raw_code": "def push_test_metrics_to_gcs\n          gcs_file_name = \"main_test-stats-#{env('CI_PIPELINE_ID') || 'local'}.json\"\n\n          retry_on_exception(sleep_interval: 30, message: 'Failed to push test metrics to GCS') do\n            gcs_client.put_object(gcs_bucket, gcs_file_name, metrics_data.to_json,\n              force: true, content_type: 'application/json')\n\n            logger.info(\"Pushed #{metrics_data.length} test execution entries to GCS\")\n          end",
    "comment": "Push test execution metrics to GCS  @return [void]",
    "label": "",
    "id": "3983"
  },
  {
    "raw_code": "def push_test_metrics_to_influxdb\n          logger.info(\"Exporting #{metrics_data.size} entries to influxdb\")\n\n          influx_client\n            .create_write_api(write_options: write_options)\n            .write(data: metrics_data, bucket: INFLUX_MAIN_TEST_METRICS_BUCKET)\n\n        rescue StandardError => e\n          logger.error(\"Failed to push test execution metrics to influxdb, error: #{e}\")\n        end",
    "comment": "Push test execution metrics to InfluxDB  @return [void]",
    "label": "",
    "id": "3984"
  },
  {
    "raw_code": "def qa_tests(from_code_path_mapping: false)\n          return [] if mr_diff.empty? || dependency_changes\n          return changed_specs if only_spec_changes?\n          return selective_tests_from_code_paths_mapping if from_code_path_mapping\n\n          []\n        end",
    "comment": "Specific specs to run  @return [Array]",
    "label": "",
    "id": "3985"
  },
  {
    "raw_code": "def framework_changes?\n          return false if mr_diff.empty?\n          return false if only_spec_changes?\n\n          changed_files\n            # TODO: expand pattern to other non spec paths that shouldn't trigger full suite\n            .select { |file_path| file_path.match?(QA_PATTERN) && !file_path.match?(SPEC_PATTERN) }\n            .any?\n        end",
    "comment": "Qa framework changes  @return [Boolean]",
    "label": "",
    "id": "3986"
  },
  {
    "raw_code": "def quarantine_changes?\n          return false if mr_diff.empty?\n          return false if mr_diff.any? { |change| change[:new_file] || change[:deleted_file] }\n\n          files_count = 0\n          specs_count = 0\n          quarantine_specs_count = 0\n\n          mr_diff.each do |change|\n            path = change[:path]\n            next if File.directory?(File.expand_path(\"../#{path}\", QA::Runtime::Path.qa_root))\n\n            files_count += 1\n            next unless path.match?(SPEC_PATTERN) && path.end_with?('_spec.rb')\n\n            specs_count += 1\n            quarantine_specs_count += 1 if change[:diff].match?(/^\\+.*,? quarantine:/)\n          end",
    "comment": "Only quarantine changes  @return [Boolean]",
    "label": "",
    "id": "3987"
  },
  {
    "raw_code": "def only_spec_removal?\n          return false if mr_diff.empty?\n\n          only_spec_changes? && mr_diff.all? { |change| change[:deleted_file] }\n        end",
    "comment": "All changes are spec removals  @return [Boolean]",
    "label": "",
    "id": "3988"
  },
  {
    "raw_code": "def only_changes_to_non_e2e_spec_files?\n          return false if mr_diff.empty?\n\n          mr_diff.all? { |change| change[:path].start_with?(\"spec/\", \"ee/spec/\") }\n        end",
    "comment": "All changes are changes to non e2e specs  @return [Boolean]",
    "label": "",
    "id": "3989"
  },
  {
    "raw_code": "def changed_specs\n          mr_diff\n            .reject { |change| change[:deleted_file] }\n            .map { |change| change[:path].delete_prefix(\"qa/\") } # make paths relative to qa directory\n        end",
    "comment": "Changed spec files  @return [Array, nil]",
    "label": "",
    "id": "3990"
  },
  {
    "raw_code": "def only_spec_changes?\n          changed_files.all? { |file_path| file_path =~ SPEC_PATTERN }\n        end",
    "comment": "Are the changed files only qa specs?  @return [Boolean] whether the changes files are only qa specs",
    "label": "",
    "id": "3991"
  },
  {
    "raw_code": "def non_qa_changes?\n          changed_files.none? { |file_path| file_path =~ QA_PATTERN }\n        end",
    "comment": "Are the changed files only outside the qa directory?  @return [Boolean] whether the changes files are outside of qa directory",
    "label": "",
    "id": "3992"
  },
  {
    "raw_code": "def dependency_changes\n          changed_files.any? { |file| file.match?(DEPENDENCY_PATTERN) }\n        end",
    "comment": "Changes to gitlab dependencies  @return [Boolean]",
    "label": "",
    "id": "3993"
  },
  {
    "raw_code": "def changed_files\n          @changed_files ||= mr_diff.pluck(:path)\n        end",
    "comment": "Change files in merge request  @return [Array<String>]",
    "label": "",
    "id": "3994"
  },
  {
    "raw_code": "def selective_tests_from_code_paths_mapping\n          logger.info(\"Fetching tests to execute based on backend code paths mapping\")\n\n          unless code_paths_map\n            logger.warn(\"Failed to obtain code mappings for test selection!\")\n            return []\n          end",
    "comment": "Selective E2E tests based on code paths mapping  @return [Array]",
    "label": "",
    "id": "3995"
  },
  {
    "raw_code": "def code_paths_map\n          @code_paths_map ||= QA::Tools::Ci::CodePathsMapping.new.import(\"master\", \"e2e-test-on-gdk\")\n        end",
    "comment": "Get the mapping hash from GCP storage  @return [Hash]",
    "label": "",
    "id": "3996"
  },
  {
    "raw_code": "def frontend_code_paths_map\n          @frontend_code_paths_map ||= QA::Tools::Ci::CodePathsMapping\n                                         .new.import(\"master\", \"e2e-test-on-gdk\",\n                                           file_name: \"js-coverage-by-example-merged-pipeline\")\n        end",
    "comment": "Get the frontend mapping hash from GCP storage  @return [Hash]",
    "label": "",
    "id": "3997"
  },
  {
    "raw_code": "def fetch\n          logger.info(\"Fetching executable examples for all scenario classes\")\n          (all_scenario_classes - ignored_scenarios).each_with_object({}) do |scenario, scenarios|\n            examples = fetch_examples(scenario)\n            skipped_examples = examples.select { |example| example[:status] == \"pending\" }\n\n            logger.info(\" detected examples, total: #{examples.size}, skipped: #{skipped_examples.size}\")\n            scenarios[scenario] = examples\n          end",
    "comment": "Return list of executable examples for each scenario class  @return [Hash<Class, Array<Hash>>]",
    "label": "",
    "id": "3998"
  },
  {
    "raw_code": "def ignored_scenarios\n          return IGNORED_SCENARIOS.map(&:constantize) if QA.const_defined?(\"QA::EE\")\n\n          IGNORED_SCENARIOS.reject { |scenario| scenario.include?(\"EE\") }.map(&:constantize)\n        end",
    "comment": "Ignored scenarios classes  @return [Array<Class>]",
    "label": "",
    "id": "3999"
  },
  {
    "raw_code": "def all_scenario_classes\n          foss_scenarios = scenario_classes(QA::Scenario::Test)\n          return foss_scenarios unless QA.const_defined?(\"QA::EE\")\n\n          foss_scenarios + scenario_classes(QA::EE::Scenario::Test)\n        end",
    "comment": "Get all defined scenarios  @return [Array<Class>]",
    "label": "",
    "id": "4000"
  },
  {
    "raw_code": "def scenario_classes(mod)\n          mod.constants.flat_map do |const|\n            c = mod.const_get(const, false)\n            next c if c.is_a?(Class)\n\n            scenario_classes(c)\n          end",
    "comment": "Fetch scenario classes recursively  @param [Module] mod @return [Array<Object>]",
    "label": "",
    "id": "4001"
  },
  {
    "raw_code": "def fetch_examples(klass)\n          logger.info(\"Fetching examples for scenario '#{klass}'\")\n\n          spec_pattern = klass.spec_pattern\n          scenario_tests = scenario_class_tests(spec_pattern)\n          return [] if spec_pattern && scenario_tests.empty? # no executable specs for this scenario class\n\n          Support::ExampleData.fetch(klass.focus, scenario_tests, logger: logger).map do |example|\n            example.slice(:id, :status)\n          end",
    "comment": "Fetch list of executable examples for scenario class  @param klass [Class] @return [Array<Hash>]",
    "label": "",
    "id": "4002"
  },
  {
    "raw_code": "def scenario_class_tests(pattern)\n          return qa_tests if pattern.nil?\n\n          scenario_tests = Dir.glob(pattern)\n          return scenario_tests if qa_tests.nil? || qa_tests.empty?\n\n          qa_tests.flat_map do |path|\n            next path if File.file?(path) && File.fnmatch(pattern, path)\n            next specs_in_path(path, scenario_tests) if File.directory?(path)\n\n            []\n          end",
    "comment": "Specs for particular scenario class if it defines specific spec pattern  @param pattern [String, nil] @return [Array]",
    "label": "",
    "id": "4003"
  },
  {
    "raw_code": "def specs_in_path(path, scenario_specs)\n          scenario_specs.select { |spec| spec.match?(%r{#{Specs::Runner::ABSOLUTE_PATH_PREFIX_PATTERN}#{path}}) }\n        end",
    "comment": "List of specs within a path  @param path [String] @param scenario_specs [Array] @return [Array]",
    "label": "",
    "id": "4004"
  },
  {
    "raw_code": "def export(mapping_files_glob, bucket: DEFAULT_BUCKET, file_name: DEFAULT_FILE_NAME)\n          mapping_files = Dir.glob(mapping_files_glob)\n          return logger.warn(\"No files matched pattern, skipping coverage mapping upload\") if mapping_files.empty?\n\n          unless ENV[\"QA_RUN_TYPE\"].present?\n            return logger.warn(\"QA_RUN_TYPE variable is not set, skipping coverage mapping upload\")\n          end",
    "comment": "Export code path mappings to GCP  @param [String] mapping_files_glob - glob pattern for mapping files @param [String] bucket - custom bucket name (optional) @param [String] file_name - custom file name (optional) @return [void]",
    "label": "",
    "id": "4005"
  },
  {
    "raw_code": "def import(branch, run_type, bucket: DEFAULT_BUCKET, file_name: DEFAULT_FILE_NAME)\n          prefix = \"#{branch}/#{run_type}/#{file_name}\"\n\n          filename = code_paths_mapping_file(prefix, bucket)\n\n          logger.info(\"The mapping file fetched in import: #{filename}\")\n          file = client.get_object(bucket, filename)\n          JSON.parse(file[:body])\n        rescue StandardError => e\n          logger.error(\"Failed to download code paths mapping from GCS. Error: #{e}\")\n          logger.error(\"Backtrace: #{e.backtrace}\")\n          nil # Ensure it returns nil in case of GCS errors\n        end",
    "comment": "Import code path mappings from GCP  @param [String] branch - branch name @param [String] run_type - run type @param [String] bucket - custom bucket name (optional) @param [String] file_name - custom file name base (optional) @return [Hash]",
    "label": "",
    "id": "4006"
  },
  {
    "raw_code": "def client\n          @client ||= Fog::Google::Storage.new(google_project: PROJECT, **gcs_credentials)\n        end",
    "comment": "GCS client  @return [Fog::Google::StorageJSON]",
    "label": "",
    "id": "4007"
  },
  {
    "raw_code": "def gcs_credentials\n          json_key =  ENV.fetch(\"QA_CODE_PATH_MAPPINGS_GCS_CREDENTIALS\") do\n            raise \"QA_CODE_PATH_MAPPINGS_GCS_CREDENTIALS env variable is required!\"\n          end",
    "comment": "GCS credentials json  @return [Hash]",
    "label": "",
    "id": "4008"
  },
  {
    "raw_code": "def code_paths_mapping_file(prefix, bucket = DEFAULT_BUCKET)\n          paginated_list(client.list_objects(bucket, prefix: prefix)).last&.name\n        end",
    "comment": "Code paths mapping file from GCS  Get most up to date mapping file based on pipeline type @return [String]",
    "label": "",
    "id": "4009"
  },
  {
    "raw_code": "def paginated_list(list)\n          return [] if list.items.nil?\n          return list.items if list.next_page_token.nil?\n\n          paginated_list(\n            client.list_objects(list.bucket, prefix: list.prefixes.first, page_token: list.next_page_token)\n          ) + list.items\n        end",
    "comment": "Paginated list of items  @param [Google::Apis::StorageV1::Objects] list @return [Array]",
    "label": "",
    "id": "4010"
  },
  {
    "raw_code": "def logger\n          @logger ||= Gitlab::QA::TestLogger.logger(\n            level: Gitlab::QA::Runtime::Env.log_level,\n            source: \"CI Tools\"\n          )\n        end",
    "comment": "Logger instance  @return [Logger]",
    "label": "",
    "id": "4011"
  },
  {
    "raw_code": "def fetch\n          logger.info(\"Detecting feature flag changes\")\n          ff_toggles = mr_diff.map do |change|\n            ff_yaml = ff_yaml_for_file(change)\n            next unless ff_yaml\n\n            state = if ff_yaml[:deleted]\n                      \"deleted\"\n                    else\n                      ff_yaml[:default_enabled] ? 'disabled' : 'enabled'\n                    end",
    "comment": "Return list of feature flags changed in mr with inverse or deleted state  @return [String]",
    "label": "",
    "id": "4012"
  },
  {
    "raw_code": "def ff_yaml_for_file(change)\n          return unless %r{/feature_flags/.*\\.yml}.match?(change[:path])\n\n          if change[:deleted_file]\n            return { name: change[:path].split(\"/\").last.gsub(/\\.(yml|yaml)/, \"\"), deleted: true }\n          end",
    "comment": "Loads the YAML feature flag definition based on changed files in merge requests. The definition is loaded from the definition file itself.  @param [Hash] change mr file change @return [Hash] a hash containing the YAML data for the feature flag definition",
    "label": "",
    "id": "4013"
  },
  {
    "raw_code": "def append_to_file(path, text)\n        File.open(path, \"a\") { |f| f.write(text) }\n      end",
    "comment": "Append text to file  @param [String] path @param [String] text @return [void]",
    "label": "",
    "id": "4014"
  },
  {
    "raw_code": "def mr_labels\n        ENV[\"CI_MERGE_REQUEST_LABELS\"]&.split(',') || []\n      end",
    "comment": "Merge request labels  @return [Array]",
    "label": "",
    "id": "4015"
  },
  {
    "raw_code": "def mr_diff\n        mr_iid = ENV[\"CI_MERGE_REQUEST_IID\"]\n        return [] unless mr_iid\n\n        gitlab_endpoint = ENV[\"CI_API_V4_URL\"]\n        gitlab_token = ENV[\"PROJECT_TOKEN_FOR_CI_SCRIPTS_API_USAGE\"]\n        project_id = ENV[\"CI_MERGE_REQUEST_PROJECT_ID\"]\n\n        response = get(\n          \"#{gitlab_endpoint}/projects/#{project_id}/merge_requests/#{mr_iid}/changes\",\n          headers: { \"PRIVATE-TOKEN\" => gitlab_token }\n        )\n\n        parse_body(response).fetch(:changes, []).map do |change|\n          {\n            path: change[:new_path],\n            **change.slice(:new_file, :deleted_file, :diff)\n          }\n        end",
    "comment": "Merge request changes  @return [Array<Hash>]",
    "label": "",
    "id": "4016"
  },
  {
    "raw_code": "def mock_response(code, body)\n    instance_double(RestClient::Response, code: code, body: body.to_json)\n  end",
    "comment": "Instance double for rest client response  @param code [Integer] @param body [Hash] @return [InstanceDouble]",
    "label": "",
    "id": "4017"
  },
  {
    "raw_code": "def request_args(verb, path, payload, token = \"token\")\n    {\n      method: verb,\n      url: \"https://gitlab.com/api/v4/projects/278964/#{path}\",\n      payload: payload,\n      verify_ssl: false,\n      headers: { \"PRIVATE-TOKEN\" => token }\n    }.compact\n  end",
    "comment": "Request args passed to rest client  @param verb [Symbol] @param path [String] @param payload [Hash] @param token [String] @return [Hash]",
    "label": "",
    "id": "4018"
  },
  {
    "raw_code": "def expect_mr_created(reviewer_ids: nil)\n    expect(knapsack_reporter).to have_received(:create_knapsack_report).with(merged_runtimes)\n    expect(RestClient::Request).to have_received(:execute).with(request_args(:post, \"repository/commits\", {\n      branch: branch,\n      commit_message: \"Update master_report.json for E2E tests\",\n      actions: [\n        {\n          action: \"update\",\n          file_path: \"qa/knapsack/example_runtimes/master_report.json\",\n          content: \"#{JSON.pretty_generate(merged_runtimes)}\\n\"\n        },\n        {\n          action: \"update\",\n          file_path: \"qa/knapsack/master_report.json\",\n          content: \"#{JSON.pretty_generate(merged_report)}\\n\"\n        }\n      ]\n    }))\n    expect(RestClient::Request).to have_received(:execute).with(request_args(:post, \"merge_requests\", {\n      source_branch: branch,\n      target_branch: \"master\",\n      title: \"Update knapsack runtime data for E2E tests\",\n      remove_source_branch: true,\n      squash: true,\n      reviewer_ids: reviewer_ids,\n      labels: [\n        \"group::development analytics\",\n        \"type::maintenance\",\n        \"maintenance::pipelines\",\n        \"automation:bot-authored\",\n        \"automation:bot-no-updates\",\n        \"pipeline::tier-3\"\n      ].join(\",\"),\n      description: \"Update fallback knapsack report and example runtime data report.\".then do |description|\n        next description unless reviewer_ids.nil?\n\n        \"#{description}\\n\\ncc: @gl-dx/maintainers\"\n      end",
    "comment": "Expect merge request was created  @param reviewer_ids [Array, nil] @return [void]",
    "label": "",
    "id": "4019"
  },
  {
    "raw_code": "def flush_after_commit_queue(merge_request)\n  # To prevent idle in transaction timeouts, defer the creation of the\n  # NewMergeRequestWorker in a real Sidekiq job.\n  Sidekiq::Testing.disable! do\n    Gitlab::ExclusiveLease.skipping_transaction_check do\n      # Seed-Fu runs this entire fixture in a transaction, so the `after_commit`\n      # hook won't run until after the fixture is loaded. That is too late\n      # since the Sidekiq::Testing block has already exited. Force clearing\n      # the `after_commit` queue to ensure the job is run now.\n      merge_request.send(:_run_after_commit_queue)\n    end",
    "comment": "Normally merge requests build their diffs in an async job via NewMergeRequestWorker, but this method will force the after_create actions to happen inline.",
    "label": "",
    "id": "4020"
  },
  {
    "raw_code": "def up\n    # no-op\n  end",
    "comment": "The migration was finalized in 17.4 https://gitlab.com/gitlab-org/gitlab/-/merge_requests/159861#note_2067062639",
    "label": "",
    "id": "4021"
  },
  {
    "raw_code": "def up\n    with_lock_retries do\n      remove_foreign_key_if_exists(\n        :packages_terraform_module_metadata,\n        column: :project_id,\n        on_delete: :nullify,\n        name: OLD_CONSTRAINT_NAME\n      )\n    end",
    "comment": "new foreign key added in ChangeProjectIdFkOnPackagesTerraformModuleMetadata",
    "label": "",
    "id": "4022"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :notes, :id, name: INDEX_NAME, where: 'project_id is NULL'\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- Already added in a async index creation db/post_migrate/20250411043427_add_temp_index_on_notes_for_projects_null_and_id.rb",
    "label": "",
    "id": "4023"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :notes, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4024"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :sbom_occurrences, [:traversal_ids, :highest_severity, :component_id, :component_version_id],\n      where: 'archived = false',\n      name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- Legacy migration",
    "label": "",
    "id": "4025"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :sbom_occurrences, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4026"
  },
  {
    "raw_code": "def up\n    return unless Gitlab.com_except_jh?\n\n    prepare_async_index :events, :personal_namespace_id, name: INDEX_NAME, # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n      where: 'personal_namespace_id IS NOT NULL'\n  end",
    "comment": "-- https://gitlab.com/gitlab-org/gitlab/-/issues/462801#note_2081632603",
    "label": "",
    "id": "4027"
  },
  {
    "raw_code": "def up\n    return unless should_run?\n\n    prepare_async_index_removal :project_statistics, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/466691",
    "label": "",
    "id": "4028"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Will be removed once the backfilling for note is complete\n    prepare_async_index(:notes,\n      :id,\n      name: NEW_INDEX_NAME,\n      where: 'project_id is NULL')\n    # rubocop:enable Migration/PreventIndexCreation\n  end",
    "comment": "Remove index issue: gitlab.com/gitlab-org/gitlab/-/issues/535969 Make the index async issue: gitlab.com/gitlab-org/gitlab/-/issues/535970",
    "label": "",
    "id": "4029"
  },
  {
    "raw_code": "def up\n    # no-op\n  end",
    "comment": "To be re-enqueued by: db/post_migrate/20240927202948_reenqueue_deduplicate_lfs_objects_projects_with_null_repository_types.rb",
    "label": "",
    "id": "4030"
  },
  {
    "raw_code": "def restore_index_names\n    RENAMED_PARTITION_INDEX_MAP.each do |renamed_index|\n      next unless index_name_exists?(TABLE_NAME, renamed_index[:old_name])\n\n      rename_index(TABLE_NAME, renamed_index[:old_name], renamed_index[:new_name])\n    end",
    "comment": "PostgreSQL is generating different names than what we already have when we create a partition. So we have to rename these indexes to restore original names for previous migrations that depends on the names.",
    "label": "",
    "id": "4031"
  },
  {
    "raw_code": "def up\n    return unless should_run?\n\n    prepare_async_index_removal :project_statistics, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/466691",
    "label": "",
    "id": "4032"
  },
  {
    "raw_code": "def should_run?\n    !Gitlab.com_except_jh?\n  end",
    "comment": "This index is primarily used for optimizing ServicePing queries, which are not needed for GitLab.com, as we rely directly on our data warehouse. See https://gitlab.com/gitlab-org/gitlab/-/merge_requests/176561 for context.",
    "label": "",
    "id": "4033"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :merge_request_diff_files, :project_id, name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- https://gitlab.com/gitlab-org/gitlab/-/issues/512949",
    "label": "",
    "id": "4034"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal TABLE_NAME, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/536403",
    "label": "",
    "id": "4035"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n    prepare_async_index :sent_notifications, COLUMN_NAMES, where: \"noteable_type = 'Issue'\", name: INDEX_NAME\n    # rubocop:enable Migration/PreventIndexCreation\n  end",
    "comment": "Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/502841  This is designed to replace existing index: \"index_sent_notifications_on_noteable_type_noteable_id\" btree (noteable_id) WHERE noteable_type = 'Issue' with \"index_sent_notifications_on_noteable_type_noteable_id_id\" btree (noteable_id, id) WHERE noteable_type = 'Issue' to improve iterating over issue related sent notification records in batches.",
    "label": "",
    "id": "4036"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- large tables\n    prepare_async_index :approval_merge_request_rules, :approval_policy_rule_id, name: INDEX_NAME\n    # rubocop:enable Migration/PreventIndexCreation\n  end",
    "comment": "TODO: Index to be created synchronously as part of https://gitlab.com/gitlab-org/gitlab/-/merge_requests/155256",
    "label": "",
    "id": "4037"
  },
  {
    "raw_code": "def up\n    prepare_partitioned_async_check_constraint_validation PARTITIONED_TABLE_NAME, name: CONSTRAINT_NAME\n  end",
    "comment": "Partitioned check constraint to be validated as part of https://gitlab.com/gitlab-org/gitlab/-/issues/549079",
    "label": "",
    "id": "4038"
  },
  {
    "raw_code": "def up\n    validate_foreign_key :push_event_payloads, :project_id, name: FK_NAME\n  end",
    "comment": "NOTE: follow up to https://gitlab.com/gitlab-org/gitlab/-/merge_requests/201179 && https://gitlab.com/gitlab-org/gitlab/-/merge_requests/201993",
    "label": "",
    "id": "4039"
  },
  {
    "raw_code": "def down\n    add_concurrent_foreign_key TABLE_NAME, :prometheus_alert_events,\n      column: :prometheus_alert_event_id,\n      name: EVENTS_FK_NAME\n\n    add_concurrent_foreign_key TABLE_NAME, :issues,\n      column: :issue_id,\n      name: ISSUES_FK_NAME\n  end",
    "comment": "Original SQL:  ALTER TABLE ONLY issues_prometheus_alert_events ADD CONSTRAINT fk_rails_b32edb790f FOREIGN KEY (prometheus_alert_event_id) REFERENCES prometheus_alert_events(id) ON DELETE CASCADE;  ALTER TABLE ONLY issues_prometheus_alert_events ADD CONSTRAINT fk_rails_db5b756534 FOREIGN KEY (issue_id) REFERENCES issues(id) ON DELETE CASCADE; ",
    "label": "",
    "id": "4040"
  },
  {
    "raw_code": "def down\n    add_concurrent_foreign_key TABLE_NAME, :self_managed_prometheus_alert_events,\n      column: :self_managed_prometheus_alert_event_id,\n      name: EVENTS_FK_NAME\n\n    add_concurrent_foreign_key TABLE_NAME, :issues,\n      column: :issue_id,\n      name: ISSUES_FK_NAME\n  end",
    "comment": "Original SQL:  ALTER TABLE ONLY issues_self_managed_prometheus_alert_events ADD CONSTRAINT fk_rails_cc5d88bbb0 FOREIGN KEY (issue_id) REFERENCES issues(id) ON DELETE CASCADE;  ALTER TABLE ONLY issues_self_managed_prometheus_alert_events ADD CONSTRAINT fk_rails_f7db2d72eb FOREIGN KEY (self_managed_prometheus_alert_event_id) REFERENCES self_managed_prometheus_alert_events(id) ON DELETE CASCADE; ",
    "label": "",
    "id": "4041"
  },
  {
    "raw_code": "def up\n    validate_foreign_key :approval_merge_request_rules_users, :project_id, name: FK_NAME\n  end",
    "comment": "NOTE: follow up to https://gitlab.com/gitlab-org/gitlab/-/merge_requests/202062",
    "label": "",
    "id": "4042"
  },
  {
    "raw_code": "def up\n    update_value = Arel.sql(\"REPLACE(flux_resource_path, '#{OLD_VERSION}', '#{NEW_VERSION}')\")\n\n    update_column_in_batches(:environments, :flux_resource_path, update_value) do |table, query|\n      query.where(table[:flux_resource_path].matches(\"%#{OLD_VERSION}%\"))\n    end",
    "comment": "Update flux_resource_path to replace v1beta1 with v1 for Kustomizations",
    "label": "",
    "id": "4043"
  },
  {
    "raw_code": "def up\n    # No-op.\n  end",
    "comment": "Migration was set to succesful due to issue with foreign key validation when loose foreign key records were unprocessed. Will be reattempted in a new migraiton.",
    "label": "",
    "id": "4044"
  },
  {
    "raw_code": "def up\n    change_column_default(:p_ci_builds, :auto_canceled_by_partition_id, nil)\n    change_column_null(:p_ci_builds, :auto_canceled_by_partition_id, true)\n  end",
    "comment": "rubocop:disable Migration/ChangeColumnNullOnHighTrafficTable -- Legacy migration",
    "label": "",
    "id": "4045"
  },
  {
    "raw_code": "def up\n    remove_column(:application_settings, :cloud_connector_keys, if_exists: true)\n  end",
    "comment": "Follow-up to RemoveCloudConnectorKeysFromApplicationSettings.  This actually removes a column that was added in a previous migration where we had to make this a no-op due to a production issue. See https://gitlab.com/gitlab-com/gl-infra/production/-/issues/19182",
    "label": "",
    "id": "4046"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :sbom_occurrences,\n      \"traversal_ids, (licenses -> 0 ->> 'spdx_identifier'), component_id, component_version_id\",\n      where: 'archived = false',\n      name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- Once complete, this index unblocks the removal of other indexes https://gitlab.com/gitlab-org/gitlab/-/issues/442486",
    "label": "",
    "id": "4047"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :sbom_occurrences, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4048"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :sbom_occurrences, [:traversal_ids, :component_id, :component_version_id],\n      where: 'archived = false',\n      name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- Legacy migration",
    "label": "",
    "id": "4049"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :sbom_occurrences, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4050"
  },
  {
    "raw_code": "def up\n    # no-op\n  end",
    "comment": "https://gitlab.com/gitlab-com/gl-infra/production/-/issues/20518",
    "label": "",
    "id": "4051"
  },
  {
    "raw_code": "def up\n    prepare_async_index :merge_request_diff_commits, :project_id, name: INDEX_NAME # rubocop:disable Migration/PreventIndexCreation -- Need while reading from the old table before the new one is swapped\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/554107",
    "label": "",
    "id": "4052"
  },
  {
    "raw_code": "def concurrent_partitioned_foreign_key_name(table, column, prefix: 'fk_rails_')\n    identifier = \"#{table}_#{column}_fk\"\n    hashed_identifier = Digest::SHA256.hexdigest(identifier).first(10)\n\n    \"#{prefix}#{hashed_identifier}\"\n  end",
    "comment": "NOTE: it seems that prepare_async_foreign_key_validation uses concurrent_foreign_key_name internally which is different from concurrent_partitioned_foreign_key_name used in add_concurrent_partitioned_foreign_key.",
    "label": "",
    "id": "4053"
  },
  {
    "raw_code": "def down; end\nend",
    "comment": "NOP because these sequences were never used, and re-adding them causes the columns to be altered to use them as default values.",
    "label": "",
    "id": "4054"
  },
  {
    "raw_code": "def up; end\n\n  def down; end\nend",
    "comment": "no-op due to gitlab.com deploy incident https://gitlab.com/gitlab-com/gl-infra/production-engineering/-/issues/26996 See db/post_migrate/20250701220912_drop_user_starred_dashboards_again.rb for replacement",
    "label": "",
    "id": "4055"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :issues, [:project_id, :external_key], name: INDEX_NAME\n  end",
    "comment": "Follow-up issue to remove index https://gitlab.com/gitlab-org/gitlab/-/issues/558770",
    "label": "",
    "id": "4056"
  },
  {
    "raw_code": "def up\n    prepare_async_index :approval_project_rules, :approval_policy_rule_id, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be created synchronously as part of https://gitlab.com/gitlab-org/gitlab/-/merge_requests/155256",
    "label": "",
    "id": "4057"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :sbom_occurrences, [:component_version_id, :traversal_ids],\n      where: 'archived = false',\n      name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- Legacy migration",
    "label": "",
    "id": "4058"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :sbom_occurrences, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4059"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :issues, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "Follow-up issue to remove index https://gitlab.com/gitlab-org/gitlab/-/issues/559053",
    "label": "",
    "id": "4060"
  },
  {
    "raw_code": "def up\n    return unless should_run?\n\n    prepare_async_index_removal :project_statistics, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/466691",
    "label": "",
    "id": "4061"
  },
  {
    "raw_code": "def up\n    # no-op\n  end",
    "comment": "The migration was finalized in 17.4 https://gitlab.com/gitlab-org/gitlab/-/merge_requests/159861#note_2067062639",
    "label": "",
    "id": "4062"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n    add_concurrent_index :sent_notifications, COLUMN_NAMES, where: \"noteable_type = 'Issue'\", name: INDEX_NAME\n    # rubocop:enable Migration/PreventIndexCreation\n  end",
    "comment": "Creating prepared index in 20241106125627_update_sent_notifications_index_on_noteable https://gitlab.com/gitlab-org/gitlab/-/merge_requests/171687",
    "label": "",
    "id": "4063"
  },
  {
    "raw_code": "def down\n    create_table TABLE_NAME, primary_key: [:issue_id, :self_managed_prometheus_alert_event_id] do |t|\n      t.bigint :issue_id, null: false\n      t.bigint :self_managed_prometheus_alert_event_id, null: false\n      t.timestamps_with_timezone null: false\n\n      t.index :self_managed_prometheus_alert_event_id, name: INDEX_NAME\n    end",
    "comment": "Original SQL:  CREATE TABLE issues_self_managed_prometheus_alert_events ( issue_id bigint NOT NULL, self_managed_prometheus_alert_event_id bigint NOT NULL, created_at timestamp with time zone NOT NULL, updated_at timestamp with time zone NOT NULL );  CREATE INDEX issue_id_issues_self_managed_rometheus_alert_events_index ON issues_self_managed_prometheus_alert_events USING btree (self_managed_prometheus_alert_event_id);  ALTER TABLE ONLY issues_self_managed_prometheus_alert_events ADD CONSTRAINT issues_self_managed_prometheus_alert_events_pkey PRIMARY KEY (issue_id, self_managed_prometheus_alert_event_id); ",
    "label": "",
    "id": "4064"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :project_statistics, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/464566",
    "label": "",
    "id": "4065"
  },
  {
    "raw_code": "def improved_delete_batched_background_migration(job_class_name, table_name, column_name, job_arguments)\n    Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_dml_mode!\n\n    Gitlab::Database::BackgroundMigration::BatchedMigration.reset_column_information\n\n    batched_migration = Gitlab::Database::BackgroundMigration::BatchedMigration\n      .for_configuration(\n        gitlab_schema_from_context, job_class_name, table_name, column_name, job_arguments,\n        include_compatible: true\n      ).take\n\n    return unless batched_migration\n\n    batched_migration.batched_jobs.each_batch(of: 100) { |b| b.delete_all }\n\n    batched_migration.delete\n  end",
    "comment": "For context on why `delete_batched_background_migration` is overloaded: https://gitlab.com/gitlab-org/gitlab/-/issues/434089#note_2696645957",
    "label": "",
    "id": "4066"
  },
  {
    "raw_code": "def up; end\n\n  def down; end\nend",
    "comment": "Introduced in 17.11 and no-op in 18.0. No-op because we decided not pursue this migration. See: https://gitlab.com/groups/gitlab-org/-/epics/16522#note_2492640881",
    "label": "",
    "id": "4067"
  },
  {
    "raw_code": "def up\n    return unless should_run?\n\n    prepare_async_index_removal :project_statistics, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/466691",
    "label": "",
    "id": "4068"
  },
  {
    "raw_code": "def up\n    add_concurrent_index(\n      :merge_request_diff_commits,\n      :merge_request_commits_metadata_id,\n      name: INDEX_NAME,\n      where: \"merge_request_commits_metadata_id IS NOT NULL\"\n    )\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- this index is already present on GitLab.com which was prepared in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/189775.",
    "label": "",
    "id": "4069"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :merge_request_diff_commits, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4070"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal TABLE_NAME, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/544929",
    "label": "",
    "id": "4071"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :merge_requests, COLUMN_NAME, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/454457",
    "label": "",
    "id": "4072"
  },
  {
    "raw_code": "def up\n    return unless Gitlab.com_except_jh?\n\n    return if column_exists?(:events, :personal_namespace_id)\n\n    with_lock_retries(raise_on_exhaustion: true) do\n      # Doing DDL in post-deployment migration is discouraged in general,\n      # this is done as a workaround to prevent production incidents when\n      # changing the schema for very high-traffic table\n      add_column :events, :personal_namespace_id, :bigint # rubocop:disable Migration/PreventAddingColumns -- Legacy migration\n    end",
    "comment": "rubocop:disable Migration/SchemaAdditionMethodsNoPost -- https://gitlab.com/gitlab-org/gitlab/-/merge_requests/165127#note_2092878736",
    "label": "",
    "id": "4073"
  },
  {
    "raw_code": "def up\n    prepare_async_index :resource_label_events, :namespace_id, name: INDEX_NAME # rubocop:disable Migration/PreventIndexCreation -- Sharding key is an exception\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/197676",
    "label": "",
    "id": "4074"
  },
  {
    "raw_code": "def down; end\nend",
    "comment": "one-way migration",
    "label": "",
    "id": "4075"
  },
  {
    "raw_code": "def up\n    prepare_async_index TABLE_NAME, INITIAL_PIPELINE_COLUMNS, name: INITIAL_PIPELINE_INDEX,\n      where: 'initial_pipeline_id IS NULL'\n    prepare_async_index TABLE_NAME, LATEST_PIPELINE_COLUMNS, name: LATEST_PIPELINE_INDEX,\n      where: 'latest_pipeline_id IS NULL'\n  end",
    "comment": "Index created synchronously in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/148514 TODO remove tmp index in https://gitlab.com/gitlab-org/gitlab/-/issues/454243",
    "label": "",
    "id": "4076"
  },
  {
    "raw_code": "def up\n    prepare_async_index TABLE_NAME, GROUP_LEVEL_COLUMNS, name: GROUP_LEVEL_INDEX, where: 'archived = false'\n    prepare_async_index TABLE_NAME, PROJECT_LEVEL_COLUMNS, name: PROJECT_LEVEL_INDEX,\n      order: { vulnerability_id: :desc }\n  end",
    "comment": "Index created synchronously in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/167603",
    "label": "",
    "id": "4077"
  },
  {
    "raw_code": "def up\n    prepare_async_index :vulnerability_occurrence_pipelines, :project_id, name: INDEX_NAME\n  end",
    "comment": "-- Legacy migration",
    "label": "",
    "id": "4078"
  },
  {
    "raw_code": "def up\n    prepare_async_index :scan_result_policy_violations, :approval_policy_rule_id, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be created synchronously as part of https://gitlab.com/gitlab-org/gitlab/-/merge_requests/155256",
    "label": "",
    "id": "4079"
  },
  {
    "raw_code": "def up\n    prepare_async_index :sent_notifications, :namespace_id, name: INDEX_NAME # rubocop:disable Migration/PreventIndexCreation -- Necessary for sharding key\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/work_items/541120",
    "label": "",
    "id": "4080"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n    add_concurrent_index :vulnerability_reads, %i[project_id resolved_on_default_branch], name: INDEX_NAME\n    # rubocop:enable Migration/PreventIndexCreation\n  end",
    "comment": "-- Legacy migration",
    "label": "",
    "id": "4081"
  },
  {
    "raw_code": "def up\n    indexes_by_definition = indexes_by_definition_for_table(TABLE_NAME)\n    parent_index_name = indexes_by_definition[INDEX_DEFINITION]\n\n    if parent_index_name.nil?\n      Gitlab::AppLogger.warn \"Index not removed because it doesn't exist (this may be due to an aborted \" \\\n        \"migration or similar): table_name: #{TABLE_NAME}, index_definition: #{INDEX_DEFINITION}\"\n\n      return\n    end",
    "comment": "The index name ends with `1` on Production and possibly on other instances, so we must find it by definition. See https://gitlab.com/gitlab-org/gitlab/-/issues/520213#note_2450943371.",
    "label": "",
    "id": "4082"
  },
  {
    "raw_code": "def up\n    add_concurrent_index TABLE_NAME, INITIAL_PIPELINE_COLUMNS, name: INITIAL_PIPELINE_INDEX,\n      where: 'initial_pipeline_id IS NULL'\n    add_concurrent_index TABLE_NAME, LATEST_PIPELINE_COLUMNS, name: LATEST_PIPELINE_INDEX,\n      where: 'latest_pipeline_id IS NULL'\n  end",
    "comment": "TODO remove in https://gitlab.com/gitlab-org/gitlab/-/issues/454243",
    "label": "",
    "id": "4083"
  },
  {
    "raw_code": "def improved_delete_batched_background_migration(job_class_name, table_name, column_name, job_arguments)\n    Gitlab::Database::QueryAnalyzers::RestrictAllowedSchemas.require_dml_mode!\n\n    Gitlab::Database::BackgroundMigration::BatchedMigration.reset_column_information\n\n    batched_migration = Gitlab::Database::BackgroundMigration::BatchedMigration\n      .for_configuration(\n        gitlab_schema_from_context, job_class_name, table_name, column_name, job_arguments,\n        include_compatible: true\n      ).take\n\n    return unless batched_migration\n\n    batched_migration.batched_jobs.each_batch(of: 100) { |b| b.delete_all }\n\n    batched_migration.delete\n  end",
    "comment": "For context on why `delete_batched_background_migration` is overloaded: https://gitlab.com/gitlab-org/gitlab/-/issues/434089#note_2696645957",
    "label": "",
    "id": "4084"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :subscriptions, COLUMN_NAMES, name: INDEX_NAME\n  end",
    "comment": "Creating prepared index in 20241106125601_update_subscriptions_index_on_noteable https://gitlab.com/gitlab-org/gitlab/-/merge_requests/171687",
    "label": "",
    "id": "4085"
  },
  {
    "raw_code": "def restore_index_names\n    RENAMED_PARTITION_INDEX_MAP.each do |renamed_index|\n      next unless index_name_exists?(TABLE_NAME, renamed_index[:old_name])\n\n      rename_index(TABLE_NAME, renamed_index[:old_name], renamed_index[:new_name])\n    end",
    "comment": "PostgreSQL is generating different names than what we already have when we create a partition. So we have to rename these indexes to restore original names for previous migrations that depends on the names.",
    "label": "",
    "id": "4086"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n    add_concurrent_index :vulnerability_reads, COLUMNS, name: INDEX_NAME, where: WHERE_CLAUSE\n    # rubocop:enable Migration/PreventIndexCreation\n  end",
    "comment": "-- Legacy migration",
    "label": "",
    "id": "4087"
  },
  {
    "raw_code": "def up; end\n\n  def down; end\nend",
    "comment": "Marking this as a noop because the backfill migration incorrectly ignores the default_enabled configuration value, causing wrong feature flag value to be added in setting. Revert MR: https://gitlab.com/gitlab-org/gitlab/-/merge_requests/190832",
    "label": "",
    "id": "4088"
  },
  {
    "raw_code": "def up\n    prepare_async_index :approval_group_rules, :approval_policy_rule_id, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be created synchronously as part of https://gitlab.com/gitlab-org/gitlab/-/merge_requests/155256",
    "label": "",
    "id": "4089"
  },
  {
    "raw_code": "def down\n    add_concurrent_foreign_key TABLE_NAME, :projects,\n      column: :project_id,\n      name: PROJECTS_FK_NAME\n\n    add_concurrent_foreign_key TABLE_NAME, :environments,\n      column: :environment_id,\n      name: ENVIRONMENTS_FK_NAME\n  end",
    "comment": "Original SQL:  ALTER TABLE ONLY self_managed_prometheus_alert_events ADD CONSTRAINT fk_rails_3936dadc62 FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE;  ALTER TABLE ONLY self_managed_prometheus_alert_events ADD CONSTRAINT fk_rails_39d83d1b65 FOREIGN KEY (environment_id) REFERENCES environments(id) ON DELETE CASCADE; ",
    "label": "",
    "id": "4090"
  },
  {
    "raw_code": "def down\n    create_table TABLE_NAME do |t|\n      t.bigint :project_id, null: false\n      t.bigint :prometheus_alert_id, null: false\n      t.datetime_with_timezone :started_at, null: false\n      t.datetime_with_timezone :ended_at\n      t.integer :status, limit: 2\n      t.string :payload_key\n\n      t.index [:prometheus_alert_id, :payload_key], name: UNIQUE_INDEX_NAME, unique: true\n      t.index [:project_id, :status], name: STATUS_INDEX\n    end",
    "comment": "Original SQL:  CREATE TABLE prometheus_alert_events ( id bigint NOT NULL, project_id bigint NOT NULL, prometheus_alert_id bigint NOT NULL, started_at timestamp with time zone NOT NULL, ended_at timestamp with time zone, status smallint, payload_key character varying );  CREATE SEQUENCE prometheus_alert_events_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1;  ALTER SEQUENCE prometheus_alert_events_id_seq OWNED BY prometheus_alert_events.id;  CREATE UNIQUE INDEX index_prometheus_alert_event_scoped_payload_key ON prometheus_alert_events USING btree (prometheus_alert_id, payload_key);  CREATE INDEX index_prometheus_alert_events_on_project_id_and_status ON prometheus_alert_events USING btree (project_id, status); ",
    "label": "",
    "id": "4091"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n    prepare_async_index :vulnerability_finding_links, :project_id, name: INDEX_NAME\n    # rubocop:enable Migration/PreventIndexCreation\n  end",
    "comment": "-- Legacy migration",
    "label": "",
    "id": "4092"
  },
  {
    "raw_code": "def down\n    Gitlab::Database.allow_cross_joins_across_databases(\n      url: 'https://gitlab.com/gitlab-org/gitlab/-/merge_requests/164505'\n    ) do\n      add_concurrent_foreign_key(:vulnerability_reads, :projects,\n        name: FOREIGN_KEY_NAME, column: :project_id,\n        target_column: :id, on_delete: :cascade, reverse_lock_order: true)\n    end",
    "comment": "This migration can only be reverted in the event that the instance database has not conducted a Sec Decomposition. GitLab.com does not run `down` operations.  As such, this allow_cross_joins_across_databases call will never be removed.",
    "label": "",
    "id": "4093"
  },
  {
    "raw_code": "def up\n    # no-op\n  end",
    "comment": "https://gitlab.com/gitlab-com/gl-infra/production/-/issues/20518",
    "label": "",
    "id": "4094"
  },
  {
    "raw_code": "def down\n    create_table TABLE_NAME do |t|\n      t.bigint :project_id, null: false\n      t.bigint :environment_id\n      t.datetime_with_timezone :started_at, null: false\n      t.datetime_with_timezone :ended_at\n      t.integer :status, limit: 2, null: false\n      t.string :title, limit: 255, null: false\n      t.string :query_expression, limit: 255\n      t.string :payload_key, limit: 255, null: false\n\n      t.index [:project_id, :payload_key], name: UNIQUE_INDEX_NAME, unique: true\n      t.index :environment_id, name: ENVIRONMENT_INDEX\n    end",
    "comment": "Original SQL:  CREATE TABLE self_managed_prometheus_alert_events ( id bigint NOT NULL, project_id bigint NOT NULL, environment_id bigint, started_at timestamp with time zone NOT NULL, ended_at timestamp with time zone, status smallint NOT NULL, title character varying(255) NOT NULL, query_expression character varying(255), payload_key character varying(255) NOT NULL );  CREATE SEQUENCE self_managed_prometheus_alert_events_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1;  ALTER SEQUENCE self_managed_prometheus_alert_events_id_seq OWNED BY self_managed_prometheus_alert_events.id;  CREATE UNIQUE INDEX idx_project_id_payload_key_self_managed_prometheus_alert_events ON self_managed_prometheus_alert_events USING btree (project_id, payload_key);  CREATE INDEX index_self_managed_prometheus_alert_events_on_environment_id ON self_managed_prometheus_alert_events USING btree (environment_id); ",
    "label": "",
    "id": "4095"
  },
  {
    "raw_code": "def up; end\n  def down; end\nend",
    "comment": "No-op for https://gitlab.com/gitlab-com/gl-infra/production/-/issues/19464",
    "label": "",
    "id": "4096"
  },
  {
    "raw_code": "def up\n    prepare_async_index :software_license_policies, :approval_policy_rule_id, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be created synchronously as part of https://gitlab.com/gitlab-org/gitlab/-/merge_requests/155256",
    "label": "",
    "id": "4097"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n    add_concurrent_index TABLE_NAME, :id, name: INDEX_NAME, where: \"state = 1\"\n    # rubocop:enable Migration/PreventIndexCreation\n  end",
    "comment": "-- Legacy migration",
    "label": "",
    "id": "4098"
  },
  {
    "raw_code": "def up\n    return if Gitlab.com_except_jh?\n\n    add_concurrent_index :events, # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n      :personal_namespace_id,\n      where: 'personal_namespace_id IS NOT NULL',\n      name: INDEX\n  end",
    "comment": "-- https://gitlab.com/gitlab-org/gitlab/-/issues/462801#note_2081632603",
    "label": "",
    "id": "4099"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n    add_concurrent_index :vulnerability_reads, [:owasp_top_10, :state, :report_type,\n      # rubocop:enable Migration/PreventIndexCreation\n      :severity, :traversal_ids, :vulnerability_id, :resolved_on_default_branch],\n      where: 'archived = false',\n      name: INDEX_NAME\n  end",
    "comment": "-- Legacy migration",
    "label": "",
    "id": "4100"
  },
  {
    "raw_code": "def up\n    validate_foreign_key :merge_request_cleanup_schedules, :project_id, name: FK_NAME\n  end",
    "comment": "NOTE: follow up to https://gitlab.com/gitlab-org/gitlab/-/merge_requests/201825 && https://gitlab.com/gitlab-org/gitlab/-/merge_requests/201986",
    "label": "",
    "id": "4101"
  },
  {
    "raw_code": "def up\n    prepare_async_index :subscriptions, COLUMN_NAMES, name: INDEX_NAME\n  end",
    "comment": "Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/502840  This is designed to improve iterating over related subscriptions records in batches:",
    "label": "",
    "id": "4102"
  },
  {
    "raw_code": "def down\n    Gitlab::Database.allow_cross_joins_across_databases(\n      url: 'https://gitlab.com/gitlab-org/gitlab/-/merge_requests/164505'\n    ) do\n      add_concurrent_foreign_key(:vulnerability_reads, :cluster_agents,\n        name: FOREIGN_KEY_NAME, column: :casted_cluster_agent_id,\n        target_column: :id, on_delete: :nullify, reverse_lock_order: true)\n    end",
    "comment": "This migration can only be reverted in the event that the instance database has not conducted a Sec Decomposition. GitLab.com does not run `down` operations.  As such, this allow_cross_joins_across_databases call will never be removed.",
    "label": "",
    "id": "4103"
  },
  {
    "raw_code": "def down\n    create_table TABLE_NAME do |t|\n      t.timestamps_with_timezone null: false\n      t.float :threshold, null: false\n      t.integer :operator, null: false\n      t.bigint :environment_id, null: false\n      t.bigint :project_id, null: false\n      t.bigint :prometheus_metric_id, null: false\n      t.text :runbook_url, limit: 255\n\n      t.index [:project_id, :prometheus_metric_id, :environment_id], name: UNIQUE_INDEX_NAME, unique: true\n      t.index :environment_id, name: ENVIRONMENT_INDEX_NAME\n      t.index :prometheus_metric_id, name: METRIC_INDEX_NAME\n    end",
    "comment": "Original SQL:  CREATE TABLE prometheus_alerts ( id bigint NOT NULL, created_at timestamp with time zone NOT NULL, updated_at timestamp with time zone NOT NULL, threshold double precision NOT NULL, operator integer NOT NULL, environment_id bigint NOT NULL, project_id bigint NOT NULL, prometheus_metric_id bigint NOT NULL, runbook_url text, CONSTRAINT check_cb76d7e629 CHECK ((char_length(runbook_url) <= 255)) );  CREATE SEQUENCE prometheus_alerts_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1;  ALTER SEQUENCE prometheus_alerts_id_seq OWNED BY prometheus_alerts.id;  CREATE UNIQUE INDEX index_prometheus_alerts_metric_environment ON prometheus_alerts USING btree (project_id, prometheus_metric_id, environment_id);  CREATE INDEX index_prometheus_alerts_on_environment_id ON prometheus_alerts USING btree (environment_id);  CREATE INDEX index_prometheus_alerts_on_prometheus_metric_id ON prometheus_alerts USING btree (prometheus_metric_id); ",
    "label": "",
    "id": "4104"
  },
  {
    "raw_code": "def up\n    # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n    add_concurrent_index :vulnerability_reads, [:owasp_top_10, :state, :report_type, :resolved_on_default_branch,\n      # rubocop:enable Migration/PreventIndexCreation\n      :severity, :traversal_ids, :vulnerability_id],\n      where: 'archived = false',\n      name: INDEX_NAME\n  end",
    "comment": "-- Legacy migration",
    "label": "",
    "id": "4105"
  },
  {
    "raw_code": "def up\n    remove_foreign_key_if_exists(\n      :approval_merge_request_rules,\n      column: :approval_policy_rule_id,\n      on_delete: :cascade,\n      name: OLD_CONSTRAINT_NAME)\n  end",
    "comment": "new foreign key added in db/migrate/20240918130318_replace_fk_on_approval_merge_request_rules_approval_policy_rule_id.rb and validated in db/migrate/20240918130409_validate_fk_on_approval_merge_request_rules_approval_policy_rule_id.rb",
    "label": "",
    "id": "4106"
  },
  {
    "raw_code": "def up\n    # no-op because there was a bug in the original migration, which has been\n    # fixed in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/203996\n    nil\n  end",
    "comment": "@return [Void]",
    "label": "",
    "id": "4107"
  },
  {
    "raw_code": "def down\n    # no-op because there was a bug in the original migration, which has been\n    # fixed in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/203996\n    nil\n  end",
    "comment": "@return [Void]",
    "label": "",
    "id": "4108"
  },
  {
    "raw_code": "def table_name\n    @table_name ||= table_exists?(P_TABLE_NAME) ? P_TABLE_NAME : 'ci_runners'\n  end",
    "comment": "https://gitlab.com/gitlab-org/gitlab/-/merge_requests/182549 renamed ci_runners_e59bb2812d to ci_runners, but then it was reverted in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/183389. That means some environments no longer have ci_runners_e59bb2812d, but they do have ci_runners. Fall back to fixing the constraint for ci_runners in that case.",
    "label": "",
    "id": "4109"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :p_ci_job_artifacts, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/187564 ",
    "label": "",
    "id": "4110"
  },
  {
    "raw_code": "def up\n    add_concurrent_index( # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n      TABLE_NAME, [:primary_identifier_id, :vulnerability_id],\n      name: NEW_INDEX_NAME\n    )\n  end",
    "comment": "-- This index was created async previously, check https://gitlab.com/gitlab-org/gitlab/-/merge_requests/131647",
    "label": "",
    "id": "4111"
  },
  {
    "raw_code": "def down\n    create_table TABLE_NAME, primary_key: [:issue_id, :prometheus_alert_event_id] do |t|\n      t.bigint :issue_id, null: false\n      t.bigint :prometheus_alert_event_id, null: false\n      t.timestamps_with_timezone null: false\n\n      t.index :prometheus_alert_event_id, name: INDEX_NAME\n    end",
    "comment": "Original SQL:  CREATE TABLE issues_prometheus_alert_events ( issue_id bigint NOT NULL, prometheus_alert_event_id bigint NOT NULL, created_at timestamp with time zone NOT NULL, updated_at timestamp with time zone NOT NULL );  CREATE INDEX issue_id_issues_prometheus_alert_events_index ON issues_prometheus_alert_events USING btree (prometheus_alert_event_id);  ALTER TABLE ONLY issues_prometheus_alert_events ADD CONSTRAINT issues_prometheus_alert_events_pkey PRIMARY KEY (issue_id, prometheus_alert_event_id); ",
    "label": "",
    "id": "4112"
  },
  {
    "raw_code": "def down\n    add_concurrent_foreign_key :alert_management_alerts, TABLE_NAME,\n      column: :prometheus_alert_id,\n      name: ALERTS_FK_NAME\n\n    add_concurrent_foreign_key TABLE_NAME, :environments,\n      column: :environment_id,\n      name: ENVIRONMENTS_FK_NAME\n\n    add_concurrent_foreign_key TABLE_NAME, :prometheus_metrics,\n      column: :prometheus_metric_id,\n      name: METRICS_FK_NAME\n\n    add_concurrent_foreign_key TABLE_NAME, :projects,\n      column: :project_id,\n      name: PROJECTS_FK_NAME\n  end",
    "comment": "Original SQL:  ALTER TABLE ONLY alert_management_alerts ADD CONSTRAINT fk_51ab4b6089 FOREIGN KEY (prometheus_alert_id) REFERENCES prometheus_alerts(id) ON DELETE CASCADE;  ALTER TABLE ONLY prometheus_alerts ADD CONSTRAINT fk_rails_6d9b283465 FOREIGN KEY (environment_id) REFERENCES environments(id) ON DELETE CASCADE;  ALTER TABLE ONLY prometheus_alerts ADD CONSTRAINT fk_rails_e6351447ec FOREIGN KEY (prometheus_metric_id) REFERENCES prometheus_metrics(id) ON DELETE CASCADE;  ALTER TABLE ONLY prometheus_alerts ADD CONSTRAINT fk_rails_f0e8db86aa FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE; ",
    "label": "",
    "id": "4113"
  },
  {
    "raw_code": "def up\n    # no-op\n  end",
    "comment": "To be re-enqueued by db/post_migrate/20240827204855_reenqueue_deduplicate_lfs_objects_projects.rb",
    "label": "",
    "id": "4114"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :lfs_objects_projects, COLUMNS, name: INDEX_NAME\n  end",
    "comment": "This index was not synchronously created This index was created in: ScheduleUniqueIndexLfsObjectsProjects This was made redundant by: - ScheduleUniqueIndexLfsObjectsProjectsWithoutRepositoryType - ScheduleUniqueIndexLfsObjectsProjectsWithRepositoryType",
    "label": "",
    "id": "4115"
  },
  {
    "raw_code": "def up\n    add_concurrent_index TABLE_NAME, GROUP_LEVEL_COLUMNS, name: GROUP_LEVEL_INDEX, where: 'archived = false'\n    add_concurrent_index TABLE_NAME, PROJECT_LEVEL_COLUMNS, name: PROJECT_LEVEL_INDEX,\n      order: { vulnerability_id: :desc }\n  end",
    "comment": "Index created asynchronously in previous MR: https://gitlab.com/gitlab-org/gitlab/-/merge_requests/166312",
    "label": "",
    "id": "4116"
  },
  {
    "raw_code": "def down\n    create_table TABLE_NAME do |t|\n      t.bigint :project_id\n      t.string :title, null: false\n      t.string :query, null: false\n      t.string :y_label, null: false\n      t.string :unit, null: false\n      t.string :legend\n      t.integer \"group\", null: false\n      t.timestamps_with_timezone null: false\n      t.boolean :common, null: false, default: false\n      t.string :identifier\n      t.text :dashboard_path, limit: 2048\n\n      t.index :common, name: COMMON_INDEX\n      t.index \"group\", name: GROUP_INDEX\n      t.index :identifier, unique: true, name: UNIQUE_NULL_PROJECT_INDEX, where: 'project_id IS NULL'\n      t.index [:identifier, :project_id], unique: true, name: UNIQUE_PROJECT_INDEX\n      t.index :project_id, name: PROJECT_INDEX\n    end",
    "comment": "Original SQL:  CREATE TABLE prometheus_metrics ( id bigint NOT NULL, project_id bigint, title character varying NOT NULL, query character varying NOT NULL, y_label character varying NOT NULL, unit character varying NOT NULL, legend character varying, \"group\" integer NOT NULL, created_at timestamp with time zone NOT NULL, updated_at timestamp with time zone NOT NULL, common boolean DEFAULT false NOT NULL, identifier character varying, dashboard_path text, CONSTRAINT check_0ad9f01463 CHECK ((char_length(dashboard_path) <= 2048)) ); CREATE SEQUENCE prometheus_metrics_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1;  ALTER SEQUENCE prometheus_metrics_id_seq OWNED BY prometheus_metrics.id;  CREATE INDEX index_prometheus_metrics_on_common ON prometheus_metrics USING btree (common);  CREATE INDEX index_prometheus_metrics_on_group ON prometheus_metrics USING btree (\"group\");  CREATE UNIQUE INDEX index_prometheus_metrics_on_identifier_and_null_project ON prometheus_metrics USING btree (identifier) WHERE (project_id IS NULL);  CREATE UNIQUE INDEX index_prometheus_metrics_on_identifier_and_project_id ON prometheus_metrics USING btree (identifier, project_id);  CREATE INDEX index_prometheus_metrics_on_project_id ON prometheus_metrics USING btree (project_id); ",
    "label": "",
    "id": "4117"
  },
  {
    "raw_code": "def up\n    add_concurrent_partitioned_index :ai_usage_events,\n      [:namespace_id, :event, :timestamp, :id],\n      name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/Datetime -- it's a column name",
    "label": "",
    "id": "4118"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :merge_requests, COLUMN_NAMES, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/454262",
    "label": "",
    "id": "4119"
  },
  {
    "raw_code": "def up\n    prepare_async_index :project_daily_statistics, [:date, :id], name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/560318",
    "label": "",
    "id": "4120"
  },
  {
    "raw_code": "def up\n    prepare_async_index :sent_notifications, [:namespace_id, :id], name: INDEX_NAME # rubocop:disable Migration/PreventIndexCreation -- Necessary for sharding key\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/192248",
    "label": "",
    "id": "4121"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :issues, [:health_status], name: INDEX_NAME\n  end",
    "comment": "Follow-up issue to remove index https://gitlab.com/gitlab-org/gitlab/-/issues/372205",
    "label": "",
    "id": "4122"
  },
  {
    "raw_code": "def up; end\n\n  def down; end\nend",
    "comment": "Introduced in 17.2 and no-op in 17.3. No-op because we decided not pursue this migration. See: https://gitlab.com/gitlab-org/gitlab/-/issues/460080#note_2007295428",
    "label": "",
    "id": "4123"
  },
  {
    "raw_code": "def up\n    return unless Gitlab.com_except_jh?\n\n    delete_batched_background_migration(MIGRATION, :namespaces, :id, [])\n\n    queue_batched_background_migration(\n      MIGRATION,\n      :namespaces,\n      :id,\n      job_interval: DELAY_INTERVAL,\n      batch_size: BATCH_SIZE,\n      sub_batch_size: SUB_BATCH_SIZE\n    )\n  end",
    "comment": "`db/post_migrate/20250130093913_queue_limit_namespace_visibility_by_organization_visibility.rb` failed because of `Sidekiq::Shutdown` so we need to re-enqueue the migration only for gitlab.com. See https://gitlab.com/gitlab-org/gitlab/-/issues/366720.",
    "label": "",
    "id": "4124"
  },
  {
    "raw_code": "def down\n    add_concurrent_foreign_key TABLE_NAME, :projects,\n      column: :project_id,\n      name: PROJECT_FK_NAME\n  end",
    "comment": "Original SQL:  ALTER TABLE ONLY prometheus_metrics ADD CONSTRAINT fk_rails_4c8957a707 FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE; ",
    "label": "",
    "id": "4125"
  },
  {
    "raw_code": "def up; end\n\n  def down; end\n\n  private\n\n  def batch_sizes\n    if Gitlab.com_except_jh?\n      {\n        batch_size: GITLAB_OPTIMIZED_BATCH_SIZE,\n        sub_batch_size: GITLAB_OPTIMIZED_SUB_BATCH_SIZE\n      }\n    else\n      {\n        batch_size: BATCH_SIZE,\n        sub_batch_size: SUB_BATCH_SIZE\n      }\n    end\n  end\nend",
    "comment": "No longer needed as we are now going to backfill only non project notes gitlab.com/gitlab-org/gitlab/-/issues/444222",
    "label": "",
    "id": "4126"
  },
  {
    "raw_code": "def up\n    add_concurrent_index(\n      :packages_packages,\n      %i[project_id name version],\n      name: INDEX_NAME,\n      unique: true,\n      where: \"package_type = #{PACKAGE_TYPE_ML_MODEL} AND status <> #{PACKAGE_STATUS_PENDING_DESTRUCTION}\"\n    )\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- Legacy migration",
    "label": "",
    "id": "4127"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :packages_packages, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4128"
  },
  {
    "raw_code": "def up\n    truncate_tables!('ai_usage_events')\n    partitioned_table = find_partitioned_table('ai_usage_events')\n\n    index_options = {\n      name: INDEX_NAME,\n      unique: true,\n      nulls_not_distinct: true\n    }\n\n    partitioned_table.postgres_partitions.order(:name).each do |partition|\n      partition_index_name = generated_index_name(partition.identifier, index_options[:name])\n      partition_options = index_options.merge(name: partition_index_name)\n\n      add_index partition.identifier, COLUMN_NAMES, **partition_options\n    end",
    "comment": "rubocop: disable Migration/AddIndex -- table is empty rubocop: disable Migration/RemoveIndex -- table is empty rubocop: disable Migration/ComplexIndexesRequireName -- index name is part of the index options hash",
    "label": "",
    "id": "4129"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :merge_requests, COLUMN_NAMES, name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/455498",
    "label": "",
    "id": "4130"
  },
  {
    "raw_code": "def up\n    # no-op\n  end",
    "comment": "To be re-enqueued by: db/post_migrate/20250708101955_requeue_backfill_terraform_modules_metadata_with_semver.rb",
    "label": "",
    "id": "4131"
  },
  {
    "raw_code": "def up\n    with_each_index do |table, columns, old_name, new_name|\n      next if index_exists?(table, columns, name: new_name)\n      next unless index_exists?(table, columns, name: old_name)\n\n      with_lock_retries { rename_index(table, old_name, new_name) }\n    end",
    "comment": "rubocop:enable Layout/LineLength",
    "label": "",
    "id": "4132"
  },
  {
    "raw_code": "def down; end\n\n  private\n\n  def with_each_index\n    INDEXES_TO_RENAME.each do |table, indexes|\n      indexes.each do |index_info|\n        columns, old_name, new_name = index_info\n        yield table, columns, old_name, new_name\n      end\n    end",
    "comment": "We have no way of tracking which misnamed indexes existed before the migration, so let's not try to restore the previous, incorrect state.",
    "label": "",
    "id": "4133"
  },
  {
    "raw_code": "def up\n    prepare_partitioned_async_index(TABLE_NAME, COLUMN_NAMES_1, name: INDEX_NAME_1)\n    prepare_partitioned_async_index(TABLE_NAME, COLUMN_NAMES_2, name: INDEX_NAME_2)\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/514158",
    "label": "",
    "id": "4134"
  },
  {
    "raw_code": "def up\n    OrganizationUser.where(access_level: 50).each do |organization_user|\n      organization_user.update!(access_level: 10) unless organization_user.user.admin? # rubocop:disable Cop/UserAdmin -- Application logic is not available here\n    end",
    "comment": "rubocop:disable Rails/FindEach -- Find Each is adding an order by id which makes this slow. Number of records is small.",
    "label": "",
    "id": "4135"
  },
  {
    "raw_code": "def down\n    # no-op\n  end",
    "comment": "rubocop:enable Rails/FindEach",
    "label": "",
    "id": "4136"
  },
  {
    "raw_code": "def up\n    validate_foreign_key(:users, :organization_id, name: FK_NAME)\n  end",
    "comment": "foreign key added in 20250709143510_add_foreign_key_to_users_on_organization_id.rb",
    "label": "",
    "id": "4137"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :sbom_occurrences, COLUMNS, where: 'archived = false', name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- Legacy migration",
    "label": "",
    "id": "4138"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :sbom_occurrences, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4139"
  },
  {
    "raw_code": "def up; end\n\n  def down; end\nend",
    "comment": "re-enqueued via https://gitlab.com/gitlab-org/gitlab/-/merge_requests/197199",
    "label": "",
    "id": "4140"
  },
  {
    "raw_code": "def up\n    validate_foreign_key(:approval_merge_request_rules, :approval_policy_rule_id, name: NEW_CONSTRAINT_NAME)\n  end",
    "comment": "foreign key added in db/migrate/20240918130318_replace_fk_on_approval_merge_request_rules_approval_policy_rule_id.rb",
    "label": "",
    "id": "4141"
  },
  {
    "raw_code": "def up\n    queue_batched_background_migration(\n      MIGRATION,\n      :workspaces,\n      :id,\n      batch_size: BATCH_SIZE,\n      sub_batch_size: SUB_BATCH_SIZE,\n      job_interval: DELAY_INTERVAL\n    )\n    nil\n  end",
    "comment": "@return [Void]",
    "label": "",
    "id": "4142"
  },
  {
    "raw_code": "def down\n    delete_batched_background_migration(MIGRATION, :workspaces, :id, [])\n    nil\n  end",
    "comment": "@return [Void]",
    "label": "",
    "id": "4143"
  },
  {
    "raw_code": "def up\n    prepare_async_index :merge_request_diff_commits, :merge_request_commits_metadata_id, name: INDEX_NAME,\n      where: \"merge_request_commits_metadata_id IS NOT NULL\"\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/527227 rubocop:disable Migration/PreventIndexCreation -- this index is required as we will be querying data from `merge_request_commits_metadata_id` and joining by this column.",
    "label": "",
    "id": "4144"
  },
  {
    "raw_code": "def down\n    unprepare_async_index :merge_request_diff_commits, :merge_request_commits_metadata_id, name: INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4145"
  },
  {
    "raw_code": "def up\n    drop_table :work_item_descriptions if table_exists? :work_item_descriptions\n  end",
    "comment": "We found that partitioning by root_namespace_id causes problems when syncing the data when a work item gets transferred between namespaces.",
    "label": "",
    "id": "4146"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :projects, [:id, :namespace_id], name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- This is part of an experiment to see if it improves certain queries See https://gitlab.com/gitlab-org/gitlab/-/issues/466236",
    "label": "",
    "id": "4147"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :projects, INDEX_NAME\n  end",
    "comment": "rubocop:enable Migration/PreventIndexCreation",
    "label": "",
    "id": "4148"
  },
  {
    "raw_code": "def up\n    prepare_async_index_removal :vulnerabilities, [:detected_at, :id], name: INDEX_NAME\n  end",
    "comment": "TODO: Index to be destroyed synchronously in follow-up issue in https://gitlab.com/gitlab-org/gitlab/-/issues/458022",
    "label": "",
    "id": "4149"
  },
  {
    "raw_code": "def up\n    add_concurrent_index(\n      TABLE,\n      'traversal_ids, package_manager COLLATE \"C\"',\n      using: 'btree',\n      name: INDEX_NAME\n    )\n  end",
    "comment": "rubocop:disable Migration/PreventIndexCreation -- needed to feature development",
    "label": "",
    "id": "4150"
  },
  {
    "raw_code": "def columns_to_backfill_exist?\n    columns(TABLE).any? { |column| column.name.ends_with?('_id_convert_to_bigint') }\n  end",
    "comment": "If we are on a newer installation where the columns are already `bigint`, the previous migration will not have added any new columns.",
    "label": "",
    "id": "4151"
  },
  {
    "raw_code": "def up\n    prepare_async_index :issues, %i[milestone_id id], name: INDEX_NAME # rubocop:disable Migration/PreventIndexCreation -- Legacy migration\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/461627",
    "label": "",
    "id": "4152"
  },
  {
    "raw_code": "def up\n    return unless index_name\n\n    prepare_async_index_removal TABLE_NAME, COLUMNS, name: index_name\n  end",
    "comment": "TODO: Index to be destroyed synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/532779",
    "label": "",
    "id": "4153"
  },
  {
    "raw_code": "def index_name\n    indexes_by_definition = indexes_by_definition_for_table(TABLE_NAME)\n    indexes_by_definition[INDEX_DEFINITION]\n  end",
    "comment": "This index has a different name on Production DB and possibly on other instances. So we must find the index by definition instead. See https://gitlab.com/gitlab-org/gitlab/-/issues/520213#note_2450943371.",
    "label": "",
    "id": "4154"
  },
  {
    "raw_code": "def up\n    remove_concurrent_partitioned_index_by_name(:ai_usage_events, INDEX_NAME)\n  end",
    "comment": "rubocop:disable Migration/Datetime -- it's a column name",
    "label": "",
    "id": "4155"
  },
  {
    "raw_code": "def up\n    queue_batched_background_migration(\n      MIGRATION,\n      TABLE_NAME,\n      COLUMN_NAME,\n      job_interval: DELAY_INTERVAL,\n      batch_size: BATCH_SIZE,\n      sub_batch_size: SUB_BATCH_SIZE\n    )\n  end",
    "comment": "Only numeric or time-based columns can be used to divide tables for batching so, the :namespace_id is used in place of the :require_dpop_for_manage_api_endpoints column",
    "label": "",
    "id": "4156"
  },
  {
    "raw_code": "def down\n    add_concurrent_foreign_key TABLE_NAME, :prometheus_alerts,\n      column: :prometheus_alert_id,\n      name: ALERTS_FK_NAME\n\n    add_concurrent_foreign_key TABLE_NAME, :projects,\n      column: :project_id,\n      name: PROJECT_FK_NAME\n  end",
    "comment": "Original SQL:  ALTER TABLE ONLY prometheus_alert_events ADD CONSTRAINT fk_rails_106f901176 FOREIGN KEY (prometheus_alert_id) REFERENCES prometheus_alerts(id) ON DELETE CASCADE;  ALTER TABLE ONLY prometheus_alert_events ADD CONSTRAINT fk_rails_4675865839 FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE; ",
    "label": "",
    "id": "4157"
  },
  {
    "raw_code": "def up\n    # index with the same name is re-created, but without \"unique: true\"\n    remove_index :p_knowledge_graph_replicas, name: INDEX_NAME\n    add_index :p_knowledge_graph_replicas, :namespace_id, name: INDEX_NAME\n  end",
    "comment": "rubocop:disable Migration/AddIndex, Migration/RemoveIndex -- these are on empty tables",
    "label": "",
    "id": "4158"
  },
  {
    "raw_code": "def up\n    with_lock_retries do\n      remove_foreign_key_if_exists(:zoekt_repositories, column: :zoekt_index_id, on_delete: :cascade,\n        name: OLD_CONSTRAINT_NAME)\n    end",
    "comment": "new foreign key added in ChangeZoektIndexFkOnZoektRepositories migration and validated in ValidateZoektIndexFkChangeOnZoektRepositories migration",
    "label": "",
    "id": "4159"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :application_settings, :workspaces_oauth_application_id, name: INDEX_NAME\n    add_concurrent_foreign_key :application_settings, :oauth_applications,\n      column: :workspaces_oauth_application_id,\n      on_delete: :nullify\n  end",
    "comment": "@return [void]",
    "label": "",
    "id": "4160"
  },
  {
    "raw_code": "def down\n    with_lock_retries do\n      remove_foreign_key :application_settings, column: :workspaces_oauth_application_id\n    end",
    "comment": "@return [void]",
    "label": "",
    "id": "4161"
  },
  {
    "raw_code": "def up\n    unless table_exists?(:oauth_access_grant_archived_records)\n      create_table :oauth_access_grant_archived_records, id: false do |t|\n        t.bigint :id, null: false\n        t.bigint :resource_owner_id, null: false\n        t.bigint :application_id, null: false\n        t.string :token, null: false\n        t.integer :expires_in, null: false\n        t.text :redirect_uri, null: false\n        t.datetime :created_at, null: false\n        t.datetime :revoked_at\n        t.string :scopes\n        t.text :code_challenge\n        t.text :code_challenge_method\n        t.references :organization, null: false, index: true, foreign_key: { on_delete: :cascade }\n        t.datetime_with_timezone :archived_at, null: false, default: -> { 'CURRENT_TIMESTAMP' }\n      end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable -- No factory needed for temporary table rubocop:disable Migration/AddLimitToTextColumns -- Keeps compatibility with existing table rubocop:disable Migration/PreventStrings -- Keeps compatibility with existing table rubocop:disable Migration/Datetime -- Keeps compatibility with existing table",
    "label": "",
    "id": "4162"
  },
  {
    "raw_code": "def down\n    drop_table :oauth_access_grant_archived_records, if_exists: true\n  end",
    "comment": "rubocop:enable Migration/EnsureFactoryForTable rubocop:enable Migration/AddLimitToTextColumns rubocop:enable Migration/PreventStrings rubocop:enable Migration/Datetime",
    "label": "",
    "id": "4163"
  },
  {
    "raw_code": "def up\n    # no-op: we need to do this in a separate deploy since the original MR reached canary\n  end",
    "comment": "This removes a column that was added in a previous migration that is now a no-op See https://gitlab.com/gitlab-com/gl-infra/production/-/issues/19182",
    "label": "",
    "id": "4164"
  },
  {
    "raw_code": "def change\n    add_column :duo_workflows_events, :correlation_id_value, :text\n  end",
    "comment": "rubocop:disable Migration/AddLimitToTextColumns -- Limit is added in 20250131175917_add_correlation_id_value_limit_to_duo_workflows_events",
    "label": "",
    "id": "4165"
  },
  {
    "raw_code": "def up\n    with_lock_retries do\n      remove_foreign_key_if_exists(\n        :work_item_colors,\n        column: :namespace_id,\n        on_delete: :nullify,\n        name: OLD_FK_NAME\n      )\n    end",
    "comment": "new foreign key added in FixWorkItemColorsCascadeOptionOnFkToNamespaceId and validated in ValidateNewNamespaceIdFkOnWorkItemColors",
    "label": "",
    "id": "4166"
  },
  {
    "raw_code": "def up\n    create_table :vulnerability_namespace_historical_statistics do |t|\n      t.bigint :namespace_id, null: false, index: { name: NAMESPACE_ID_FK_INDEX_NAME }\n      t.timestamps_with_timezone null: false\n      t.bigint :traversal_ids, array: true, default: [], null: false\n      t.integer :total, default: 0, null: false\n      t.integer :critical, default: 0, null: false\n      t.integer :high, default: 0, null: false\n      t.integer :medium, default: 0, null: false\n      t.integer :low, default: 0, null: false\n      t.integer :unknown, default: 0, null: false\n      t.integer :info, default: 0, null: false\n      t.date :date, null: false\n      t.integer :letter_grade, limit: 1, null: false\n      t.index [:traversal_ids, :date], unique: true, name: TRAVERSAL_IDS_DATE_INDEX_NAME\n    end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable -- False Positive",
    "label": "",
    "id": "4167"
  },
  {
    "raw_code": "def down\n    drop_table :vulnerability_namespace_historical_statistics\n  end",
    "comment": "rubocop:enable Migration/EnsureFactoryForTable",
    "label": "",
    "id": "4168"
  },
  {
    "raw_code": "def change\n    add_column :merge_request_diff_commits, :merge_request_commits_metadata_id, :bigint, null: true\n  end",
    "comment": "rubocop:disable Migration/PreventAddingColumns -- this column is required as we will be querying data from `merge_request_commits_metadata` table using this column",
    "label": "",
    "id": "4169"
  },
  {
    "raw_code": "def up\n    create_table :oauth_access_token_archived_records, id: false do |t|\n      t.bigint :id, null: false\n      t.bigint :resource_owner_id\n      t.bigint :application_id\n      t.string :token, null: false\n      t.string :refresh_token\n      t.integer :expires_in, default: 7200, null: false\n      t.datetime :revoked_at\n      t.datetime :created_at, null: false\n      t.string :scopes\n      t.references :organization, null: false, index: true, foreign_key: { on_delete: :cascade }\n      t.datetime_with_timezone :archived_at, null: false, default: -> { 'CURRENT_TIMESTAMP' }\n    end",
    "comment": "rubocop:disable Migration/PreventStrings -- Keeps compatibility with existing table rubocop:disable Migration/Datetime -- Keeps compatibility with existing table",
    "label": "",
    "id": "4170"
  },
  {
    "raw_code": "def down\n    drop_table :oauth_access_token_archived_records\n  end",
    "comment": "rubocop:enable Migration/PreventStrings rubocop:enable Migration/Datetime",
    "label": "",
    "id": "4171"
  },
  {
    "raw_code": "def change\n    add_column :duo_workflows_workflows, :goal, :text, null: true\n  end",
    "comment": "rubocop:disable Migration/AddLimitToTextColumns -- limit is added in 20240918153150_add_text_limit_to_duo_workflows_goal",
    "label": "",
    "id": "4172"
  },
  {
    "raw_code": "def up\n    # these tables are currently unused, there should be no data in them\n    # truncating for unexpected cases (local dev, failed migrations, etc)\n    truncate_tables! 'system_access_group_microsoft_graph_access_tokens', 'system_access_group_microsoft_applications'\n\n    remove_index :system_access_group_microsoft_applications,\n      name: 'index_system_access_group_microsoft_applications_on_group_id'\n\n    add_index :system_access_group_microsoft_applications, :group_id, unique: true\n  end",
    "comment": "rubocop:disable Migration/AddIndex, Migration/RemoveIndex -- these are on empty tables",
    "label": "",
    "id": "4173"
  },
  {
    "raw_code": "def up\n    add_column :namespace_settings, :step_up_auth_required_oauth_provider, :text\n  end",
    "comment": "rubocop:disable Migration/AddLimitToTextColumns -- Limit is added in 20250820070257_add_text_limit_to_step_up_auth_required_oauth_provider",
    "label": "",
    "id": "4174"
  },
  {
    "raw_code": "def down\n    remove_column :namespace_settings, :step_up_auth_required_oauth_provider\n  end",
    "comment": "rubocop:enable Migration/AddLimitToTextColumns",
    "label": "",
    "id": "4175"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :topics, [:organization_id, :name], name: NEW_INDEX, unique: true\n\n    remove_concurrent_index_by_name :topics, name: OLD_INDEX\n  end",
    "comment": "Replaces the current schema index: CREATE UNIQUE INDEX index_topics_on_name ON topics USING btree (name);",
    "label": "",
    "id": "4176"
  },
  {
    "raw_code": "def change\n    add_column :analytics_cycle_analytics_value_stream_settings, :namespace_id, :bigint, null: false\n  end",
    "comment": "rubocop:disable Rails/NotNullColumn -- NamespaceId can not have a default value",
    "label": "",
    "id": "4177"
  },
  {
    "raw_code": "def change\n    create_table :work_item_type_user_preferences do |t|\n      t.timestamps_with_timezone null: false\n\n      t.bigint :user_id, null: false\n      t.bigint :namespace_id, null: false\n      t.bigint :work_item_type_id\n      t.text :sort, limit: 255\n\n      t.index %i[user_id namespace_id work_item_type_id], name: UNIQUE_INDEX_NAME\n    end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable -- https://gitlab.com/gitlab-org/gitlab/-/issues/468630 the factory exist in spec/factories/work_items/user_preference.rb",
    "label": "",
    "id": "4178"
  },
  {
    "raw_code": "def change\n    add_column :users, :last_access_from_pipl_country_at, :datetime_with_timezone, if_not_exists: true\n  end",
    "comment": "This column prevents additional queries (e.g. 'SELECT ... FROM country_access_logs ...') when checking if the user's access from a specific country should be tracked (insert/update to country_access_logs table). rubocop:disable Migration/PreventAddingColumns -- see previous lines",
    "label": "",
    "id": "4179"
  },
  {
    "raw_code": "def up\n    validate_foreign_key(:work_item_colors, :namespace_id, name: NEW_FK_NAME)\n  end",
    "comment": "foreign key added in FixWorkItemColorsCascadeOptionOnFkToNamespaceId",
    "label": "",
    "id": "4180"
  },
  {
    "raw_code": "def change\n    create_table :vulnerability_export_parts do |t|\n      t.references :vulnerability_export, foreign_key: { on_delete: :cascade }, null: false, index: true\n      t.bigint :start_id, null: false\n      t.bigint :end_id, null: false\n      t.references :organization, foreign_key: { on_delete: :cascade }, null: false, default: 1\n      t.integer :file_store\n      t.text :file, limit: 255\n\n      t.timestamps_with_timezone null: false\n    end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable -- False Positive",
    "label": "",
    "id": "4181"
  },
  {
    "raw_code": "def up\n    add_column :p_ci_builds, :scoped_user_id, :bigint, if_not_exists: true\n    add_column :p_ci_builds, :timeout, :integer, if_not_exists: true\n    add_column :p_ci_builds, :timeout_source, :integer, limit: 2, if_not_exists: true\n    add_column :p_ci_builds, :exit_code, :integer, limit: 2, if_not_exists: true\n    add_column :p_ci_builds, :debug_trace_enabled, :boolean, if_not_exists: true\n  end",
    "comment": "rubocop:disable Migration/PreventAddingColumns -- adding them to new table will add overhead",
    "label": "",
    "id": "4182"
  },
  {
    "raw_code": "def up\n    with_lock_retries do\n      remove_foreign_key_if_exists(:zoekt_indices, column: :zoekt_replica_id, on_delete: :cascade,\n        name: OLD_CONSTRAINT_NAME)\n    end",
    "comment": "new foreign key added in ChangeZoektReplicaFkOnZoektIndices migration and validated in ValidateZoektReplicaFKOnZoektIndices migration",
    "label": "",
    "id": "4183"
  },
  {
    "raw_code": "def up\n    with_lock_retries do\n      remove_foreign_key_if_exists(:epics, column: :parent_id, on_delete: :cascade, name: FK_NAME)\n    end",
    "comment": "new foreign key added in db/migrate/20240403113607_replace_epics_fk_on_parent_id.rb and validated in db/migrate/20240403114400_validate_epics_fk_on_parent_id_with_on_delete_nullify.rb",
    "label": "",
    "id": "4184"
  },
  {
    "raw_code": "def change\n    add_column :namespace_settings, :resource_access_token_notify_inherited, :boolean\n    add_column :namespace_settings, :lock_resource_access_token_notify_inherited, :boolean, default: false, null: false\n  end",
    "comment": "not using the CascadingNamespaceSettings helper here, since the application_settings changes are in a jsonb column",
    "label": "",
    "id": "4185"
  },
  {
    "raw_code": "def create_partitioned_table(name)\n    options = 'PARTITION BY LIST (model_type)'\n\n    # Table name should by provided by `partition_table_by_list`, but when using variable some\n    # Rubocop rules fail to handle this, so we use the name that would be generated instead.\n    create_table :uploads_9ba88c4165, primary_key: PARTITIONED_TABLE_PK, options: options do |t|\n      t.bigint :id, null: false\n      t.bigint :size, null: false\n      t.bigint :model_id, null: false\n      t.references :uploaded_by_user, index: false, foreign_key: { to_table: :users, on_delete: :nullify }\n      t.bigint :organization_id\n      t.bigint :namespace_id\n      t.bigint :project_id\n      t.timestamp :created_at\n      t.integer :store, null: false, default: 1\n      t.integer :version, default: 1\n      t.text :path, null: false, limit: 511\n      t.text :checksum, limit: 64\n      t.text :model_type\n      t.text :uploader, null: false\n      t.text :mount_point\n      t.text :secret\n\n      t.index :checksum, name: \"index_#{name}_on_checksum\"\n      t.index [:model_id, :model_type, :uploader, :created_at], name: \"index_#{name}_on_model_uploader_created_at\"\n      t.index :store, name: \"index_#{name}_on_store\"\n      t.index :uploaded_by_user_id, name: \"index_#{name}_on_uploaded_by_user_id\"\n      t.index [:uploader, :path], name: \"index_#{name}_on_uploader_and_path\"\n      t.index :organization_id, name: \"index_#{name}_on_organization_id\"\n      t.index :namespace_id, name: \"index_#{name}_on_namespace_id\"\n      t.index :project_id, name: \"index_#{name}_on_project_id\"\n    end",
    "comment": "rubocop:disable Migration/Datetime -- Creating a copy of existing table rubocop:disable Migration/AddLimitToTextColumns -- Creating a copy of existing table rubocop:disable Migration/EnsureFactoryForTable -- Creating a copy of existing table",
    "label": "",
    "id": "4186"
  },
  {
    "raw_code": "def partition_mappings\n    {\n      abuse_report: \"AbuseReport\",\n      achievement: \"Achievements::Achievement\",\n      ai_vectorizable_file: \"Ai::VectorizableFile\",\n      alert_management_alert_metric_image: \"AlertManagement::MetricImage\",\n      appearance: \"Appearance\",\n      bulk_import_export_upload: \"BulkImports::ExportUpload\",\n      dependency_list_export: \"Dependencies::DependencyListExport\",\n      dependency_list_export_part: \"Dependencies::DependencyListExport::Part\",\n      design_management_action: \"DesignManagement::Action\",\n      note: \"Note\",\n      namespace: \"Namespace\",\n      import_export_upload: \"ImportExportUpload\",\n      issuable_metric_image: \"IssuableMetricImage\",\n      organization_detail: \"Organizations::OrganizationDetail\",\n      snippet: \"Snippet\",\n      project: \"Project\",\n      project_import_export_relation_export_upload: \"Projects::ImportExport::RelationExportUpload\",\n      project_topic: \"Projects::Topic\",\n      user: \"User\",\n      user_permission_export_upload: \"UserPermissionExportUpload\",\n      vulnerability_archive_export: \"Vulnerabilities::ArchiveExport\",\n      vulnerability_export: \"Vulnerabilities::Export\",\n      vulnerability_export_part: \"Vulnerabilities::Export::Part\",\n      vulnerability_remediation: \"Vulnerabilities::Remediation\"\n    }.transform_values { |value| \"'#{value}'\" }\n  end",
    "comment": "rubocop:enable Migration/EnsureFactoryForTable rubocop:enable Migration/AddLimitToTextColumns rubocop:enable Migration/Datetime",
    "label": "",
    "id": "4187"
  },
  {
    "raw_code": "def up\n    unless table_exists?(TABLE_NAME)\n      create_table TABLE_NAME,\n        options: 'PARTITION BY RANGE(project_id)',\n        primary_key: [:id, :project_id] do |t|\n        # rubocop:disable Migration/Datetime -- We are keeping it the same with\n        # `merge_request_diff_commits` wherein they are timestamps without timezone\n        t.datetime :authored_date\n        t.datetime :committed_date\n        # rubocop:enable Migration/Datetime\n\n        t.bigserial :id, null: false\n        t.bigserial :project_id, null: false\n        t.bigserial :commit_author_id\n        t.bigserial :committer_id\n\n        t.binary :sha, null: false\n\n        # rubocop:disable Migration/AddLimitToTextColumns -- We are keeping it the\n        # same with `merge_request_diff_commits` wherein message has no limit\n        t.text :message\n        # rubocop:enable Migration/AddLimitToTextColumns\n\n        t.jsonb :trailers, null: false, default: {}\n        t.index [:project_id, :sha], unique: true\n      end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable, Lint/RedundantCopDisableDirective -- False positive",
    "label": "",
    "id": "4188"
  },
  {
    "raw_code": "def down\n    drop_table :merge_request_commits_metadata\n  end",
    "comment": "rubocop:enable Migration/EnsureFactoryForTable, Lint/RedundantCopDisableDirective",
    "label": "",
    "id": "4189"
  },
  {
    "raw_code": "def change\n    add_column :application_settings, :workspaces_oauth_application_id, :bigint, if_not_exists: true\n  end",
    "comment": "@return [void]",
    "label": "",
    "id": "4190"
  },
  {
    "raw_code": "def up\n    connection.execute(\n      <<~SQL\n        CREATE FUNCTION #{function_name}() RETURNS trigger\n            LANGUAGE plpgsql\n            AS $$\n        BEGIN\n          INSERT INTO loose_foreign_keys_deleted_records\n          (fully_qualified_table_name, primary_key_value)\n          SELECT current_schema() || '.' || TG_ARGV[0], old_table.id FROM old_table;\n\n          RETURN NULL;\n        END\n        $$;\n      SQL\n    )\n  end",
    "comment": "TG_ARGV[0] is the table name. It should always be on the default schema (public)",
    "label": "",
    "id": "4191"
  },
  {
    "raw_code": "def up\n    remove_foreign_key_if_exists(\n      :epics,\n      :issues,\n      column: :issue_id,\n      on_delete: :nullify,\n      name: OLD_FK_NAME,\n      reverse_lock_order: true\n    )\n  end",
    "comment": "new foreign key added in FixEpicsCascadeOptionOnFkToIssueId and validated in FixEpicsCascadeOptionOnFkToIssueId",
    "label": "",
    "id": "4192"
  },
  {
    "raw_code": "def up\n    remove_foreign_key_if_exists(\n      :work_item_dates_sources,\n      column: :namespace_id,\n      on_delete: :nullify,\n      name: OLD_FK_NAME\n    )\n  end",
    "comment": "new foreign key added in FixWorkItemSourceDatesCascadeOptionOnFkToNamespaceId and validated in ValidateNewNamespaceIdFkOnWorkItemDatesSources",
    "label": "",
    "id": "4193"
  },
  {
    "raw_code": "def change\n    add_column :gpg_signatures, :author_email, :text\n  end",
    "comment": "rubocop:disable Migration/AddLimitToTextColumns -- limit is added in a separate migration 20250502053033",
    "label": "",
    "id": "4194"
  },
  {
    "raw_code": "def down\n    create_table TABLE_NAME do |t|\n      t.timestamps_with_timezone null: false\n      t.bigint \"cluster_agent_id\", null: false\n      t.bigint \"workspaces_quota\", default: -1, null: false\n      t.bigint \"workspaces_per_user_quota\", default: -1, null: false\n      t.bigint \"project_id\"\n      t.column \"default_max_hours_before_termination\", :smallint, default: 24, null: false\n      t.column \"max_hours_before_termination_limit\", :smallint, default: 120, null: false\n      t.boolean \"enabled\", null: false\n      t.boolean \"network_policy_enabled\", default: true, null: false\n      t.text \"dns_zone\", limit: 256, null: false\n      # Kubernetes namespaces are limited to 63 characters\n      t.text \"gitlab_workspaces_proxy_namespace\", limit: 63, default: \"gitlab-workspaces\", null: false\n      t.jsonb \"network_policy_egress\",\n        default: [{ \"allow\" => \"0.0.0.0/0\", \"except\" => %w[10.0.0.0/8 172.16.0.0/12 192.168.0.0/16] }], null: false\n      t.jsonb \"default_resources_per_workspace_container\", default: {}, null: false\n      t.jsonb \"max_resources_per_workspace\", default: {}, null: false\n\n      t.index :cluster_agent_id, unique: true, name: \"index_remote_development_agent_configs_on_unique_agent_id\"\n      t.index :project_id\n    end",
    "comment": "noinspection RubyResolve -- RubyMine doesn't resolve t.bigint. TODO: Open ticket and link on https://handbook.gitlab.com/handbook/tools-and-tips/editors-and-ides/jetbrains-ides/tracked-jetbrains-issues/",
    "label": "",
    "id": "4195"
  },
  {
    "raw_code": "def up\n    with_lock_retries do\n      add_column :users, :organization_id, :bigint, default: DEFAULT_ORGANIZATION_ID, null: false\n    end",
    "comment": "rubocop:disable Migration/PreventAddingColumns, Migration/PreventIndexCreation -- required for sharding",
    "label": "",
    "id": "4196"
  },
  {
    "raw_code": "def down\n    remove_concurrent_index_by_name :users, INDEX_NAME\n    with_lock_retries do\n      remove_column :users, :organization_id\n    end",
    "comment": "rubocop:enable Migration/PreventAddingColumns, Migration/PreventIndexCreation",
    "label": "",
    "id": "4197"
  },
  {
    "raw_code": "def change\n    create_table(:p_ci_pipelines_config, primary_key: [:pipeline_id, :partition_id],\n      options: 'PARTITION BY LIST (partition_id)') do |t|\n      t.bigint :pipeline_id, null: false\n      t.bigint :partition_id, null: false\n      t.text :content, null: false\n    end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable -- No factory needed rubocop:disable Migration/AddLimitToTextColumns -- keeps compatibility with existing table",
    "label": "",
    "id": "4198"
  },
  {
    "raw_code": "def up\n    validate_foreign_key(:work_item_dates_sources, :namespace_id, name: NEW_FK_NAME)\n  end",
    "comment": "foreign key added in FixWorkItemSourceDatesCascadeOptionOnFkToNamespaceId",
    "label": "",
    "id": "4199"
  },
  {
    "raw_code": "def change\n    create_table :vulnerability_representation_information, id: false do |t|\n      t.timestamps_with_timezone null: false\n      t.bigint :vulnerability_id, null: false, primary_key: true, default: nil\n      t.bigint :project_id, null: false\n      t.binary :resolved_in_commit_sha\n    end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable -- False Positive",
    "label": "",
    "id": "4200"
  },
  {
    "raw_code": "def up\n    create_table :authentication_event_archived_records, id: false do |t|\n      t.bigint :id, null: false\n      t.datetime_with_timezone :created_at, null: false\n      t.bigint :user_id\n      t.integer :result, limit: 2, null: false\n      t.inet :ip_address\n      t.text :provider, null: false\n      t.text :user_name, null: false\n      t.datetime_with_timezone :archived_at, default: -> { 'CURRENT_TIMESTAMP' }\n    end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable -- there is no corresponding AR model for this temp table",
    "label": "",
    "id": "4201"
  },
  {
    "raw_code": "def down\n    drop_table :authentication_event_archived_records\n  end",
    "comment": "rubocop:enable Migration/EnsureFactoryForTable",
    "label": "",
    "id": "4202"
  },
  {
    "raw_code": "def up\n    add_column :ssh_signatures, :author_email, :text\n  end",
    "comment": "rubocop:disable Migration/AddLimitToTextColumns -- limit is added in a separate migration 20250425111203",
    "label": "",
    "id": "4203"
  },
  {
    "raw_code": "def down\n    remove_column :ssh_signatures, :author_email if column_exists?(:ssh_signatures, :author_email)\n  end",
    "comment": "rubocop:enable Migration/AddLimitToTextColumns",
    "label": "",
    "id": "4204"
  },
  {
    "raw_code": "def change\n    add_column :project_compliance_violations, :audit_event_table_name, :smallint, null: false\n  end",
    "comment": "rubocop:disable Rails/NotNullColumn -- table is empty",
    "label": "",
    "id": "4205"
  },
  {
    "raw_code": "def archived_table_present?\n    table_exists?(ARCHIVED_TABLE_NAME)\n  end",
    "comment": "ci_runner_machines_archived was dropped in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/189308 (18.0) as a post-deployment migration. This migration might end up being executed earlier than that migration in self-managed environments. Instead of waiting for a required stop in 18.2, we'll look at whether the archive table is still there. If it is, we'll add the column there as well, and the table will end up being dropped later.",
    "label": "",
    "id": "4206"
  },
  {
    "raw_code": "def up\n    validate_foreign_key(:zoekt_indices, :zoekt_replica_id, name: NEW_CONSTRAINT_NAME)\n  end",
    "comment": "foreign key added in ChangeZoektReplicaFKOnZoektIndices migration",
    "label": "",
    "id": "4207"
  },
  {
    "raw_code": "def archived_table_present?\n    table_exists?(ARCHIVED_TABLE_NAME)\n  end",
    "comment": "ci_runners_archived was dropped in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/184670 (18.0) as a post-deployment migration. This migration might end up being executed earlier than that migration in self-managed environments. Instead of waiting for a required stop in 18.2, we'll look at whether the archive table is still there. If it is, we'll add the column there as well, and the table will end up being dropped later.",
    "label": "",
    "id": "4208"
  },
  {
    "raw_code": "def up\n    create_table :vulnerability_severity_overrides do |t|\n      t.references :vulnerability, index: true, null: false, foreign_key: { on_delete: :cascade }\n      t.references :author, index: true\n      t.timestamps_with_timezone null: false\n      t.bigint :project_id, index: true, null: false\n      t.column :original_severity, :smallint, null: false\n      t.column :new_severity, :smallint, null: false\n    end",
    "comment": "rubocop:disable Migration/EnsureFactoryForTable -- False positive",
    "label": "",
    "id": "4209"
  },
  {
    "raw_code": "def down\n    drop_table :vulnerability_severity_overrides\n  end",
    "comment": "rubocop:enable Migration/EnsureFactoryForTable",
    "label": "",
    "id": "4210"
  },
  {
    "raw_code": "def restore_index_names\n    RENAMED_PARTITION_INDEX_MAP.each do |table_name, renamed_indexes|\n      renamed_indexes.each do |renamed_index|\n        next unless index_name_exists?(table_name, renamed_index[:old_name])\n\n        rename_index(table_name, renamed_index[:old_name], renamed_index[:new_name])\n      end",
    "comment": "When the type is changed, all indexes on the column are recreated for partitions and PostgreSQL is generating different names than what we already have. So we have to rename these indexes to restore original names.",
    "label": "",
    "id": "4211"
  },
  {
    "raw_code": "def change\n    create_table(:workspaces_agent_configs) do |t|\n      t.timestamps_with_timezone null: false\n      t.bigint \"cluster_agent_id\", null: false\n      t.bigint \"workspaces_quota\", default: -1, null: false\n      t.bigint \"workspaces_per_user_quota\", default: -1, null: false\n      t.bigint \"project_id\", null: false\n      t.column \"default_max_hours_before_termination\", :smallint, default: 24, null: false\n      t.column \"max_hours_before_termination_limit\", :smallint, default: 120, null: false\n      t.boolean \"enabled\", null: false\n      t.boolean \"network_policy_enabled\", default: true, null: false\n      t.text \"dns_zone\", limit: 256, null: false\n      # Kubernetes namespaces are limited to 63 characters\n      t.text \"gitlab_workspaces_proxy_namespace\", limit: 63, default: \"gitlab-workspaces\", null: false\n      t.jsonb \"network_policy_egress\",\n        default: [{ \"allow\" => \"0.0.0.0/0\", \"except\" => %w[10.0.0.0/8 172.16.0.0/12 192.168.0.0/16] }], null: false\n      t.jsonb \"default_resources_per_workspace_container\", default: {}, null: false\n      t.jsonb \"max_resources_per_workspace\", default: {}, null: false\n\n      t.index :cluster_agent_id, unique: true, name: \"index_workspaces_agent_configs_on_unique_cluster_agent_id\"\n      t.index :project_id\n    end",
    "comment": "noinspection RubyResolve -- RubyMine doesn't resolve t.bigint. TODO: Open ticket and link on https://handbook.gitlab.com/handbook/tools-and-tips/editors-and-ides/jetbrains-ides/tracked-jetbrains-issues/",
    "label": "",
    "id": "4212"
  },
  {
    "raw_code": "def up\n    unprepare_partitioned_async_index(TABLE_NAME, COLUMN_NAMES, name: INDEX_NAME)\n  end",
    "comment": "Reverting the index added in https://gitlab.com/gitlab-org/gitlab/-/merge_requests/151048",
    "label": "",
    "id": "4213"
  },
  {
    "raw_code": "def up\n    validate_foreign_key(:epics, :parent_id, name: FK_NAME)\n  end",
    "comment": "foreign key added in db/migrate/20240403113607_replace_epics_fk_on_parent_id.rb",
    "label": "",
    "id": "4214"
  },
  {
    "raw_code": "def change\n    add_column(:ci_runner_taggings, :tag_name, :text, null: true)\n  end",
    "comment": "rubocop:disable Migration/AddLimitToTextColumns -- limit is added in 20250917172856_add_text_limit_to_ci_runner_taggings_tag_name",
    "label": "",
    "id": "4215"
  },
  {
    "raw_code": "def up\n    validate_foreign_key(:zoekt_repositories, :zoekt_index_id, name: NEW_CONSTRAINT_NAME)\n  end",
    "comment": "foreign key added in ChangeZoektIndexFkOnZoektRepositories migration",
    "label": "",
    "id": "4216"
  },
  {
    "raw_code": "def up\n    validate_foreign_key(:epics, :issue_id, name: NEW_FK_NAME)\n  end",
    "comment": "foreign key added in FixEpicsCascadeOptionOnFkToIssueId",
    "label": "",
    "id": "4217"
  },
  {
    "raw_code": "def up\n    prepare_partitioned_async_index(TABLE_NAME, COLUMN_NAMES, name: INDEX_NAME)\n  end",
    "comment": "TODO: Index to be created synchronously in https://gitlab.com/gitlab-org/gitlab/-/issues/465539",
    "label": "",
    "id": "4218"
  },
  {
    "raw_code": "def change\n    create_table :workspace_tokens do |t|\n      t.references :workspace, foreign_key: { on_delete: :cascade }, null: false, index: { unique: true }\n\n      t.timestamps_with_timezone null: false\n      t.bigint :project_id, null: false, index: true\n      t.text :token_encrypted, limit: 512, null: false\n\n      t.check_constraint 'char_length(token_encrypted) <= 512'\n    end",
    "comment": "@return [void]",
    "label": "",
    "id": "4219"
  },
  {
    "raw_code": "def up\n    add_concurrent_index :topics, :name, name: NAME_INDEX\n    add_concurrent_index :topics, :slug, name: SLUG_INDEX, where: '(slug IS NOT NULL)'\n  end",
    "comment": "Add temporary indexes to support old queries (without organization_id) for topics",
    "label": "",
    "id": "4220"
  },
  {
    "raw_code": "def with_multi_connection(conn, &block)\n      return yield(conn) if conn\n\n      Sidekiq.redis do |c|\n        c.multi do |multi|\n          yield(multi)\n        end",
    "comment": "Yield block inside an existing multi connection or creates new one",
    "label": "",
    "id": "4221"
  },
  {
    "raw_code": "def queues_cmd\n      if @strictly_ordered_queues\n        @queues\n      else\n        permute = @queues.shuffle\n        permute.uniq!\n        permute\n      end",
    "comment": "This method was copied from https://github.com/sidekiq/sidekiq/blob/v8.0.1/lib/sidekiq/fetch.rb#L73-L86  Creating the Redis#brpop command takes into account any configured queue weights. By default Redis#brpop returns data from the first queue that has pending elements. We recreate the queue command each time we invoke Redis#brpop to honor weights and avoid queue starvation.",
    "label": "",
    "id": "4222"
  },
  {
    "raw_code": "def requeue_job(queue, msg, conn)\n      with_connection(conn) do |conn|\n        conn.lpush(queue, Sidekiq.dump_json(msg))\n      end",
    "comment": "If you want this method to be run in a scope of multi connection you need to pass it",
    "label": "",
    "id": "4223"
  },
  {
    "raw_code": "def clean_working_queues!\n      Sidekiq.logger.info('Cleaning working queues')\n\n      Sidekiq.redis do |conn|\n        conn.scan(match: \"#{WORKING_QUEUE_PREFIX}:queue:*\", count: SCAN_COUNT) do |key|\n          original_queue, identity = extract_queue_and_identity(key)\n\n          next if original_queue.nil? || identity.nil?\n\n          clean_working_queue!(original_queue, key) if self.class.worker_dead?(identity, conn)\n        end",
    "comment": "Detect \"old\" jobs and requeue them because the worker they were assigned to probably failed miserably.",
    "label": "",
    "id": "4224"
  },
  {
    "raw_code": "def with_connection(conn)\n      return yield(conn) if conn\n\n      Sidekiq.redis { |redis_conn| yield(redis_conn) }\n    end",
    "comment": "Yield block with an existing connection or creates another one",
    "label": "",
    "id": "4225"
  },
  {
    "raw_code": "def split_array(arr)\n  first_arr = arr.take(arr.size / 2)\n  second_arr = arr - first_arr\n  [first_arr, second_arr]\nend",
    "comment": "Splits array into two halves",
    "label": "",
    "id": "4226"
  },
  {
    "raw_code": "def stop_workers(pids)\n  pids.each do |pid|\n    Process.kill('KILL', pid)\n    Process.wait pid\n  end",
    "comment": "Stop Sidekiq workers",
    "label": "",
    "id": "4227"
  },
  {
    "raw_code": "def self.handle_interruptions_exhausted(msg); end\n        end",
    "comment": "mock method to test interruptions exhausted behavior",
    "label": "",
    "id": "4228"
  },
  {
    "raw_code": "def to_s\n    if length1 == 0\n      coords1 = start1.to_s + \",0\"\n    elsif length1 == 1\n      coords1 = (start1 + 1).to_s\n    else\n      coords1 = (start1 + 1).to_s + \",\" + length1.to_s\n    end",
    "comment": "Emulate GNU diff's format Header: @@ -382,8 +481,9 @@ Indices are printed as 1-based, not 0-based.",
    "label": "",
    "id": "4229"
  },
  {
    "raw_code": "def diff_main(text1, text2, checklines=true, deadline=nil)\n    # Set a deadline by which time the diff must be complete.\n    if deadline.nil? && diff_timeout > 0\n        deadline = Time.now + diff_timeout\n    end",
    "comment": "Find the differences between two texts.  Simplifies the problem by stripping any common prefix or suffix off the texts before diffing.",
    "label": "",
    "id": "4230"
  },
  {
    "raw_code": "def diff_compute(text1, text2, checklines, deadline)\n    # Just add some text (speedup).\n    return [[:insert, text2]] if text1.empty?\n  \n    # Just delete some text (speedup).\n    return [[:delete, text1]] if text2.empty?\n  \n    shorttext, longtext = [text1, text2].sort_by(&:length)\n    if i = longtext.index(shorttext)\n      # Shorter text is inside the longer text (speedup).\n      diffs = [[:insert, longtext[0...i]], [:equal, shorttext],\n               [:insert, longtext[(i + shorttext.length)..-1]]]\n\n      # Swap insertions for deletions if diff is reversed.\n      if text1.length > text2.length\n          diffs[0][0] = :delete\n          diffs[2][0] = :delete\n      end",
    "comment": "Find the differences between two texts.  Assumes that the texts do not have any common prefix or suffix.",
    "label": "",
    "id": "4231"
  },
  {
    "raw_code": "def diff_lineMode(text1, text2, deadline)\n    # Scan the text on a line-by-line basis first.\n    text1, text2, line_array = diff_linesToChars(text1, text2)\n  \n    diffs = diff_main(text1, text2, false, deadline)\n  \n    # Convert the diff back to original text.\n    diff_charsToLines(diffs, line_array)\n    # Eliminate freak matches (e.g. blank lines)\n    diff_cleanupSemantic(diffs)\n  \n    # Rediff any replacement blocks, this time character-by-character.\n    # Add a dummy entry at the end.\n    diffs.push([:equal, ''])\n    pointer = 0\n    count_delete = 0\n    count_insert = 0\n    text_delete = ''\n    text_insert = ''\n\n    while pointer < diffs.length\n      case diffs[pointer][0]\n        when :insert\n          count_insert += 1\n          text_insert += diffs[pointer][1]\n        when :delete\n          count_delete += 1\n          text_delete += diffs[pointer][1]\n        when :equal\n          # Upon reaching an equality, check for prior redundancies.\n          if count_delete >= 1 && count_insert >= 1\n            # Delete the offending records and add the merged ones.\n            a = diff_main(text_delete, text_insert, false, deadline)\n            diffs[pointer - count_delete - count_insert, \n              count_delete + count_insert] = []\n            pointer = pointer - count_delete - count_insert\n            diffs[pointer, 0] = a\n            pointer = pointer + a.length\n          end",
    "comment": "Do a quick line-level diff on both strings, then rediff the parts for greater accuracy. This speedup can produce non-minimal diffs.",
    "label": "",
    "id": "4232"
  },
  {
    "raw_code": "def diff_bisect(text1, text2, deadline)\n    # Cache the text lengths to prevent multiple calls.\n    text1_length = text1.length\n    text2_length = text2.length\n    max_d = (text1_length + text2_length + 1) / 2\n    v_offset = max_d\n    v_length = 2 * max_d\n    v1 = Array.new(v_length, -1)\n    v2 = Array.new(v_length, -1)\n    v1[v_offset + 1] = 0\n    v2[v_offset + 1] = 0\n    delta = text1_length - text2_length\n  \n    # If the total number of characters is odd, then the front path will \n    # collide with the reverse path.\n    front = (delta % 2 != 0)\n    # Offsets for start and end of k loop.\n    # Prevents mapping of space beyond the grid.\n    k1start = 0\n    k1end = 0\n    k2start = 0\n    k2end = 0\n    max_d.times do |d|\n      # Bail out if deadline is reached.\n      break if deadline && Time.now >= deadline\n  \n      # Walk the front path one step.\n      (-d + k1start).step(d - k1end, 2) do |k1|\n        k1_offset = v_offset + k1\n        if k1 == -d || k1 != d && v1[k1_offset - 1] < v1[k1_offset + 1]\n          x1 = v1[k1_offset + 1]\n        else\n          x1 = v1[k1_offset - 1] + 1\n        end",
    "comment": "Find the 'middle snake' of a diff, split the problem in two and return the recursively constructed diff. See Myers 1986 paper: An O(ND) Difference Algorithm and Its Variations.",
    "label": "",
    "id": "4233"
  },
  {
    "raw_code": "def diff_bisectSplit(text1, text2, x, y, deadline)\n    text1a = text1[0...x]\n    text2a = text2[0...y]\n    text1b = text1[x..-1]\n    text2b = text2[y..-1]\n  \n    # Compute both diffs serially.\n    diffs = diff_main(text1a, text2a, false, deadline)\n    diffsb = diff_main(text1b, text2b, false, deadline)\n  \n    diffs + diffsb\n  end",
    "comment": "Given the location of the 'middle snake', split the diff in two parts and recurse.",
    "label": "",
    "id": "4234"
  },
  {
    "raw_code": "def diff_linesToChars(text1, text2)\n    line_array = ['']  # e.g. line_array[4] == \"Hello\\n\"\n    line_hash = {}     # e.g. line_hash[\"Hello\\n\"] == 4\n\n    [text1, text2].map do |text|\n      # Split text into an array of strings.  Reduce the text to a string of\n      # hashes where each Unicode character represents one line.\n      chars = ''\n      text.each_line do |line|\n        if line_hash[line]\n          chars += line_hash[line].chr(Encoding::UTF_8)\n        else\n          chars += line_array.length.chr(Encoding::UTF_8)\n          line_hash[line] = line_array.length\n          line_array.push(line)\n        end",
    "comment": "Split two texts into an array of strings.  Reduce the texts to a string of hashes where each Unicode character represents one line.",
    "label": "",
    "id": "4235"
  },
  {
    "raw_code": "def diff_charsToLines(diffs, line_array)\n    diffs.each do |diff|\n      diff[1] = diff[1].chars.map{ |c| line_array[c.ord] }.join\n    end",
    "comment": "Rehydrate the text in a diff from a string of line hashes to real lines of text.",
    "label": "",
    "id": "4236"
  },
  {
    "raw_code": "def diff_commonPrefix(text1, text2)\n    # Quick check for common null cases.\n    return 0 if text1.empty? || text2.empty? || text1[0] != text2[0]\n\n    # Binary search.\n    # Performance analysis: http://neil.fraser.name/news/2007/10/09/\n    pointer_min = 0\n    pointer_max = [text1.length, text2.length].min\n    pointer_mid = pointer_max\n    pointer_start = 0\n\n    while pointer_min < pointer_mid\n      if text1[pointer_start...pointer_mid] == text2[pointer_start...pointer_mid]\n        pointer_min = pointer_mid\n        pointer_start = pointer_min\n      else\n        pointer_max = pointer_mid\n      end",
    "comment": "Determine the common prefix of two strings.",
    "label": "",
    "id": "4237"
  },
  {
    "raw_code": "def diff_commonSuffix(text1, text2)\n    # Quick check for common null cases.\n    return 0 if text1.empty? || text2.empty? || text1[-1] != text2[-1]\n  \n    # Binary search.\n    # Performance analysis: http://neil.fraser.name/news/2007/10/09/\n    pointer_min = 0\n    pointer_max = [text1.length, text2.length].min\n    pointer_mid = pointer_max\n    pointer_end = 0\n\n    while pointer_min < pointer_mid\n      if text1[-pointer_mid..(-pointer_end-1)] == text2[-pointer_mid..(-pointer_end-1)]\n        pointer_min = pointer_mid\n        pointer_end = pointer_min\n      else\n        pointer_max = pointer_mid\n      end",
    "comment": "Determine the common suffix of two strings.",
    "label": "",
    "id": "4238"
  },
  {
    "raw_code": "def diff_commonOverlap(text1, text2)\n    # Cache the text lengths to prevent multiple calls.\n    text1_length = text1.length\n    text2_length = text2.length\n\n    # Eliminate the null case.\n    return 0  if text1_length.zero? || text2_length.zero?\n  \n    # Truncate the longer string.\n    if text1_length > text2_length\n      text1 = text1[-text2_length..-1]\n    else\n      text2 = text2[0...text1_length]\n    end",
    "comment": "Determine if the suffix of one string is the prefix of another.",
    "label": "",
    "id": "4239"
  },
  {
    "raw_code": "def diff_halfMatchI(longtext, shorttext, i)\n    seed = longtext[i, longtext.length / 4]\n    j = -1\n    best_common = ''\n    while j = shorttext.index(seed, j + 1)\n      prefix_length = diff_commonPrefix(longtext[i..-1], shorttext[j..-1])\n      suffix_length = diff_commonSuffix(longtext[0...i], shorttext[0...j])\n      if best_common.length < suffix_length + prefix_length\n        best_common = shorttext[(j - suffix_length)...j] + shorttext[j...(j + prefix_length)]\n        best_longtext_a = longtext[0...(i - suffix_length)]\n        best_longtext_b = longtext[(i + prefix_length)..-1]\n        best_shorttext_a = shorttext[0...(j - suffix_length)]\n        best_shorttext_b = shorttext[(j + prefix_length)..-1]\n      end",
    "comment": "Does a substring of shorttext exist within longtext such that the substring is at least half the length of longtext?",
    "label": "",
    "id": "4240"
  },
  {
    "raw_code": "def diff_halfMatch(text1, text2)\n    # Don't risk returning a non-optimal diff if we have unlimited time\n    return nil if diff_timeout <= 0\n\n    shorttext, longtext = [text1, text2].sort_by(&:length)\n    if longtext.length < 4 || shorttext.length * 2 < longtext.length\n      return nil # Pointless.\n    end",
    "comment": "Do the two texts share a substring which is at least half the length of the longer text? This speedup can produce non-minimal diffs.",
    "label": "",
    "id": "4241"
  },
  {
    "raw_code": "def diff_cleanupSemantic(diffs)\n    changes = false\n    equalities = []  # Stack of indices where equalities are found.\n    last_equality = nil # Always equal to equalities.last[1]\n    pointer = 0 # Index of current position.\n    # Number of characters that changed prior to the equality.\n    length_insertions1 = 0\n    length_deletions1 = 0\n    # Number of characters that changed after the equality.\n    length_insertions2 = 0\n    length_deletions2 = 0\n  \n    while pointer < diffs.length\n      if diffs[pointer][0] == :equal # Equality found.\n        equalities.push(pointer)\n        length_insertions1 = length_insertions2\n        length_deletions1 = length_deletions2\n        length_insertions2 = 0\n        length_deletions2 = 0\n        last_equality = diffs[pointer][1]\n      else  # An insertion or deletion.\n        if diffs[pointer][0] == :insert\n          length_insertions2 += diffs[pointer][1].length\n        else\n          length_deletions2 += diffs[pointer][1].length\n        end",
    "comment": "Reduce the number of edits by eliminating semantically trivial equalities.",
    "label": "",
    "id": "4242"
  },
  {
    "raw_code": "def diff_cleanupSemanticScore(one, two)\n    if one.empty? || two.empty?\n      # Edges are the best.\n      return 5\n    end",
    "comment": "Given two strings, compute a score representing whether the internal boundary falls on logical boundaries. Scores range from 5 (best) to 0 (worst).",
    "label": "",
    "id": "4243"
  },
  {
    "raw_code": "def diff_cleanupSemanticLossless(diffs)\n    pointer = 1\n    # Intentionally ignore the first and last element (don't need checking).\n    while pointer < diffs.length - 1\n      if diffs[pointer - 1][0] == :equal && diffs[pointer + 1][0] == :equal\n        # This is a single edit surrounded by equalities.\n        equality1 = diffs[pointer - 1][1]\n        edit = diffs[pointer][1]\n        equality2 = diffs[pointer + 1][1]\n  \n        # First, shift the edit as far left as possible.\n        common_offset = diff_commonSuffix(equality1, edit)\n        if common_offset != 0\n          common_string = edit[-common_offset..-1]\n          equality1 = equality1[0...-common_offset]\n          edit = common_string + edit[0...-common_offset]\n          equality2 = common_string + equality2\n        end",
    "comment": "Look for single edits surrounded on both sides by equalities which can be shifted sideways to align the edit to a word boundary. e.g: The c<ins>at c</ins>ame. -> The <ins>cat </ins>came.",
    "label": "",
    "id": "4244"
  },
  {
    "raw_code": "def diff_cleanupEfficiency(diffs)\n    changes = false\n    equalities = []  # Stack of indices where equalities are found.\n    last_equality = ''  # Always equal to equalities.last[1]\n    pointer = 0  # Index of current position.    \n    pre_ins = false # Is there an insertion operation before the last equality.      \n    pre_del = false # Is there a deletion operation before the last equality.      \n    post_ins = false # Is there an insertion operation after the last equality.      \n    post_del = false # Is there a deletion operation after the last equality.\n\n    while pointer < diffs.length\n      if diffs[pointer][0] == :equal # Equality found.\n        if diffs[pointer][1].length < diff_editCost && (post_ins || post_del)\n          # Candidate found.\n          equalities.push(pointer)\n          pre_ins = post_ins\n          pre_del = post_del\n          last_equality = diffs[pointer][1]\n        else\n          # Not a candidate, and can never become one.\n          equalities.clear\n          last_equality = ''\n        end",
    "comment": "Reduce the number of edits by eliminating operationally trivial equalities.",
    "label": "",
    "id": "4245"
  },
  {
    "raw_code": "def diff_cleanupMerge(diffs)\n    diffs.push([:equal, '']) # Add a dummy entry at the end.\n    pointer = 0\n    count_delete = 0\n    count_insert = 0\n    text_delete = ''\n    text_insert = ''\n\n    while pointer < diffs.length\n      case diffs[pointer][0]\n        when :insert\n          count_insert += 1\n          text_insert += diffs[pointer][1]\n          pointer += 1\n        when :delete\n          count_delete += 1\n          text_delete += diffs[pointer][1]\n          pointer += 1\n        when :equal\n          # Upon reaching an equality, check for prior redundancies.\n          if count_delete + count_insert > 1\n            if count_delete != 0 && count_insert != 0\n              # Factor out any common prefixies.\n              common_length = diff_commonPrefix(text_insert, text_delete)\n              if common_length != 0\n                if (pointer - count_delete - count_insert) > 0 &&\n                   diffs[pointer - count_delete - count_insert - 1][0] == :equal\n                  diffs[pointer - count_delete - count_insert - 1][1] +=\n                    text_insert[0...common_length]\n                else\n                  diffs.unshift([:equal, text_insert[0...common_length]])\n                  pointer += 1\n                end",
    "comment": "Reorder and merge like edit sections.  Merge equalities. Any edit section can move as long as it doesn't cross an equality.",
    "label": "",
    "id": "4246"
  },
  {
    "raw_code": "def diff_xIndex(diffs, loc)\n    chars1 = 0\n    chars2 = 0\n    last_chars1 = 0\n    last_chars2 = 0\n    x = diffs.index do |diff|\n      if diff[0] != :insert\n        chars1 += diff[1].length\n      end",
    "comment": "loc is a location in text1, compute and return the equivalent location in text2. e.g. 'The cat' vs 'The big cat', 1->1, 5->8",
    "label": "",
    "id": "4247"
  },
  {
    "raw_code": "def diff_prettyHtml(diffs)\n    diffs.map do |op, data|\n      text = data.gsub('&', '&amp;').gsub('<', '&lt;').gsub('>', '&gt;').gsub('\\n', '&para;<br>')\n      case op\n        when :insert\n          \"<ins style=\\\"background:#e6ffe6;\\\">#{text}</ins>\"\n        when :delete\n          \"<del style=\\\"background:#ffe6e6;\\\">#{text}</del>\"\n        when :equal\n          \"<span>#{text}</span>\"\n      end",
    "comment": "Convert a diff array into a pretty HTML report.",
    "label": "",
    "id": "4248"
  },
  {
    "raw_code": "def diff_text1(diffs)\n    diffs.map do |op, data|\n      if op == :insert\n        ''\n      else\n        data\n      end",
    "comment": "Compute and return the source text (all equalities and deletions).",
    "label": "",
    "id": "4249"
  },
  {
    "raw_code": "def diff_text2(diffs)\n    diffs.map do |op, data|\n      if op == :delete\n        ''\n      else\n        data\n      end",
    "comment": "Compute and return the destination text (all equalities and insertions).",
    "label": "",
    "id": "4250"
  },
  {
    "raw_code": "def diff_levenshtein(diffs)\n    levenshtein = 0\n    insertions = 0\n    deletions = 0\n\n    diffs.each do |op, data|\n      case op\n        when :insert\n          insertions += data.length\n        when :delete\n          deletions += data.length\n        when :equal\n          # A deletion and an insertion is one substitution.\n          levenshtein += [insertions, deletions].max\n          insertions = 0\n          deletions = 0\n      end",
    "comment": "Compute the Levenshtein distance; the number of inserted, deleted or substituted characters.",
    "label": "",
    "id": "4251"
  },
  {
    "raw_code": "def diff_toDelta(diffs)\n    diffs.map do |op, data|\n      case op\n        when :insert\n          '+' + PatchObj.uri_encode(data, /[^0-9A-Za-z_.;!~*'(),\\/?:@&=+$\\#-]/)\n        when :delete\n          '-' + data.length.to_s\n        when :equal\n          '=' + data.length.to_s\n      end",
    "comment": "Crush the diff into an encoded string which describes the operations required to transform text1 into text2. E.g. =3\\t-2\\t+ing  -> Keep 3 chars, delete 2 chars, insert 'ing'. Operations are tab-separated.  Inserted text is escaped using %xx notation.",
    "label": "",
    "id": "4252"
  },
  {
    "raw_code": "def diff_fromDelta(text1, delta)\n    # Deltas should be composed of a subset of ascii chars, Unicode not required.\n    delta.encode('ascii')\n    diffs = []\n    pointer = 0 # Cursor in text1\n    delta.split(\"\\t\").each do |token|\n      # Each token begins with a one character parameter which specifies the\n      # operation of this token (delete, insert, equality).\n      param = token[1..-1]\n      case token[0]\n        when '+'\n          diffs.push([:insert, PatchObj.uri_decode(param.force_encoding(Encoding::UTF_8))])\n        when '-', '='\n          begin\n            n = Integer(param)\n            raise if n < 0\n            text = text1[pointer...(pointer + n)]\n            pointer += n\n            if token[0] == '='\n              diffs.push([:equal, text])\n            else\n              diffs.push([:delete, text])\n            end",
    "comment": "Given the original text1, and an encoded string which describes the operations required to transform text1 into text2, compute the full diff.",
    "label": "",
    "id": "4253"
  },
  {
    "raw_code": "def match_main(text, pattern, loc)\n    # Check for null inputs.\n    if [text, pattern].any?(&:nil?)\n      raise ArgumentError.new(\"Null input. (match_main)\")\n    end",
    "comment": "Locate the best instance of 'pattern' in 'text' near 'loc'.",
    "label": "",
    "id": "4254"
  },
  {
    "raw_code": "def match_bitap(text, pattern, loc)\n    if pattern.length > match_maxBits\n      throw ArgumentError.new(\"Pattern too long\")\n    end",
    "comment": "Locate the best instance of 'pattern' in 'text' near 'loc' using the Bitap algorithm.",
    "label": "",
    "id": "4255"
  },
  {
    "raw_code": "def match_alphabet(pattern)\n    s = {}\n    pattern.chars.each_with_index do |c, i|\n      s[c] ||= 0\n      s[c] |= 1 << (pattern.length - i - 1)\n    end",
    "comment": "Initialise the alphabet for the Bitap algorithm.",
    "label": "",
    "id": "4256"
  },
  {
    "raw_code": "def patch_fromText(textline)\n    return []  if textline.empty?\n\n    patches = []\n    text = textline.split(\"\\n\")\n    text_pointer = 0\n    patch_header = /^@@ -(\\d+),?(\\d*) \\+(\\d+),?(\\d*) @@$/\n    while text_pointer < text.length\n      m = text[text_pointer].match(patch_header)\n      if m.nil?\n        raise ArgumentError.new(\"Invalid patch string: #{text[text_pointer]}\")\n      end",
    "comment": "Parse a textual representation of patches and return a list of patch objects.",
    "label": "",
    "id": "4257"
  },
  {
    "raw_code": "def patch_toText(patches)\n    patches.join\n  end",
    "comment": "Take a list of patches and return a textual representation",
    "label": "",
    "id": "4258"
  },
  {
    "raw_code": "def patch_addContext(patch, text)\n    return  if text.empty?\n    pattern = text[patch.start2, patch.length1]\n    padding = 0\n  \n    # Look for the first and last matches of pattern in text.  If two different\n    # matches are found, increase the pattern length.\n    while text.index(pattern) != text.rindex(pattern) &&\n          pattern.length < match_maxBits - 2 * patch_margin\n      padding += patch_margin\n      pattern = text[[0, patch.start2 - padding].max...(patch.start2 + patch.length1 + padding)]\n    end",
    "comment": "Increase the context until it is unique, but don't let the pattern expand beyond match_maxBits",
    "label": "",
    "id": "4259"
  },
  {
    "raw_code": "def patch_make(*args)\n    text1 = nil\n    diffs = nil\n    if args.length == 2 && args[0].is_a?(String) && args[1].is_a?(String)\n      # Compute diffs from text1 and text2.\n      text1 = args[0]\n      text2 = args[1]\n      diffs = diff_main(text1, text2, true)\n      if diffs.length > 2\n        diff_cleanupSemantic(diffs)\n        diff_cleanupEfficiency(diffs)\n      end",
    "comment": "Compute a list of patches to turn text1 into text2. Use diffs if provided, otherwise compute it ourselves. There are four ways to call this function, depending on what data is available to the caller: Method 1: a = text1, b = text2 Method 2: a = diffs Method 3 (optimal): a = text1, b = diffs Method 4 (deprecated, use method 3): a = text1, b = text2, c = diffs",
    "label": "",
    "id": "4260"
  },
  {
    "raw_code": "def patch_apply(patches, text)  \n    return [text, []]  if patches.empty?\n  \n    # Deep copy the patches so that no changes are made to originals.\n    patches = Marshal.load(Marshal.dump(patches))\n\n    null_padding = patch_addPadding(patches)\n    text = null_padding + text + null_padding\n    patch_splitMax(patches)\n\n    # delta keeps track of the offset between the expected and actual location\n    # of the previous patch.  If there are patches expected at positions 10 and\n    # 20, but the first patch was found at 12, delta is 2 and the second patch\n    # has an effective expected position of 22.\n    delta = 0\n    results = []\n    patches.each_with_index do |patch, x|\n      expected_loc = patch.start2 + delta\n      text1 = diff_text1(patch.diffs)\n      end_loc = -1      \n      if text1.length > match_maxBits\n        # patch_splitMax will only provide an oversized pattern in the case of\n        # a monster delete.\n        start_loc = match_main(text, text1[0, match_maxBits], expected_loc)\n        if start_loc != -1\n          end_loc = match_main(text, text1[(text1.length - match_maxBits)..-1],\n            expected_loc + text1.length - match_maxBits)\n          if end_loc == -1 || start_loc >= end_loc\n            # Can't find valid trailing context.  Drop this patch.\n            start_loc = -1\n          end",
    "comment": "Merge a set of patches onto the text.  Return a patched text, as well as a list of true/false values indicating which patches were applied.",
    "label": "",
    "id": "4261"
  },
  {
    "raw_code": "def patch_addPadding(patches)\n    padding_length = patch_margin\n    null_padding = (1..padding_length).map{ |x| x.chr(Encoding::UTF_8) }.join\n  \n    # Bump all the patches forward.\n    patches.each do |patch|\n      patch.start1 += padding_length\n      patch.start2 += padding_length\n    end",
    "comment": "Add some padding on text start and end so that edges can match something. Intended to be called only from within patch_apply.",
    "label": "",
    "id": "4262"
  },
  {
    "raw_code": "def patch_splitMax(patches)\n    patch_size = match_maxBits\n\n    x = 0\n    while x < patches.length\n      if patches[x].length1 > patch_size\n        big_patch = patches[x]\n        # Remove the big old patch\n        patches[x, 1] = []\n        x -= 1\n        start1 = big_patch.start1\n        start2 = big_patch.start2\n        pre_context = ''\n        while !big_patch.diffs.empty?\n          # Create one of several smaller patches.\n          patch = PatchObj.new\n          empty = true\n          patch.start1 = start1 - pre_context.length\n          patch.start2 = start2 - pre_context.length\n          unless pre_context.empty?\n            patch.length1 = patch.length2 = pre_context.length\n            patch.diffs.push([:equal, pre_context])\n          end",
    "comment": "Look through the patches and break up any which are longer than the maximum limit of the match algorithm.",
    "label": "",
    "id": "4263"
  },
  {
    "raw_code": "def self.configure_embed(&block)\n    raise \"Sidekiq global configuration is frozen, you must create all embedded instances BEFORE calling `run`\" if @frozen\n\n    require \"sidekiq/embedded\"\n    cfg = default_configuration\n    cfg.concurrency = 2\n    @config_blocks&.each { |block| block.call(cfg) }\n    yield cfg\n\n    Sidekiq::Embedded.new(cfg)\n  end",
    "comment": "Creates a Sidekiq::Config instance that is more tuned for embedding within an arbitrary Ruby process. Notably it reduces concurrency by default so there is less contention for CPU time with other threads.  instance = Sidekiq.configure_embed do |config| config.queues = %w[critical default low] end instance.run sleep 10 instance.stop  NB: it is really easy to overload a Ruby process with threads due to the GIL. I do not recommend setting concurrency higher than 2-3.  NB: Sidekiq only supports one instance in memory. You will get undefined behavior if you try to embed Sidekiq twice in the same process.",
    "label": "",
    "id": "4264"
  },
  {
    "raw_code": "def global(jobstr, queue)\n      yield\n    rescue Handled => ex\n      raise ex\n    rescue Sidekiq::Shutdown => ey\n      # ignore, will be pushed back onto queue during hard_shutdown\n      raise ey\n    rescue Exception => e\n      # ignore, will be pushed back onto queue during hard_shutdown\n      raise Sidekiq::Shutdown if exception_caused_by_shutdown?(e)\n\n      msg = Sidekiq.load_json(jobstr)\n      if msg[\"retry\"]\n        process_retry(nil, msg, queue, e)\n      else\n        @capsule.config.death_handlers.each do |handler|\n          handler.call(msg, e)\n        rescue => handler_ex\n          handle_exception(handler_ex, {context: \"Error calling death handler\", job: msg})\n        end",
    "comment": "The global retry handler requires only the barest of data. We want to be able to retry as much as possible so we don't require the job to be instantiated.",
    "label": "",
    "id": "4265"
  },
  {
    "raw_code": "def local(jobinst, jobstr, queue)\n      yield\n    rescue Handled => ex\n      raise ex\n    rescue Sidekiq::Shutdown => ey\n      # ignore, will be pushed back onto queue during hard_shutdown\n      raise ey\n    rescue Exception => e\n      # ignore, will be pushed back onto queue during hard_shutdown\n      raise Sidekiq::Shutdown if exception_caused_by_shutdown?(e)\n\n      msg = Sidekiq.load_json(jobstr)\n      if msg[\"retry\"].nil?\n        msg[\"retry\"] = jobinst.class.get_sidekiq_options[\"retry\"]\n      end",
    "comment": "The local retry support means that any errors that occur within this block can be associated with the given job instance. This is required to support the `sidekiq_retries_exhausted` block.  Note that any exception from the block is wrapped in the Skip exception so the global block does not reprocess the error.  The Skip exception is unwrapped within Sidekiq::Processor#process before calling the handle_exception handlers.",
    "label": "",
    "id": "4266"
  },
  {
    "raw_code": "def process_retry(jobinst, msg, queue, exception)\n      max_retry_attempts = retry_attempts_from(msg[\"retry\"], @max_retries)\n\n      msg[\"queue\"] = (msg[\"retry_queue\"] || queue)\n\n      m = exception_message(exception)\n      if m.respond_to?(:scrub!)\n        m.force_encoding(\"utf-8\")\n        m.scrub!\n      end",
    "comment": "Note that +jobinst+ can be nil here if an error is raised before we can instantiate the job instance.  All access must be guarded and best effort.",
    "label": "",
    "id": "4267"
  },
  {
    "raw_code": "def delay_for(jobinst, count, exception, msg)\n      rv = begin\n        # sidekiq_retry_in can return two different things:\n        # 1. When to retry next, as an integer of seconds\n        # 2. A symbol which re-routes the job elsewhere, e.g. :discard, :kill, :default\n        block = jobinst&.sidekiq_retry_in_block\n\n        # the sidekiq_retry_in_block can be defined in a wrapped class (ActiveJob for instance)\n        unless msg[\"wrapped\"].nil?\n          wrapped = Object.const_get(msg[\"wrapped\"])\n          block = wrapped.respond_to?(:sidekiq_retry_in_block) ? wrapped.sidekiq_retry_in_block : nil\n        end",
    "comment": "returns (strategy, seconds)",
    "label": "",
    "id": "4268"
  },
  {
    "raw_code": "def exception_message(exception)\n      # App code can stuff all sorts of crazy binary data into the error message\n      # that won't convert to JSON.\n      exception.message.to_s[0, 10_000]\n    rescue\n      +\"!!! ERROR MESSAGE THREW AN ERROR !!!\"\n    end",
    "comment": "Extract message from exception. Set a default if the message raises an error",
    "label": "",
    "id": "4269"
  },
  {
    "raw_code": "def run(boot_app: true, warmup: true)\n      boot_application if boot_app\n\n      if environment == \"development\" && $stdout.tty? && @config.logger.formatter.is_a?(Sidekiq::Logger::Formatters::Pretty)\n        print_banner\n      end",
    "comment": "Code within this method is not tested because it alters global process state irreversibly.  PRs which improve the test coverage of Sidekiq::CLI are welcomed.",
    "label": "",
    "id": "4270"
  },
  {
    "raw_code": "def run(async_beat: true)\n      logger.debug { @config.merge!({}) }\n      Sidekiq.freeze!\n      @thread = safe_thread(\"heartbeat\", &method(:start_heartbeat)) if async_beat\n      @poller.start\n      @managers.each(&:start)\n    end",
    "comment": "Start this Sidekiq instance. If an embedding process already has a heartbeat thread, caller can use `async_beat: false` and instead have thread call Launcher#heartbeat every N seconds.",
    "label": "",
    "id": "4271"
  },
  {
    "raw_code": "def quiet\n      return if @done\n\n      @done = true\n      @managers.each(&:quiet)\n      @poller.terminate\n      fire_event(:quiet, reverse: true)\n    end",
    "comment": "Stops this instance from processing any more jobs,",
    "label": "",
    "id": "4272"
  },
  {
    "raw_code": "def stop\n      deadline = ::Process.clock_gettime(::Process::CLOCK_MONOTONIC) + @config[:timeout]\n\n      quiet\n      stoppers = @managers.map do |mgr|\n        Thread.new do\n          mgr.stop(deadline)\n        end",
    "comment": "Shuts down this Sidekiq instance. Waits up to the deadline for all jobs to complete.",
    "label": "",
    "id": "4273"
  },
  {
    "raw_code": "def heartbeat\n      \n    end",
    "comment": "If embedding Sidekiq, you can have the process heartbeat call this method to regularly heartbeat rather than creating a separate thread.",
    "label": "",
    "id": "4274"
  },
  {
    "raw_code": "def middleware(&block)\n      if block\n        @chain = @chain.dup\n        yield @chain\n      end",
    "comment": " Define client-side middleware:  client = Sidekiq::Client.new client.middleware do |chain| chain.use MyClientMiddleware end client.push('class' => 'SomeJob', 'args' => [1,2,3])  All client instances default to the globally-defined Sidekiq.client_middleware but you can change as necessary. ",
    "label": "",
    "id": "4275"
  },
  {
    "raw_code": "def initialize(*args, **kwargs)\n      if args.size == 1 && kwargs.size == 0\n        warn \"Sidekiq::Client.new(pool) is deprecated, please use Sidekiq::Client.new(pool: pool), #{caller(0..3)}\"\n        # old calling method, accept 1 pool argument\n        @redis_pool = args[0]\n        @chain = Sidekiq.default_configuration.client_middleware\n        @config = Sidekiq.default_configuration\n      else\n        # new calling method: keyword arguments\n        @config = kwargs[:config] || Sidekiq.default_configuration\n        @redis_pool = kwargs[:pool] || Thread.current[:sidekiq_redis_pool] || @config&.redis_pool\n        @chain = kwargs[:chain] || @config&.client_middleware\n        raise ArgumentError, \"No Redis pool available for Sidekiq::Client\" unless @redis_pool\n      end",
    "comment": "Sidekiq::Client is responsible for pushing job payloads to Redis. Requires the :pool or :config keyword argument.  Sidekiq::Client.new(pool: Sidekiq::RedisConnection.create)  Inside the Sidekiq process, you can reuse the configured resources:  Sidekiq::Client.new(config: config)  @param pool [ConnectionPool] explicit Redis pool to use @param config [Sidekiq::Config] use the pool and middleware from the given Sidekiq container @param chain [Sidekiq::Middleware::Chain] use the given middleware chain",
    "label": "",
    "id": "4276"
  },
  {
    "raw_code": "def cancel!(jid)\n      key = \"it-#{jid}\"\n      _, result, _ = Sidekiq.redis do |c|\n        c.pipelined do |p|\n          p.hsetnx(key, \"cancelled\", Time.now.to_i)\n          p.hget(key, \"cancelled\")\n          p.expire(key, Sidekiq::Job::Iterable::STATE_TTL)\n          # TODO When Redis 7.2 is required\n          # p.expire(key, Sidekiq::Job::Iterable::STATE_TTL, \"nx\")\n        end",
    "comment": "Cancel the IterableJob with the given JID. **NB: Cancellation is asynchronous.** Iteration checks every five seconds so this will not immediately stop the given job.",
    "label": "",
    "id": "4277"
  },
  {
    "raw_code": "def push(item)\n      normed = normalize_item(item)\n      payload = middleware.invoke(item[\"class\"], normed, normed[\"queue\"], @redis_pool) do\n        normed\n      end",
    "comment": " The main method used to push a job to Redis.  Accepts a number of options:  queue - the named queue to use, default 'default' class - the job class to call, required args - an array of simple arguments to the perform method, must be JSON-serializable at - timestamp to schedule the job (optional), must be Numeric (e.g. Time.now.to_f) retry - whether to retry this job if it fails, default true or an integer number of retries retry_for - relative amount of time to retry this job if it fails, default nil backtrace - whether to save any error backtrace, default false  If class is set to the class name, the jobs' options will be based on Sidekiq's default job options. Otherwise, they will be based on the job class's options.  Any options valid for a job class's sidekiq_options are also available here.  All keys must be strings, not symbols.  NB: because we are serializing to JSON, all symbols in 'args' will be converted to strings.  Note that +backtrace: true+ can take quite a bit of space in Redis; a large volume of failing jobs can start Redis swapping if you aren't careful.  Returns a unique Job ID.  If middleware stops the job, nil will be returned instead.  Example: push('queue' => 'my_queue', 'class' => MyJob, 'args' => ['foo', 1, :bat => 'bar']) ",
    "label": "",
    "id": "4278"
  },
  {
    "raw_code": "def push_bulk(items)\n      batch_size = items.delete(:batch_size) || items.delete(\"batch_size\") || 1_000\n      args = items[\"args\"]\n      at = items.delete(\"at\")\n      raise ArgumentError, \"Job 'at' must be a Numeric or an Array of Numeric timestamps\" if at && (Array(at).empty? || !Array(at).all? { |entry| entry.is_a?(Numeric) })\n      raise ArgumentError, \"Job 'at' Array must have same size as 'args' Array\" if at.is_a?(Array) && at.size != args.size\n\n      jid = items.delete(\"jid\")\n      raise ArgumentError, \"Explicitly passing 'jid' when pushing more than one job is not supported\" if jid && args.size > 1\n\n      normed = normalize_item(items)\n      slice_index = 0\n      result = args.each_slice(batch_size).flat_map do |slice|\n        raise ArgumentError, \"Bulk arguments must be an Array of Arrays: [[1], [2]]\" unless slice.is_a?(Array) && slice.all?(Array)\n        break [] if slice.empty? # no jobs to push\n\n        payloads = slice.map.with_index { |job_args, index|\n          copy = normed.merge(\"args\" => job_args, \"jid\" => SecureRandom.hex(12))\n          copy[\"at\"] = (at.is_a?(Array) ? at[slice_index + index] : at) if at\n          result = middleware.invoke(items[\"class\"], copy, copy[\"queue\"], @redis_pool) do\n            verify_json(copy)\n            copy\n          end",
    "comment": " Push a large number of jobs to Redis. This method cuts out the redis network round trip latency. It pushes jobs in batches if more than `:batch_size` (1000 by default) of jobs are passed. I wouldn't recommend making `:batch_size` larger than 1000 but YMMV based on network quality, size of job args, etc. A large number of jobs can cause a bit of Redis command processing latency.  Takes the same arguments as #push except that args is expected to be an Array of Arrays.  All other keys are duplicated for each job.  Each job is run through the client middleware pipeline and each job gets its own Job ID as normal.  Returns an array of the of pushed jobs' jids, may contain nils if any client middleware prevented a job push.  Example (pushing jobs in batches): push_bulk('class' => MyJob, 'args' => (1..100_000).to_a, batch_size: 1_000) ",
    "label": "",
    "id": "4279"
  },
  {
    "raw_code": "def self.via(pool)\n      raise ArgumentError, \"No pool given\" if pool.nil?\n      current_sidekiq_pool = Thread.current[:sidekiq_redis_pool]\n      Thread.current[:sidekiq_redis_pool] = pool\n      yield\n    ensure\n      Thread.current[:sidekiq_redis_pool] = current_sidekiq_pool\n    end",
    "comment": "Allows sharding of jobs across any number of Redis instances.  All jobs defined within the block will use the given Redis connection pool.  pool = ConnectionPool.new { Redis.new } Sidekiq::Client.via(pool) do SomeJob.perform_async(1,2,3) SomeOtherJob.perform_async(1,2,3) end  Generally this is only needed for very large Sidekiq installs processing thousands of jobs per second.  I do not recommend sharding unless you cannot scale any other way (e.g. splitting your app into smaller apps).",
    "label": "",
    "id": "4280"
  },
  {
    "raw_code": "def enqueue(klass, *args)\n        klass.client_push(\"class\" => klass, \"args\" => args)\n      end",
    "comment": "Resque compatibility helpers.  Note all helpers should go through Sidekiq::Job#client_push.  Example usage: Sidekiq::Client.enqueue(MyJob, 'foo', 1, :bat => 'bar')  Messages are enqueued to the 'default' queue. ",
    "label": "",
    "id": "4281"
  },
  {
    "raw_code": "def enqueue_to(queue, klass, *args)\n        klass.client_push(\"queue\" => queue, \"class\" => klass, \"args\" => args)\n      end",
    "comment": "Example usage: Sidekiq::Client.enqueue_to(:queue_name, MyJob, 'foo', 1, :bat => 'bar') ",
    "label": "",
    "id": "4282"
  },
  {
    "raw_code": "def enqueue_to_in(queue, interval, klass, *args)\n        int = interval.to_f\n        now = Time.now.to_f\n        ts = ((int < 1_000_000_000) ? now + int : int)\n\n        item = {\"class\" => klass, \"args\" => args, \"at\" => ts, \"queue\" => queue}\n        item.delete(\"at\") if ts <= now\n\n        klass.client_push(item)\n      end",
    "comment": "Example usage: Sidekiq::Client.enqueue_to_in(:queue_name, 3.minutes, MyJob, 'foo', 1, :bat => 'bar') ",
    "label": "",
    "id": "4283"
  },
  {
    "raw_code": "def enqueue_in(interval, klass, *args)\n        klass.perform_in(interval, *args)\n      end",
    "comment": "Example usage: Sidekiq::Client.enqueue_in(3.minutes, MyJob, 'foo', 1, :bat => 'bar') ",
    "label": "",
    "id": "4284"
  },
  {
    "raw_code": "def wait_for(deadline, &condblock)\n      remaining = deadline - ::Process.clock_gettime(::Process::CLOCK_MONOTONIC)\n      while remaining > PAUSE_TIME\n        return if condblock.call\n        sleep PAUSE_TIME\n        remaining = deadline - ::Process.clock_gettime(::Process::CLOCK_MONOTONIC)\n      end",
    "comment": "Wait for the orblock to be true or the deadline passed.",
    "label": "",
    "id": "4285"
  },
  {
    "raw_code": "def method_missing(*args, &block)\n        warn(\"[sidekiq#5788] Redis has deprecated the `#{args.first}`command, called at #{caller(1..1)}\") if DEPRECATED_COMMANDS.include?(args.first)\n        @client.call(*args, *block)\n      end",
    "comment": "this allows us to use methods like `conn.hmset(...)` instead of having to use redis-client's native `conn.call(\"hmset\", ...)`",
    "label": "",
    "id": "4286"
  },
  {
    "raw_code": "def inspect\n      \"#<#{self.class.name} #{\n        instance_variables.map do |name|\n          value = instance_variable_get(name)\n          case value\n          when Proc\n            \"#{name}=#{value}\"\n          when Sidekiq::Config\n            \"#{name}=#{value}\"\n          when Sidekiq::Component\n            \"#{name}=#{value}\"\n          else\n            \"#{name}=#{value.inspect}\"\n          end",
    "comment": "When you have a large tree of components, the `inspect` output can get out of hand, especially with lots of Sidekiq::Config references everywhere. We avoid calling `inspect` on more complex state and use `to_s` instead to keep output manageable, #6553",
    "label": "",
    "id": "4287"
  },
  {
    "raw_code": "def push_bulk(items)\n      @redis_client.push_bulk(items)\n    end",
    "comment": " We don't provide transactionality for push_bulk because we don't want to hold potentially hundreds of thousands of job records in memory due to a long running enqueue process.",
    "label": "",
    "id": "4288"
  },
  {
    "raw_code": "def queues=(val)\n      @weights = {}\n      @queues = Array(val).each_with_object([]) do |qstr, memo|\n        arr = qstr\n        arr = qstr.split(\",\") if qstr.is_a?(String)\n        name, weight = arr\n        @weights[name] = weight.to_i\n        [weight.to_i, 1].max.times do\n          memo << name\n        end",
    "comment": "Sidekiq checks queues in three modes: - :strict - all queues have 0 weight and are checked strictly in order - :weighted - queues have arbitrary weight between 1 and N - :random - all queues have weight of 1",
    "label": "",
    "id": "4289"
  },
  {
    "raw_code": "def client_middleware\n      @client_chain ||= config.client_middleware.copy_for(self)\n      yield @client_chain if block_given?\n      @client_chain\n    end",
    "comment": "Allow the middleware to be different per-capsule. Avoid if possible and add middleware globally so all capsules share the same chains. Easier to debug that way.",
    "label": "",
    "id": "4290"
  },
  {
    "raw_code": "def fetch_stats_fast!\n      pipe1_res = Sidekiq.redis { |conn|\n        conn.pipelined do |pipeline|\n          pipeline.get(\"stat:processed\")\n          pipeline.get(\"stat:failed\")\n          pipeline.zcard(\"schedule\")\n          pipeline.zcard(\"retry\")\n          pipeline.zcard(\"dead\")\n          pipeline.scard(\"processes\")\n          pipeline.lindex(\"queue:default\", -1)\n        end",
    "comment": "O(1) redis calls @api private",
    "label": "",
    "id": "4291"
  },
  {
    "raw_code": "def fetch_stats_slow!\n      processes = Sidekiq.redis { |conn|\n        conn.sscan(\"processes\").to_a\n      }\n\n      queues = Sidekiq.redis { |conn|\n        conn.sscan(\"queues\").to_a\n      }\n\n      pipe2_res = Sidekiq.redis { |conn|\n        conn.pipelined do |pipeline|\n          processes.each { |key| pipeline.hget(key, \"busy\") }\n          queues.each { |queue| pipeline.llen(\"queue:#{queue}\") }\n        end",
    "comment": "O(number of processes + number of queues) redis calls @api private",
    "label": "",
    "id": "4292"
  },
  {
    "raw_code": "def fetch_stats!\n      fetch_stats_fast!\n      fetch_stats_slow!\n    end",
    "comment": "@api private",
    "label": "",
    "id": "4293"
  },
  {
    "raw_code": "def reset(*stats)\n      all = %w[failed processed]\n      stats = stats.empty? ? all : all & stats.flatten.compact.map(&:to_s)\n\n      mset_args = []\n      stats.each do |stat|\n        mset_args << \"stat:#{stat}\"\n        mset_args << 0\n      end",
    "comment": "@api private",
    "label": "",
    "id": "4294"
  },
  {
    "raw_code": "def self.all\n      Sidekiq.redis { |c| c.sscan(\"queues\").to_a }.sort.map { |q| Sidekiq::Queue.new(q) }\n    end",
    "comment": " Fetch all known queues within Redis.  @return [Array<Sidekiq::Queue>]",
    "label": "",
    "id": "4295"
  },
  {
    "raw_code": "def initialize(name = \"default\")\n      @name = name.to_s\n      @rname = \"queue:#{name}\"\n    end",
    "comment": "@param name [String] the name of the queue",
    "label": "",
    "id": "4296"
  },
  {
    "raw_code": "def size\n      Sidekiq.redis { |con| con.llen(@rname) }\n    end",
    "comment": "The current size of the queue within Redis. This value is real-time and can change between calls.  @return [Integer] the size",
    "label": "",
    "id": "4297"
  },
  {
    "raw_code": "def paused?\n      false\n    end",
    "comment": "@return [Boolean] if the queue is currently paused",
    "label": "",
    "id": "4298"
  },
  {
    "raw_code": "def latency\n      entry = Sidekiq.redis { |conn|\n        conn.lindex(@rname, -1)\n      }\n      return 0 unless entry\n      job = Sidekiq.load_json(entry)\n      now = Time.now.to_f\n      thence = job[\"enqueued_at\"] || now\n      now - thence\n    end",
    "comment": " Calculates this queue's latency, the difference in seconds since the oldest job in the queue was enqueued.  @return [Float] in seconds",
    "label": "",
    "id": "4299"
  },
  {
    "raw_code": "def find_job(jid)\n      detect { |j| j.jid == jid }\n    end",
    "comment": " Find the job with the given JID within this queue.  This is a *slow, inefficient* operation.  Do not use under normal conditions.  @param jid [String] the job_id to look for @return [Sidekiq::JobRecord] @return [nil] if not found",
    "label": "",
    "id": "4300"
  },
  {
    "raw_code": "def clear\n      Sidekiq.redis do |conn|\n        conn.multi do |transaction|\n          transaction.unlink(@rname)\n          transaction.srem(\"queues\", [name])\n        end",
    "comment": "delete all jobs within this queue @return [Boolean] true",
    "label": "",
    "id": "4301"
  },
  {
    "raw_code": "def as_json(options = nil)\n      {name: name} # 5336\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4302"
  },
  {
    "raw_code": "def initialize(item, queue_name = nil)\n      @args = nil\n      @value = item\n      @item = item.is_a?(Hash) ? item : parse(item)\n      @queue = queue_name || @item[\"queue\"]\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4303"
  },
  {
    "raw_code": "def parse(item)\n      Sidekiq.load_json(item)\n    rescue JSON::ParserError\n      # If the job payload in Redis is invalid JSON, we'll load\n      # the item as an empty hash and store the invalid JSON as\n      # the job 'args' for display in the Web UI.\n      @invalid = true\n      @args = [item]\n      {}\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4304"
  },
  {
    "raw_code": "def klass\n      self[\"class\"]\n    end",
    "comment": "This is the job class which Sidekiq will execute. If using ActiveJob, this class will be the ActiveJob adapter class rather than a specific job.",
    "label": "",
    "id": "4305"
  },
  {
    "raw_code": "def delete\n      count = Sidekiq.redis { |conn|\n        conn.lrem(\"queue:#{@queue}\", 1, @value)\n      }\n      count != 0\n    end",
    "comment": "Remove this job from the queue",
    "label": "",
    "id": "4306"
  },
  {
    "raw_code": "def [](name)\n      # nil will happen if the JSON fails to parse.\n      # We don't guarantee Sidekiq will work with bad job JSON but we should\n      # make a best effort to minimize the damage.\n      @item ? @item[name] : nil\n    end",
    "comment": "Access arbitrary attributes within the job hash",
    "label": "",
    "id": "4307"
  },
  {
    "raw_code": "def initialize(parent, score, item)\n      super(item)\n      @score = Float(score)\n      @parent = parent\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4308"
  },
  {
    "raw_code": "def at\n      Time.at(score).utc\n    end",
    "comment": "The timestamp associated with this entry",
    "label": "",
    "id": "4309"
  },
  {
    "raw_code": "def delete\n      if @value\n        @parent.delete_by_value(@parent.name, @value)\n      else\n        @parent.delete_by_jid(score, jid)\n      end",
    "comment": "remove this entry from the sorted set",
    "label": "",
    "id": "4310"
  },
  {
    "raw_code": "def reschedule(at)\n      Sidekiq.redis do |conn|\n        conn.zincrby(@parent.name, at.to_f - @score, Sidekiq.dump_json(@item))\n      end",
    "comment": "Change the scheduled time for this job.  @param at [Time] the new timestamp for this job",
    "label": "",
    "id": "4311"
  },
  {
    "raw_code": "def add_to_queue\n      remove_job do |message|\n        msg = Sidekiq.load_json(message)\n        Sidekiq::Client.push(msg)\n      end",
    "comment": "Enqueue this job from the scheduled or dead set so it will be executed at some point in the near future.",
    "label": "",
    "id": "4312"
  },
  {
    "raw_code": "def retry\n      remove_job do |message|\n        msg = Sidekiq.load_json(message)\n        msg[\"retry_count\"] -= 1 if msg[\"retry_count\"]\n        Sidekiq::Client.push(msg)\n      end",
    "comment": "enqueue this job from the retry set so it will be executed at some point in the near future.",
    "label": "",
    "id": "4313"
  },
  {
    "raw_code": "def kill\n      remove_job do |message|\n        DeadSet.new.kill(message)\n      end",
    "comment": "Move this job from its current set into the Dead set.",
    "label": "",
    "id": "4314"
  },
  {
    "raw_code": "def initialize(name)\n      @name = name\n      @_size = size\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4315"
  },
  {
    "raw_code": "def size\n      Sidekiq.redis { |c| c.zcard(name) }\n    end",
    "comment": "real-time size of the set, will change",
    "label": "",
    "id": "4316"
  },
  {
    "raw_code": "def scan(match, count = 100)\n      return to_enum(:scan, match, count) unless block_given?\n\n      match = \"*#{match}*\" unless match.include?(\"*\")\n      Sidekiq.redis do |conn|\n        conn.zscan(name, match: match, count: count) do |entry, score|\n          yield SortedEntry.new(self, score, entry)\n        end",
    "comment": "Scan through each element of the sorted set, yielding each to the supplied block. Please see Redis's <a href=\"https://redis.io/commands/scan/\">SCAN documentation</a> for implementation details.  @param match [String] a snippet or regexp to filter matches. @param count [Integer] number of elements to retrieve at a time, default 100 @yieldparam [Sidekiq::SortedEntry] each entry",
    "label": "",
    "id": "4317"
  },
  {
    "raw_code": "def clear\n      Sidekiq.redis do |conn|\n        conn.unlink(name)\n      end",
    "comment": "@return [Boolean] always true",
    "label": "",
    "id": "4318"
  },
  {
    "raw_code": "def as_json(options = nil)\n      {name: name} # 5336\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4319"
  },
  {
    "raw_code": "def schedule(timestamp, job)\n      Sidekiq.redis do |conn|\n        conn.zadd(name, timestamp.to_f.to_s, Sidekiq.dump_json(job))\n      end",
    "comment": "Add a job with the associated timestamp to this set. @param timestamp [Time] the score for the job @param job [Hash] the job data",
    "label": "",
    "id": "4320"
  },
  {
    "raw_code": "def kill_all(notify_failure: false, ex: nil)\n      ds = DeadSet.new\n      opts = {notify_failure: notify_failure, ex: ex, trim: false}\n\n      begin\n        pop_each do |msg, _|\n          ds.kill(msg, opts)\n        end",
    "comment": "Move all jobs from this Set to the Dead Set. See DeadSet#kill",
    "label": "",
    "id": "4321"
  },
  {
    "raw_code": "def fetch(score, jid = nil)\n      begin_score, end_score =\n        if score.is_a?(Range)\n          [score.first, score.last]\n        else\n          [score, score]\n        end",
    "comment": " Fetch jobs that match a given time or Range. Job ID is an optional second argument.  @param score [Time,Range] a specific timestamp or range @param jid [String, optional] find a specific JID within the score @return [Array<SortedEntry>] any results found, can be empty",
    "label": "",
    "id": "4322"
  },
  {
    "raw_code": "def find_job(jid)\n      Sidekiq.redis do |conn|\n        conn.zscan(name, match: \"*#{jid}*\", count: 100) do |entry, score|\n          job = Sidekiq.load_json(entry)\n          matched = job[\"jid\"] == jid\n          return SortedEntry.new(self, score, entry) if matched\n        end",
    "comment": " Find the job with the given JID within this sorted set. *This is a slow O(n) operation*.  Do not use for app logic.  @param jid [String] the job identifier @return [SortedEntry] the record or nil",
    "label": "",
    "id": "4323"
  },
  {
    "raw_code": "def delete_by_value(name, value)\n      Sidekiq.redis do |conn|\n        ret = conn.zrem(name, value)\n        @_size -= 1 if ret\n        ret\n      end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4324"
  },
  {
    "raw_code": "def delete_by_jid(score, jid)\n      Sidekiq.redis do |conn|\n        elements = conn.zrangebyscore(name, score, score)\n        elements.each do |element|\n          if element.index(jid)\n            message = Sidekiq.load_json(element)\n            if message[\"jid\"] == jid\n              ret = conn.zrem(name, element)\n              @_size -= 1 if ret\n              break ret\n            end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4325"
  },
  {
    "raw_code": "def trim\n      hash = Sidekiq.default_configuration\n      now = Time.now.to_f\n      Sidekiq.redis do |conn|\n        conn.multi do |transaction|\n          transaction.zremrangebyscore(name, \"-inf\", now - hash[:dead_timeout_in_seconds])\n          transaction.zremrangebyrank(name, 0, - hash[:dead_max_jobs])\n        end",
    "comment": "Trim dead jobs which are over our storage limits",
    "label": "",
    "id": "4326"
  },
  {
    "raw_code": "def kill(message, opts = {})\n      now = Time.now.to_f\n      Sidekiq.redis do |conn|\n        conn.zadd(name, now.to_s, message)\n      end",
    "comment": "Add the given job to the Dead set. @param message [String] the job data as JSON @option opts [Boolean] :notify_failure (true) Whether death handlers should be called @option opts [Boolean] :trim (true) Whether Sidekiq should trim the structure to keep it within configuration @option opts [Exception] :ex (RuntimeError) An exception to pass to the death handlers",
    "label": "",
    "id": "4327"
  },
  {
    "raw_code": "def initialize(clean_plz = true)\n      cleanup if clean_plz\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4328"
  },
  {
    "raw_code": "def cleanup\n      # dont run cleanup more than once per minute\n      return 0 unless Sidekiq.redis { |conn| conn.set(\"process_cleanup\", \"1\", \"NX\", \"EX\", \"60\") }\n\n      count = 0\n      Sidekiq.redis do |conn|\n        procs = conn.sscan(\"processes\").to_a\n        heartbeats = conn.pipelined { |pipeline|\n          procs.each do |key|\n            pipeline.hget(key, \"info\")\n          end",
    "comment": "Cleans up dead processes recorded in Redis. Returns the number of processes cleaned. :nodoc: @api private",
    "label": "",
    "id": "4329"
  },
  {
    "raw_code": "def size\n      Sidekiq.redis { |conn| conn.scard(\"processes\") }\n    end",
    "comment": "This method is not guaranteed accurate since it does not prune the set based on current heartbeat.  #each does that and ensures the set only contains Sidekiq processes which have sent a heartbeat within the last 60 seconds. @return [Integer] current number of registered Sidekiq processes",
    "label": "",
    "id": "4330"
  },
  {
    "raw_code": "def total_concurrency\n      sum { |x| x[\"concurrency\"].to_i }\n    end",
    "comment": "Total number of threads available to execute jobs. For Sidekiq Enterprise customers this number (in production) must be less than or equal to your licensed concurrency. @return [Integer] the sum of process concurrency",
    "label": "",
    "id": "4331"
  },
  {
    "raw_code": "def total_rss_in_kb\n      sum { |x| x[\"rss\"].to_i }\n    end",
    "comment": "@return [Integer] total amount of RSS memory consumed by Sidekiq processes",
    "label": "",
    "id": "4332"
  },
  {
    "raw_code": "def leader\n      @leader ||= begin\n        x = Sidekiq.redis { |c| c.get(\"dear-leader\") }\n        # need a non-falsy value so we can memoize\n        x ||= \"\"\n        x\n      end",
    "comment": "Returns the identity of the current cluster leader or \"\" if no leader. This is a Sidekiq Enterprise feature, will always return \"\" in Sidekiq or Sidekiq Pro. @return [String] Identity of cluster leader @return [String] empty string if no leader",
    "label": "",
    "id": "4333"
  },
  {
    "raw_code": "def initialize(hash)\n      @attribs = hash\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4334"
  },
  {
    "raw_code": "def quiet!\n      raise \"Can't quiet an embedded process\" if embedded?\n\n      signal(\"TSTP\")\n    end",
    "comment": "Signal this process to stop processing new jobs. It will continue to execute jobs it has already fetched. This method is *asynchronous* and it can take 5-10 seconds for the process to quiet.",
    "label": "",
    "id": "4335"
  },
  {
    "raw_code": "def stop!\n      raise \"Can't stop an embedded process\" if embedded?\n\n      signal(\"TERM\")\n    end",
    "comment": "Signal this process to shutdown. It will shutdown within its configured :timeout value, default 25 seconds. This method is *asynchronous* and it can take 5-10 seconds for the process to start shutting down.",
    "label": "",
    "id": "4336"
  },
  {
    "raw_code": "def dump_threads\n      signal(\"TTIN\")\n    end",
    "comment": "Signal this process to log backtraces for all threads. Useful if you have a frozen or deadlocked process which is still sending a heartbeat. This method is *asynchronous* and it can take 5-10 seconds.",
    "label": "",
    "id": "4337"
  },
  {
    "raw_code": "def stopping?\n      self[\"quiet\"] == \"true\"\n    end",
    "comment": "@return [Boolean] true if this process is quiet or shutting down",
    "label": "",
    "id": "4338"
  },
  {
    "raw_code": "def size\n      Sidekiq.redis do |conn|\n        procs = conn.sscan(\"processes\").to_a\n        if procs.empty?\n          0\n        else\n          conn.pipelined { |pipeline|\n            procs.each do |key|\n              pipeline.hget(key, \"busy\")\n            end",
    "comment": "Note that #size is only as accurate as Sidekiq's heartbeat, which happens every 5 seconds.  It is NOT real-time.  Not very efficient if you have lots of Sidekiq processes but the alternative is a global counter which can easily get out of sync with crashy processes.",
    "label": "",
    "id": "4339"
  },
  {
    "raw_code": "def find_work_by_jid(jid)\n      each do |_process_id, _thread_id, work|\n        job = work.job\n        return work if job.jid == jid\n      end",
    "comment": " Find the work which represents a job with the given JID. *This is a slow O(n) operation*.  Do not use for app logic.  @param jid [String] the job identifier @return [Sidekiq::Work] the work or nil",
    "label": "",
    "id": "4340"
  },
  {
    "raw_code": "def [](key)\n      kwargs = {uplevel: 1}\n      kwargs[:category] = :deprecated if RUBY_VERSION > \"3.0\" # TODO\n      warn(\"Direct access to `Sidekiq::Work` attributes is deprecated, please use `#payload`, `#queue`, `#run_at` or `#job` instead\", **kwargs)\n\n      @hsh[key]\n    end",
    "comment": "deprecated",
    "label": "",
    "id": "4341"
  },
  {
    "raw_code": "def raw(name)\n      @hsh[name]\n    end",
    "comment": ":nodoc: @api private",
    "label": "",
    "id": "4342"
  },
  {
    "raw_code": "def configure\n        yield self\n      end",
    "comment": "Forward compatibility with 8.0",
    "label": "",
    "id": "4343"
  },
  {
    "raw_code": "def self.register(extension, name: nil, tab: nil, index: nil, root_dir: nil, cache_for: 86400, asset_paths: nil)\n      tab = Array(tab)\n      index = Array(index)\n      tab.zip(index).each do |tab, index|\n        tabs[tab] = index\n      end",
    "comment": "Register a class as a Sidekiq Web UI extension. The class should provide one or more tabs which map to an index route. Options:  @param extension [Class] Class which contains the HTTP actions, required @param name [String] the name of the extension, used to namespace assets @param tab [String | Array] labels(s) of the UI tabs @param index [String | Array] index route(s) for each tab @param root_dir [String] directory location to find assets, locales and views, typically `web/` within the gemfile @param asset_paths [Array] one or more directories under {root}/assets/{name} to be publicly served, e.g. [\"js\", \"css\", \"img\"] @param cache_for [Integer] amount of time to cache assets, default one day  TODO name, tab and index will be mandatory in 8.0  Web extensions will have a root `web/` directory with `locales/`, `assets/` and `views/` subdirectories.",
    "label": "",
    "id": "4344"
  },
  {
    "raw_code": "def __set_test_mode(mode)\n        if block_given?\n          # Reentrant testing modes will lead to a rat's nest of code which is\n          # hard to reason about. You can set the testing mode once globally and\n          # you can override that global setting once per-thread.\n          raise TestModeAlreadySetError, \"Nesting test modes is not supported\" if __local_test_mode\n\n          self.__local_test_mode = mode\n          begin\n            yield\n          ensure\n            self.__local_test_mode = nil\n          end",
    "comment": "Calling without a block sets the global test mode, affecting all threads. Calling with a block only affects the current Thread.",
    "label": "",
    "id": "4345"
  },
  {
    "raw_code": "def queue\n        get_sidekiq_options[\"queue\"]\n      end",
    "comment": "Queue for this worker",
    "label": "",
    "id": "4346"
  },
  {
    "raw_code": "def jobs\n        Queues.jobs_by_class[to_s]\n      end",
    "comment": "Jobs queued for this worker",
    "label": "",
    "id": "4347"
  },
  {
    "raw_code": "def clear\n        Queues.clear_for(queue, to_s)\n      end",
    "comment": "Clear all jobs for this worker",
    "label": "",
    "id": "4348"
  },
  {
    "raw_code": "def drain\n        while jobs.any?\n          next_job = jobs.first\n          Queues.delete_for(next_job[\"jid\"], next_job[\"queue\"], to_s)\n          process_job(next_job)\n        end",
    "comment": "Drain and run all jobs for this worker",
    "label": "",
    "id": "4349"
  },
  {
    "raw_code": "def perform_one\n        raise(EmptyQueueError, \"perform_one called with empty job queue\") if jobs.empty?\n        next_job = jobs.first\n        Queues.delete_for(next_job[\"jid\"], next_job[\"queue\"], to_s)\n        process_job(next_job)\n      end",
    "comment": "Pop out a single job and perform it",
    "label": "",
    "id": "4350"
  },
  {
    "raw_code": "def clear_all\n        Queues.clear_all\n      end",
    "comment": "Clear all queued jobs",
    "label": "",
    "id": "4351"
  },
  {
    "raw_code": "def drain_all\n        while jobs.any?\n          job_classes = jobs.map { |job| job[\"class\"] }.uniq\n\n          job_classes.each do |job_class|\n            Object.const_get(job_class).drain\n          end",
    "comment": "Drain (execute) all queued jobs",
    "label": "",
    "id": "4352"
  },
  {
    "raw_code": "def log_at(level)\n      old_local_level = local_level\n      self.local_level = level\n      yield\n    ensure\n      self.local_level = old_local_level\n    end",
    "comment": "Change the thread-local level for the duration of the given block.",
    "label": "",
    "id": "4353"
  },
  {
    "raw_code": "def terminate\n        @done = true\n        @enq.terminate\n\n        @sleeper << 0\n        @thread&.value\n      end",
    "comment": "Shut down this instance, will pause until the thread is dead.",
    "label": "",
    "id": "4354"
  },
  {
    "raw_code": "def poll_interval_average(count)\n        @config[:poll_interval_average] || scaled_poll_interval(count)\n      end",
    "comment": "We do our best to tune the poll interval to the size of the active Sidekiq cluster.  If you have 30 processes and poll every 15 seconds, that means one Sidekiq is checking Redis every 0.5 seconds - way too often for most people and really bad if the retry or scheduled sets are large.  Instead try to avoid polling more than once every 15 seconds.  If you have 30 Sidekiq processes, we'll poll every 30 * 15 or 450 seconds. To keep things statistically random, we'll sleep a random amount between 225 and 675 seconds for each poll or 450 seconds on average.  Otherwise restarting all your Sidekiq processes at the same time will lead to them all polling at the same time: the thundering herd problem.  We only do this if poll_interval_average is unset (the default).",
    "label": "",
    "id": "4355"
  },
  {
    "raw_code": "def scaled_poll_interval(process_count)\n        process_count * @config[:average_scheduled_poll_interval]\n      end",
    "comment": "Calculates an average poll interval based on the number of known Sidekiq processes. This minimizes a single point of failure by dispersing check-ins but without taxing Redis if you run many Sidekiq processes.",
    "label": "",
    "id": "4356"
  },
  {
    "raw_code": "def cleanup\n        # dont run cleanup more than once per minute\n        return 0 unless redis { |conn| conn.set(\"process_cleanup\", \"1\", \"NX\", \"EX\", \"60\") }\n\n        count = 0\n        redis do |conn|\n          procs = conn.sscan(\"processes\").to_a\n          heartbeats = conn.pipelined { |pipeline|\n            procs.each do |key|\n              pipeline.hget(key, \"info\")\n            end",
    "comment": "A copy of Sidekiq::ProcessSet#cleanup because server should never depend on sidekiq/api.",
    "label": "",
    "id": "4357"
  },
  {
    "raw_code": "def sidekiq_options(opts = {})\n          # stringify 2 levels of keys\n          opts = opts.to_h do |k, v|\n            [k.to_s, (Hash === v) ? v.transform_keys(&:to_s) : v]\n          end",
    "comment": " Allows customization for this type of Job. Legal options:  queue - name of queue to use for this job type, default *default* retry - enable retries for this Job in case of error during execution, *true* to use the default or *Integer* count backtrace - whether to save any error backtrace in the retry payload to display in web UI, can be true, false or an integer number of lines to save, default *false*  In practice, any option is allowed.  This is the main mechanism to configure the options for a specific job.",
    "label": "",
    "id": "4358"
  },
  {
    "raw_code": "def perform_inline(*args)\n        raw = @opts.merge(\"args\" => args, \"class\" => @klass)\n\n        # validate and normalize payload\n        item = normalize_item(raw)\n        queue = item[\"queue\"]\n\n        # run client-side middleware\n        cfg = Sidekiq.default_configuration\n        result = cfg.client_middleware.invoke(item[\"class\"], item, queue, cfg.redis_pool) do\n          item\n        end",
    "comment": "Explicit inline execution of a job. Returns nil if the job did not execute, true otherwise.",
    "label": "",
    "id": "4359"
  },
  {
    "raw_code": "def perform_in(interval, *args)\n        at(interval).perform_async(*args)\n      end",
    "comment": "+interval+ must be a timestamp, numeric or something that acts numeric (like an activesupport time interval).",
    "label": "",
    "id": "4360"
  },
  {
    "raw_code": "def perform_inline(*args)\n        Setter.new(self, {}).perform_inline(*args)\n      end",
    "comment": "Inline execution of job's perform method after passing through Sidekiq.client_middleware and Sidekiq.server_middleware",
    "label": "",
    "id": "4361"
  },
  {
    "raw_code": "def perform_bulk(*args, **kwargs)\n        Setter.new(self, {}).perform_bulk(*args, **kwargs)\n      end",
    "comment": " Push a large number of jobs to Redis, while limiting the batch of each job payload to 1,000. This method helps cut down on the number of round trips to Redis, which can increase the performance of enqueueing large numbers of jobs.  +items+ must be an Array of Arrays.  For finer-grained control, use `Sidekiq::Client.push_bulk` directly.  Example (3 Redis round trips):  SomeJob.perform_async(1) SomeJob.perform_async(2) SomeJob.perform_async(3)  Would instead become (1 Redis round trip):  SomeJob.perform_bulk([[1], [2], [3]]) ",
    "label": "",
    "id": "4362"
  },
  {
    "raw_code": "def perform_in(interval, *args)\n        int = interval.to_f\n        now = Time.now.to_f\n        ts = ((int < 1_000_000_000) ? now + int : int)\n\n        item = {\"class\" => self, \"args\" => args}\n\n        # Optimization to enqueue something now that is scheduled to go out now or in the past\n        item[\"at\"] = ts if ts > now\n\n        client_push(item)\n      end",
    "comment": "+interval+ must be a timestamp, numeric or something that acts numeric (like an activesupport time interval).",
    "label": "",
    "id": "4363"
  },
  {
    "raw_code": "def sidekiq_options(opts = {})\n        super\n      end",
    "comment": " Allows customization for this type of Job. Legal options:  queue - use a named queue for this Job, default 'default' retry - enable the RetryJobs middleware for this Job, *true* to use the default or *Integer* count backtrace - whether to save any error backtrace in the retry payload to display in web UI, can be true, false or an integer number of lines to save, default *false* pool - use the given Redis connection pool to push this type of job to a given shard.  In practice, any option is allowed.  This is the main mechanism to configure the options for a specific job.",
    "label": "",
    "id": "4364"
  },
  {
    "raw_code": "def concurrency=(val)\n      default_capsule.concurrency = Integer(val)\n    end",
    "comment": "LEGACY: edits the default capsule config.concurrency = 5",
    "label": "",
    "id": "4365"
  },
  {
    "raw_code": "def queues=(val)\n      default_capsule.queues = val\n    end",
    "comment": "Edit the default capsule. config.queues = %w( high default low )                 # strict config.queues = %w( high,3 default,2 low,1 )           # weighted config.queues = %w( feature1,1 feature2,1 feature3,1 ) # random  With weighted priority, queue will be checked first (weight / total) of the time. high will be checked first (3/6) or 50% of the time. I'd recommend setting weights between 1-10. Weights in the hundreds or thousands are ridiculous and unnecessarily expensive. You can get random queue ordering by explicitly setting all weights to 1.",
    "label": "",
    "id": "4366"
  },
  {
    "raw_code": "def capsule(name)\n      nm = name.to_s\n      cap = @capsules.fetch(nm) do\n        cap = Sidekiq::Capsule.new(nm, self)\n        @capsules[nm] = cap\n      end",
    "comment": "register a new queue processing subsystem",
    "label": "",
    "id": "4367"
  },
  {
    "raw_code": "def redis=(hash)\n      @redis_config = @redis_config.merge(hash)\n    end",
    "comment": "All capsules must use the same Redis configuration",
    "label": "",
    "id": "4368"
  },
  {
    "raw_code": "def register(name, instance)\n      # logger.debug(\"register[#{name}] = #{instance}\")\n      # Sidekiq Enterprise lazy registers a few services so we\n      # can't lock down this hash completely.\n      hash = @directory.dup\n      hash[name] = instance\n      @directory = hash.freeze\n      instance\n    end",
    "comment": "register global singletons which can be accessed elsewhere",
    "label": "",
    "id": "4369"
  },
  {
    "raw_code": "def lookup(name, default_class = nil)\n      # JNDI is just a fancy name for a hash lookup\n      @directory.fetch(name) do |key|\n        return nil unless default_class\n        register(key, default_class.new(self))\n      end",
    "comment": "find a singleton",
    "label": "",
    "id": "4370"
  },
  {
    "raw_code": "def death_handlers\n      @options[:death_handlers]\n    end",
    "comment": " Death handlers are called when all retries for a job have been exhausted and the job dies.  It's the notification to your application that this job will not succeed without manual intervention.  Sidekiq.configure_server do |config| config.death_handlers << ->(job, ex) do end end",
    "label": "",
    "id": "4371"
  },
  {
    "raw_code": "def average_scheduled_poll_interval=(interval)\n      @options[:average_scheduled_poll_interval] = interval\n    end",
    "comment": "How frequently Redis should be checked by a random Sidekiq process for scheduled and retriable jobs. Each individual process will take turns by waiting some multiple of this value.  See sidekiq/scheduled.rb for an in-depth explanation of this value",
    "label": "",
    "id": "4372"
  },
  {
    "raw_code": "def error_handlers\n      @options[:error_handlers]\n    end",
    "comment": "Register a proc to handle any error which occurs within the Sidekiq process.  Sidekiq.configure_server do |config| config.error_handlers << proc {|ex,ctx_hash| MyErrorService.notify(ex, ctx_hash) } end  The default error handler logs errors to @logger.",
    "label": "",
    "id": "4373"
  },
  {
    "raw_code": "def on(event, &block)\n      raise ArgumentError, \"Symbols only please: #{event}\" unless event.is_a?(Symbol)\n      raise ArgumentError, \"Invalid event name: #{event}\" unless @options[:lifecycle_events].key?(event)\n      @options[:lifecycle_events][event] << block\n    end",
    "comment": "Register a block to run at a point in the Sidekiq lifecycle. :startup, :quiet or :shutdown are valid events.  Sidekiq.configure_server do |config| config.on(:shutdown) do puts \"Goodbye cruel world!\" end end",
    "label": "",
    "id": "4374"
  },
  {
    "raw_code": "def handle_exception(ex, ctx = {})\n      if @options[:error_handlers].size == 0\n        p [\"!!!!!\", ex]\n      end",
    "comment": "INTERNAL USE ONLY",
    "label": "",
    "id": "4375"
  },
  {
    "raw_code": "def queues_cmd\n      if @strictly_ordered_queues\n        @queues\n      else\n        permute = @queues.shuffle\n        permute.uniq!\n        permute\n      end",
    "comment": "Creating the Redis#brpop command takes into account any configured queue weights. By default Redis#brpop returns data from the first queue that has pending elements. We recreate the queue command each time we invoke Redis#brpop to honor weights and avoid queue starvation.",
    "label": "",
    "id": "4376"
  },
  {
    "raw_code": "def self.status(status, unset_env = false)\n      notify(\"#{STATUS}#{status}\", unset_env)\n    end",
    "comment": "@param status [String] a custom status string that describes the current state of the service",
    "label": "",
    "id": "4377"
  },
  {
    "raw_code": "def self.errno(errno, unset_env = false)\n      notify(\"#{ERRNO}#{errno}\", unset_env)\n    end",
    "comment": "@param errno [Integer]",
    "label": "",
    "id": "4378"
  },
  {
    "raw_code": "def self.mainpid(pid, unset_env = false)\n      notify(\"#{MAINPID}#{pid}\", unset_env)\n    end",
    "comment": "@param pid [Integer]",
    "label": "",
    "id": "4379"
  },
  {
    "raw_code": "def self.watchdog?\n      wd_usec = ENV[\"WATCHDOG_USEC\"]\n      wd_pid = ENV[\"WATCHDOG_PID\"]\n\n      return false unless wd_usec\n\n      begin\n        wd_usec = Integer(wd_usec)\n      rescue\n        return false\n      end",
    "comment": "@return [Boolean] true if the service manager expects watchdog keep-alive notification messages to be sent from this process.  If the $WATCHDOG_USEC environment variable is set, and the $WATCHDOG_PID variable is unset or set to the PID of the current process  @note Unlike sd_watchdog_enabled(3), this method does not mutate the environment.",
    "label": "",
    "id": "4380"
  },
  {
    "raw_code": "def self.notify(state, unset_env = false)\n      sock = ENV[\"NOTIFY_SOCKET\"]\n\n      return nil unless sock\n\n      ENV.delete(\"NOTIFY_SOCKET\") if unset_env\n\n      begin\n        Addrinfo.unix(sock, :DGRAM).connect do |s|\n          s.close_on_exec = true\n          s.write(state)\n        end",
    "comment": "Notify systemd with the provided state, via the notification socket, if any.  Generally this method will be used indirectly through the other methods of the library.  @param state [String] @param unset_env [Boolean]  @return [Fixnum, nil] the number of bytes written to the notification socket or nil if there was no socket to report to (eg. the program wasn't started by systemd)  @raise [NotifyError] if there was an error communicating with the systemd socket  @see https://www.freedesktop.org/software/systemd/man/sd_notify.html",
    "label": "",
    "id": "4381"
  },
  {
    "raw_code": "def each(&block)\n        entries.each(&block)\n      end",
    "comment": "Iterate through each middleware in the chain",
    "label": "",
    "id": "4382"
  },
  {
    "raw_code": "def initialize(config = nil) # :nodoc:\n        @config = config\n        @entries = nil\n        yield self if block_given?\n      end",
    "comment": "@api private",
    "label": "",
    "id": "4383"
  },
  {
    "raw_code": "def remove(klass)\n        entries.delete_if { |entry| entry.klass == klass }\n      end",
    "comment": "Remove all middleware matching the given Class @param klass [Class]",
    "label": "",
    "id": "4384"
  },
  {
    "raw_code": "def add(klass, *args)\n        remove(klass)\n        entries << Entry.new(@config, klass, *args)\n      end",
    "comment": "Add the given middleware to the end of the chain. Sidekiq will call `klass.new(*args)` to create a clean copy of your middleware for every job executed.  chain.add(Statsd::Metrics, { collector: \"localhost:8125\" })  @param klass [Class] Your middleware class @param *args [Array<Object>] Set of arguments to pass to every instance of your middleware",
    "label": "",
    "id": "4385"
  },
  {
    "raw_code": "def prepend(klass, *args)\n        remove(klass)\n        entries.insert(0, Entry.new(@config, klass, *args))\n      end",
    "comment": "Identical to {#add} except the middleware is added to the front of the chain.",
    "label": "",
    "id": "4386"
  },
  {
    "raw_code": "def insert_before(oldklass, newklass, *args)\n        i = entries.index { |entry| entry.klass == newklass }\n        new_entry = i.nil? ? Entry.new(@config, newklass, *args) : entries.delete_at(i)\n        i = entries.index { |entry| entry.klass == oldklass } || 0\n        entries.insert(i, new_entry)\n      end",
    "comment": "Inserts +newklass+ before +oldklass+ in the chain. Useful if one middleware must run before another middleware.",
    "label": "",
    "id": "4387"
  },
  {
    "raw_code": "def insert_after(oldklass, newklass, *args)\n        i = entries.index { |entry| entry.klass == newklass }\n        new_entry = i.nil? ? Entry.new(@config, newklass, *args) : entries.delete_at(i)\n        i = entries.index { |entry| entry.klass == oldklass } || entries.count - 1\n        entries.insert(i + 1, new_entry)\n      end",
    "comment": "Inserts +newklass+ after +oldklass+ in the chain. Useful if one middleware must run after another middleware.",
    "label": "",
    "id": "4388"
  },
  {
    "raw_code": "def exists?(klass)\n        any? { |entry| entry.klass == klass }\n      end",
    "comment": "@return [Boolean] if the given class is already in the chain",
    "label": "",
    "id": "4389"
  },
  {
    "raw_code": "def empty?\n        @entries.nil? || @entries.empty?\n      end",
    "comment": "@return [Boolean] if the chain contains no middleware",
    "label": "",
    "id": "4390"
  },
  {
    "raw_code": "def invoke(*args, &block)\n        return yield if empty?\n\n        chain = retrieve\n        traverse(chain, 0, args, &block)\n      end",
    "comment": "Used by Sidekiq to execute the middleware at runtime @api private",
    "label": "",
    "id": "4391"
  },
  {
    "raw_code": "def top_jobs(class_filter: nil, minutes: 60)\n        result = Result.new\n\n        time = @time\n        redis_results = @pool.with do |conn|\n          conn.pipelined do |pipe|\n            minutes.times do |idx|\n              key = \"j|#{time.strftime(\"%Y%m%d\")}|#{time.hour}:#{time.min}\"\n              pipe.hgetall key\n              result.prepend_bucket time\n              time -= 60\n            end",
    "comment": "Get metric data for all jobs from the last hour +class_filter+: return only results for classes matching filter",
    "label": "",
    "id": "4392"
  },
  {
    "raw_code": "def params\n      indifferent_hash = Hash.new { |hash, key| hash[key.to_s] if Symbol === key }\n\n      indifferent_hash.merge! request.params\n      route_params.each { |k, v| indifferent_hash[k.to_s] = v }\n\n      indifferent_hash\n    end",
    "comment": "deprecated, will warn in 8.0",
    "label": "",
    "id": "4393"
  },
  {
    "raw_code": "def url_params(key)\n      request.params[key]\n    end",
    "comment": "Use like `url_params(\"page\")` within your action blocks",
    "label": "",
    "id": "4394"
  },
  {
    "raw_code": "def route_params(key = nil)\n      if key\n        env[WebRouter::ROUTE_PARAMS][key]\n      else\n        env[WebRouter::ROUTE_PARAMS]\n      end",
    "comment": "Use like `route_params(:name)` within your action blocks key is required in 8.0, nil is only used for backwards compatibility",
    "label": "",
    "id": "4395"
  },
  {
    "raw_code": "def add_to_head\n      @head_html ||= []\n      @head_html << yield.dup if block_given?\n    end",
    "comment": "This view helper provide ability display you html code in to head of page. Example:  <% add_to_head do %> <link rel=\"stylesheet\" .../> <meta .../> <% end %> ",
    "label": "",
    "id": "4396"
  },
  {
    "raw_code": "def user_preferred_languages\n      languages = env[\"HTTP_ACCEPT_LANGUAGE\"]\n      languages.to_s.downcase.gsub(/\\s+/, \"\").split(\",\").map { |language|\n        locale, quality = language.split(\";q=\", 2)\n        locale = nil if locale == \"*\" # Ignore wildcards\n        quality = quality ? quality.to_f : 1.0\n        [locale, quality]\n      }.sort { |(_, left), (_, right)|\n        right <=> left\n      }.map(&:first).compact\n    end",
    "comment": "See https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4",
    "label": "",
    "id": "4397"
  },
  {
    "raw_code": "def locale\n      # session[:locale] is set via the locale selector from the footer\n      @locale ||= if (l = session&.fetch(:locale, nil)) && available_locales.include?(l)\n        l\n      else\n        matched_locale = user_preferred_languages.map { |preferred|\n          preferred_language = preferred.split(\"-\", 2).first\n\n          lang_group = available_locales.select { |available|\n            preferred_language == available.split(\"-\", 2).first\n          }\n\n          lang_group.find { |lang| lang == preferred } || lang_group.min_by(&:length)\n        }.compact.first\n\n        matched_locale || \"en\"\n      end",
    "comment": "Given an Accept-Language header like \"fr-FR,fr;q=0.8,en-US;q=0.6,en;q=0.4,ru;q=0.2\" this method will try to best match the available locales to the user's preferred languages.  Inspiration taken from https://github.com/iain/http_accept_language/blob/master/lib/http_accept_language/parser.rb",
    "label": "",
    "id": "4398"
  },
  {
    "raw_code": "def unfiltered?\n      s = url_params(\"substr\")\n      yield unless s && s.size > 0\n    end",
    "comment": "sidekiq/sidekiq#3243",
    "label": "",
    "id": "4399"
  },
  {
    "raw_code": "def sorted_processes\n      @sorted_processes ||= begin\n        return processes unless processes.all? { |p| p[\"hostname\"] }\n\n        processes.to_a.sort_by do |process|\n          # Kudos to `shurikk` on StackOverflow\n          # https://stackoverflow.com/a/15170063/575547\n          process[\"hostname\"].split(/(\\d+)/).map { |a| /\\d+/.match?(a) ? a.to_i : a }\n        end",
    "comment": "Sorts processes by hostname following the natural sort order",
    "label": "",
    "id": "4400"
  },
  {
    "raw_code": "def qparams(options)\n      stringified_options = options.transform_keys(&:to_s)\n\n      to_query_string(params.merge(stringified_options))\n    end",
    "comment": "Merge options with current params, filter safe params, and stringify to query string",
    "label": "",
    "id": "4401"
  },
  {
    "raw_code": "def redirect_with_query(url)\n      r = request.referer\n      if r && r =~ /\\?/\n        ref = URI(r)\n        redirect(\"#{url}?#{ref.query}\")\n      else\n        redirect url\n      end",
    "comment": "Any paginated list that performs an action needs to redirect back to the proper page after performing that action.",
    "label": "",
    "id": "4402"
  },
  {
    "raw_code": "def valid_token?(env, giventoken)\n        return false if giventoken.nil? || giventoken.empty?\n\n        begin\n          token = decode_token(giventoken)\n        rescue ArgumentError # client input is invalid\n          return false\n        end",
    "comment": "Checks that the token given to us as a parameter matches the token stored in the session.",
    "label": "",
    "id": "4403"
  },
  {
    "raw_code": "def mask_token(token)\n        token = decode_token(token)\n        one_time_pad = SecureRandom.random_bytes(token.length)\n        encrypted_token = xor_byte_strings(one_time_pad, token)\n        masked_token = one_time_pad + encrypted_token\n        encode_token(masked_token)\n      end",
    "comment": "Creates a masked version of the authenticity token that varies on each request. The masking is used to mitigate SSL attacks like BREACH.",
    "label": "",
    "id": "4404"
  },
  {
    "raw_code": "def unmask_token(masked_token)\n        # Split the token into the one-time pad and the encrypted\n        # value and decrypt it\n        token_length = masked_token.length / 2\n        one_time_pad = masked_token[0...token_length]\n        encrypted_token = masked_token[token_length..]\n        xor_byte_strings(one_time_pad, encrypted_token)\n      end",
    "comment": "Essentially the inverse of +mask_token+.",
    "label": "",
    "id": "4405"
  },
  {
    "raw_code": "def self.included(base)\n        base.extend(ClassMethods)\n      end",
    "comment": "@api private",
    "label": "",
    "id": "4406"
  },
  {
    "raw_code": "def initialize\n        super\n\n        @_executions = 0\n        @_cursor = nil\n        @_start_time = nil\n        @_runtime = 0\n        @_args = nil\n        @_cancelled = nil\n      end",
    "comment": "@api private",
    "label": "",
    "id": "4407"
  },
  {
    "raw_code": "def cancel!\n        return @_cancelled if cancelled?\n\n        key = \"it-#{jid}\"\n        _, result, _ = Sidekiq.redis do |c|\n          c.pipelined do |p|\n            p.hsetnx(key, \"cancelled\", Time.now.to_i)\n            p.hget(key, \"cancelled\")\n            # TODO When Redis 7.2 is required\n            # p.expire(key, Sidekiq::Job::Iterable::STATE_TTL, \"nx\")\n            p.expire(key, Sidekiq::Job::Iterable::STATE_TTL)\n          end",
    "comment": "Set a flag in Redis to mark this job as cancelled. Cancellation is asynchronous and is checked at the start of iteration and every 5 seconds thereafter as part of the recurring state flush.",
    "label": "",
    "id": "4408"
  },
  {
    "raw_code": "def on_start\n      end",
    "comment": "A hook to override that will be called when the job starts iterating.  It is called only once, for the first time. ",
    "label": "",
    "id": "4409"
  },
  {
    "raw_code": "def around_iteration\n        yield\n      end",
    "comment": "A hook to override that will be called around each iteration.  Can be useful for some metrics collection, performance tracking etc. ",
    "label": "",
    "id": "4410"
  },
  {
    "raw_code": "def on_resume\n      end",
    "comment": "A hook to override that will be called when the job resumes iterating. ",
    "label": "",
    "id": "4411"
  },
  {
    "raw_code": "def on_stop\n      end",
    "comment": "A hook to override that will be called each time the job is interrupted.  This can be due to interruption or sidekiq stopping. ",
    "label": "",
    "id": "4412"
  },
  {
    "raw_code": "def on_complete\n      end",
    "comment": "A hook to override that will be called when the job finished iterating. ",
    "label": "",
    "id": "4413"
  },
  {
    "raw_code": "def build_enumerator(*)\n        raise NotImplementedError, \"#{self.class.name} must implement a '#build_enumerator' method\"\n      end",
    "comment": "The enumerator to be iterated over.  @return [Enumerator]  @raise [NotImplementedError] with a message advising subclasses to implement an override for this method. ",
    "label": "",
    "id": "4414"
  },
  {
    "raw_code": "def each_iteration(*)\n        raise NotImplementedError, \"#{self.class.name} must implement an '#each_iteration' method\"\n      end",
    "comment": "The action to be performed on each item from the enumerator.  @return [void]  @raise [NotImplementedError] with a message advising subclasses to implement an override for this method. ",
    "label": "",
    "id": "4415"
  },
  {
    "raw_code": "def perform(*args)\n        @_args = args.dup.freeze\n        fetch_previous_iteration_state\n\n        @_executions += 1\n        @_start_time = ::Process.clock_gettime(::Process::CLOCK_MONOTONIC)\n\n        enumerator = build_enumerator(*args, cursor: @_cursor)\n        unless enumerator\n          logger.info(\"'#build_enumerator' returned nil, skipping the job.\")\n          return\n        end",
    "comment": "@api private",
    "label": "",
    "id": "4416"
  },
  {
    "raw_code": "def array_enumerator(array, cursor:)\n          raise ArgumentError, \"array must be an Array\" unless array.is_a?(Array)\n\n          x = array.each_with_index.drop(cursor || 0)\n          x.to_enum { x.size }\n        end",
    "comment": "Builds Enumerator object from a given array, using +cursor+ as an offset.  @param array [Array] @param cursor [Integer] offset to start iteration from  @return [Enumerator]  @example array_enumerator(['build', 'enumerator', 'from', 'any', 'array'], cursor: cursor) ",
    "label": "",
    "id": "4417"
  },
  {
    "raw_code": "def active_record_records_enumerator(relation, cursor:, **options)\n          ActiveRecordEnumerator.new(relation, cursor: cursor, **options).records\n        end",
    "comment": "Builds Enumerator from `ActiveRecord::Relation`. Each Enumerator tick moves the cursor one row forward.  @param relation [ActiveRecord::Relation] relation to iterate @param cursor [Object] offset id to start iteration from @param options [Hash] additional options that will be passed to relevant ActiveRecord batching methods  @return [ActiveRecordEnumerator]  @example def build_enumerator(cursor:) active_record_records_enumerator(User.all, cursor: cursor) end  def each_iteration(user) user.notify_about_something end ",
    "label": "",
    "id": "4418"
  },
  {
    "raw_code": "def active_record_batches_enumerator(relation, cursor:, **options)\n          ActiveRecordEnumerator.new(relation, cursor: cursor, **options).batches\n        end",
    "comment": "Builds Enumerator from `ActiveRecord::Relation` and enumerates on batches of records. Each Enumerator tick moves the cursor `:batch_size` rows forward. @see #active_record_records_enumerator  @example def build_enumerator(product_id, cursor:) active_record_batches_enumerator( Comment.where(product_id: product_id).select(:id), cursor: cursor, batch_size: 100 ) end  def each_iteration(batch_of_comments, product_id) comment_ids = batch_of_comments.map(&:id) CommentService.call(comment_ids: comment_ids) end ",
    "label": "",
    "id": "4419"
  },
  {
    "raw_code": "def active_record_relations_enumerator(relation, cursor:, **options)\n          ActiveRecordEnumerator.new(relation, cursor: cursor, **options).relations\n        end",
    "comment": "Builds Enumerator from `ActiveRecord::Relation` and enumerates on batches, yielding `ActiveRecord::Relation`s. @see #active_record_records_enumerator  @example def build_enumerator(product_id, cursor:) active_record_relations_enumerator( Product.find(product_id).comments, cursor: cursor, batch_size: 100, ) end  def each_iteration(batch_of_comments, product_id) # batch_of_comments will be a Comment::ActiveRecord_Relation batch_of_comments.update_all(deleted: true) end ",
    "label": "",
    "id": "4420"
  },
  {
    "raw_code": "def csv_enumerator(csv, cursor:)\n          CsvEnumerator.new(csv).rows(cursor: cursor)\n        end",
    "comment": "Builds Enumerator from a CSV file.  @param csv [CSV] an instance of CSV object @param cursor [Integer] offset to start iteration from  @example def build_enumerator(import_id, cursor:) import = Import.find(import_id) csv_enumerator(import.csv, cursor: cursor) end  def each_iteration(csv_row) # insert csv_row into database end ",
    "label": "",
    "id": "4421"
  },
  {
    "raw_code": "def csv_batches_enumerator(csv, cursor:, **options)\n          CsvEnumerator.new(csv).batches(cursor: cursor, **options)\n        end",
    "comment": "Builds Enumerator from a CSV file and enumerates on batches of records.  @param csv [CSV] an instance of CSV object @param cursor [Integer] offset to start iteration from @option options :batch_size [Integer] (100) size of the batch  @example def build_enumerator(import_id, cursor:) import = Import.find(import_id) csv_batches_enumerator(import.csv, cursor: cursor) end  def each_iteration(batch_of_csv_rows) # ... end ",
    "label": "",
    "id": "4422"
  },
  {
    "raw_code": "def enqueue_after_transaction_commit?\n        true\n      end",
    "comment": "Defines whether enqueuing should happen implicitly to after commit when called from inside a transaction. @api private",
    "label": "",
    "id": "4423"
  },
  {
    "raw_code": "def enqueue(job)\n        job.provider_job_id = JobWrapper.set(\n          wrapped: job.class,\n          queue: job.queue_name\n        ).perform_async(job.serialize)\n      end",
    "comment": "@api private",
    "label": "",
    "id": "4424"
  },
  {
    "raw_code": "def enqueue_at(job, timestamp)\n        job.provider_job_id = JobWrapper.set(\n          wrapped: job.class,\n          queue: job.queue_name\n        ).perform_at(timestamp, job.serialize)\n      end",
    "comment": "@api private",
    "label": "",
    "id": "4425"
  },
  {
    "raw_code": "def enqueue_all(jobs)\n        enqueued_count = 0\n        jobs.group_by(&:class).each do |job_class, same_class_jobs|\n          same_class_jobs.group_by(&:queue_name).each do |queue, same_class_and_queue_jobs|\n            immediate_jobs, scheduled_jobs = same_class_and_queue_jobs.partition { |job| job.scheduled_at.nil? }\n\n            if immediate_jobs.any?\n              jids = Sidekiq::Client.push_bulk(\n                \"class\" => JobWrapper,\n                \"wrapped\" => job_class,\n                \"queue\" => queue,\n                \"args\" => immediate_jobs.map { |job| [job.serialize] }\n              )\n              enqueued_count += jids.compact.size\n            end",
    "comment": "@api private",
    "label": "",
    "id": "4426"
  },
  {
    "raw_code": "def password_digest(password)\n        remove_instance_variable('@split_encrypted_password') if defined?(@split_encrypted_password)\n\n        encryptor_class.digest(password, encryptor_class::STRETCHES, Devise.friendly_token[0, 16])\n      end",
    "comment": "Used by Devise DatabaseAuthenticatable when setting a password",
    "label": "",
    "id": "4427"
  },
  {
    "raw_code": "def encryptor_class\n          @encryptor_class ||= case encryptor\n                               when :bcrypt\n                                 raise \"In order to use bcrypt as encryptor, simply remove :pbkdf2_encryptable from your devise model\"\n                               when nil\n                                 raise \"You need to specify an :encryptor in Devise configuration in order to use :pbkdf2_encryptable\"\n                               else\n                                 Devise::Pbkdf2Encryptable::Encryptors.const_get(encryptor.to_s.classify)\n                               end",
    "comment": "Returns the class for the configured encryptor.",
    "label": "",
    "id": "4428"
  },
  {
    "raw_code": "def generate_otp_backup_codes_pbkdf2!\n        codes           = []\n        number_of_codes = self.class.otp_number_of_backup_codes\n        code_length     = self.class.otp_backup_code_length\n\n        number_of_codes.times do\n          codes << SecureRandom.hex(code_length / 2) # Hexstring has length 2*n\n        end",
    "comment": "1) Invalidates all existing backup codes 2) Generates otp_number_of_backup_codes backup codes 3) Stores the hashed backup codes in the database 4) Returns a plaintext array of the generated backup codes ",
    "label": "",
    "id": "4429"
  },
  {
    "raw_code": "def invalidate_otp_backup_code_pdkdf2!(code)\n        codes = self.otp_backup_codes || []\n\n        codes.each do |backup_code|\n          next unless Devise::Pbkdf2Encryptable::Encryptors::Pbkdf2Sha512.compare(backup_code, code)\n\n          codes.delete(backup_code)\n          self.otp_backup_codes = codes\n          return true\n        end",
    "comment": "Returns true and invalidates the given code if that code is a valid backup code. ",
    "label": "",
    "id": "4430"
  },
  {
    "raw_code": "def make_authentication_request_body(password)\n          request_body = Nokogiri::XML(AUTHENTICATION_REQUEST_BODY)\n          password_value = request_body.at_css \"value\"\n          password_value.content = password\n          return request_body.root.to_s # return the body without the xml header\n        end",
    "comment": "create the body using Nokogiri so proper encoding of passwords can be ensured",
    "label": "",
    "id": "4431"
  },
  {
    "raw_code": "def initialize(params)\n          parse_params params\n        end",
    "comment": "@param [Hash] params configuration options @option params [String, nil] :crowd_server_url the Crowd server root URL; probably something like `https://crowd.mycompany.com` or `https://crowd.mycompany.com/crowd`; optional. @option params [String, nil] :crowd_authentication_url (:crowd_server_url + '/rest/usermanagement/latest/authentication') the URL to which to use for authenication; optional if `:crowd_server_url` is specified, required otherwise. @option params [String, nil] :application_name the application name specified in Crowd for this application, required. @option params [String, nil] :application_password the application password specified in Crowd for this application, required. @option params [Boolean, nil] :disable_ssl_verification disable verification for SSL cert, helpful when you developing with a fake cert. @option params [Boolean, true] :   include a list of user groups when getting information ont he user @option params [String, nil] :crowd_user_group_url (:crowd_server_url + '/rest/usermanagement/latest/user/group/direct') the URL to which to use for retrieving users groups optional if `:crowd_server_url` is specified, or if `:include_user_groups` is false required otherwise. @option params [Boolean, false] :use_sessions Use Crowd sessions. If the user logins with user and password create a new Crowd session. Update the session if only a session token is sent (Cookie name set by option session_cookie) @option params [String, 'crowd.token_key'] :session_cookie Session cookie name. Defaults to: 'crowd.token_key' @option params [String, nil] :sso_url URL of the external SSO page. If this parameter is defined the login form will have a link which will redirect to the SSO page. The SSO must return to the URL of the page using omniauth_crowd (Path portion '/users/auth/crowd/callback' is appended to the URL) @option params [String, nil] :sso_url_image Optional image URL to be used in SSO link in the login form",
    "label": "",
    "id": "4432"
  },
  {
    "raw_code": "def authentication_url(username)\n          append_username @authentication_url, username\n        end",
    "comment": "Build a Crowd authentication URL from +username+.  @param [String] username the username to validate  @return [String] a URL like `https://crowd.myhost.com/crowd/rest/usermanagement/latest/authentication?username=USERNAME`",
    "label": "",
    "id": "4433"
  },
  {
    "raw_code": "def append_username(base, username)\n          result = base.dup\n          result << (result.include?('?') ? '&' : '?')\n          result << 'username='\n          result << Rack::Utils.escape(username)\n        end",
    "comment": "Adds +service+ as an URL-escaped parameter to +base+.  @param [String] base the base URL @param [String] service the service (a.k.a. return-to) URL.  @return [String] the new joined URL.",
    "label": "",
    "id": "4434"
  },
  {
    "raw_code": "def initialize(\n    pipeline_template_path:, rspec_files_path: nil, knapsack_report_path: nil, test_suite_prefix: nil,\n    job_tags: [], generated_pipeline_path: nil)\n    @pipeline_template_path = pipeline_template_path.to_s\n    @rspec_files_path = rspec_files_path.to_s\n    @knapsack_report_path = knapsack_report_path.to_s\n    @test_suite_prefix = test_suite_prefix\n    @job_tags = job_tags\n    @generated_pipeline_path = generated_pipeline_path || \"#{pipeline_template_path}.yml\"\n\n    raise ArgumentError unless File.exist?(@pipeline_template_path)\n  end",
    "comment": "pipeline_template_path: A YAML pipeline configuration template to generate the final pipeline config from rspec_files_path: A file containing RSpec files to run, separated by a space knapsack_report_path: A file containing a Knapsack report test_suite_prefix: An optional test suite folder prefix (e.g. `ee/` or `jh/`) generated_pipeline_path: An optional filename where to write the pipeline config (defaults to `\"#{pipeline_template_path}.yml\"`)",
    "label": "",
    "id": "4435"
  },
  {
    "raw_code": "def downstream_client\n      com_gitlab_client\n    end",
    "comment": "This client is used for downstream build and pipeline status Can be overridden",
    "label": "",
    "id": "4436"
  },
  {
    "raw_code": "def downstream_project_path\n      raise NotImplementedError\n    end",
    "comment": "Must be overridden",
    "label": "",
    "id": "4437"
  },
  {
    "raw_code": "def ref_param_name\n      raise NotImplementedError\n    end",
    "comment": "Must be overridden",
    "label": "",
    "id": "4438"
  },
  {
    "raw_code": "def primary_ref\n      'main'\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "4439"
  },
  {
    "raw_code": "def trigger_token\n      ENV['CI_JOB_TOKEN']\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "4440"
  },
  {
    "raw_code": "def extra_variables\n      {}\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "4441"
  },
  {
    "raw_code": "def version_param_value(version_file)\n      ENV[version_file]&.strip || File.read(version_file).strip\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "4442"
  },
  {
    "raw_code": "def trigger_stable_branch_if_detected?\n      false\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "4443"
  },
  {
    "raw_code": "def version_file_variables\n      Dir.glob(\"*_VERSION\").each_with_object({}) do |version_file, params| # rubocop:disable Rails/IndexWith -- Non-rails CI script\n        params[version_file] = version_param_value(version_file)\n      end",
    "comment": "Read version files from all components",
    "label": "",
    "id": "4444"
  },
  {
    "raw_code": "def downstream_project_path\n      ENV.fetch('CNG_PROJECT_PATH', 'gitlab-org/build/CNG-mirror')\n    end",
    "comment": "overridden base class methods",
    "label": "",
    "id": "4445"
  },
  {
    "raw_code": "def logger\n      @logger ||= Logger.new(ENV.fetch(\"CNG_VAR_SETUP_LOG_FILE\", \"tmp/cng-var-setup.log\"))\n    end",
    "comment": "overridden base class methods Logger with file output  @return [Logger]",
    "label": "",
    "id": "4446"
  },
  {
    "raw_code": "def cng_commit_sha\n      @cng_commit_sha ||= ENV['CNG_COMMIT_SHA']\n    end",
    "comment": "Specific commit sha to be used instead of branch if defined  @return [String]",
    "label": "",
    "id": "4447"
  },
  {
    "raw_code": "def default_build_vars\n      @default_build_vars ||= {\n        \"CONTAINER_VERSION_SUFFIX\" => ENV.fetch(\"CI_PROJECT_PATH_SLUG\", \"upstream-trigger\"),\n        \"CACHE_BUSTER\" => \"false\",\n        \"ARCH_LIST\" => ENV.fetch(\"ARCH_LIST\", \"amd64\")\n      }\n    end",
    "comment": "Default variables used in CNG builds that affect container version values  @return [Hash]",
    "label": "",
    "id": "4448"
  },
  {
    "raw_code": "def skip_redundant_jobs?\n      ENV[\"CNG_SKIP_REDUNDANT_JOBS\"] == \"true\"\n    end",
    "comment": "Skip redundant build jobs by calculating if container images are already present in the registry  @return [Boolean]",
    "label": "",
    "id": "4449"
  },
  {
    "raw_code": "def ref_update_mr?\n      ENV[\"CI_MERGE_REQUEST_TARGET_BRANCH_NAME\"]&.match?(%r{renovate-e2e/cng\\S+digest})\n    end",
    "comment": "Pipeline is part of MR that updates cng-mirror ref  @return [Boolean]",
    "label": "",
    "id": "4450"
  },
  {
    "raw_code": "def skip_job_regex\n      \"/#{[*DEFAULT_SKIPPED_JOBS, *STABLE_BASE_JOBS, *skippable_jobs].join('|')}/\"\n    end",
    "comment": "Skipped job regex based on existing container tags in the registry  @return [String]",
    "label": "",
    "id": "4451"
  },
  {
    "raw_code": "def branch_exists?(branch_name)\n      !!downstream_client.branch(downstream_project_path, branch_name)\n    rescue Gitlab::Error::ResponseError\n      false\n    end",
    "comment": "Branch existence check  @param branch_name [String] @return [Boolean]",
    "label": "",
    "id": "4452"
  },
  {
    "raw_code": "def repo_tree\n      logger.info(\"Fetching repo tree for ref '#{ref}'\")\n      downstream_client\n        .repo_tree(downstream_project_path, ref: ref, per_page: 100).auto_paginate\n        .select { |node| node[\"type\"] == \"tree\" }\n        .map { |node| \"#{node['mode']} #{node['type']} #{node['id']}  #{node['path']}\" }\n        .join(\"\\n\")\n    end",
    "comment": "Repository file tree in form of the output of `git ls-tree` command  @return [String]",
    "label": "",
    "id": "4453"
  },
  {
    "raw_code": "def container_versions_script\n      logger.info(\"Fetching container versions script for ref '#{ref}'\")\n      downstream_client.file_contents(\n        downstream_project_path,\n        \"build-scripts/container_versions.sh\",\n        ref\n      )\n    end",
    "comment": "Script used for container version calculations in CNG build jobs  @return [String]",
    "label": "",
    "id": "4454"
  },
  {
    "raw_code": "def debian_image\n      @debian_image ||= docker_image_with_digest(cng_versions[\"DEBIAN_IMAGE\"])\n    end",
    "comment": "Debian image with digest  @return [String]",
    "label": "",
    "id": "4455"
  },
  {
    "raw_code": "def alpine_image\n      @alpine_image ||= docker_image_with_digest(cng_versions[\"ALPINE_IMAGE\"])\n    end",
    "comment": "Alpine image with digest  @return [String]",
    "label": "",
    "id": "4456"
  },
  {
    "raw_code": "def edition\n      @edition ||= Trigger.ee? ? \"ee\" : \"ce\"\n    end",
    "comment": "Edition postfix  @return [String]",
    "label": "",
    "id": "4457"
  },
  {
    "raw_code": "def cng_versions\n      @cng_versions ||= YAML\n        .safe_load(downstream_client.file_contents(downstream_project_path, \"ci_files/variables.yml\", ref))\n        .fetch(\"variables\")\n    end",
    "comment": "Component versions used in CNG builds  @return [Hash]",
    "label": "",
    "id": "4458"
  },
  {
    "raw_code": "def version_fetch_env_variables\n      {\n        **cng_versions,\n        **version_file_variables,\n        **default_build_vars,\n        \"GITLAB_VERSION\" => gitlab_version,\n        \"RUBY_VERSION\" => RUBY_VERSION,\n        \"DEBIAN_DIGEST\" => debian_image.split(\"@\").last,\n        \"ALPINE_DIGEST\" => alpine_image.split(\"@\").last,\n        \"REPOSITORY_TREE\" => repo_tree\n      }\n    end",
    "comment": "Environment variables required for container version fetching All these variables influence final container version values  @return [Hash]",
    "label": "",
    "id": "4459"
  },
  {
    "raw_code": "def deploy_component_tag_variables\n      {\n        \"GITALY_TAG\" => container_versions[\"gitaly\"],\n        \"GITLAB_SHELL_TAG\" => container_versions[\"gitlab-shell\"],\n        \"GITLAB_TOOLBOX_TAG\" => container_versions[\"gitlab-toolbox-#{edition}\"],\n        \"GITLAB_SIDEKIQ_TAG\" => container_versions[\"gitlab-sidekiq-#{edition}\"],\n        \"GITLAB_WEBSERVICE_TAG\" => container_versions[\"gitlab-webservice-#{edition}\"],\n        \"GITLAB_WORKHORSE_TAG\" => container_versions[\"gitlab-workhorse-#{edition}\"],\n        \"GITLAB_KAS_TAG\" => container_versions[\"gitlab-kas\"],\n        \"GITLAB_CONTAINER_REGISTRY_TAG\" => container_versions[\"gitlab-container-registry\"]\n      }\n    end",
    "comment": "Image tags used by CNG deployments  @return [Hash]",
    "label": "",
    "id": "4460"
  },
  {
    "raw_code": "def container_versions\n      @container_versions ||= Tempfile.create('container-versions') do |file|\n        file.write(container_versions_script)\n        file.close\n\n        build_vars = version_fetch_env_variables.transform_values(&:to_s)\n        logger.info(\"Computing container versions using following env variables:\\n#{JSON.pretty_generate(build_vars)}\")\n        out, status = Open3.capture2e(build_vars, \"bash -c 'source #{file.path} && get_all_versions'\")\n        raise \"Failed to fetch container versions! #{out}\" unless status.success?\n\n        component_versions = out.split(\"\\n\")\n        unless component_versions.all? { |line| line.match?(/^[A-Za-z0-9_\\-]+=[^=]+$/) }\n          raise \"Invalid container versions output format! Expected key=value pairs got:\\n#{out}\"\n        end",
    "comment": "Container versions for all components in CNG build pipeline  @return [Hash]",
    "label": "",
    "id": "4461"
  },
  {
    "raw_code": "def skippable_jobs\n      jobs = container_versions.keys\n      logger.info(\"Fetching container registry repositories for project '#{downstream_project_path}'\")\n      repositories = downstream_client.registry_repositories(downstream_project_path, per_page: 100).auto_paginate\n      build_repositories = repositories.each_with_object({}) do |repo, hash|\n        job = jobs.find { |job| repo.name.end_with?(job) }\n        next unless job\n\n        hash[job] = repo.id\n      end",
    "comment": "List of jobs that can be skipped because tag is already present in the registry  @return [Array]",
    "label": "",
    "id": "4462"
  },
  {
    "raw_code": "def docker_image_with_digest(docker_image)\n      image, tag = docker_image.split(\":\")\n\n      logger.info(\"Fetching digest for image '#{docker_image}'\")\n      auth_url = \"https://auth.docker.io/token?service=registry.docker.io&scope=repository:library/#{image}:pull\"\n      auth_response = HTTParty.get(auth_url)\n      raise \"Failed to get auth token\" unless auth_response.success?\n\n      token = JSON.parse(auth_response.body)['token']\n      manifest_url = \"https://registry.hub.docker.com/v2/library/#{image}/manifests/#{tag}\"\n      response = HTTParty.head(manifest_url, headers: {\n        'Authorization' => \"Bearer #{token}\",\n        'Accept' => 'application/vnd.docker.distribution.manifest.v2+json'\n      })\n      raise \"Failed to fetch image '#{docker_image}' digest\" unless response.success?\n\n      digest = response.headers['docker-content-digest'] || raise(\"Failed to get image digest\")\n      \"#{image}:#{tag}@#{digest}\"\n    end",
    "comment": "rubocop:disable Gitlab/HTTParty -- CI script Fetch Docker image with digest from DockerHub  @param docker_image [String] @return [String]",
    "label": "",
    "id": "4463"
  },
  {
    "raw_code": "def cleanup!\n      environment = com_gitlab_client.environments(downstream_project_path, name: downstream_environment).first\n      return unless environment\n\n      environment = com_gitlab_client.stop_environment(downstream_project_path, environment.id)\n      if environment.state == 'stopped'\n        puts \"=> Downstream environment '#{downstream_environment}' stopped.\"\n      else\n        puts \"=> Downstream environment '#{downstream_environment}' failed to stop.\"\n      end",
    "comment": " Remove a remote environment in the docs-gitlab-com project. ",
    "label": "",
    "id": "4464"
  },
  {
    "raw_code": "def extract_queries_from_file(file_path)\n    logger.info \"Extracting queries from file: #{file_path}\" if logger\n    queries = []\n\n    begin\n      if file_path.end_with?('.gz')\n        Zlib::GzipReader.open(file_path) do |gz|\n          gz.each_line do |line|\n            process_json_line(line, queries)\n          end",
    "comment": "Extract fingerprints from a local file (compressed or uncompressed) Returns an array of query objects with fingerprints",
    "label": "",
    "id": "4465"
  },
  {
    "raw_code": "def extract_fingerprints_from_file(file_path)\n    queries = extract_queries_from_file(file_path)\n    Set.new(queries.filter_map { |q| q['fingerprint'] })\n  end",
    "comment": "Extract just the fingerprint strings from a file Returns a Set of fingerprint strings",
    "label": "",
    "id": "4466"
  },
  {
    "raw_code": "def extract_from_tar_gz(content, max_size_mb = 250)\n    fingerprints = Set.new\n    max_size = max_size_mb * (1024**2) # guardrail to prevent issues if unexpectedly large\n\n    begin\n      io = StringIO.new(content)\n      gz = Zlib::GzipReader.new(io)\n      tar = Gem::Package::TarReader.new(gz)\n\n      tar&.each do |entry|\n        # Now looking for raw fingerprint files (any text file)\n        next unless entry.file? && !entry.directory?\n\n        # Check file size before reading\n        if entry.header.size > max_size\n          logger.error(\n            \"File too large: #{entry.header.size / (1024**2)}MB exceeds limit #{max_size_mb}MB\"\n          )\n          return fingerprints\n        end",
    "comment": "Extract fingerprints from a tar.gz content Returns a Set of fingerprint strings",
    "label": "",
    "id": "4467"
  },
  {
    "raw_code": "def write_fingerprints_to_file(fingerprints, output_file)\n    File.open(output_file, 'w') do |f|\n      fingerprints.each { |fp| f.puts(fp) }\n    end",
    "comment": "Write a set of fingerprints to file",
    "label": "",
    "id": "4468"
  },
  {
    "raw_code": "def project_slug\n    case ENV['CI_PROJECT_PATH']\n    when 'gitlab-org/gitlab'\n      ''\n    when 'gitlab-org/gitlab-runner'\n      'runner'\n    when 'gitlab-org/omnibus-gitlab'\n      'omnibus'\n    when 'gitlab-org/charts/gitlab'\n      'charts'\n    when 'gitlab-org/cloud-native/gitlab-operator'\n      'operator'\n    end",
    "comment": "Website root path based on project path",
    "label": "",
    "id": "4469"
  },
  {
    "raw_code": "def docs_path\n    ENV['CI_PROJECT_PATH'] == 'gitlab-org/gitlab-runner' ? 'docs/' : 'doc/'\n  end",
    "comment": "Location of docs files in the project",
    "label": "",
    "id": "4470"
  },
  {
    "raw_code": "def check_for_missing_nav_entry(file)\n    # Translate the file path to its website path:\n    # 1. gsub(docs_path, project_slug) - Replaces the local docs directory with the appropriate project URL prefix\n    # 2. gsub(/_?index\\.md/, '') - Removes both index.md and _index.md\n    # 3. gsub('.md', '/') - Converts .md to a trailing slash\n    file_sub = file[\"old_path\"]\n      .gsub(docs_path, project_slug)\n      .gsub(/_?index\\.md/, '')\n      .gsub('.md', '/')\n\n    result = navigation_file.include?(\"'#{file_sub}'\")\n    return unless result\n\n    # If we're here, the path exists in navigation\n    # Now check if this is a rename between index.md and _index.md\n    if renamed_doc_file?(file)\n      old_basename = File.basename(file['old_path'])\n      new_basename = File.basename(file['new_path'])\n\n      # Allow renames between index.md and _index.md\n      return if %w[index.md _index.md].include?(old_basename) &&\n        %w[index.md _index.md].include?(new_basename)\n\n      # Handle the case where page.md is moved to page/_index.md\n      if !old_basename.start_with?('_') &&\n          new_basename == '_index.md' &&\n          File.dirname(file['old_path']) == File.dirname(File.dirname(file['new_path']))\n        # The path structure looks like:\n        # old: doc/path/page.md\n        # new: doc/path/page/_index.md\n        return\n      end",
    "comment": " Check if the deleted/renamed file exists in https://gitlab.com/gitlab-org/technical-writing/docs-gitlab-com/-/blob/main/data/navigation.yaml.  We need to first convert the Markdown file path to HTML. There are two cases:  - A source doc entry with _index.md looks like: doc/administration/_index.md The navigation.yaml equivalent is:           administration/ - A source doc entry without _index.md looks like: doc/administration/appearance.md The navigation.yaml equivalent is:              administration/appearance/ ",
    "label": "",
    "id": "4471"
  },
  {
    "raw_code": "def rake_command(file)\n    # The Rake task is only available for gitlab-org/gitlab\n    return unless ENV['CI_PROJECT_PATH'] == 'gitlab-org/gitlab'\n\n    if renamed_doc_file?(file)\n      rake = \"bundle exec rake \\\"gitlab:docs:redirect[#{file['old_path']}, #{file['new_path']}]\\\"\"\n      msg = \"It seems you renamed a page, run the following Rake task locally and commit the changes.\\n\"\n    elsif deleted_doc_file?(file)\n      rake = \"bundle exec rake \\\"gitlab:docs:redirect[#{file['old_path']}, doc/new/path.md]\\\"\"\n      msg = \"It seems you deleted a page. Run the following Rake task by replacing\\n\" \\\n            \"'doc/new/path.md' with the page to redirect to, and commit the changes.\\n\"\n    end",
    "comment": "Rake task to use depending on the file being deleted or renamed",
    "label": "",
    "id": "4472"
  },
  {
    "raw_code": "def gitlab_api_url\n    ENV.fetch('CI_API_V4_URL', 'https://gitlab.com/api/v4')\n  end",
    "comment": "GitLab API URL",
    "label": "",
    "id": "4473"
  },
  {
    "raw_code": "def url_encoded_project_path\n    project_path = ENV.fetch('CI_PROJECT_PATH', nil)\n    return unless project_path\n\n    CGI.escape(project_path)\n  end",
    "comment": "Take the project path from the CI_PROJECT_PATH predefined variable.",
    "label": "",
    "id": "4474"
  },
  {
    "raw_code": "def merge_request_iid\n    ENV.fetch('CI_MERGE_REQUEST_IID', nil)\n  end",
    "comment": "Take the merge request ID from the CI_MERGE_REQUEST_IID predefined variable.",
    "label": "",
    "id": "4475"
  },
  {
    "raw_code": "def project_supported?\n    PROJECT_PATHS.include? ENV['CI_PROJECT_PATH']\n  end",
    "comment": "Skip if CI_PROJECT_PATH is not in the designated project paths",
    "label": "",
    "id": "4476"
  },
  {
    "raw_code": "def merge_request_diff\n    @merge_request_diff ||= begin\n      uri = URI.parse(\n        \"#{gitlab_api_url}/projects/#{url_encoded_project_path}/merge_requests/#{merge_request_iid}/diffs?per_page=30\"\n      )\n      response = Net::HTTP.get_response(uri)\n\n      unless response.code == '200'\n        raise \"API call to get MR diffs failed. Response code: #{response.code}. Response message: #{response.message}\"\n      end",
    "comment": "Fetch the merge request diff JSON object",
    "label": "",
    "id": "4477"
  },
  {
    "raw_code": "def check_renamed_deleted_files\n    renamed_files = merge_request_diff.select do |file|\n      renamed_doc_file?(file)\n    end",
    "comment": "Create a list of hashes of the renamed documentation files",
    "label": "",
    "id": "4478"
  },
  {
    "raw_code": "def redirect_to(diff_file)\n    redirect_to = diff_file[\"diff\"]\n                    .lines\n                    .find { |e| e.include?('+redirect_to') }\n                    &.delete_prefix('+')\n\n    return if redirect_to.nil?\n\n    YAML.safe_load(redirect_to)['redirect_to']\n  end",
    "comment": "Search for '+redirect_to' in the diff to find the new value. It should return a string of \"+redirect_to: 'file.md'\", in which case, delete the '+' prefix. If not found, skip and go to next file.",
    "label": "",
    "id": "4479"
  },
  {
    "raw_code": "def check_for_circular_redirects\n    all_doc_files.each do |file|\n      next if redirect_to(file).nil?\n\n      basename = File.basename(file['old_path'])\n\n      # Fail if the 'redirect_to' value is the same as the file's basename.\n      next unless redirect_to(file) == basename\n\n      warn <<~WARNING\n        #{COLOR_CODE_RED} ERROR: Circular redirect detected. The 'redirect_to' value points to the same file.#{COLOR_CODE_RESET}\n      WARNING\n\n      puts\n      puts \"File        : #{file['old_path']}\"\n      puts \"Redirect to : #{redirect_to(file)}\"\n\n      abort\n    end",
    "comment": "Check if a page redirects to itself",
    "label": "",
    "id": "4480"
  },
  {
    "raw_code": "def test_report_for_build(pipeline_url, build_id)\n    fetch(\"#{pipeline_url}/tests/suite.json?build_ids[]=#{build_id}\").tap do |suite|\n      suite['job_url'] = job_url(pipeline_url, build_id)\n    end",
    "comment": "Method uses the test suite endpoint to gather test results for a particular build. Here we request individual builds, even though it is possible to supply multiple build IDs. The reason for this; it is possible to lose the job context and name when requesting multiple builds. Please see for more info: https://gitlab.com/gitlab-org/gitlab/-/merge_requests/69053#note_709939709",
    "label": "",
    "id": "4481"
  },
  {
    "raw_code": "def nakayoshi_gc\n  4.times { GC.start(full_mark: false) }\n  GC.compact\nend",
    "comment": "frozen_string_literal: true Promotes survivors from eden to old gen and runs a compaction.  aka \"Nakayoshi GC\"  https://github.com/puma/puma/blob/de632261ac45d7dd85230c83f6af6dd720f1cbd9/lib/puma/util.rb#L26-L35",
    "label": "",
    "id": "4482"
  },
  {
    "raw_code": "def assets_impacting_compilation\n      assets_folders = FOSS_ASSET_FOLDERS\n      assets_folders += EE_ASSET_FOLDERS if ::Gitlab.ee?\n      assets_folders += JH_ASSET_FOLDERS if ::Gitlab.jh?\n\n      asset_files = Dir.glob(JS_ASSET_PATTERNS)\n      asset_files += JS_ASSET_FILES\n      asset_files += RAILS_ASSET_FILES\n\n      assets_folders.each do |folder|\n        asset_files.concat(Dir.glob([\"#{folder}/**/*.*\"]))\n      end",
    "comment": "Files listed here should match the list in: .assets-compilation-patterns in .gitlab/ci/rules.gitlab-ci.yml So we make sure that any impacting changes we do rebuild cache",
    "label": "",
    "id": "4483"
  },
  {
    "raw_code": "def initialize(api_endpoint:, gitlab_access_token:)\n    @api_endpoint        = api_endpoint\n    @gitlab_access_token = gitlab_access_token\n  end",
    "comment": "We need an access token that isn't CI_JOB_TOKEN because we are querying the pipelines API to fetch jobs and bridge jobs. We are still using CI_JOB_TOKEN to update the pipeline name.  See https://docs.gitlab.com/ee/ci/jobs/ci_job_token.html for more info.",
    "label": "",
    "id": "4484"
  },
  {
    "raw_code": "def allow_to_fail_return_code\n    3\n  end",
    "comment": "Exit with a different error code, so that we can allow the CI job to fail",
    "label": "",
    "id": "4485"
  },
  {
    "raw_code": "def unique_ids\n      prefix = [\n        operator,\n        (actions || []).sort.join('+'),\n        'filter-',\n        filtered?\n      ].join('_')\n\n      Array(time_frame).map { |t| prefix + t }\n    end",
    "comment": "Enables comparison with new metrics",
    "label": "",
    "id": "4486"
  },
  {
    "raw_code": "def unique_ids\n      prefix = [\n        operator.reference(identifier),\n        actions.sort.join('+'),\n        'filter-',\n        filtered?\n      ].join('_')\n\n      time_frame.value.map { |t| prefix + t }\n    end",
    "comment": "Enables comparison with existing metrics",
    "label": "",
    "id": "4487"
  },
  {
    "raw_code": "def events\n      if filters.assigned?\n        self[:filters].map { |(action, filter)| event_params(action, filter) }\n      else\n        actions.map { |action| event_params(action) }\n      end",
    "comment": "Returns value for the `events` key in the metric definition. Requires #actions or #filters to be set by the caller first.  @return [Hash]",
    "label": "",
    "id": "4488"
  },
  {
    "raw_code": "def filtered?\n      filters.assigned? || filters.expected?\n    end",
    "comment": "How to interpret different values for filters: nil --> not expected, assigned or filtered (metric not initialized with filters) [] --> both expected and filtered (metric initialized with filters, but not yet assigned by user) [['event', {}]] --> not expected, assigned or filtered (filters were expected, but then skipped by user) [['event', { 'label' => 'a' }]] --> both assigned and filtered (filters exist for any event; user is done assigning)",
    "label": "",
    "id": "4489"
  },
  {
    "raw_code": "def description_prefix\n      [\n        (time_frame.description if time_frame.single?),\n        (operator.description if event_metric?),\n        *(identifier.plural if identifier.default? && event_metric?)\n      ].compact.join(' ').capitalize\n    end",
    "comment": "Automatically prepended to all new descriptions ex) Total count of ex) Weekly/Monthly count of unique ex) Count of",
    "label": "",
    "id": "4490"
  },
  {
    "raw_code": "def technical_description\n      event_name = actions.first if events.length == 1 && !filtered?\n      event_name ||= 'the selected events'\n      [\n        (time_frame.description if time_frame.single?),\n        (operator.description if event_metric?),\n        ((identifier.description % event_name).to_s if event_metric?)\n      ].compact.join(' ').capitalize\n    end",
    "comment": "Provides simplified but technically accurate description to be used before the user has provided a description",
    "label": "",
    "id": "4491"
  },
  {
    "raw_code": "def assign_time_frame\n      time_frame.single? ? time_frame.value.first : time_frame.value\n    end",
    "comment": "Maintain current functionality of string time_frame for backward compatibility TODO: Remove once we can deduplicate and merge metric files",
    "label": "",
    "id": "4492"
  },
  {
    "raw_code": "def single?\n        !value.is_a?(Array) || value.length == 1\n      end",
    "comment": "TODO: Delete once we are able to deduplicate and merge metric files",
    "label": "",
    "id": "4493"
  },
  {
    "raw_code": "def description\n        if value.nil?\n          \"%s occurrences\"\n        elsif value == 'user'\n          \"users who triggered %s\"\n        elsif %w[project namespace].include?(value)\n          \"#{plural} where %s occurred\"\n        else\n          \"#{plural} from %s occurrences\"\n        end",
    "comment": "returns a description of the identifier with appropriate grammar to interpolate a description of events",
    "label": "",
    "id": "4494"
  },
  {
    "raw_code": "def plural\n        default? ? \"#{value}s\" : \"values for '#{value}'\"\n      end",
    "comment": "handles generic pluralization for unknown indentifers",
    "label": "",
    "id": "4495"
  },
  {
    "raw_code": "def key_path(operator)\n        case operator.value\n        when 'unique_count'\n          \"distinct_#{reference.tr('.', '_')}_from\"\n        when 'count'\n          'total'\n        when 'sum'\n          \"#{reference.tr('.', '_')}_from\"\n        end",
    "comment": "returns a slug which can be used in the metric's key_path and filepath",
    "label": "",
    "id": "4496"
  },
  {
    "raw_code": "def reference\n        default? ? \"#{value}.id\" : value\n      end",
    "comment": "Returns the identifier string that will be included in the yml",
    "label": "",
    "id": "4497"
  },
  {
    "raw_code": "def default?\n        %w[user project namespace].include?(value)\n      end",
    "comment": "Refers to the top-level identifiers not included in additional_properties",
    "label": "",
    "id": "4498"
  },
  {
    "raw_code": "def value(name_to_display = nil)\n        [\n          operator.verb,\n          identifier&.key_path(operator),\n          name_to_display || name_for_events,\n          time_frame&.key_path\n        ].compact.join('_')\n      end",
    "comment": "@param name_to_display [String] return the key with the provided name instead of a list of event names",
    "label": "",
    "id": "4499"
  },
  {
    "raw_code": "def name_for_events\n        # user may have defined a different name for events\n        return events unless events.respond_to?(:join)\n\n        events.join('_and_')\n      end",
    "comment": "Refers to the middle portion of a metric's `key_path` pertaining to the relevent events; This does not include identifier/time_frame/etc",
    "label": "",
    "id": "4500"
  },
  {
    "raw_code": "def events\n        @events ||= events_by_filepath(@selected_event_paths)\n      end",
    "comment": "----- Memoization Helpers -----------------",
    "label": "",
    "id": "4501"
  },
  {
    "raw_code": "def prompt_for_metric_type\n        return :event_metric if @selected_event_paths.any?\n\n        new_page!(on_step: 'Type', steps: STEPS)\n\n        cli.select(\"Which best describes what the metric should track?\", **select_opts) do |menu|\n          menu.enum \".\"\n\n          menu.choice 'Single event    -- count occurrences of a specific event or user interaction',\n            :event_metric\n          menu.choice 'Multiple events -- count occurrences of several separate events or interactions',\n            :aggregate_metric\n          menu.choice 'Database        -- record value of a particular field or count of database rows',\n            :database_metric\n        end",
    "comment": "----- Prompts -----------------------------",
    "label": "",
    "id": "4502"
  },
  {
    "raw_code": "def prompt_for_copying_event_properties\n        shared_values = collect_values_for_shared_event_properties\n        defaults = shared_values.except(:stage, :section)\n\n        return {} if shared_values.none?\n\n        return shared_values if defaults.none?\n\n        new_page!(on_step: 'Defaults', steps: STEPS)\n\n        cli.say <<~TEXT\n          #{format_info('Convenient! We can copy these attributes from the event definition(s):')}\n\n          #{defaults.compact.transform_keys(&:to_s).to_yaml(line_width: 150)}\n          #{format_info('If any of these attributes are incorrect, you can also change them manually from your text editor later.')}\n\n        TEXT\n\n        cli.select('What would you like to do?', **select_opts) do |menu|\n          menu.enum '.'\n          menu.choice 'Copy & continue', -> { bulk_assign(defaults) }\n          menu.choice 'Modify attributes'\n        end",
    "comment": "Check existing event files for attributes to copy over",
    "label": "",
    "id": "4503"
  },
  {
    "raw_code": "def selected_events_filter_options\n        filterable_events_selected = selected_events.any? { |event| event.additional_properties&.any? }\n\n        selected_events.map do |event|\n          filters = event.additional_properties&.keys\n          filter_phrase = if filters\n                            \" (filterable by #{filters&.join(', ')})\"\n                          elsif filterable_events_selected\n                            ' -- not filterable'\n                          end",
    "comment": "----- Prompt-specific Helpers ------------- Helper for #prompt_for_metrics",
    "label": "",
    "id": "4504"
  },
  {
    "raw_code": "def print_event_filter_header(event, idx, total)\n        cli.say \"\\n\"\n        cli.say format_info(format_subheader('SETTING EVENT FILTERS', event.action, idx, total))\n\n        return unless event.additional_properties&.any?\n\n        event_filter_options = event.additional_properties.map do |property, attrs|\n          \"  #{property}: #{attrs['description']}\\n\"\n        end",
    "comment": "Helper for #prompt_for_event_filters",
    "label": "",
    "id": "4505"
  },
  {
    "raw_code": "def deselect_nonfilterable_event?(event)\n        cli.say \"\\n\"\n\n        return false if event.additional_properties&.any?\n        return false if cli.yes?(\"This event is not filterable. Should it be included in the metric?\", **yes_no_opts)\n\n        selected_events.delete(event)\n        bulk_assign(actions: selected_events.map(&:action).sort)\n\n        true\n      end",
    "comment": "Helper for #prompt_for_event_filters",
    "label": "",
    "id": "4506"
  },
  {
    "raw_code": "def prompt_for_property_filter(action, property, default)\n        formatted_prop = format_info(property)\n        prompt = \"Count where #{formatted_prop} equals any of (comma-sep):\"\n\n        inputs = prompt_for_text(prompt, default, **input_opts) do |q|\n          if property == 'value'\n            q.convert ->(input) { input.split(',').map(&:to_i).uniq }\n            q.validate %r{^(\\d|\\s|,)*$}\n            q.messages[:valid?] = \"Inputs for #{formatted_prop} must be numeric\"\n          elsif property == 'property' || property == 'label'\n            q.convert ->(input) { input.split(',').map(&:strip).uniq }\n          else\n            q.convert ->(input) do\n              input.split(',').map do |value|\n                val = value.strip\n                cast_if_numeric(val)\n              end.uniq\n            end",
    "comment": "Helper for #prompt_for_event_filters",
    "label": "",
    "id": "4507"
  },
  {
    "raw_code": "def find_filter_permutations(action, filters)\n        # Define a filter for all events, regardless of the available props so NewMetric#events is correct\n        return [[action, {}]] unless filters&.any?\n\n        # Uses proc syntax to avoid spliting & type-checking `filters`\n        :product.to_proc.call(*filters).map do |filter|\n          [action, filter.reduce(&:merge)]\n        end",
    "comment": "Helper for #prompt_for_event_filters  Gets all the permutations of the provided property values. @param filters [Array] ex) [{ 'label' => 'red' }, { 'label' => 'blue' }, { value => 16 }] @return ex) [{ 'label' => 'red', value => 16 }, { 'label' => 'blue', value => 16 }]",
    "label": "",
    "id": "4508"
  },
  {
    "raw_code": "def selected_event_descriptions\n        selected_events.map do |event|\n          filters = @selected_filters[event.action]\n\n          if filters&.any?\n            filter_phrase = filters.map { |k, v| \"#{k}=#{v}\" }.join(' ')\n            filter_phrase = format_help(\"(#{filter_phrase})\")\n          end",
    "comment": "Helper for #prompt_for_description",
    "label": "",
    "id": "4509"
  },
  {
    "raw_code": "def name_requirement_reason\n        if metric.filters.assigned?\n          NAME_REQUIREMENT_REASONS[:filters]\n        elsif metric.file_name.length > MAX_FILENAME_LENGTH\n          NAME_REQUIREMENT_REASONS[:length]\n        elsif conflicting_key_path?(metric.key_path)\n          NAME_REQUIREMENT_REASONS[:conflict]\n        end",
    "comment": "Helper for #prompt_for_description",
    "label": "",
    "id": "4510"
  },
  {
    "raw_code": "def conflicting_key_path?(key_path)\n        cli.global.metrics.any? do |existing_metric|\n          existing_metric.key_path == key_path\n        end",
    "comment": "Helper for #prompt_for_description",
    "label": "",
    "id": "4511"
  },
  {
    "raw_code": "def collect_values_for_shared_event_properties\n        fields = Hash.new { |h, k| h[k] = [] }\n\n        selected_events.each do |event|\n          fields[:introduced_by_url] << event.introduced_by_url\n          fields[:product_group] << event.product_group\n          fields[:stage] << find_stage(event.product_group)\n          fields[:section] << find_section(event.product_group)\n          fields[:product_categories] << event.product_categories\n          fields[:tiers] << event.tiers&.sort\n        end",
    "comment": "Helper for #prompt_for_copying_event_properties",
    "label": "",
    "id": "4512"
  },
  {
    "raw_code": "def get_next_step\n        cli.select(\"How would you like to proceed?\", **select_opts) do |menu|\n          menu.enum \".\"\n\n          menu.choice \"New Event -- define a new event\", :new_event\n\n          if metric.event_metric?\n            actions = selected_events.map(&:action).join(', ')\n            menu.choice \"New Metric -- define another metric for #{actions}\", :new_metric_with_events\n          end",
    "comment": "Helper for #prompt_for_next_steps",
    "label": "",
    "id": "4513"
  },
  {
    "raw_code": "def assign_shared_attrs(...)\n        attrs = metric.to_h.slice(...)\n        attrs = yield(metric) unless attrs.values.all?\n\n        bulk_assign(attrs)\n      end",
    "comment": "----- Shared Helpers ----------------------",
    "label": "",
    "id": "4514"
  },
  {
    "raw_code": "def prompt_for_text(message, value = nil, multiline: false, **opts)\n        prompt = message.dup # mutable for concat in #ask callback\n\n        options = { **input_opts, **opts }\n        value ||= options.delete(:value)\n        options.delete(:prefix) if multiline\n\n        cli.ask(prompt, **options) do |q|\n          q.value(value) if value\n\n          yield q if block_given?\n\n          if multiline\n            # wrap error messages so they render nicely with prompt\n            q.messages.each do |key, error|\n              closing_text = \"\\n#{format_error('<<|')}\" if error.lines.length > 1\n\n              q.messages[key] = [error, closing_text, \"\\n\\n\\n\"].join('')\n            end",
    "comment": "Prompts the user to input text. Prefer this over calling cli#ask directly (so styling is consistent).   @return [String, nil] user-provided text @param message [String] a single line prompt/question or last line of a prompt @param value [String, nil] prepopulated as the answer which user can accept/modify @option multiline [Boolean] indicates that any help text or prompt prefix will be printed on another line before calling #prompt_for_text -->  ex) see MetricDefiner#prompt_for_description @yield [TTY::Prompt::Question] @see https://github.com/piotrmurach/tty-prompt?tab=readme-ov-file#21-ask",
    "label": "",
    "id": "4515"
  },
  {
    "raw_code": "def select_opts\n        {\n          prefix: format_prompt('Select one: '),\n          cycle: true,\n          show_help: :always,\n          # Strip colors so #format_selection is applied uniformly\n          active_color: ->(choice) { format_selection(clear_format(choice)) }\n        }\n      end",
    "comment": "Provide to cli#select as kwargs for consistent style/ux",
    "label": "",
    "id": "4516"
  },
  {
    "raw_code": "def multiselect_opts\n        {\n          **select_opts,\n          prefix: format_prompt('Select multiple: '),\n          min: 1,\n          help: \"(Space to select, Enter to submit, /// to move, Ctrl+A|R to select all|none, letters to filter)\"\n        }\n      end",
    "comment": "Provide to cli#multiselect as kwargs for consistent style/ux",
    "label": "",
    "id": "4517"
  },
  {
    "raw_code": "def filter_opts(header_size: nil)\n        {\n          filter: true,\n          per_page: header_size ? [(window_height - header_size), 10].max : 30\n        }\n      end",
    "comment": "Accepts a number of lines occupied by text, so remaining screen real estate can be filled with select options",
    "label": "",
    "id": "4518"
  },
  {
    "raw_code": "def select_option_divider(text)\n        { name: \"-- #{text} --\", value: nil, disabled: '' }\n      end",
    "comment": "Creates divider to be passed to a select or multiselect as a menu item. Use with #format_disabled_options_as_dividers for best formatting.",
    "label": "",
    "id": "4519"
  },
  {
    "raw_code": "def format_disabled_options_as_dividers(select_menu)\n        select_menu.symbols(cross: '')\n      end",
    "comment": "Styling all disabled options in a menu without indication of being a selectable option @param select_menu [TTY::Prompt]",
    "label": "",
    "id": "4520"
  },
  {
    "raw_code": "def disabled_format_callback\n        proc { |menu| menu.symbols(cross: format_help(\"\")) }\n      end",
    "comment": "For use when menu options are disabled by being grayed out",
    "label": "",
    "id": "4521"
  },
  {
    "raw_code": "def input_required_text\n        format_help(\"(leave blank for help)\")\n      end",
    "comment": "Help text to use with required, multiline cli#ask prompts. Otherwise, prefer #prompt_for_text.",
    "label": "",
    "id": "4522"
  },
  {
    "raw_code": "def input_optional_text(value)\n        format_help(\"(enter to #{value ? 'submit' : 'skip'})\")\n      end",
    "comment": "Help text to use with optional, multiline cli#ask prompts. Otherwise, prefer #prompt_for_text.",
    "label": "",
    "id": "4523"
  },
  {
    "raw_code": "def group_choices\n        available_groups.map do |group, ownership|\n          {\n            name: ownership.values_at(:section, :stage, :group).compact.join(':'),\n            value: group\n          }\n        end",
    "comment": "@return Array[<Hash - matches #prompt_for_ownership_manually output format>]",
    "label": "",
    "id": "4524"
  },
  {
    "raw_code": "def category_choices(groups)\n        options = []\n\n        # List likeliest categories for group as the first options\n        Array(groups).compact.uniq.flat_map do |group|\n          divider = select_option_divider(\"Categories for #{group}\")\n          categories = (find_categories(group) || []) & KNOWN_CATEGORIES\n\n          options.push(divider, *categories.sort) if categories.any?\n        end",
    "comment": "Returns the list of product category options for use in select menu. Prioritizes categories from related groups.",
    "label": "",
    "id": "4525"
  },
  {
    "raw_code": "def available_groups\n        # rubocop:disable Gitlab/ModuleWithInstanceVariables -- memoization is acceptable use\n        # https://docs.gitlab.com/ee/development/module_with_instance_variables.html#acceptable-use\n        return @available_groups if @available_groups\n\n        response = Timeout.timeout(5) { Net::HTTP.get(URI(STAGES_YML)) }\n        data = YAML.safe_load(response)\n\n        @available_groups = data['stages'].flat_map do |stage_name, stage_data|\n          stage_data['groups'].map do |group_name, group_data|\n            [\n              group_name,\n              {\n                group: group_name,\n                stage: stage_name,\n                section: stage_data['section'],\n                categories: group_data['categories']\n              }\n            ]\n          end",
    "comment": "Output looks like: { \"import\" => { stage: \"manage\", section: \"dev\", group: \"import\" }, ... }",
    "label": "",
    "id": "4526"
  },
  {
    "raw_code": "def format_info(string)\n        pastel.cyan(string)\n      end",
    "comment": "When to format as \"info\": - When a header is needed to organize contextual information. These headers should always be all caps. - As a supplemental way to highlight the most important text within a menu or informational text. - Optionally, for URLs",
    "label": "",
    "id": "4527"
  },
  {
    "raw_code": "def format_warning(string)\n        pastel.yellow(string)\n      end",
    "comment": "When to format as \"warning\": - To highlight the first sentence/phrase describing a problem the user needs to address. Any further text explantion should be left unformatted. - To highlight an explanation of why the user cannot take a particular action.",
    "label": "",
    "id": "4528"
  },
  {
    "raw_code": "def format_selection(string)\n        pastel.green(string)\n      end",
    "comment": "When to format as \"selection\": - As a supplemental way of indicating something was selected or the current state of an interaction.",
    "label": "",
    "id": "4529"
  },
  {
    "raw_code": "def format_help(string)\n        pastel.bright_black(string)\n      end",
    "comment": "When to format as \"help\": - To format supplemental information on how to interact with prompts. This should always be in parenthesis. - To indicate disabled or unavailable menu options. - To indicate meta-information in menu options or informational text.",
    "label": "",
    "id": "4530"
  },
  {
    "raw_code": "def format_prompt(string)\n        pastel.magenta(string)\n      end",
    "comment": "When to format as \"prompt\": - When we need the user to input information. The text should describe the action the user should take to move forward, like `Input text` or `Select one` - As header text on multi-screen steps in a flow. Always include a counter when this is the case.",
    "label": "",
    "id": "4531"
  },
  {
    "raw_code": "def format_error(string)\n        pastel.red(string)\n      end",
    "comment": "When to format as \"error\": - When the CLI encounters unexpected problems that may require broader changes by the Analytics Instrumentation Group or out of band configuration. - To highlight special characters used to symbolize that there was an error or that an option is not available.",
    "label": "",
    "id": "4532"
  },
  {
    "raw_code": "def clear_format(string)\n        pastel.strip(string)\n      end",
    "comment": "Strips all existing color/text style",
    "label": "",
    "id": "4533"
  },
  {
    "raw_code": "def format_heading(string)\n        [divider, pastel.cyan(string), divider].join(\"\\n\")\n      end",
    "comment": "When to format as \"heading\": - At the beginning or end of complete flows, to create visual separation and indicate logical breakpoints.",
    "label": "",
    "id": "4534"
  },
  {
    "raw_code": "def format_subheader(subject, item, count = 1, total = 1)\n        formatting_end = \"\\e[0m\"\n        suffix = formatting_end if subject[-formatting_end.length..] == formatting_end\n\n        \"-- #{[subject.chomp(formatting_end), counter(count, total)].compact.join(' ')}:#{suffix} #{item} --\"\n      end",
    "comment": "Used for grouping prompts that occur on the same screen or as part of the same step of a flow.  Counter is exluded if total is 1. The subject's formatting is extended to the counter.  @return [String] ex) -- EATING COOKIES (2/3): Chocolate Chip -- @param subject [String] describes task generically ex) EATING COOKIES @param item [String] describes specific context ex) Chocolate Chip @param count [Integer] ex) 2 @param total [Integer] ex) 3",
    "label": "",
    "id": "4535"
  },
  {
    "raw_code": "def divider\n        \"-\" * window_size\n      end",
    "comment": "When to use a divider: - As separation between whole flows or format the layout of a screen or the layout of CLI outputs. - Dividers should not be used to differentiate between prompts on the same screen.",
    "label": "",
    "id": "4536"
  },
  {
    "raw_code": "def progress_bar(current_title, titles = [])\n        step = titles.index(current_title)\n        total = titles.length - 1\n\n        raise ArgumentError, \"Invalid selection #{current_title} in progress bar\" unless step\n\n        status = \" Step #{step} / #{total} : #{titles.join(' > ')}\"\n        status.gsub!(current_title, format_selection(current_title))\n\n        total_length = window_size - 4\n        step_length = step / total.to_f * total_length\n\n        incomplete = '-' * [(total_length - step_length - 1), 0].max\n        complete = '=' * [(step_length - 1), 0].max\n\n        \"#{status}\\n|==#{complete}>#{incomplete}|\\n\"\n      end",
    "comment": "Prints a progress bar on the screen at the current location @param current_title [String] title to highlight @param titles [Array<String>] progression to follow; -> first element is expected to be a title for the entire flow",
    "label": "",
    "id": "4537"
  },
  {
    "raw_code": "def counter(idx, total)\n        \"(#{idx + 1}/#{total})\" if total > 1\n      end",
    "comment": "Formats a counter if there's anything to count  @return [String, nil] ex) \"(3/4)\"\"",
    "label": "",
    "id": "4538"
  },
  {
    "raw_code": "def get_metric_options(events)\n        selection = EventSelection.new(events)\n        available_options = []\n        disabled_options = []\n\n        options = get_all_metric_options(selection.actions)\n\n        options.each do |metric|\n          # Filters & breaks up menu items based on existing metrics\n          # and supported functionality, appending formatted options\n          # to available_options and disabled_options arrays\n          collect_options!(metric, selection, available_options, disabled_options)\n        end",
    "comment": "Creates a list of metrics to be used as options in a select/multiselect menu; existing metrics and metrics for unavailable identifiers are marked as disabled  @param events [Array<ExistingEvent>] @return [Array<Hash>] hash (compact) has keys/values: value: [Array<NewMetric>] name: [String] formatted description of the metrics disabled: [String] reason metrics are disabled",
    "label": "",
    "id": "4539"
  },
  {
    "raw_code": "def get_all_metric_options(actions)\n        [\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'user' },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'project' },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'namespace' },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'user', filters: [] },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'project', filters: [] },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'namespace', filters: [] },\n          { time_frame: %w[7d 28d all], operator: 'count' },\n          { time_frame: %w[7d 28d all], operator: 'count', filters: [] },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'label' },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'property' },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'value' },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'label', filters: [] },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'property', filters: [] },\n          { time_frame: %w[7d 28d], operator: 'unique_count', identifier: 'value', filters: [] },\n          { time_frame: %w[7d 28d all], operator: 'sum', identifier: 'value' },\n          { time_frame: %w[7d 28d all], operator: 'sum', identifier: 'value', filters: [] }\n        ].map do |attributes|\n          Metric.new(**attributes.merge(actions: actions))\n        end",
    "comment": "Lists all potential metrics supported in service ping, ordered by: identifier > filters > time_frame  @param actions [Array<String>] event names @return [Array<NewMetric>]",
    "label": "",
    "id": "4540"
  },
  {
    "raw_code": "def events_name\n          return actions.first if actions.length == 1\n\n          \"any of #{actions.length} events\"\n        end",
    "comment": "Very brief summary of the provided events to use in a basic description of the metric This ignores filters for simplicity & skimability",
    "label": "",
    "id": "4541"
  },
  {
    "raw_code": "def filter_name(identifier)\n          filter_options.difference([identifier]).join('/')\n        end",
    "comment": "Formatted list of filter options for these events, given the provided uniqueness constraint",
    "label": "",
    "id": "4542"
  },
  {
    "raw_code": "def filter_options\n          events.flat_map(&:available_filters).uniq\n        end",
    "comment": "We accept different filters for each event, so we want any filter options available for any event",
    "label": "",
    "id": "4543"
  },
  {
    "raw_code": "def operable_identifiers\n          [*shared_identifiers, *shared_filters, nil]\n        end",
    "comment": "We require the same uniqueness constraint for all events, so we want only the options they have in common",
    "label": "",
    "id": "4544"
  },
  {
    "raw_code": "def can_filter_when_operated_on?(identifier)\n          supports_operations?(identifier) && filter_options.difference([identifier]).any?\n        end",
    "comment": "Whether there are any filtering options other than the selected uniqueness constraint",
    "label": "",
    "id": "4545"
  },
  {
    "raw_code": "def supports_operations?(identifier)\n          operable_identifiers.include?(identifier)\n        end",
    "comment": "Whether the given identifier is available for all events and can be used as a uniqueness constraint",
    "label": "",
    "id": "4546"
  },
  {
    "raw_code": "def shared_identifiers\n          events.map(&:identifiers).reduce(&:&)\n        end",
    "comment": "Common values for identifiers shared across all the events",
    "label": "",
    "id": "4547"
  },
  {
    "raw_code": "def shared_filters\n          events.map(&:available_filters).reduce(&:&)\n        end",
    "comment": "Common values for filters shared across all the events",
    "label": "",
    "id": "4548"
  },
  {
    "raw_code": "def exclude_filter_identifier?(identifier)\n          return false if identifier.nil? || Metric::Identifier.new(identifier).default?\n\n          filter_options.empty?\n        end",
    "comment": "Whether none of the events have additional properties and the given identifier is an additional property. In this case, it makes sense to exclude these from the menu to keep the flow simple when the use-case is simple",
    "label": "",
    "id": "4549"
  },
  {
    "raw_code": "def formatted\n          name = [time_frame_phrase, operator_phrase, filter_phrase].compact.join(' ')\n          name = format_help(name) if disabled\n\n          { name: name, disabled: disabled, value: metric }.compact\n        end",
    "comment": "@return [Hash] see #get_metric_options for format ex) Monthly/Weekly count of unique users who triggered cli_template_included where label/property is... ex) Monthly/Weekly count of unique users who triggered cli_template_included (user unavailable)",
    "label": "",
    "id": "4550"
  },
  {
    "raw_code": "def time_frame_phrase\n          phrase = metric.time_frame.description\n\n          disabled ? phrase : format_info(phrase)\n        end",
    "comment": "ex) \"Monthly/Weekly\"",
    "label": "",
    "id": "4551"
  },
  {
    "raw_code": "def operator_phrase\n          phrase = \"#{metric.operator.description} #{identifier.description % events_name}\"\n          phrase.gsub!(highlighted_phrase, format_info(highlighted_phrase)) unless disabled\n\n          phrase\n        end",
    "comment": "ex) \"count of unique users who triggered cli_template_included\"",
    "label": "",
    "id": "4552"
  },
  {
    "raw_code": "def highlighted_phrase\n          \"#{metric.operator.qualifier} #{identifier.plural}\"\n        end",
    "comment": "ex) \"unique users\"",
    "label": "",
    "id": "4553"
  },
  {
    "raw_code": "def filter_phrase\n          return unless filter_name\n          return \"where filtered\" if disabled\n\n          \"#{format_info(\"where #{filter_name}\")} is...\"\n        end",
    "comment": "ex) \"where label/property is...\"",
    "label": "",
    "id": "4554"
  },
  {
    "raw_code": "def disabled\n          if defined\n            pastel.bold(format_help(\"(already defined)\"))\n          elsif !supported\n            pastel.bold(format_help(\"(#{identifier.value} unavailable)\"))\n          end",
    "comment": "Returns the string to include at the end of disabled menu items. Nil if menu item shouldn't be disabled",
    "label": "",
    "id": "4555"
  },
  {
    "raw_code": "def initialize(\n      project_path: ENV['CI_PROJECT_PATH'],\n      gitlab_token: ENV['GITLAB_PROJECT_PACKAGES_CLEANUP_API_TOKEN'],\n      api_endpoint: ENV['CI_API_V4_URL'],\n      options: {}\n    )\n      @project_path = project_path\n      @gitlab_token = gitlab_token\n      @api_endpoint = api_endpoint\n      @dry_run = options[:dry_run]\n\n      puts \"Dry-run mode.\" if dry_run\n    end",
    "comment": "$GITLAB_PROJECT_PACKAGES_CLEANUP_API_TOKEN => `Packages Cleanup` project token",
    "label": "",
    "id": "4556"
  },
  {
    "raw_code": "def fetch_version\n    versions_data.fetch('gitlab', nil)\n  end",
    "comment": "Get the version from the VERSIONS environment variable All components use the same version, so we can just get the version of gitlab",
    "label": "",
    "id": "4557"
  },
  {
    "raw_code": "def generate_json\n    output_json = {}\n    COMPONENTS.each do |component|\n      output_json[component.to_s] = image_tag.to_s\n    end",
    "comment": "Will generate a json object that has a key for every component and a value which is the environment combined with short sha Example: { \"gitaly\": \"15-10-stable-c7c5131c\", \"registry\": \"15-10-stable-c7c5131c\", \"kas\": \"15-10-stable-c7c5131c\", \"mailroom\": \"15-10-stable-c7c5131c\", \"pages\": \"15-10-stable-c7c5131c\", \"gitlab\": \"15-10-stable-c7c5131c\", \"shell\": \"15-10-stable-c7c5131c\" }",
    "label": "",
    "id": "4558"
  },
  {
    "raw_code": "def environment_base\n    @environment_base ||= if release_tag_match\n                            \"#{release_tag_match[1]}-#{release_tag_match[2]}-stable\"\n                          else\n                            ENV['CI_COMMIT_REF_NAME'].delete_suffix('-ee')\n                          end",
    "comment": "This is to generate the environment name without \"-security\". It is used by the image tag",
    "label": "",
    "id": "4559"
  },
  {
    "raw_code": "def security_omnibus_stable_branch\n    # Transform RC tags to stable branch format\n    ref_name = ENV['CI_COMMIT_REF_NAME']&.match(/^v?([\\d]+)\\.([\\d]+)\\.[\\d]+-rc\\d+-ee$/)\n\n    if ref_name\n      major = ref_name[1]\n      minor = ref_name[2]\n\n      \"#{major}-#{minor}-stable\"\n    else\n      ENV['CI_COMMIT_BRANCH'].gsub(\"-ee\", \"\")\n    end",
    "comment": "Omnibus security stable branch has no -ee suffix",
    "label": "",
    "id": "4560"
  },
  {
    "raw_code": "def initialize(\n      project_path: ENV['CI_PROJECT_PATH'],\n      gitlab_token: ENV['GITLAB_PROJECT_REVIEW_APP_CLEANUP_API_TOKEN'],\n      api_endpoint: ENV['CI_API_V4_URL'],\n      options: {}\n    )\n      @project_path                     = project_path\n      @gitlab_token                     = gitlab_token\n      @api_endpoint                     = api_endpoint\n      @dry_run                          = options[:dry_run]\n    end",
    "comment": "$GITLAB_PROJECT_REVIEW_APP_CLEANUP_API_TOKEN => `Automated Review App Cleanup` project token",
    "label": "",
    "id": "4561"
  },
  {
    "raw_code": "def execute\n    options = parse_options\n    puts \"Options: #{options.inspect}\" if options[:debug]\n\n    files = get_changed_files(branch_name: BRANCH_NAME)\n    puts \"Files: #{files.inspect}\" if options[:debug]\n\n    base_files = files.map { |path| File.basename(path) }\n    puts \"Base files: #{base_files.inspect}\" if options[:debug]\n\n    if base_files.empty?\n      puts 'No migration files found'\n      exit 1\n    end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity,Metrics/PerceivedComplexity -- we can skip it for this script",
    "label": "",
    "id": "4562"
  },
  {
    "raw_code": "def virtual?\n      viewer_component_instance.virtual_rendering_params != nil\n    end",
    "comment": "enables virtual rendering through content-visibility: auto, significantly boosts client performance",
    "label": "",
    "id": "4563"
  },
  {
    "raw_code": "def filter_attribute(value, allowed_values, default: nil)\n      return default unless value\n      return value if allowed_values.include?(value)\n\n      default\n    end",
    "comment": "Filter a given value against a list of allowed values If no value is given or value is not allowed return default one  @param [Object] value @param [Enumerable] allowed_values @param [Object] default",
    "label": "",
    "id": "4564"
  },
  {
    "raw_code": "def format_options(options:, css_classes: [], additional_options: {})\n      options.merge({ class: [*css_classes, options[:class]].flatten.compact }, additional_options)\n    end",
    "comment": "Add CSS classes and additional options to an existing options hash  @param [Hash] options @param [Array] css_classes @param [Hash] additional_option",
    "label": "",
    "id": "4565"
  },
  {
    "raw_code": "def initialize(\n      title: nil,\n      description: nil,\n      icon: nil,\n      href: nil,\n      link_options: {},\n      **html_options\n    )\n      @title = title\n      @description = description\n      @icon = icon.to_s\n      @href = href\n      @html_options = html_options\n      @link_options = link_options\n    end",
    "comment": "@param [String] title @param [String] description @param [String] icon @param [String] href @param [Hash] html_options @param [Hash] link_options",
    "label": "",
    "id": "4566"
  },
  {
    "raw_code": "def initialize(\n      heading, description: nil, id: nil, testid: nil, expanded: nil, button_options: {},\n      css_class: nil)\n      @heading = heading\n      @description = description\n      @id = id\n      @testid = testid\n      @expanded = expanded\n      @button_options = button_options\n      @css_class = css_class\n    end",
    "comment": "@param [String] heading @param [String] description @param [String] id @param [String] testid @param [Boolean] expanded @param [Hash] button_options",
    "label": "",
    "id": "4567"
  },
  {
    "raw_code": "def initialize(border: true, options: {})\n      @border = border\n      @options = options\n    end",
    "comment": "@param [Boolean] border @param [Hash] options",
    "label": "",
    "id": "4568"
  },
  {
    "raw_code": "def initialize(\n      type: :search,\n      **html_options\n    )\n      @type = filter_attribute(type.to_sym, TYPE_OPTIONS, default: :search)\n      @html_options = html_options\n    end",
    "comment": "@param [Symbol] type @param [Hash] html_options",
    "label": "",
    "id": "4569"
  },
  {
    "raw_code": "def initialize(\n      title, description: nil, count: nil, icon: nil, icon_class: nil,\n      toggle_text: nil, options: {}, count_options: {}, body_options: {},\n      form_options: {}, toggle_options: {}, footer_options: {},\n      is_collapsible: false, container_tag: 'section'\n    )\n      @title = title\n      @description = description\n      @count = count\n      @icon = icon\n      @icon_class = icon_class\n      @toggle_text = toggle_text\n      @options = options\n      @count_options = count_options\n      @body_options = body_options\n      @form_options = form_options\n      @toggle_options = toggle_options\n      @footer_options = footer_options\n      @is_collapsible = is_collapsible\n      @container_tag = container_tag\n    end",
    "comment": "@param [String] title @param [String] description @param [Number] count @param [String] icon @param [String] icon_class @param [String] toggle_text @param [Hash] options @param [Hash] count_options @param [Hash] body_options @param [Hash] form_options @param [Hash] toggle_options @param [Hash] footer_options @param [Boolean] is_collapsible @param [String] container_tag",
    "label": "",
    "id": "4570"
  },
  {
    "raw_code": "def initialize(heading, description: nil, options: {})\n      @heading = heading\n      @description = description\n      @options = options\n    end",
    "comment": "@param [String] heading @param [Hash] options",
    "label": "",
    "id": "4571"
  },
  {
    "raw_code": "def initialize(heading, description: nil, id: nil, testid: nil, options: {})\n      @heading = heading\n      @description = description\n      @id = id\n      @testid = testid\n      @options = options\n    end",
    "comment": "@param [String] heading @param [String] description @param [String] id @param [String] testid @param [Hash] options",
    "label": "",
    "id": "4572"
  },
  {
    "raw_code": "def initialize(\n      compact: false,\n      title: nil,\n      svg_path: nil,\n      primary_button_text: nil,\n      primary_button_link: nil,\n      primary_button_options: {},\n      secondary_button_text: nil,\n      secondary_button_link: nil,\n      secondary_button_options: {},\n      empty_state_options: {}\n    )\n      @compact = compact\n      @title = title\n      @svg_path = svg_path.to_s\n      @primary_button_text = primary_button_text\n      @primary_button_link = primary_button_link\n      @primary_button_options = primary_button_options\n      @secondary_button_text = secondary_button_text\n      @secondary_button_link = secondary_button_link\n      @secondary_button_options = secondary_button_options\n      @empty_state_options = empty_state_options\n    end",
    "comment": "@param [Boolean] compact @param [String] title @param [String] svg_path @param [String] primary_button_text @param [String] primary_button_link @param [Hash] primary_button_options @param [String] secondary_button_text @param [String] secondary_button_link @param [Hash] secondary_button_options @param [Hash] empty_state_options",
    "label": "",
    "id": "4573"
  },
  {
    "raw_code": "def initialize(item, alt: nil, class: \"\", size: 64, avatar_options: {})\n      @item = item\n      @alt = alt\n      @class = binding.local_variable_get(:class)\n      @size = filter_attribute(size.to_i, SIZE_OPTIONS, default: 64)\n      @avatar_options = avatar_options\n    end",
    "comment": "@param item [User, Project, Group, AvatarEmail, String] @param alt [String] text for the alt attribute @param class [String] custom CSS class(es) @param size [Integer] size in pixel @param [Hash] avatar_options",
    "label": "",
    "id": "4574"
  },
  {
    "raw_code": "def text\n      content || @text\n    end",
    "comment": "Determines the rendered text content. The content slot takes presedence over the text param.",
    "label": "",
    "id": "4575"
  },
  {
    "raw_code": "def initialize(\n      button_text: 'OK',\n      button_link: '#',\n      variant: :promotion,\n      svg_path: nil,\n      banner_options: {},\n      button_options: {},\n      close_options: {}\n    )\n      @button_text = button_text\n      @button_link = button_link\n      @variant = filter_attribute(variant.to_sym, VARIANT_OPTIONS, default: :promotion)\n      @svg_path = svg_path.to_s\n      @banner_options = banner_options\n      @button_options = button_options\n      @close_options = format_options(options: close_options, css_classes: %w[js-close gl-banner-close])\n    end",
    "comment": "@param [String] button_text @param [String] button_link @param [Symbol] variant @param [String] svg_path @param [Hash] banner_options @param [Hash] button_options @param [Hash] close_options",
    "label": "",
    "id": "4576"
  },
  {
    "raw_code": "def initialize(\n      message:, id:, theme:, dismissable:, expire_date:, cookie_key:, dismissal_path: nil,\n      button_testid: nil, banner: nil)\n      @message = message\n      @id = id\n      @theme = theme\n      @dismissable = dismissable\n      @expire_date = expire_date\n      @cookie_key = cookie_key\n      @dismissal_path = dismissal_path\n      @button_testid = button_testid\n      @banner = banner\n    end",
    "comment": "@param [String] message @param [String] id @param [String] theme @param [Boolean] dismissable @param [String] expire_date @param [String] cookie_key @param [String] dismissal_path @param [String] button_testid",
    "label": "",
    "id": "4577"
  },
  {
    "raw_code": "def initialize(title: nil, state: :closed, button_options: {})\n      @title = title\n      @state = filter_attribute(state.to_sym, STATE_OPTIONS)\n      @button_options = button_options\n    end",
    "comment": "@param [String] title @param [Symbol] state @param [Hash] button_options",
    "label": "",
    "id": "4578"
  },
  {
    "raw_code": "def filter_attribute(value, allowed_values, default: nil)\n      return default unless value\n      return value if allowed_values.include?(value)\n\n      default\n    end",
    "comment": "Filter a given a value against a list of allowed values If no value is given or value is not allowed return default one  @param [Object] value @param [Enumerable] allowed_values @param [Object] default",
    "label": "",
    "id": "4579"
  },
  {
    "raw_code": "def format_options(options:, css_classes: [], additional_options: {})\n      options.merge({ class: [*css_classes, options[:class]].flatten.compact }, additional_options)\n    end",
    "comment": "Add CSS classes and additional options to an existing options hash  @param [Hash] options @param [Array] css_classes @param [Hash] additional_option",
    "label": "",
    "id": "4580"
  },
  {
    "raw_code": "def initialize(\n      title: nil, variant: :info, dismissible: true, show_icon: true,\n      alert_options: {}, close_button_options: {})\n      @title = title.presence\n      @variant = filter_attribute(variant&.to_sym, VARIANT_ICONS.keys, default: :info)\n      @dismissible = dismissible\n      @show_icon = show_icon\n      @alert_options = alert_options\n      @close_button_options = close_button_options\n    end",
    "comment": "@param [String] title @param [Symbol] variant @param [Boolean] dismissible @param [Boolean] show_icon @param [Hash] alert_options @param [Hash] close_button_options",
    "label": "",
    "id": "4581"
  },
  {
    "raw_code": "def initialize(\n    classes:, label: nil, label_position: nil,\n    id: nil, name: nil, help: nil, data: {},\n    is_disabled: false, is_checked: false, is_loading: false\n  )\n    @id = id\n    @name = name\n    @classes = classes\n    @label = label\n    @label_position = filter_attribute(label_position, LABEL_POSITION_OPTIONS)\n    @help = help\n    @data = data\n    @is_disabled = is_disabled\n    @is_checked = is_checked\n    @is_loading = is_loading\n  end",
    "comment": "@param [String] classes @param [String] label @param [Symbol] label_position :top, :left or :hidden @param [String] id @param [String] name @param [String] help @param [Hash] data @param [Boolean] is_disabled @param [Boolean] is_checked @param [Boolean] is_loading",
    "label": "",
    "id": "4582"
  },
  {
    "raw_code": "def initialize(color: :dark, inline: false, label: _(\"Loading\"), size: :sm, **html_options)\n      @color = filter_attribute(color.to_sym, COLOR_OPTIONS)\n      @inline = inline\n      @label = label.presence\n      @size = filter_attribute(size.to_sym, SIZE_OPTIONS)\n      @html_options = html_options\n    end",
    "comment": "@param [Symbol] color @param [Boolean] inline @param [String] label @param [Symbol] size",
    "label": "",
    "id": "4583"
  },
  {
    "raw_code": "def initialize(card: {}, card_options: {}, header_options: {}, body_options: {}, footer_options: {})\n      @card_options = card_options\n      @header_options = header_options\n      @body_options = body_options\n      @footer_options = footer_options\n      @card = card\n    end",
    "comment": "@param [Hash] card_options @param [Hash] header_options @param [Hash] body_options @param [Hash] footer_options @card [Hash] card structure as an object. This enables .with_collection functionality.",
    "label": "",
    "id": "4584"
  },
  {
    "raw_code": "def initialize(icon: :chevron_down_up, variant: :current, is_on: false, icon_options: {})\n      @icon = filter_attribute(icon.to_sym, ICONS)\n      @variant = filter_attribute(variant&.to_sym, VARIANT_CLASSES.keys, default: :current)\n      @is_on = is_on\n      @icon_options = icon_options\n    end",
    "comment": "@param [Symbol] icon @param [Symbol] variant @param [Boolean] is_on @param [Hash] icon_options",
    "label": "",
    "id": "4585"
  },
  {
    "raw_code": "def initialize(\n      category: :primary,\n      variant: :default,\n      size: :medium,\n      type: :button,\n      disabled: false,\n      loading: false,\n      block: false,\n      label: false,\n      selected: false,\n      icon: nil,\n      href: nil,\n      form: false,\n      target: nil,\n      method: nil,\n      button_options: {},\n      button_text_classes: nil,\n      icon_classes: nil\n    )\n      @category = filter_attribute(category.to_sym, CATEGORY_OPTIONS)\n      @variant = filter_attribute(variant.to_sym, VARIANT_OPTIONS)\n      @size = filter_attribute(size.to_sym, SIZE_OPTIONS)\n      @type = filter_attribute(type.to_sym, TYPE_OPTIONS, default: :button)\n      @disabled = disabled\n      @loading = loading\n      @block = block\n      @label = label\n      @selected = selected\n      @icon = icon\n      @href = href\n      @form = form\n      @target = filter_attribute(target, TARGET_OPTIONS)\n      @method = filter_attribute(method, METHOD_OPTIONS)\n      @button_options = button_options\n      @button_text_classes = button_text_classes\n      @icon_classes = icon_classes\n    end",
    "comment": "@param [Symbol] category @param [Symbol] variant @param [Symbol] size @param [Symbol] type @param [Boolean] disabled @param [Boolean] loading @param [Boolean] block @param [Boolean] label @param [Boolean] selected @param [String] icon @param [String] href @param [Boolean] form @param [String] target @param [Symbol] method @param [Hash] button_options @param [String] button_text_classes @param [String] icon_classes",
    "label": "",
    "id": "4586"
  },
  {
    "raw_code": "def initialize(\n      title: nil,\n      title_tag: 'span',\n      stat_value: nil,\n      stat_value_testid: 'non-animated-value',\n      unit: nil,\n      title_icon: nil,\n      meta_text: nil,\n      meta_icon: nil,\n      text_color: nil,\n      variant: :neutral\n    )\n      @title = title\n      @title_tag = title_tag\n      @stat_value = stat_value\n      @stat_value_testid = stat_value_testid\n      @unit = unit\n      @title_icon = title_icon.to_s.presence\n      @meta_text = meta_text\n      @meta_icon = meta_icon\n      @text_color = text_color\n      @variant = filter_attribute(variant.to_sym, Pajamas::BadgeComponent::VARIANT_OPTIONS, default: :neutral)\n    end",
    "comment": "@param [String] title @param [String] title_tag @param [String] stat_value @param [String] stat_value_testid @param [String] unit @param [String] title_icon @param [String] meta_text @param [String] meta_icon @param [Symbol] variant",
    "label": "",
    "id": "4587"
  },
  {
    "raw_code": "def content_tag(...)\n    ActionController::Base.helpers.content_tag(...)\n  end",
    "comment": "Avoid including ActionView::Helpers::UrlHelper",
    "label": "",
    "id": "4588"
  },
  {
    "raw_code": "def environments_cluster_path(cluster)\n    nil\n  end",
    "comment": "Will be overridden in EE",
    "label": "",
    "id": "4589"
  },
  {
    "raw_code": "def can_override?\n    false\n  end",
    "comment": "This functionality is only available in EE.",
    "label": "",
    "id": "4590"
  },
  {
    "raw_code": "def code_owners\n    []\n  end",
    "comment": "Will be overridden in EE",
    "label": "",
    "id": "4591"
  },
  {
    "raw_code": "def resource_parent_name\n    resource_parent&.full_name || ''\n  end",
    "comment": "implement cache here",
    "label": "",
    "id": "4592"
  },
  {
    "raw_code": "def link_to(...)\n    ApplicationController.helpers.link_to(...)\n  end",
    "comment": "Avoid including ActionView::Helpers::UrlHelper",
    "label": "",
    "id": "4593"
  },
  {
    "raw_code": "def precalculate_data_by_commit!\n      groups.each { |group| commit_data(group[:commit], group[:previous_path]) }\n    end",
    "comment": "Huge source files with a high churn rate (e.g. 'locale/gitlab.pot') could have 10x times more blame groups than unique commits across all the groups. That means we could cache per-commit data we need to avoid recalculating it multiple times. For such files, it could significantly improve the performance of the Blame.",
    "label": "",
    "id": "4594"
  },
  {
    "raw_code": "def present\n      {\n        candidate: {\n          info: {\n            iid: candidate.iid,\n            eid: candidate.eid,\n            gid: candidate.to_global_id.to_s,\n            path_to_artifact: link_to_artifact,\n            experiment_name: candidate.experiment.name,\n            path_to_experiment: link_to_experiment,\n            path: link_to_details,\n            status: candidate.status,\n            ci_job: job_info,\n            created_at: candidate.created_at,\n            author_web_url: candidate.user&.namespace&.web_url,\n            author_name: candidate.user&.name,\n            promote_path: url_helpers.promote_project_ml_candidate_path(candidate.project, candidate.iid),\n            can_promote: can_promote\n          },\n          params: candidate.params,\n          metrics: candidate.metrics,\n          metadata: candidate.metadata,\n          projectPath: candidate.project.full_path,\n          can_write_model_experiments: current_user&.can?(:write_model_experiments, candidate.project),\n          markdown_preview_path: url_helpers.project_preview_markdown_path(candidate.project),\n          model_gid: candidate.experiment.model&.to_global_id.to_s,\n          model_name: candidate.experiment.model&.name,\n          latest_version: candidate.experiment.model&.latest_version&.version\n        }\n      }\n    end",
    "comment": "rubocop:disable Metrics/AbcSize -- Monoton complexity",
    "label": "",
    "id": "4595"
  },
  {
    "raw_code": "def present_as_json\n      Gitlab::Json.generate(present.deep_transform_keys { |k| k.to_s.camelize(:lower) })\n    end",
    "comment": "rubocop:enable Metrics/AbcSize",
    "label": "",
    "id": "4596"
  },
  {
    "raw_code": "def tags\n      super.map do |tag|\n        name = tag.delete_prefix(Gitlab::Git::TAG_REF_PREFIX)\n        {\n          name: name,\n          path: tag.delete_prefix('refs/'),\n          web_path: project_tag_path(project, id: name)\n        }\n      end",
    "comment": "Note: this returns the path key as 'tags/tag_name' but it is used as a URL in the UI",
    "label": "",
    "id": "4597"
  },
  {
    "raw_code": "def compare_versions(version_a, version_b)\n        return 0 if version_a == version_b\n        return 1 if version_b.nil?\n        return -1 if version_a.nil?\n\n        a_without_build_meta, a_build_meta = version_a.split('+', 2)\n        b_without_build_meta, b_build_meta = version_b.split('+', 2)\n\n        a_core, a_pre = a_without_build_meta.split(/-/, 2)\n        b_core, b_pre = b_without_build_meta.split(/-/, 2)\n\n        a_core_parts = a_core.split('.')\n        b_core_parts = b_core.split('.')\n\n        compare_core_parts(a_core_parts, b_core_parts) ||\n          compare_pre_release_parts(a_pre, b_pre) ||\n          pick_non_nil(a_pre, b_pre) ||\n          compare_build_meta_parts(a_build_meta, b_build_meta)\n      end",
    "comment": "NuGet version sorting algorithm as per https://semver.org/spec/v2.0.0.html#spec-item-11",
    "label": "",
    "id": "4598"
  },
  {
    "raw_code": "def self.failure_reasons\n      { unknown_failure: 'The reason for the pipeline failure is unknown.',\n        config_error: 'The pipeline failed due to an error on the CI/CD configuration file.',\n        external_validation_failure: 'The external pipeline validation failed.',\n        user_not_verified: 'The pipeline failed due to the user not being verified.',\n        size_limit_exceeded: 'The pipeline size limit was exceeded.',\n        job_activity_limit_exceeded: 'The pipeline job activity limit was exceeded.',\n        deployments_limit_exceeded: 'The pipeline deployments limit was exceeded.',\n        project_deleted: 'The project associated with this pipeline was deleted.',\n        filtered_by_rules: Ci::Pipeline.rules_failure_message,\n        filtered_by_workflow_rules: Ci::Pipeline.workflow_rules_failure_message,\n        composite_identity_forbidden: 'This pipeline did not run because the code should be reviewed by a non-AI ' \\\n          'user first. Verify that all changes in this merge request are safe before running a new pipeline.',\n        pipeline_ref_creation_failure: 'Failed to create pipeline ref.' }\n    end",
    "comment": "We use a class method here instead of a constant, allowing EE to redefine the returned `Hash` more easily.",
    "label": "",
    "id": "4599"
  },
  {
    "raw_code": "def registration_omniauth_params\n      {}\n    end",
    "comment": "overridden in EE",
    "label": "",
    "id": "4600"
  },
  {
    "raw_code": "def enabled_keys\n        strong_memoize(:enabled_keys) do\n          project.deploy_keys.with_projects\n        end",
    "comment": "It includes: - The deploy keys enabled in the project.",
    "label": "",
    "id": "4601"
  },
  {
    "raw_code": "def available_keys\n        strong_memoize(:available_keys) do\n          current_user\n            .accessible_deploy_keys\n            .id_not_in(enabled_keys.select(:id))\n            .with_projects\n        end",
    "comment": "NOTE: This method is redundant. Use `available_project_keys` and `available_public_keys` instead. It includes: - Enabled deploy keys in projects that can be accessed by the user. - Instance-level public deploy keys. It excludes: - The deploy keys enabled in the project.",
    "label": "",
    "id": "4602"
  },
  {
    "raw_code": "def available_project_keys\n        strong_memoize(:available_project_keys) do\n          current_user.project_deploy_keys.with_projects - enabled_keys\n        end",
    "comment": "It includes: - Enabled deploy keys in projects that can be accessed by the user. It excludes: - The deploy keys enabled in the project",
    "label": "",
    "id": "4603"
  },
  {
    "raw_code": "def available_public_keys\n        strong_memoize(:available_public_keys) do\n          DeployKey.are_public.with_projects - enabled_keys\n        end",
    "comment": "It includes: - Instance-level public deploy keys. It excludes: - The deploy keys enabled in the project.",
    "label": "",
    "id": "4604"
  },
  {
    "raw_code": "def readable_project_ids\n        strong_memoize(:readable_projects_by_id) do\n          Set.new(user_readable_project_ids)\n        end",
    "comment": "Caching all readable project ids for the user that are associated with the queried deploy keys",
    "label": "",
    "id": "4605"
  },
  {
    "raw_code": "def user_readable_project_ids\n        project_ids = (available_project_keys + available_public_keys)\n          .flat_map { |deploy_key| deploy_key.deploy_keys_projects.map(&:project_id) }\n          .compact\n          .uniq\n\n        current_user.authorized_projects(Gitlab::Access::GUEST).id_in(project_ids).pluck(:id)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4606"
  },
  {
    "raw_code": "def group_members\n        return [] unless current_user.can?(:admin_group, project.group)\n\n        # We need `.connected_to_user` here otherwise when a group has an\n        # invitee, it would make the following query return 0 rows since a NULL\n        # user_id would be present in the subquery\n        non_null_user_ids = project.project_members.connected_to_user.select(:user_id)\n        GroupMembersFinder.new(project.group).execute.where.not(user_id: non_null_user_ids).preload(:user)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4607"
  },
  {
    "raw_code": "def logs_project\n      strong_memoize(:logs_project) do\n        cluster.all_projects.first\n      end",
    "comment": "currently log explorer is only available in the scope of the project for group and instance level cluster selected project does not affects fetching logs from gitlab managed apps namespace, therefore any project available to user will be sufficient.",
    "label": "",
    "id": "4608"
  },
  {
    "raw_code": "def diff_lines(with_positions_and_indent: false)\n      diff_lines = lines.map.with_index do |line, index|\n        full_line = limited_blob_lines[index].delete(\"\\n\")\n\n        if with_positions_and_indent\n          new_pos = index + since\n          old_pos = new_pos - offset\n          line[0, 0] = ' '\n          Gitlab::Diff::Line.new(full_line, nil, nil, old_pos, new_pos, rich_text: line)\n        else\n          Gitlab::Diff::Line.new(full_line, nil, nil, nil, nil, rich_text: line)\n        end",
    "comment": "Returns an array of Gitlab::Diff::Line with match line added",
    "label": "",
    "id": "4609"
  },
  {
    "raw_code": "def initialize(event, push_data)\n    @event = event\n    @push_data = push_data\n  end",
    "comment": "event - The event this push payload belongs to. push_data - A Hash produced by `Gitlab::DataBuilder::Push.build` to use for building the push payload.",
    "label": "",
    "id": "4610"
  },
  {
    "raw_code": "def execute\n    @event.build_push_event_payload(\n      commit_count: commit_count,\n      action: action,\n      ref_type: ref_type,\n      commit_from: commit_from_id,\n      commit_to: commit_to_id,\n      ref: trimmed_ref,\n      commit_title: commit_title,\n      event_id: @event.id\n    )\n\n    @event.push_event_payload.save!\n    @event.push_event_payload\n  end",
    "comment": "Creates and returns a new PushEventPayload row.  This method will raise upon encountering validation errors.  Returns an instance of PushEventPayload.",
    "label": "",
    "id": "4611"
  },
  {
    "raw_code": "def commit_title\n    commit = @push_data.fetch(:commits).last\n\n    return unless commit && commit[:message]\n\n    raw_msg = commit[:message]\n\n    # Find where the first line ends, without turning the entire message into an\n    # Array of lines (this is a waste of memory for large commit messages).\n    index = raw_msg.index(\"\\n\")\n    message = index ? raw_msg[0..index] : raw_msg\n\n    message.strip.truncate(70)\n  end",
    "comment": "Returns the commit title to use.  The commit title is limited to the first line and a maximum of 70 characters.",
    "label": "",
    "id": "4612"
  },
  {
    "raw_code": "def get_html_checkbox(html)\n    html.css(\".task-list-item[data-sourcepos^='#{line_number}:'] input.task-list-item-checkbox\").first\n  end",
    "comment": "When using CommonMark, we should be able to use the embedded `sourcepos` attribute to target the exact line in the DOM.",
    "label": "",
    "id": "4613"
  },
  {
    "raw_code": "def new_issue(issue, current_user)\n    new_issuable(issue, current_user)\n  end",
    "comment": "When create an issue we should:  * create a todo for assignee if issue is assigned * create a todo for each mentioned user on issue ",
    "label": "",
    "id": "4614"
  },
  {
    "raw_code": "def update_issue(issue, current_user, skip_users = [])\n    update_issuable(issue, current_user, skip_users)\n  end",
    "comment": "When update an issue we should:  * mark all pending todos related to the issue for the current user as done ",
    "label": "",
    "id": "4615"
  },
  {
    "raw_code": "def close_issue(issue, current_user)\n    resolve_todos_for_target(issue, current_user)\n  end",
    "comment": "When close an issue we should:  * mark all pending todos related to the target for the current user as done ",
    "label": "",
    "id": "4616"
  },
  {
    "raw_code": "def destroy_target(target)\n    todo_user_ids = target.todos.distinct_user_ids\n\n    yield target\n\n    Users::UpdateTodoCountCacheService.new(todo_user_ids).execute if todo_user_ids.present?\n  end",
    "comment": "When we destroy a todo target we should:  * refresh the todos count cache for all users with todos on the target  This needs to yield back to the caller to destroy the target, because it collects the todo users before the todos themselves are deleted, then updates the todo counts for those users. ",
    "label": "",
    "id": "4617"
  },
  {
    "raw_code": "def reassigned_assignable(issuable, current_user, old_assignees = [])\n    create_assignment_todo(issuable, current_user, old_assignees)\n  end",
    "comment": "When we reassign an assignable object (issuable, alert) we should:  * create a pending todo for new assignee if object is assigned ",
    "label": "",
    "id": "4618"
  },
  {
    "raw_code": "def reassigned_reviewable(issuable, current_user, old_reviewers = [])\n    create_reviewer_todo(issuable, current_user, old_reviewers)\n  end",
    "comment": "When we reassign an reviewable object (merge request) we should:  * create a pending todo for new reviewer if object is assigned ",
    "label": "",
    "id": "4619"
  },
  {
    "raw_code": "def new_merge_request(merge_request, current_user)\n    new_issuable(merge_request, current_user)\n  end",
    "comment": "When create a merge request we should:  * creates a pending todo for assignee if merge request is assigned * create a todo for each mentioned user on merge request ",
    "label": "",
    "id": "4620"
  },
  {
    "raw_code": "def update_merge_request(merge_request, current_user, skip_users = [])\n    update_issuable(merge_request, current_user, skip_users)\n  end",
    "comment": "When update a merge request we should:  * create a todo for each mentioned user on merge request ",
    "label": "",
    "id": "4621"
  },
  {
    "raw_code": "def close_merge_request(merge_request, current_user)\n    resolve_todos_for_target(merge_request, current_user)\n  end",
    "comment": "When close a merge request we should:  * mark all pending todos related to the target for the current user as done ",
    "label": "",
    "id": "4622"
  },
  {
    "raw_code": "def merge_merge_request(merge_request, current_user)\n    resolve_todos_for_target(merge_request, current_user)\n  end",
    "comment": "When merge a merge request we should:  * mark all pending todos related to the target for the current user as done ",
    "label": "",
    "id": "4623"
  },
  {
    "raw_code": "def merge_request_build_failed(merge_request)\n    merge_request.merge_participants.each do |user|\n      create_build_failed_todo(merge_request, user)\n    end",
    "comment": "When a build fails on the HEAD of a merge request we should:  * create a todo for each merge participant ",
    "label": "",
    "id": "4624"
  },
  {
    "raw_code": "def merge_request_push(merge_request, current_user)\n    resolve_todos_for_target(merge_request, current_user)\n  end",
    "comment": "When a new commit is pushed to a merge request we should:  * mark all pending todos related to the merge request for that user as done ",
    "label": "",
    "id": "4625"
  },
  {
    "raw_code": "def merge_request_build_retried(merge_request)\n    merge_request.merge_participants.each do |user|\n      resolve_todos_for_target(merge_request, user)\n    end",
    "comment": "When a build is retried to a merge request we should:  * mark all pending todos related to the merge request as done for each merge participant ",
    "label": "",
    "id": "4626"
  },
  {
    "raw_code": "def merge_request_became_unmergeable(merge_request)\n    merge_request.merge_participants.each do |user|\n      create_unmergeable_todo(merge_request, user)\n    end",
    "comment": "When a merge request could not be merged due to its unmergeable state we should:  * create a todo for each merge participant ",
    "label": "",
    "id": "4627"
  },
  {
    "raw_code": "def new_note(note, current_user)\n    handle_note(note, current_user)\n  end",
    "comment": "When create a note we should:  * mark all pending todos related to the noteable for the note author as done * create a todo for each mentioned user on note ",
    "label": "",
    "id": "4628"
  },
  {
    "raw_code": "def update_note(note, current_user, skip_users = [])\n    handle_note(note, current_user, skip_users)\n  end",
    "comment": "When update a note we should:  * mark all pending todos related to the noteable for the current user as done * create a todo for each new user mentioned on note ",
    "label": "",
    "id": "4629"
  },
  {
    "raw_code": "def new_award_emoji(awardable, current_user)\n    resolve_todos_for_target(awardable, current_user)\n  end",
    "comment": "When an emoji is awarded we should:  * mark all pending todos related to the awardable for the current user as done ",
    "label": "",
    "id": "4630"
  },
  {
    "raw_code": "def ssh_key_expiring_soon(ssh_keys)\n    create_ssh_key_todos(Array(ssh_keys), ::Todo::SSH_KEY_EXPIRING_SOON)\n  end",
    "comment": "When a SSH key is expiring soon we should:  * create a todo for the user owning that SSH key ",
    "label": "",
    "id": "4631"
  },
  {
    "raw_code": "def ssh_key_expired(ssh_keys)\n    ssh_keys = Array(ssh_keys)\n\n    # Resolve any pending \"expiring soon\" todos for these keys\n    expiring_key_todos = ::Todo.pending_for_expiring_ssh_keys(ssh_keys.map(&:id))\n    expiring_key_todos.batch_update(state: :done, resolved_by_action: :system_done)\n\n    create_ssh_key_todos(ssh_keys, ::Todo::SSH_KEY_EXPIRED)\n  end",
    "comment": "When a SSH key expired we should:  * resolve any corresponding \"expiring soon\" todo * create a todo for the user owning that SSH key ",
    "label": "",
    "id": "4632"
  },
  {
    "raw_code": "def new_review(merge_request, current_user)\n    resolve_todos_for_target(merge_request, current_user)\n\n    # Create a new todo for assignees and author\n    create_review_submitted_todo(merge_request, current_user)\n  end",
    "comment": "When a merge request receives a review  * Mark all outstanding todos on this MR for the current user as done ",
    "label": "",
    "id": "4633"
  },
  {
    "raw_code": "def mark_todo(target, current_user)\n    project = target.project\n    attributes = attributes_for_todo(project, target, current_user, Todo::MARKED)\n\n    todos = create_todos(current_user, attributes, target_namespace(target), project)\n    work_item_activity_counter.track_work_item_mark_todo_action(author: current_user) if target.is_a?(WorkItem)\n\n    todos\n  end",
    "comment": "When user marks a target as todo",
    "label": "",
    "id": "4634"
  },
  {
    "raw_code": "def resolve_todos_for_target(target, current_user)\n    attributes = attributes_for_target(target)\n\n    resolve_todos(pending_todos([current_user], attributes), current_user)\n\n    GraphqlTriggers.issuable_todo_updated(target)\n  end",
    "comment": "Resolves all todos related to target for the current_user",
    "label": "",
    "id": "4635"
  },
  {
    "raw_code": "def resolve_todos_with_attributes_for_target(target, attributes, resolution: :done, resolved_by_action: :system_done)\n    target_attributes = { target_id: target.id, target_type: target.class.polymorphic_name }\n    attributes.merge!(target_attributes)\n    attributes[:preload_user_association] = true\n\n    todos = PendingTodosFinder.new(attributes).execute\n    users = todos.map(&:user)\n    todos_ids = todos.batch_update(state: resolution, resolved_by_action: resolved_by_action)\n    users.each(&:update_todos_count_cache)\n    todos_ids\n  end",
    "comment": "Resolves all todos related to target for all users",
    "label": "",
    "id": "4636"
  },
  {
    "raw_code": "def self.from_legacy_hash(response)\n    return response if response.is_a?(ServiceResponse)\n    return ServiceResponse.new(**response) if response.is_a?(Hash)\n\n    raise ArgumentError, \"argument must be a ServiceResponse or a Hash\"\n  end",
    "comment": "This is used to help wrap old service responses that were just hashes",
    "label": "",
    "id": "4637"
  },
  {
    "raw_code": "def new_key(key)\n    if key.user&.can?(:receive_notifications)\n      mailer.new_ssh_key_email(key.id).deliver_later\n    end",
    "comment": "Always notify user about ssh key added only if ssh key is not deploy key  This is security email so it will be sent even if user disabled notifications. However, it won't be sent to internal users like the ghost user or the EE support bot.",
    "label": "",
    "id": "4638"
  },
  {
    "raw_code": "def new_gpg_key(gpg_key)\n    if gpg_key.user&.can?(:receive_notifications)\n      mailer.new_gpg_key_email(gpg_key.id).deliver_later\n    end",
    "comment": "Always notify the user about gpg key added  This is a security email so it will be sent even if the user disabled notifications",
    "label": "",
    "id": "4639"
  },
  {
    "raw_code": "def access_token_created(user, token_name)\n    return unless user.can?(:receive_notifications)\n\n    mailer.access_token_created_email(user, token_name).deliver_later\n  end",
    "comment": "Notify the owner of the account when a new personal access token is created",
    "label": "",
    "id": "4640"
  },
  {
    "raw_code": "def access_token_about_to_expire(user, token_names, params = {})\n    return unless user.can?(:receive_notifications)\n\n    log_info(\"Notifying User about expiring tokens\", user)\n\n    mailer.access_token_about_to_expire_email(user, token_names, params).deliver_later\n  end",
    "comment": "Notify the owner of the personal access token, when it is about to expire And mark the token with about_to_expire_delivered",
    "label": "",
    "id": "4641"
  },
  {
    "raw_code": "def access_token_expired(user, token_names = [])\n    return unless user.can?(:receive_notifications)\n\n    mailer.access_token_expired_email(user, token_names).deliver_later\n  end",
    "comment": "Notify the user when at least one of their personal access tokens has expired today",
    "label": "",
    "id": "4642"
  },
  {
    "raw_code": "def access_token_revoked(user, token_name, source = nil)\n    return unless user.can?(:receive_notifications)\n\n    mailer.access_token_revoked_email(user, token_name, source).deliver_later\n  end",
    "comment": "Notify the user when one of their personal access tokens is revoked",
    "label": "",
    "id": "4643"
  },
  {
    "raw_code": "def access_token_rotated(user, token_name)\n    return unless user.can?(:receive_notifications)\n\n    mailer.access_token_rotated_email(user, token_name).deliver_later\n  end",
    "comment": "Notify the user when one of their personal access tokens is rotated",
    "label": "",
    "id": "4644"
  },
  {
    "raw_code": "def deploy_token_about_to_expire(user, token_name, project, params = {})\n    return unless user.can?(:receive_notifications)\n    return unless project.team.owner?(user) || project.team.maintainer?(user)\n\n    log_info(\"Notifying user about expiring deploy tokens\", user)\n\n    mailer.deploy_token_about_to_expire_email(user, token_name, project, params).deliver_later\n  end",
    "comment": "Notify the owner of the deploy token, when it is about to expire",
    "label": "",
    "id": "4645"
  },
  {
    "raw_code": "def ssh_key_expired(user, fingerprints)\n    return unless user.can?(:receive_notifications)\n\n    mailer.ssh_key_expired_email(user, fingerprints).deliver_later\n  end",
    "comment": "Notify the user when at least one of their ssh key has expired today",
    "label": "",
    "id": "4646"
  },
  {
    "raw_code": "def ssh_key_expiring_soon(user, fingerprints)\n    return unless user.can?(:receive_notifications)\n\n    mailer.ssh_key_expiring_soon_email(user, fingerprints).deliver_later\n  end",
    "comment": "Notify the user when at least one of their ssh key is expiring soon",
    "label": "",
    "id": "4647"
  },
  {
    "raw_code": "def unknown_sign_in(user, ip, time, request_info)\n    return unless user.can?(:receive_notifications)\n\n    mailer.unknown_sign_in_email(user, ip, time, country: request_info.country, city: request_info.city).deliver_later\n  end",
    "comment": "Notify a user when a previously unknown IP or device is used to sign in to their account",
    "label": "",
    "id": "4648"
  },
  {
    "raw_code": "def two_factor_otp_attempt_failed(user, ip)\n    return unless user.can?(:receive_notifications)\n\n    mailer.two_factor_otp_attempt_failed_email(user, ip).deliver_later\n  end",
    "comment": "Notify a user when a wrong 2FA OTP has been entered to try to sign in to their account",
    "label": "",
    "id": "4649"
  },
  {
    "raw_code": "def new_email_address_added(user, email)\n    return unless user.can?(:receive_notifications)\n\n    mailer.new_email_address_added_email(user, email).deliver_later\n  end",
    "comment": "Notify a user when a new email address is added to the their account",
    "label": "",
    "id": "4650"
  },
  {
    "raw_code": "def new_issue(issue, current_user)\n    new_resource_email(issue, current_user, :new_issue_email)\n  end",
    "comment": "When create an issue we should send an email to:  * issue assignee if their notification level is not Disabled * project team members with notification level higher then Participating * watchers of the issue's labels * users with custom level checked with \"new issue\" ",
    "label": "",
    "id": "4651"
  },
  {
    "raw_code": "def new_mentions_in_issue(issue, new_mentioned_users, current_user)\n    new_mentions_in_resource_email(\n      issue,\n      new_mentioned_users,\n      current_user,\n      :new_mention_in_issue_email\n    )\n  end",
    "comment": "When issue text is updated, we should send an email to:  * newly mentioned project team members with notification level higher than Participating ",
    "label": "",
    "id": "4652"
  },
  {
    "raw_code": "def close_issue(issue, current_user, params = {})\n    close_resource_email(issue, current_user, :closed_issue_email, closed_via: params[:closed_via])\n  end",
    "comment": "When we close an issue we should send an email to:  * issue author if their notification level is not Disabled * issue assignee if their notification level is not Disabled * project team members with notification level higher then Participating * users with custom level checked with \"close issue\" ",
    "label": "",
    "id": "4653"
  },
  {
    "raw_code": "def reassigned_issue(issue, current_user, previous_assignees = [])\n    recipients = NotificationRecipients::BuildService.build_recipients(\n      issue,\n      current_user,\n      action: \"reassign\",\n      previous_assignees: previous_assignees\n    )\n\n    previous_assignee_ids = previous_assignees.map(&:id)\n\n    recipients.each do |recipient|\n      mailer.send(\n        :reassigned_issue_email,\n        recipient.user.id,\n        issue.id,\n        previous_assignee_ids,\n        current_user.id,\n        recipient.reason\n      ).deliver_later\n    end",
    "comment": "When we reassign an issue we should send an email to:  * issue old assignees if their notification level is not Disabled * issue new assignees if their notification level is not Disabled * users with custom level checked with \"reassign issue\" ",
    "label": "",
    "id": "4654"
  },
  {
    "raw_code": "def relabeled_issue(issue, added_labels, current_user)\n    relabeled_resource_email(issue, added_labels, current_user, :relabeled_issue_email)\n  end",
    "comment": "When we add labels to an issue we should send an email to:  * watchers of the issue's labels ",
    "label": "",
    "id": "4655"
  },
  {
    "raw_code": "def new_merge_request(merge_request, current_user)\n    new_resource_email(merge_request, current_user, :new_merge_request_email)\n  end",
    "comment": "When create a merge request we should send an email to:  * mr author * mr assignees if their notification level is not Disabled * project team members with notification level higher then Participating * watchers of the mr's labels * users with custom level checked with \"new merge request\"  In EE, approvers of the merge request are also included",
    "label": "",
    "id": "4656"
  },
  {
    "raw_code": "def merge_request_unmergeable(merge_request)\n    merge_request_unmergeable_email(merge_request)\n  end",
    "comment": "When a merge request is found to be unmergeable, we should send an email to:  * mr author * mr merge user if set ",
    "label": "",
    "id": "4657"
  },
  {
    "raw_code": "def new_mentions_in_merge_request(merge_request, new_mentioned_users, current_user)\n    new_mentions_in_resource_email(\n      merge_request,\n      new_mentioned_users,\n      current_user,\n      :new_mention_in_merge_request_email\n    )\n  end",
    "comment": "When merge request text is updated, we should send an email to:  * newly mentioned project team members with notification level higher than Participating ",
    "label": "",
    "id": "4658"
  },
  {
    "raw_code": "def reassigned_merge_request(merge_request, current_user, previous_assignees = [])\n    recipients = NotificationRecipients::BuildService.build_recipients(\n      merge_request,\n      current_user,\n      action: \"reassign\",\n      previous_assignees: previous_assignees\n    )\n\n    previous_assignee_ids = previous_assignees.map(&:id)\n\n    recipients.each do |recipient|\n      mailer.reassigned_merge_request_email(\n        recipient.user.id,\n        merge_request.id,\n        previous_assignee_ids,\n        current_user.id,\n        recipient.reason\n      ).deliver_later\n    end",
    "comment": "When we reassign a merge_request we should send an email to:  * merge_request old assignees if their notification level is not Disabled * merge_request new assignees if their notification level is not Disabled * users with custom level checked with \"reassign merge request\" ",
    "label": "",
    "id": "4659"
  },
  {
    "raw_code": "def changed_reviewer_of_merge_request(merge_request, current_user, previous_reviewers = [])\n    recipients = NotificationRecipients::BuildService.build_recipients(\n      merge_request,\n      current_user,\n      action: \"change_reviewer\",\n      previous_assignees: previous_reviewers\n    )\n\n    previous_reviewer_ids = previous_reviewers.map(&:id)\n\n    recipients.each do |recipient|\n      mailer.changed_reviewer_of_merge_request_email(\n        recipient.user.id,\n        merge_request.id,\n        previous_reviewer_ids,\n        current_user.id,\n        recipient.reason\n      ).deliver_later\n    end",
    "comment": "When we change reviewer in a merge_request we should send an email to:  * merge_request old reviewers if their notification level is not Disabled * merge_request new reviewers if their notification level is not Disabled * users with custom level checked with \"change reviewer merge request\" ",
    "label": "",
    "id": "4660"
  },
  {
    "raw_code": "def relabeled_merge_request(merge_request, added_labels, current_user)\n    relabeled_resource_email(merge_request, added_labels, current_user, :relabeled_merge_request_email)\n  end",
    "comment": "When we add labels to a merge request we should send an email to:  * watchers of the mr's labels ",
    "label": "",
    "id": "4661"
  },
  {
    "raw_code": "def new_user(user, token = nil)\n    return true unless notifiable?(user, :mention)\n\n    # Don't email omniauth created users\n    mailer.new_user_email(user.id, token).deliver_later unless user.identities.any?\n  end",
    "comment": "Notify new user with email after creation",
    "label": "",
    "id": "4662"
  },
  {
    "raw_code": "def new_note(note)\n    return true unless note.noteable_type.present?\n\n    # ignore gitlab service messages\n    return true if note.system_note_with_references?\n\n    send_new_note_notifications(note)\n    send_service_desk_notification(note)\n  end",
    "comment": "Notify users on new note in system",
    "label": "",
    "id": "4663"
  },
  {
    "raw_code": "def send_new_release_notifications(release)\n    unless release.author&.can_trigger_notifications?\n      warn_skipping_notifications(release.author, release)\n      return false\n    end",
    "comment": "Notify users when a new release is created",
    "label": "",
    "id": "4664"
  },
  {
    "raw_code": "def new_review(review)\n    recipients = NotificationRecipients::BuildService.build_new_review_recipients(review)\n    deliver_options = new_review_deliver_options(review)\n\n    recipients.each do |recipient|\n      mailer\n        .new_review_email(recipient.user.id, review.id)\n        .deliver_later(deliver_options)\n    end",
    "comment": "Notify users on new review in system",
    "label": "",
    "id": "4665"
  },
  {
    "raw_code": "def expired?\n    domain.enabled_until&.past?\n  end",
    "comment": "A domain is only expired until `disable!` has been called",
    "label": "",
    "id": "4666"
  },
  {
    "raw_code": "def initialize(author, entity, details = {}, save_type = :database_and_stream, created_at = DateTime.current)\n    @author = build_author(author)\n    @entity = entity\n    @details = details\n    @ip_address = resolve_ip_address(@author)\n    @save_type = save_type\n    @created_at = created_at\n\n    validate_scope!(@entity)\n\n    log_initialization\n  end",
    "comment": "Instantiates a new service  @deprecated This service is deprecated. Use Gitlab::Audit::Auditor instead. More information: https://docs.gitlab.com/ee/development/audit_event_guide/#how-to-instrument-new-audit-events  @param [User, token String] author the entity who authors the change @param [User, Project, Group] entity the scope which audit event belongs to This param is also used to determine the visibility of the audit event. - Project: events are visible at Project and Instance level - Group: events are visible at Group and Instance level - User: events are visible at Instance level @param [Hash] details extra data of audit event @param [Symbol] save_type the type to save the event Can be selected from the following, :database, :stream, :database_and_stream . @params [DateTime] created_at the time the action occured  @return [AuditEventService]",
    "label": "",
    "id": "4667"
  },
  {
    "raw_code": "def for_authentication\n    mark_as_authentication_event!\n\n    @details = {\n      with: @details[:with],\n      target_id: @author.id,\n      target_type: 'User',\n      target_details: @author.name\n    }\n\n    self\n  end",
    "comment": "Builds the @details attribute for authentication  This uses the @author as the target object being audited  @return [AuditEventService]",
    "label": "",
    "id": "4668"
  },
  {
    "raw_code": "def security_event\n    log_security_event_to_file\n    log_authentication_event_to_database\n    log_security_event_to_database\n  end",
    "comment": "Writes event to a file and creates an event record in DB  @return [AuditEvent] persisted if saves and non-persisted if fails",
    "label": "",
    "id": "4669"
  },
  {
    "raw_code": "def log_security_event_to_file\n    file_logger.info(base_payload.merge(formatted_details))\n  end",
    "comment": "Writes event to a file",
    "label": "",
    "id": "4670"
  },
  {
    "raw_code": "def execute_without_params(callback_class)\n    # We should update execute without params only during creation.\n    # During update the services are not build to run without params.\n    return false unless create_service?\n\n    callback_class.execute_without_params?\n  end",
    "comment": "Helper method, that is being used when we initialize the callbacks, to dynamically determine if a callback class can be executed without params",
    "label": "",
    "id": "4671"
  },
  {
    "raw_code": "def create_service?\n    class_name = self.class.name\n\n    class_name.include?('WorkItems::CreateService') || class_name.include?('Issues::CreateService')\n  end",
    "comment": "Method to determine if this is a create service for a WorkItem or Issue",
    "label": "",
    "id": "4672"
  },
  {
    "raw_code": "def merge_quick_actions_into_params!(issuable, params:, only: nil)\n    target_description = params.fetch(:description, issuable.description)\n\n    description, command_params = QuickActions::InterpretService.new(\n      container: container,\n      current_user: current_user,\n      params: quick_action_options\n    ).execute_with_original_text(target_description, issuable, only: only, original_text: issuable.description_was)\n\n    # Avoid a description already set on an issuable to be overwritten by a nil\n    params[:description] = description if description && description != target_description\n\n    params.merge!(command_params)\n  end",
    "comment": "Notes: When the description has been edited, then we need to sanitize and compare with the original description, removing any extra quick actions. If the description has not been edited, then just remove any quick actions in the current description.",
    "label": "",
    "id": "4673"
  },
  {
    "raw_code": "def trigger_update_subscriptions(issuable, old_associations); end\n\n  def transaction_update(issuable, opts = {})\n    touch = opts[:save_with_touch] || false\n\n    issuable.save(touch: touch).tap do |saved|\n      if saved\n        @callbacks.each(&:after_update)\n        @callbacks.each(&:after_save)\n      end\n    end",
    "comment": "Overriden in child class",
    "label": "",
    "id": "4674"
  },
  {
    "raw_code": "def update_task_event(issuable)\n    update_task_params = params.delete(:update_task)\n    return unless update_task_params\n\n    tasklist_toggler = TaskListToggleService.new(\n      issuable.description,\n      issuable.description_html,\n      line_source: update_task_params[:line_source],\n      line_number: update_task_params[:line_number].to_i,\n      toggle_as_checked: update_task_params[:checked]\n    )\n\n    unless tasklist_toggler.execute\n      # if we make it here, the data is much newer than we thought it was - fail fast\n      raise ActiveRecord::StaleObjectError\n    end",
    "comment": "Handle the `update_task` event sent from UI.  Attempts to update a specific line in the markdown and cached html, bypassing any unnecessary updates or checks.",
    "label": "",
    "id": "4675"
  },
  {
    "raw_code": "def change_todo(issuable)\n    case params.delete(:todo_event)\n    when 'add'\n      todo_service.mark_todo(issuable, current_user)\n    when 'done'\n      todo = TodosFinder.new(users: current_user).find_by(target: issuable)\n      todo_service.resolve_todo(todo, current_user) if todo\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4676"
  },
  {
    "raw_code": "def assign_requested_crm_contacts(issuable)\n    add_crm_contact_emails = params.delete(:add_contacts)\n    remove_crm_contact_emails = params.delete(:remove_contacts)\n    set_crm_contacts(issuable, add_crm_contact_emails, remove_crm_contact_emails)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4677"
  },
  {
    "raw_code": "def ids_changing?(old_array, new_array)\n    old_array.sort != new_array.sort\n  end",
    "comment": "Arrays of ids are used, but we should really use sets of ids, so let's have an helper to properly check if some ids are changing",
    "label": "",
    "id": "4678"
  },
  {
    "raw_code": "def handle_label_changes(issuable, old_labels)\n    return false unless has_label_changes?(issuable, old_labels)\n\n    # reset to preserve the label sort order (title ASC)\n    issuable.labels.reset\n\n    GraphqlTriggers.issuable_labels_updated(issuable)\n\n    # return true here to avoid checking for label changes in sub classes\n    true\n  end",
    "comment": "override if needed",
    "label": "",
    "id": "4679"
  },
  {
    "raw_code": "def handle_changes(issuable, options); end\n\n  # override if needed\n  def handle_task_changes(issuable); end\n\n  # override if needed\n  def execute_hooks(issuable, action = 'open', params = {}); end\n\n  def update_project_counter_caches?(issuable)\n    issuable.state_id_changed?\n  end\n\n  def parent\n    project\n  end\n\n  def update_timestamp?(issuable)\n    issuable.changes.keys != [\"relative_position\"]\n  end\n\n  def allowed_create_params(params)\n    params.except(:observability_links)\n  end\n\n  def allowed_update_params(params)\n    params\n  end\n\n  def update_issuable_sla(issuable)\n    return unless issuable_sla = issuable.issuable_sla\n\n    issuable_sla.update(issuable_closed: issuable.closed?)\n  end\n\n  def filter_contact_params(issuable)\n    return if params.slice(:add_contacts, :remove_contacts).empty?\n    return if can?(current_user, :set_issue_crm_contacts, issuable)\n\n    params.extract!(:add_contacts, :remove_contacts)\n  end\nend\n\nIssuableBaseService.prepend_mod_with('IssuableBaseService')",
    "comment": "override if needed",
    "label": "",
    "id": "4680"
  },
  {
    "raw_code": "def cohorts\n    months = Array.new(MONTHS_INCLUDED) { |i| i.months.ago.beginning_of_month.to_date }\n\n    Array.new(MONTHS_INCLUDED) do\n      registration_month = months.last\n      activity_months = running_totals(months, registration_month)\n\n      # Even if no users registered in this month, we always want to have a\n      # value to fill in the table.\n      inactive = counts_by_month[[registration_month, nil]].to_i\n\n      months.pop\n\n      {\n        registration_month: registration_month,\n        activity_months: activity_months[1..],\n        total: activity_months.first[:total],\n        inactive: inactive\n      }\n    end",
    "comment": "Get an array of hashes that looks like:  [ { registration_month: Date.new(2017, 3), activity_months: [3, 2, 1], total: 3 inactive: 0 }, etc.  The `months` array is always from oldest to newest, so it's always non-strictly decreasing from left to right.",
    "label": "",
    "id": "4681"
  },
  {
    "raw_code": "def running_totals(all_months, registration_month)\n    month_totals =\n      all_months\n        .map { |activity_month| counts_by_month[[registration_month, activity_month]] }\n        .reduce([]) { |result, total| result << (result.last.to_i + total.to_i) }\n        .reverse\n\n    overall_total = month_totals.first\n\n    month_totals.map do |total|\n      { total: total, percentage: total == 0 ? 0 : 100 * total / overall_total }\n    end",
    "comment": "Calculate a running sum of active users, so users active in later months count as active in this month, too. Start with the most recent month first, for calculating the running totals, and then reverse for displaying in the table.  Each month has a total, and a percentage of the overall total, as keys.",
    "label": "",
    "id": "4682"
  },
  {
    "raw_code": "def counts_by_month\n    @counts_by_month ||=\n      begin\n        created_at_month = column_to_date('created_at')\n        last_activity_on_month = column_to_date('last_activity_on')\n\n        User\n          .where('created_at > ?', MONTHS_INCLUDED.months.ago.end_of_month)\n          .group(created_at_month, last_activity_on_month)\n          .reorder(Arel.sql(\"#{created_at_month} ASC, #{last_activity_on_month} ASC\"))\n          .count\n      end",
    "comment": "Get a hash that looks like:  { [created_at_month, last_activity_on_month] => count, [created_at_month, last_activity_on_month_2] => count_2, # etc. }  created_at_month can never be nil, but last_activity_on_month can (when a user has never logged in, just been created). This covers the last MONTHS_INCLUDED months. rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4683"
  },
  {
    "raw_code": "def column_to_date(column)\n    \"CAST(DATE_TRUNC('month', #{column}) AS date)\"\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4684"
  },
  {
    "raw_code": "def add_commits(noteable, project, author, new_commits, existing_commits = [], oldrev = nil)\n    ::SystemNotes::CommitService.new(noteable: noteable, container: project, author: author).add_commits(new_commits, existing_commits, oldrev)\n  end",
    "comment": "Called when commits are added to a merge request  noteable         - Noteable object container        - Project or Namespace(Group or ProjectNamespace) owning noteable author           - User performing the change new_commits      - Array of Commits added since last push existing_commits - Array of Commits added in a previous push oldrev           - Optional String SHA of a previous Commit  Returns the created Note object",
    "label": "",
    "id": "4685"
  },
  {
    "raw_code": "def tag_commit(noteable, project, author, tag_name)\n    ::SystemNotes::CommitService.new(noteable: noteable, container: project, author: author).tag_commit(tag_name)\n  end",
    "comment": "Called when a commit was tagged  noteable  - Noteable object container - Project or Namespace(Group or ProjectNamespace) owning noteable author    - User performing the tag tag_name  - The created tag name  Returns the created Note object",
    "label": "",
    "id": "4686"
  },
  {
    "raw_code": "def change_start_date_or_due_date(noteable, project, author, changed_dates)\n    ::SystemNotes::TimeTrackingService.new(\n      noteable: noteable,\n      container: project,\n      author: author\n    ).change_start_date_or_due_date(changed_dates)\n  end",
    "comment": "Called when the due_date or start_date of a Noteable is changed  noteable  - Noteable object project   - Project owning noteable author    - User performing the change due_date  - Due date being assigned, or nil  Example Note text:  \"removed due date\"  \"changed due date to September 20, 2018 and changed start date to September 25, 2018\"  Returns the created Note object",
    "label": "",
    "id": "4687"
  },
  {
    "raw_code": "def change_time_estimate(noteable, project, author)\n    ::SystemNotes::TimeTrackingService.new(noteable: noteable, container: project, author: author).change_time_estimate\n  end",
    "comment": "Called when the estimated time of a Noteable is changed  noteable      - Noteable object project       - Project owning noteable author        - User performing the change time_estimate - Estimated time  Example Note text:  \"removed time estimate\"  \"changed time estimate to 3d 5h\"  Returns the created Note object",
    "label": "",
    "id": "4688"
  },
  {
    "raw_code": "def change_time_spent(noteable, project, author)\n    ::SystemNotes::TimeTrackingService.new(noteable: noteable, container: project, author: author).change_time_spent\n  end",
    "comment": "Called when the spent time of a Noteable is changed  noteable   - Noteable object project    - Project owning noteable author     - User performing the change time_spent - Spent time  Example Note text:  \"removed time spent\"  \"added 2h 30m of time spent\"  Returns the created Note object",
    "label": "",
    "id": "4689"
  },
  {
    "raw_code": "def created_timelog(issuable, project, author, timelog)\n    ::SystemNotes::TimeTrackingService.new(noteable: issuable, container: project, author: author).created_timelog(timelog)\n  end",
    "comment": "Called when a timelog is added to an issuable  issuable   - Issuable object (Issue, WorkItem or MergeRequest) project    - Project owning the issuable author     - User performing the change timelog    - Created timelog  Example Note text:  \"subtracted 1h 15m of time spent\"  \"added 2h 30m of time spent\"  Returns the created Note object",
    "label": "",
    "id": "4690"
  },
  {
    "raw_code": "def remove_timelog(noteable, project, author, timelog)\n    ::SystemNotes::TimeTrackingService.new(noteable: noteable, container: project, author: author).remove_timelog(timelog)\n  end",
    "comment": "Called when a timelog is removed from a Noteable  noteable  - Noteable object project   - Project owning the noteable author    - User performing the change timelog   - The removed timelog  Example Note text: \"deleted 2h 30m of time spent from 22-03-2022\"  Returns the created Note object",
    "label": "",
    "id": "4691"
  },
  {
    "raw_code": "def merge_when_checks_pass(noteable, project, author, sha)\n    ::SystemNotes::MergeRequestsService.new(noteable: noteable, container: project, author: author).merge_when_checks_pass(sha)\n  end",
    "comment": "Called when 'merge when checks pass' is executed",
    "label": "",
    "id": "4692"
  },
  {
    "raw_code": "def cancel_auto_merge(noteable, project, author)\n    ::SystemNotes::MergeRequestsService.new(noteable: noteable, container: project, author: author).cancel_auto_merge\n  end",
    "comment": "Called when 'auto merge' is canceled",
    "label": "",
    "id": "4693"
  },
  {
    "raw_code": "def abort_auto_merge(noteable, project, author, reason)\n    ::SystemNotes::MergeRequestsService.new(noteable: noteable, container: project, author: author).abort_auto_merge(reason)\n  end",
    "comment": "Called when 'auto merge' is aborted",
    "label": "",
    "id": "4694"
  },
  {
    "raw_code": "def change_branch(noteable, project, author, branch_type, event_type, old_branch, new_branch)\n    ::SystemNotes::MergeRequestsService.new(noteable: noteable, container: project, author: author)\n      .change_branch(branch_type, event_type, old_branch, new_branch)\n  end",
    "comment": "Called when a branch in Noteable is changed  noteable    - Noteable object container   - Project or Namespace(Group or ProjectNamespace) owning noteable author      - User performing the change branch_type - 'source' or 'target' event_type  - the source of event: 'update' or 'delete' old_branch  - old branch name new_branch  - new branch name  Example Note text is based on event_type:  update: \"changed target branch from `Old` to `New`\" delete: \"deleted the `Old` branch. This merge request now targets the `New` branch\"  Returns the created Note object",
    "label": "",
    "id": "4695"
  },
  {
    "raw_code": "def change_branch_presence(noteable, project, author, branch_type, branch, presence)\n    ::SystemNotes::MergeRequestsService.new(noteable: noteable, container: project, author: author).change_branch_presence(branch_type, branch, presence)\n  end",
    "comment": "Called when a branch in Noteable is added or deleted  noteable    - Noteable object container   - Project or Namespace(Group or ProjectNamespace) owning noteable author      - User performing the change branch_type - :source or :target branch      - branch name presence    - :add or :delete  Example Note text:  \"restored target branch `feature`\"  Returns the created Note object",
    "label": "",
    "id": "4696"
  },
  {
    "raw_code": "def new_issue_branch(issue, project, author, branch, branch_project: nil)\n    ::SystemNotes::MergeRequestsService.new(noteable: issue, container: project, author: author).new_issue_branch(branch, branch_project: branch_project)\n  end",
    "comment": "Called when a branch is created from the 'new branch' button on a issue Example note text:  \"created branch `201-issue-branch-button`\"",
    "label": "",
    "id": "4697"
  },
  {
    "raw_code": "def design_version_added(version)\n    ::SystemNotes::DesignManagementService.new(noteable: version.issue, container: version.issue.project, author: version.author).design_version_added(version)\n  end",
    "comment": "Parameters: - version [DesignManagement::Version]  Example Note text:  \"added [1 designs](link-to-version)\" \"changed [2 designs](link-to-version)\"  Returns [Array<Note>]: the created Note objects",
    "label": "",
    "id": "4698"
  },
  {
    "raw_code": "def design_discussion_added(discussion_note)\n    design = discussion_note.noteable\n\n    ::SystemNotes::DesignManagementService.new(noteable: design.issue, container: design.project, author: discussion_note.author).design_discussion_added(discussion_note)\n  end",
    "comment": "Called when a new discussion is created on a design  discussion_note - DiscussionNote  Example Note text:  \"started a discussion on screen.png\"  Returns the created Note object",
    "label": "",
    "id": "4699"
  },
  {
    "raw_code": "def approve_mr(noteable, user)\n    merge_requests_service(noteable, noteable.project, user).approve_mr\n  end",
    "comment": "Called when the merge request is approved by user  noteable - Noteable object user     - User performing approve  Example Note text:  \"approved this merge request\"  Returns the created Note object",
    "label": "",
    "id": "4700"
  },
  {
    "raw_code": "def include_any_scope?(required_scopes)\n    if required_scopes.blank?\n      true\n    else\n      # We're comparing each required_scope against all token scopes, which would\n      # take quadratic time. This consideration is irrelevant here because of the\n      # small number of records involved.\n      # https://gitlab.com/gitlab-org/gitlab-foss/merge_requests/12300/#note_33689006\n      token_scopes = token.scopes.map(&:to_sym)\n\n      required_scopes.any? do |scope|\n        scope = API::Scope.new(scope) unless scope.is_a?(API::Scope)\n        scope.sufficient?(token_scopes, request)\n      end",
    "comment": "True if the token's scope contains any of the passed scopes.",
    "label": "",
    "id": "4701"
  },
  {
    "raw_code": "def wiki_event(wiki_page_meta, author, action, fingerprint)\n    raise IllegalActionError, action unless Event::WIKI_ACTIONS.include?(action)\n\n    Gitlab::UsageDataCounters::HLLRedisCounter.track_event(:git_write_action, values: author.id)\n\n    track_internal_event(\"performed_wiki_action\",\n      project: wiki_page_meta.project,\n      user: author,\n      additional_properties: { label: action.to_s }\n    )\n\n    duplicate = Event.for_wiki_meta(wiki_page_meta).for_fingerprint(fingerprint).first\n    return duplicate if duplicate.present?\n\n    create_record_event(wiki_page_meta, author, action, fingerprint.presence)\n  end",
    "comment": "Create a new wiki page event  @param [WikiPage::Meta] wiki_page_meta The event target @param [User] author The event author @param [Symbol] action One of the Event::WIKI_ACTIONS @param [String] fingerprint The de-duplication fingerprint  The fingerprint, if provided, should be sufficient to find duplicate events. Suitable values would be, for example, the current page SHA.  @return [Event] the event",
    "label": "",
    "id": "4702"
  },
  {
    "raw_code": "def create_record_events(tuples, current_user)\n    base_attrs = {\n      created_at: Time.now.utc,\n      updated_at: Time.now.utc,\n      author_id: current_user.id\n    }\n\n    attribute_sets = tuples.map do |record, status, fingerprint|\n      action = Event.actions[status]\n      raise IllegalActionError, \"#{status} is not a valid status\" if action.nil?\n\n      parent_attrs(record.resource_parent, current_user)\n        .merge(base_attrs)\n        .merge(action: action, fingerprint: fingerprint, target_id: record.id, target_type: record.class.name)\n    end",
    "comment": "If creating several events, this method will insert them all in a single statement  @param [[Eventable, Symbol, String]] a list of tuples of records, a valid status, and fingerprint @param [User] the author of the event",
    "label": "",
    "id": "4703"
  },
  {
    "raw_code": "def safe_response_headers(response)\n    response.headers.each_capitalized.first(RESPONSE_HEADERS_COUNT_LIMIT).to_h do |header_key, header_value|\n      [enforce_utf8(header_key), string_size_limit(enforce_utf8(header_value), RESPONSE_HEADERS_SIZE_LIMIT)]\n    end",
    "comment": "Make response headers more stylish Net::HTTPHeader has downcased hash with arrays: { 'content-type' => ['text/html; charset=utf-8'] } This method format response to capitalized hash with strings: { 'Content-Type' => 'text/html; charset=utf-8' }",
    "label": "",
    "id": "4704"
  },
  {
    "raw_code": "def rate_limit!\n    Gitlab::WebHooks::RateLimiter.new(hook).rate_limit!\n  end",
    "comment": "Increments rate-limit counter. Returns true if hook should be rate-limited.",
    "label": "",
    "id": "4705"
  },
  {
    "raw_code": "def cache_options\n    { raw: raw? }\n  end",
    "comment": "subclasses can override to add any specific options, such as super.merge({ expires_in: 5.minutes })",
    "label": "",
    "id": "4706"
  },
  {
    "raw_code": "def initialize(current_user, project, start_at)\n      @current_user = current_user\n      @project = project\n      @jira_integration = project.jira_integration\n      @start_at = start_at\n    end",
    "comment": "The class is called from UsersImporter and small batches of users are expected In case the mapping of a big batch of users is expected to be passed here the implementation needs to change here and handles the matching in batches",
    "label": "",
    "id": "4707"
  },
  {
    "raw_code": "def self.disable_ropc_available?\n      false\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4708"
  },
  {
    "raw_code": "def initialize(current_user, request, params)\n      @current_user = current_user\n      @request = request\n      @params = params.except(:ip_address)\n    end",
    "comment": "EE would override and use `request` arg",
    "label": "",
    "id": "4709"
  },
  {
    "raw_code": "def update_targets\n      target.targets.each do |target|\n        if enable?\n          enable(target)\n        elsif disable?\n          Feature.disable(name, target)\n        elsif opt_out?\n          Feature.opt_out(name, target)\n        elsif remove_opt_out?\n          remove_opt_out(target)\n        else\n          raise UnknownOperationError, \"Cannot set '#{name}' to #{value.inspect} for #{target}\"\n        end",
    "comment": "Note: the if expressions in `update_targets` and `update_global` are order dependant.",
    "label": "",
    "id": "4710"
  },
  {
    "raw_code": "def disable?\n      value.in?(%w[0 0.0 false])\n    end",
    "comment": "Note: `key` is NOT considered - setting to a percentage to 0 is the same as disabling.",
    "label": "",
    "id": "4711"
  },
  {
    "raw_code": "def enable?\n      value.in?(%w[100 100.0 true])\n    end",
    "comment": "Note: `key` is NOT considered - setting to a percentage to 100 is the same",
    "label": "",
    "id": "4712"
  },
  {
    "raw_code": "def parsed_params\n        params\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4713"
  },
  {
    "raw_code": "def post_success\n      resource.reset\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "4714"
  },
  {
    "raw_code": "def post_success\n      resource.reset\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "4715"
  },
  {
    "raw_code": "def notify(merge_request); end\n\n    def strategy\n      strong_memoize(:strategy) do\n        self.class.name.demodulize.remove('Service').underscore\n      end\n    end",
    "comment": "Overridden in child classes",
    "label": "",
    "id": "4716"
  },
  {
    "raw_code": "def clearable_auto_merge_parameters\n      %w[\n        should_remove_source_branch\n        commit_message\n        squash_commit_message\n        auto_merge_strategy\n      ]\n    end",
    "comment": "Overridden in EE child classes",
    "label": "",
    "id": "4717"
  },
  {
    "raw_code": "def user_has_group_membership?(user)\n      ::GroupMember\n        .with_user(user)\n        .with_source_id(group.self_and_descendants)\n        .any? ||\n        ::ProjectMember\n        .with_user(user)\n        .in_namespaces(group.self_and_descendants)\n        .any?\n    end",
    "comment": "Validate whether the user has access to a group or any of its descendants. Includes membership that might not be active, but could be later, e.g. bans. Includes membership of non-human users.",
    "label": "",
    "id": "4718"
  },
  {
    "raw_code": "def transfer_status_data(old_root_ancestor_id); end\n\n    # Overridden in EE\n    def post_update_hooks(updated_project_ids, old_root_ancestor_id)\n      refresh_project_authorizations\n      refresh_descendant_groups if @new_parent_group\n      publish_event(old_root_ancestor_id)\n    end\n\n    # Overridden in EE\n    def ensure_allowed_transfer\n      raise_transfer_error(:group_is_already_root) if group_is_already_root?\n      raise_transfer_error(:same_parent_as_current) if same_parent?\n      raise_transfer_error(:has_subscription) if has_subscription?\n      raise_transfer_error(:invalid_policies) unless valid_policies?\n      raise_transfer_error(:namespace_with_same_path) if namespace_with_same_path?\n      raise_transfer_error(:group_contains_images) if group_projects_contain_registry_images?\n      raise_transfer_error(:cannot_transfer_to_subgroup) if transfer_to_subgroup?\n      raise_transfer_error(:group_contains_namespaced_npm_packages) if group_with_namespaced_npm_packages?\n      raise_transfer_error(:no_permissions_to_migrate_crm) if no_permissions_to_migrate_crm?\n    end\n\n    def no_permissions_to_migrate_crm?\n      return false unless group && @new_parent_group\n      return false if group.crm_settings&.source_group\n      return false if group.crm_group == @new_parent_group.crm_group\n\n      return true if group.crm_group.contacts.exists? && !current_user.can?(:admin_crm_contact, @new_parent_group.root_ancestor)\n      return true if group.crm_group.crm_organizations.exists? && !current_user.can?(:admin_crm_organization, @new_parent_group.root_ancestor)\n\n      false\n    end\n\n    def group_with_namespaced_npm_packages?\n      return false unless group.packages_feature_enabled?\n\n      # TODO: Remove `packages_class` with the rollout of the FF packages_refactor_group_packages_finder\n      # https://gitlab.com/gitlab-org/gitlab/-/issues/568923\n      npm_packages = ::Packages::GroupPackagesFinder\n                       .new(current_user, group, packages_class: ::Packages::Npm::Package, preload_pipelines: false, package_type: :npm)\n                       .execute\n\n      npm_packages = npm_packages.with_npm_scope(group.root_ancestor.path)\n\n      different_root_ancestor? && npm_packages.exists?\n    end\n\n    def different_root_ancestor?\n      group.root_ancestor != new_parent_group&.root_ancestor\n    end\n\n    def group_is_already_root?\n      !@new_parent_group && !@group.has_parent?\n    end\n\n    def same_parent?\n      @new_parent_group && @new_parent_group.id == @group.parent_id\n    end\n\n    def has_subscription?\n      @group.paid?\n    end\n\n    def transfer_to_subgroup?\n      @new_parent_group && \\\n        @group.self_and_descendants.pluck_primary_key.include?(@new_parent_group.id)\n    end\n\n    def valid_policies?\n      return false unless can?(current_user, :change_group, @group)\n\n      if @new_parent_group\n        can?(current_user, :create_subgroup, @new_parent_group)\n      else\n        can?(current_user, :create_group)\n      end\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4719"
  },
  {
    "raw_code": "def namespace_with_same_path?\n      Namespace.exists?(path: @group.path, parent: @new_parent_group)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4720"
  },
  {
    "raw_code": "def group_projects_contain_registry_images?\n      @group.has_container_repository_including_subgroups?\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4721"
  },
  {
    "raw_code": "def remove_paid_features_for_projects(old_root_ancestor_id); end\n\n    # rubocop: disable CodeReuse/ActiveRecord\n    def update_children_and_projects_visibility\n      descendants = @group.descendants.with_visibility_level_greater_than(@new_parent_group.visibility_level)\n\n      Group\n        .where(id: descendants.select(:id))\n        .update_all(visibility_level: @new_parent_group.visibility_level)\n\n      projects_to_update = @group\n        .all_projects\n        .with_visibility_level_greater_than(@new_parent_group.visibility_level)\n\n      # Used in post_update_hooks in EE. Must use pluck (and not select)\n      # here as after we perform the update below we won't be able to find\n      # these records again.\n      @updated_project_ids = projects_to_update.pluck(:id)\n\n      Namespaces::ProjectNamespace\n        .where(id: projects_to_update.select(:project_namespace_id))\n        .update_all(visibility_level: @new_parent_group.visibility_level)\n\n      projects_to_update\n        .update_all(visibility_level: @new_parent_group.visibility_level)\n\n      update_project_settings(@updated_project_ids)\n    end\n\n    # Overridden in EE\n    def update_project_settings(updated_project_ids); end\n\n    def update_two_factor_authentication\n      return if namespace_parent_allows_two_factor_auth\n\n      @group.require_two_factor_authentication = false\n    end\n\n    def refresh_descendant_groups\n      return if namespace_parent_allows_two_factor_auth\n\n      if @group.descendants.where(require_two_factor_authentication: true).any?\n        DisallowTwoFactorForSubgroupsWorker.perform_async(@group.id)\n      end\n    end\n    # rubocop: enable CodeReuse/ActiveRecord\n\n    def namespace_parent_allows_two_factor_auth\n      @new_parent_group.namespace_settings.allow_mfa_for_subgroups\n    end\n\n    def ensure_ownership\n      return if @new_parent_group\n      return unless @group.non_invite_owner_members.empty?\n\n      add_owner_on_transferred_group\n    end\n\n    # Overridden in EE\n    def add_owner_on_transferred_group\n      @group.add_owner(current_user)\n    end\n\n    def refresh_project_authorizations\n      project_ids = Groups::ProjectsRequiringAuthorizationsRefresh::OnTransferFinder.new(@group).execute\n\n      AuthorizedProjectUpdate::ProjectAccessChangedService.new(project_ids).execute\n    end\n\n    def raise_transfer_error(message)\n      raise TransferError, localized_error_messages[message]\n    end\n\n    # Overridden in EE\n    def localized_error_messages\n      {\n        database_not_supported: s_('TransferGroup|Database is not supported.'),\n        namespace_with_same_path: s_('TransferGroup|The parent group already has a subgroup or a project with the same path.'),\n        group_is_already_root: s_('TransferGroup|Group is already a root group.'),\n        same_parent_as_current: s_('TransferGroup|Group is already associated to the parent group.'),\n        invalid_policies: s_(\"TransferGroup|You don't have enough permissions.\"),\n        group_contains_images: s_('TransferGroup|Cannot update the path because there are projects under this group that contain Docker images in their container registry. Please remove the images from your projects first and try again.'),\n        cannot_transfer_to_subgroup: s_('TransferGroup|Cannot transfer group to one of its subgroup.'),\n        group_contains_namespaced_npm_packages: s_('TransferGroup|Group contains projects with NPM packages scoped to the current root level group.'),\n        no_permissions_to_migrate_crm: s_(\"TransferGroup|Group contains contacts/organizations and you don't have enough permissions to move them to the new root group.\")\n      }.freeze\n    end\n\n    def inherit_group_shared_runners_settings\n      parent_setting = @group.parent&.shared_runners_setting\n      return unless parent_setting\n\n      if @group.shared_runners_setting_higher_than?(parent_setting)\n        result = Groups::UpdateSharedRunnersService.new(@group, current_user, shared_runners_setting: parent_setting).execute\n\n        raise TransferError, result[:message] unless result[:status] == :success\n      end\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4722"
  },
  {
    "raw_code": "def execute\n      # TODO - add a policy check here https://gitlab.com/gitlab-org/gitlab/-/issues/353082\n      raise DestroyError, \"You can't delete this group because you're blocked.\" if current_user.blocked?\n\n      mark_deleted\n\n      group.projects.includes(:project_feature).find_each do |project|\n        # Execute the destruction of the models immediately to ensure atomic cleanup.\n        success = ::Projects::DestroyService.new(project, current_user).execute\n\n        raise DestroyError, \"Project #{project.id} can't be deleted\" unless success\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4723"
  },
  {
    "raw_code": "def obtain_user_ids_for_project_authorizations_refresh\n      return unless any_projects_shared_with_this_group? || any_groups_shared_with_this_group?\n      return group.user_ids_for_project_authorizations if any_projects_shared_with_this_group?\n\n      group.users_ids_of_direct_members\n    end",
    "comment": "Destroying a group automatically destroys all project authorizations directly associated with the group and descendents. However, project authorizations for projects and groups this group is shared with are not. Without a manual refresh, the project authorization records of these users to shared projects and projects within the shared groups will never be removed, causing inconsistencies with access permissions.  This method retrieves the user IDs that need to be refreshed. If only groups are shared with this group, only direct members need to be refreshed. If projects are also shared with the group, direct members *and* shared members of other groups need to be refreshed. `Group#user_ids_for_project_authorizations` returns both direct and shared members' user IDs.",
    "label": "",
    "id": "4724"
  },
  {
    "raw_code": "def users_to_destroy\n      group.members_and_requesters.joins(:user)\n        .merge(User.project_bot)\n        .allow_cross_joins_across_databases(url: 'https://gitlab.com/gitlab-org/gitlab/-/issues/422405')\n        .pluck(:user_id)\n    end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4725"
  },
  {
    "raw_code": "def publish_event\n      event_data = {\n        group_id: group.id,\n        root_namespace_id: group.root_ancestor&.id.to_i # remove safe navigation and `.to_i` with https://gitlab.com/gitlab-org/gitlab/-/issues/508611\n      }\n      event_data[:parent_namespace_id] = group.parent_id if group.parent_id.present?\n\n      Gitlab::EventStore.publish(Groups::GroupDeletedEvent.new(data: event_data))\n    end",
    "comment": "rubocop:enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4726"
  },
  {
    "raw_code": "def issues(confidential_only: false, issue_types: nil)\n      finder_params = { group_id: group.id, state: 'opened' }\n      finder_params[:confidential] = true if confidential_only.present?\n      finder_params[:issue_types] = issue_types if issue_types.present?\n      finder_params[:include_descendants] = true\n\n      relation = WorkItems::WorkItemsFinder.new(current_user, finder_params).execute\n\n      relation = relation.gfm_autocomplete_search(params[:search]).limit(SEARCH_LIMIT) if params[:search]\n\n      relation\n        .preload(project: :namespace)\n        .with_work_item_type\n        .select(:iid, :title, :project_id, :namespace_id, 'work_item_types.icon_name')\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4727"
  },
  {
    "raw_code": "def merge_requests\n      MergeRequestsFinder.new(current_user, group_id: group.id, include_subgroups: true, state: 'opened')\n        .execute\n        .preload(target_project: :namespace)\n        .select(:iid, :title, :target_project_id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4728"
  },
  {
    "raw_code": "def milestones\n      group_ids = group.self_and_ancestors.public_or_visible_to_user(current_user).pluck(:id)\n\n      MilestonesFinder.new(group_ids: group_ids).execute.select(:iid, :title, :due_date)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4729"
  },
  {
    "raw_code": "def labels_as_hash(target)\n      super(target, group_id: group.id, only_group_labels: true, include_ancestor_groups: true, archived: false)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4730"
  },
  {
    "raw_code": "def remove_unallowed_params\n      if Feature.disabled?(:omniauth_step_up_auth_for_namespace, group)\n        params.delete(:step_up_auth_required_oauth_provider)\n      end",
    "comment": "overridden in EE",
    "label": "",
    "id": "4731"
  },
  {
    "raw_code": "def should_store_generated_ref_commits?\n      false # only available in ee for merge trains for now\n    end",
    "comment": "Default CE implementation - can be overridden in EE",
    "label": "",
    "id": "4732"
  },
  {
    "raw_code": "def post_merge_manually_merged\n      commit_ids = @commits.map(&:id)\n      merge_requests = @project.merge_requests.opened\n        .preload_project_and_latest_diff\n        .preload_latest_diff_commit\n        .where(target_branch: @push.branch_name).to_a\n        .select(&:diff_head_commit)\n        .select do |merge_request|\n          commit_ids.include?(merge_request.diff_head_sha) &&\n            merge_request.merge_request_diff.state != 'empty'\n        end",
    "comment": "Collect open merge requests that target same branch we push into and close if push to master include last commit from merge request We need this to close(as merged) merge requests that were merged into target branch manually rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4733"
  },
  {
    "raw_code": "def link_forks_lfs_objects\n      return unless @push.branch_updated?\n\n      merge_requests_for_forks.find_each do |mr|\n        LinkLfsObjectsService\n          .new(project: mr.target_project)\n          .execute(mr, oldrev: @push.oldrev, newrev: @push.newrev)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Link LFS objects that exists in forks but does not exists in merge requests target project",
    "label": "",
    "id": "4734"
  },
  {
    "raw_code": "def reload_merge_requests\n      merge_requests = @project.merge_requests.opened\n        .by_source_or_target_branch(@push.branch_name)\n        .preload_project_and_latest_diff\n\n      merge_requests_from_forks = merge_requests_for_forks\n        .preload_project_and_latest_diff\n\n      merge_requests_array = merge_requests.to_a + merge_requests_from_forks.to_a\n      filter_merge_requests(merge_requests_array).each do |merge_request|\n        skip_merge_status_trigger = true\n\n        if branch_and_project_match?(merge_request) || @push.force_push?\n          merge_request.reload_diff(current_user)\n          schedule_duo_code_review(merge_request)\n          # Clear existing merge error if the push were directed at the\n          # source branch. Clearing the error when the target branch\n          # changes will hide the error from the user.\n          merge_request.merge_error = nil\n\n          # Don't skip trigger since we to update the MR's merge status in real-time\n          # when the push if for the MR's source branch and project.\n          skip_merge_status_trigger = false\n        elsif merge_request.merge_request_diff.includes_any_commits?(push_commit_ids)\n          merge_request.reload_diff(current_user)\n        end",
    "comment": "Refresh merge request diff if we push to source or target branch of merge request Note: we should update merge requests from forks too",
    "label": "",
    "id": "4735"
  },
  {
    "raw_code": "def comment_mr_branch_presence_changed(merge_request)\n      presence = @push.branch_added? ? :add : :delete\n\n      SystemNoteService.change_branch_presence(\n        merge_request, merge_request.project, @current_user,\n        :source, @push.branch_name, presence)\n    end",
    "comment": "Add comment about branches being deleted or added to merge requests",
    "label": "",
    "id": "4736"
  },
  {
    "raw_code": "def notify_about_push(merge_request)\n      return unless @commits.present?\n\n      mr_commit_ids = Set.new(merge_request.commit_shas)\n\n      new_commits, existing_commits = @commits.partition do |commit|\n        mr_commit_ids.include?(commit.id)\n      end",
    "comment": "Add comment about pushing new commits to merge requests and send notification emails ",
    "label": "",
    "id": "4737"
  },
  {
    "raw_code": "def execute_mr_web_hooks(merge_request)\n      execute_hooks(merge_request, 'update', old_rev: @push.oldrev)\n    end",
    "comment": "Call merge request webhook with update branches",
    "label": "",
    "id": "4738"
  },
  {
    "raw_code": "def cache_merge_requests_closing_issues\n      @project.merge_requests.where(source_branch: @push.branch_name).find_each do |merge_request|\n        merge_request.cache_merge_request_closes_issues!(@current_user)\n      end",
    "comment": "If the merge requests closes any issues, save this information in the `MergeRequestsClosingIssues` model (as a performance optimization). rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4739"
  },
  {
    "raw_code": "def filter_merge_requests(merge_requests)\n      merge_requests.uniq.select(&:source_project)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4740"
  },
  {
    "raw_code": "def initialize(project:, current_user:, mr_params: {})\n      # branch - the name of new branch\n      # ref    - the source of new branch.\n\n      @branch_name       = mr_params[:branch_name]\n      @issue_iid         = mr_params[:issue_iid]\n      @ref               = mr_params[:ref]\n      @target_project_id = mr_params[:target_project_id]\n\n      super(project: project, current_user: current_user)\n    end",
    "comment": "TODO: This constructor does not use the \"params:\" argument from the superclass, but instead has a custom \"mr_params:\" argument. This is because historically, prior to named arguments being introduced to the constructor, it never passed along the third positional argument when calling `super`. This should be changed, in order to be consistent (all subclasses should pass along all of the arguments to the superclass, otherwise it is probably not an \"is a\" relationship). However, we need to be sure that passing the params argument to `super` (especially target_project_id) will not cause any unexpected behavior in the superclass. Since the addition of the named arguments is intended to be a low-risk pure refactor, we will defer this fix to this follow-on issue: https://gitlab.com/gitlab-org/gitlab/-/issues/328726",
    "label": "",
    "id": "4741"
  },
  {
    "raw_code": "def issue\n      @issue ||= IssuesFinder.new(current_user, project_id: project.id).find_by(iid: @issue_iid)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4742"
  },
  {
    "raw_code": "def branch_name\n      @branch ||= @branch_name || issue.to_branch_name\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4743"
  },
  {
    "raw_code": "def merge_requests\n      @merge_requests ||= MergeRequest.from_project(target_project)\n                                      .opened\n                                      .from_source_branches(branches)\n                                      .index_by(&:source_branch)\n    end",
    "comment": "Returns a Hash of branch => MergeRequest",
    "label": "",
    "id": "4744"
  },
  {
    "raw_code": "def delete_source_branch?\n      params.fetch('should_remove_source_branch', @merge_request.force_remove_source_branch?) &&\n        @merge_request.can_remove_source_branch?(branch_deletion_user)\n    end",
    "comment": "Verify again that the source branch can be removed, since branch may be protected, or the source branch may have been updated, or the user may not have permission ",
    "label": "",
    "id": "4745"
  },
  {
    "raw_code": "def clear_cache(new_diff)\n      # Remove cache for all diffs on this MR. Do not use the association on the\n      # model, as that will interfere with other actions happening when\n      # reloading the diff.\n      MergeRequestDiff\n        .where(merge_request: merge_request)\n        .preload(merge_request: :target_project)\n        .find_each do |merge_request_diff|\n        next if merge_request_diff == new_diff\n\n        cacheable_collection(merge_request_diff).clear_cache\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4746"
  },
  {
    "raw_code": "def cacheable_collection(diff)\n      # There are scenarios where we don't need to request Diff Stats.\n      # Mainly when clearing / writing diff caches.\n      diff.diffs(include_stats: false)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4747"
  },
  {
    "raw_code": "def execute(merge_request, skip_updating_state: false, skip_system_note: false, skip_notification: false)\n      return unless merge_request.approved_by?(current_user)\n      return if merge_request.merged?\n\n      # paranoid protection against running wrong deletes\n      return unless merge_request.id && current_user.id\n\n      approval = merge_request.approvals.where(user: current_user)\n\n      trigger_approval_hooks(merge_request, skip_notification) do\n        next unless approval.destroy_all # rubocop: disable Cop/DestroyAll\n\n        update_reviewer_state(merge_request, current_user, 'unapproved') unless skip_updating_state\n        reset_approvals_cache(merge_request)\n\n        unless skip_system_note\n          create_note(merge_request)\n          merge_request_activity_counter.track_unapprove_mr_action(user: current_user)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4748"
  },
  {
    "raw_code": "def execute\n      return [] if branch_names.blank?\n\n      source_branches = project.source_of_merge_requests.open_and_closed\n        .from_source_branches(branch_names).pluck(:source_branch)\n\n      target_branches = project.merge_requests.opened\n        .by_target_branch(branch_names).distinct.pluck(:target_branch)\n\n      source_branches.concat(target_branches).to_set\n    end",
    "comment": "Skip moving this logic into models since it's too specific rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4749"
  },
  {
    "raw_code": "def target_ref\n      params[:target_ref] || merge_request.merge_ref_path\n    end",
    "comment": " The parameter `target_ref` is where the merge result will be written. Default is the merge ref i.e. `refs/merge-requests/:iid/merge`.",
    "label": "",
    "id": "4750"
  },
  {
    "raw_code": "def first_parent_ref\n      params[:first_parent_ref] || merge_request.target_branch_ref\n    end",
    "comment": " The parameter `first_parent_ref` is the main line of the merge commit. Default is the target branch ref of the merge request.",
    "label": "",
    "id": "4751"
  },
  {
    "raw_code": "def hooks_validation_pass?(merge_request, validate_squash_message: false)\n      true\n    end",
    "comment": "Overridden in EE.",
    "label": "",
    "id": "4752"
  },
  {
    "raw_code": "def hooks_validation_error(merge_request, validate_squash_message: false)\n      # No-op\n    end",
    "comment": "Overridden in EE.",
    "label": "",
    "id": "4753"
  },
  {
    "raw_code": "def check_size_limit\n      # No-op\n    end",
    "comment": "Overridden in EE.",
    "label": "",
    "id": "4754"
  },
  {
    "raw_code": "def error_check!\n      # No-op\n    end",
    "comment": "Overridden in EE.",
    "label": "",
    "id": "4755"
  },
  {
    "raw_code": "def merge_requests_batch(ids)\n      MergeRequest.id_in(ids)\n    end",
    "comment": "This method is overridden in EE to extend its functionality like preloading associations.",
    "label": "",
    "id": "4756"
  },
  {
    "raw_code": "def attempt_to_unstick_mrs_with_merge_jid(merge_requests)\n      return if merge_requests.empty?\n\n      jids = merge_requests.map(&:merge_jid)\n\n      # Find the jobs that aren't currently running or that exceeded the threshold.\n      completed_jids = Gitlab::SidekiqStatus.completed_jids(jids)\n\n      return if completed_jids.empty?\n\n      completed_ids = merge_requests.select do |merge_request|\n        completed_jids.include?(merge_request.merge_jid)\n      end.map(&:id)\n\n      completed_merge_requests = MergeRequest.id_in(completed_ids)\n\n      mark_merge_requests_as_merged(completed_merge_requests.where.not(merge_commit_sha: nil))\n      unlock_merge_requests(completed_merge_requests.where(merge_commit_sha: nil))\n\n      log_info(\"Updated state of locked merge jobs. JIDs: #{completed_jids.join(', ')}\")\n    end",
    "comment": "The logic in this method is the same as in `StuckMergeJobsWorker`. This service is intended to replace the logic in that worker once feature flag is fully rolled out.  Any changes that needs to be applied here should be applied to the worker as well.  rubocop: disable CodeReuse/ActiveRecord -- TODO: Introduce new AR scopes for queries used in this method",
    "label": "",
    "id": "4757"
  },
  {
    "raw_code": "def attempt_to_unstick_mrs_without_merge_jid(merge_requests)\n      return if merge_requests.empty?\n\n      merge_requests_to_reopen = []\n      merge_request_ids_to_mark_as_merged = []\n\n      merge_requests.each do |merge_request|\n        next unless Feature.enabled?(:unstick_locked_mrs_without_merge_jid, merge_request.project)\n        next unless should_unstick?(merge_request)\n\n        # Reset merge request record to ensure we get updated record state before\n        # we check attributes. It is possible that after we queried the MRs, they\n        # got merged or unlocked and marked as such successfully. If so, skip MR.\n        next unless merge_request.reset.locked?\n\n        # Set MR to be marked as merged if one of the following is true:\n        # - it already has merged_commit_sha in the DB\n        # - it alreeady has merge_commit_sha in the DB\n        # - it has no diffs where source and target branches are compared\n        #\n        # This means the MR changes were already merged.\n        #\n        # We read the value of the column from the DB instead of MergeRequest#merged_commit_sha\n        # as that method can return nil when MR is still not merged.\n        #\n        # We also check the `merge_commit_sha` if present as there are older MRs that do not have\n        # `merged_commit_sha` set on merge.\n        #\n        # When both attributes aren't set, we check if the MR still has diffs to see\n        # if the MR changes are already merged or not.\n        if merge_request.read_attribute(:merged_commit_sha).present? ||\n            merge_request.merge_commit_sha.present? ||\n            (merge_request.source_and_target_branches_exist? && !merge_request.has_diffs?)\n          merge_request_ids_to_mark_as_merged << merge_request.id\n        else\n          # Set MR to be unlocked since it's stuck and maybe not merged yet.\n          merge_requests_to_reopen << merge_request\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4758"
  },
  {
    "raw_code": "def should_unstick?(merge_request)\n      !merge_request.merge_exclusive_lease.exists?\n    end",
    "comment": "Check if MR is still in the process of merging so we don't interrupt the process. MergeRequest::MergeService will acquire a lease when merging and keep it for 15 minutes so we can check if the lease still exists and we can consider the MR as still merging.",
    "label": "",
    "id": "4759"
  },
  {
    "raw_code": "def unlock_merge_requests(merge_requests)\n      errors = Hash.new { |h, k| h[k] = [] }\n\n      merge_requests.each do |mr|\n        mjid = mr.merge_jid\n\n        if mr.unlock_mr\n          mr.remove_from_locked_set\n          next\n        end",
    "comment": "Do not reopen merge requests using direct queries. We rely on state machine callbacks to update head_pipeline_id",
    "label": "",
    "id": "4760"
  },
  {
    "raw_code": "def assign_title_and_description\n      assign_description_from_repository_template\n      replace_variables_in_description\n      assign_title_and_description_from_commits\n      merge_request.title ||= title_from_issue if target_project.issues_enabled? || target_project.external_issue_tracker\n      merge_request.title ||= branch_name_to_title(source_branch)\n      set_draft_title_if_needed\n\n      append_closes_description\n    end",
    "comment": "When your branch name starts with an iid followed by a dash this pattern will be interpreted as the user wants to close that issue on this project.  For example: - Issue 112 exists - title: Emoji don't show up in commit title - Source branch is: 112-fix-mep-mep  Will lead to: - Appending `Closes #112` to the description - Setting the title as 'Resolves \"Emoji don't show up in commit title\"' if there is more than one commit in the MR ",
    "label": "",
    "id": "4761"
  },
  {
    "raw_code": "def execute(merge_request)\n      return merge_request unless current_user&.can?(:set_merge_request_metadata, merge_request)\n\n      old_assignees = merge_request.assignees.to_a\n      old_ids = old_assignees.map(&:id)\n      new_ids = new_user_ids(merge_request, update_attrs[:assignee_ids], :assignees)\n\n      return merge_request if merge_request.errors.any?\n      return merge_request if new_ids.size != update_attrs[:assignee_ids].size\n      return merge_request if old_ids.to_set == new_ids.to_set # no-change\n\n      attrs = update_attrs.merge(assignee_ids: new_ids)\n\n      merge_request.update(**attrs)\n\n      return merge_request unless merge_request.valid?\n\n      # Defer the more expensive operations (handle_assignee_changes) to the background\n      MergeRequests::HandleAssigneesChangeService\n        .new(project: project, current_user: current_user)\n        .async_execute(merge_request, old_assignees, execute_hooks: true)\n\n      merge_request\n    end",
    "comment": "a stripped down service that only does what it must to update the assignees, and knows that it does not have to check for other updates. This saves a lot of queries for irrelevant things that cannot possibly change in the execution of this service.",
    "label": "",
    "id": "4762"
  },
  {
    "raw_code": "def execute(recheck: false, retry_lease: true)\n      return service_error if service_error\n\n      in_write_lock(retry_lease: retry_lease) do |retried|\n        # When multiple calls are waiting for the same lock (retry_lease),\n        # it's possible that when granted, the MR status was already updated for\n        # that object, therefore we reset if there was a lease retry.\n        merge_request.reset if retried\n\n        check_mergeability(recheck)\n      end",
    "comment": "Updates the MR merge_status. Whenever it switches to a can_be_merged state, the merge-ref is refreshed.  recheck - When given, it'll enforce a merge-ref refresh if the current merge_status is can_be_merged or cannot_be_merged and merge-ref is outdated. Given MergeRequests::RefreshService is called async, it might happen that the target branch gets updated, but the MergeRequest#merge_status lags behind. So in scenarios where we need the current state of the merge ref in repository, the `recheck` argument is required.  retry_lease - Concurrent calls wait for at least 10 seconds until the lease is granted (other process finishes running). Returns an error ServiceResponse if the lease is not granted during this time.  Returns a ServiceResponse indicating merge_status is/became can_be_merged and the merge-ref is synced. Success in case of being/becoming mergeable, error otherwise.",
    "label": "",
    "id": "4763"
  },
  {
    "raw_code": "def in_write_lock(retry_lease:, &block)\n      lease_key = \"mergeability_check:#{merge_request.id}\"\n\n      lease_opts = {\n        ttl: 1.minute,\n        retries: retry_lease ? 10 : 0,\n        sleep_sec: retry_lease ? 1.second : 0\n      }\n\n      in_lock(lease_key, **lease_opts, &block)\n    end",
    "comment": "It's possible for this service to send concurrent requests to Gitaly in order to \"git update-ref\" the same ref. Therefore we handle a light exclusive lease here. ",
    "label": "",
    "id": "4764"
  },
  {
    "raw_code": "def outdated_merge_ref?\n      return false unless merge_request.open?\n\n      return true unless ref_head = merge_request.merge_ref_head\n      return true unless target_sha = merge_request.target_branch_sha\n      return true unless source_sha = merge_request.source_branch_sha\n\n      ref_head.parent_ids != [target_sha, source_sha]\n    end",
    "comment": "Checks if the existing merge-ref is synced with the target branch.  Returns true if the merge-ref does not exists or is out of sync.",
    "label": "",
    "id": "4765"
  },
  {
    "raw_code": "def inspect\n      return \"#<#{self.class}>\" unless respond_to?(:merge_request) && merge_request\n\n      \"#<#{self.class} #{merge_request.to_reference(full: true)}>\"\n    end",
    "comment": "Don't try to print expensive instance variables.",
    "label": "",
    "id": "4766"
  },
  {
    "raw_code": "def merge_requests_for(source_branch, mr_states: [:opened])\n      @project.source_of_merge_requests\n        .with_state(mr_states)\n        .where(source_branch: source_branch)\n        .preload(:source_project) # we don't need #includes since we're just preloading for the #select\n        .select(&:source_project)\n    end",
    "comment": "Returns all origin and fork merge requests from `@project` satisfying passed arguments. rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4767"
  },
  {
    "raw_code": "def pipeline_merge_requests(pipeline)\n      pipeline.all_merge_requests.opened.each do |merge_request|\n        next unless pipeline.id == merge_request.head_pipeline_id\n\n        yield merge_request\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4768"
  },
  {
    "raw_code": "def execute(commit_status)\n      return if commit_status.allow_failure? || commit_status.retried?\n\n      pipeline_merge_requests(commit_status.pipeline) do |merge_request|\n        todo_service.merge_request_build_failed(merge_request)\n      end",
    "comment": "Adds a todo to the parent merge_request when a CI build fails ",
    "label": "",
    "id": "4769"
  },
  {
    "raw_code": "def close(commit_status)\n      close_all(commit_status.pipeline)\n    end",
    "comment": "Closes any pending build failed todos for the parent MRs when a build is retried ",
    "label": "",
    "id": "4770"
  },
  {
    "raw_code": "def no_result_unsuccessful?\n        results.none?(&:unsuccessful?)\n      end",
    "comment": "This name may seem like a double-negative, but it is meaningful because #success? is _not_ the inverse of #unsuccessful?",
    "label": "",
    "id": "4771"
  },
  {
    "raw_code": "def cacheable?\n        raise NotImplementedError\n      end",
    "comment": "When this method is true, we need to implement a cache_key",
    "label": "",
    "id": "4772"
  },
  {
    "raw_code": "def use_create_ref_service?\n        Feature.enabled?(:rebase_on_merge_automatic, project) &&\n          project.ff_merge_must_be_possible? &&\n          merge_request.should_be_rebased?\n      end",
    "comment": "We only want to use the service when we can not directly fast_forward and when ff merge must be possible",
    "label": "",
    "id": "4773"
  },
  {
    "raw_code": "def add_in_query_with_limit(query, limit)\n      columns = Arel::Nodes::Grouping.new(primary_keys)\n      query.where(columns.in(in_query_with_limit(limit))).to_sql\n    end",
    "comment": "IN query with one or composite primary key WHERE (primary_key1, primary_key2) IN (subselect)",
    "label": "",
    "id": "4774"
  },
  {
    "raw_code": "def in_query_with_limit(limit)\n      in_query = Arel::SelectManager.new\n      in_query.from(quoted_table_name)\n      in_query.where(arel_table[loose_foreign_key_definition.column].in(deleted_parent_records.map(&:primary_key_value)))\n      loose_foreign_key_definition.options[:conditions]&.each do |condition|\n        in_query.where(arel_table[condition[:column]].eq(condition[:value]))\n      end",
    "comment": "Builds the following sub-query SELECT primary_keys FROM table WHERE foreign_key IN (1, 2, 3) LIMIT N",
    "label": "",
    "id": "4775"
  },
  {
    "raw_code": "def initialize(timeline_event, user)\n        @timeline_event = timeline_event\n        @user = user\n        @incident = timeline_event.incident\n        @project = @incident.project\n      end",
    "comment": "@param timeline_event [IncidentManagement::TimelineEvent] @param user [User]",
    "label": "",
    "id": "4776"
  },
  {
    "raw_code": "def initialize(incident, current_user, alert_references)\n        @incident = incident\n        @current_user = current_user\n        @alert_references = alert_references\n\n        super(project: incident.project, current_user: current_user)\n      end",
    "comment": "@param incident [Issue] an incident to link alerts @param current_user [User] @param alert_references [[String]] a list of alert references. Can be either a short reference or URL Examples: \"^alert#IID\" \"https://gitlab.com/company/project/-/alert_management/IID/details\"",
    "label": "",
    "id": "4777"
  },
  {
    "raw_code": "def initialize(incident, current_user, alert)\n        @incident = incident\n        @current_user = current_user\n        @alert = alert\n\n        super(project: incident.project, current_user: current_user)\n      end",
    "comment": "@param incident [Issue] an incident to unlink alert from @param current_user [User] @param alert [AlertManagement::Alert] an alert to unlink from the incident",
    "label": "",
    "id": "4778"
  },
  {
    "raw_code": "def process_batch(batch)\n        batch.each do |version|\n          version.file.remove!\n        end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4779"
  },
  {
    "raw_code": "def validate(_protection_rule); end\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4780"
  },
  {
    "raw_code": "def extra_create_service_params\n      {}\n    end",
    "comment": "Overidden in subclasses to support specific parameters",
    "label": "",
    "id": "4781"
  },
  {
    "raw_code": "def merge_request_branch_names\n      # without_order is necessary for SELECT DISTINCT because default scope adds an ORDER BY\n      source_names = project.origin_merge_requests.opened.without_order.distinct.pluck(:source_branch)\n      target_names = project.merge_requests.opened.without_order.distinct.pluck(:target_branch)\n      (source_names + target_names).uniq\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4782"
  },
  {
    "raw_code": "def design_version_added(version)\n      events = DesignManagement::Action.events\n      link_href = designs_path(version: version.id)\n\n      version.designs_by_event.map do |(event_name, designs)|\n        note_data = self.class.design_event_note_data(events[event_name])\n        icon_name = note_data[:icon]\n        n = designs.size\n\n        body = \"%s [%d %s](%s)\" % [note_data[:past_tense], n, 'design'.pluralize(n), link_href]\n\n        create_note(NoteSummary.new(noteable, project, author, body, action: icon_name))\n      end",
    "comment": "Parameters: - version [DesignManagement::Version]  Example Note text:  \"added [1 designs](link-to-version)\" \"changed [2 designs](link-to-version)\"  Returns [Array<Note>]: the created Note objects",
    "label": "",
    "id": "4783"
  },
  {
    "raw_code": "def design_discussion_added(discussion_note)\n      design = discussion_note.noteable\n\n      body = _('started a discussion on %{design_link}') % {\n        design_link: '[%s](%s)' % [\n          design.filename,\n          designs_path(vueroute: design.filename, anchor: dom_id(discussion_note))\n        ]\n      }\n\n      action = :designs_discussion_added\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: action))\n    end",
    "comment": "Called when a new discussion is created on a design  discussion_note - DiscussionNote  Example Note text:  \"started a discussion on screen.png\"  Returns the created Note object",
    "label": "",
    "id": "4784"
  },
  {
    "raw_code": "def self.design_event_note_data(event)\n      case event\n      when DesignManagement::Action.events[:creation]\n        { icon: 'designs_added', past_tense: 'added' }\n      when DesignManagement::Action.events[:modification]\n        { icon: 'designs_modified', past_tense: 'updated' }\n      when DesignManagement::Action.events[:deletion]\n        { icon: 'designs_removed', past_tense: 'removed' }\n      else\n        raise \"Unknown event: #{event}\"\n      end",
    "comment": "Take one of the `DesignManagement::Action.events` and return: * an English past-tense verb. * the name of an icon used in renderin a system note  We do not currently internationalize our system notes, instead we just produce English-language descriptions. See: https://gitlab.com/gitlab-org/gitlab/issues/30408 See: https://gitlab.com/gitlab-org/gitlab/issues/14056",
    "label": "",
    "id": "4785"
  },
  {
    "raw_code": "def change_incident_severity\n      severity = noteable.severity\n\n      if severity_label = IssuableSeverity::SEVERITY_LABELS[severity.to_sym]\n        body = \"changed the severity to **#{severity_label}**\"\n\n        create_note(NoteSummary.new(noteable, project, author, body, action: 'severity'))\n      else\n        Gitlab::AppLogger.error(\n          message: 'Cannot create a system note for severity change',\n          noteable_class: noteable.class.to_s,\n          noteable_id: noteable.id,\n          severity: severity\n        )\n      end",
    "comment": "Called when the severity of an Incident has changed  Example Note text:  \"changed the severity to Medium - S3\"  Returns the created Note object",
    "label": "",
    "id": "4786"
  },
  {
    "raw_code": "def change_incident_status(reason)\n      status = noteable.escalation_status.status_name.to_s.titleize\n      body = \"changed the incident status to **#{status}**#{reason}\"\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'status'))\n    end",
    "comment": "Called when the status of an IncidentManagement::IssuableEscalationStatus has changed  reason - String.  Example Note text:  \"changed the incident status to Acknowledged\" \"changed the incident status to Acknowledged by changing the status of ^alert#540\"  Returns the created Note object",
    "label": "",
    "id": "4787"
  },
  {
    "raw_code": "def add_commits(new_commits, existing_commits = [], oldrev = nil)\n      total_count  = new_commits.length + existing_commits.length\n      commits_text = \"#{total_count} commit\".pluralize(total_count)\n\n      text_parts = [\"added #{commits_text}\"]\n      text_parts << commits_list(noteable, new_commits, existing_commits, oldrev)\n      text_parts << \"[Compare with previous version](#{diff_comparison_path(noteable, project, oldrev)})\"\n\n      body = text_parts.join(\"\\n\\n\")\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'commit', commit_count: total_count))\n    end",
    "comment": "Called when commits are added to a merge request  new_commits      - Array of Commits added since last push existing_commits - Array of Commits added in a previous push oldrev           - Optional String SHA of a previous Commit  See new_commit_summary and existing_commit_summary.  Returns the created Note object",
    "label": "",
    "id": "4788"
  },
  {
    "raw_code": "def tag_commit(tag_name)\n      link = url_helpers.project_tag_path(project, id: tag_name)\n      body = \"tagged commit #{noteable.sha} to [`#{tag_name}`](#{link})\"\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'tag'))\n    end",
    "comment": "Called when a commit was tagged  tag_name  - The created tag name  Returns the created Note object",
    "label": "",
    "id": "4789"
  },
  {
    "raw_code": "def new_commits_list(new_commits)\n      new_commits.collect do |commit|\n        content_tag('li', \"#{commit.short_id} - #{commit.title}\")\n      end",
    "comment": "Build an Array of lines detailing each commit added in a merge request  new_commits - Array of new Commit objects  Returns an Array of Strings",
    "label": "",
    "id": "4790"
  },
  {
    "raw_code": "def new_commit_summary(commits, start_rev)\n      if commits.size > NEW_COMMIT_DISPLAY_LIMIT\n        no_of_commits_to_truncate = commits.size - NEW_COMMIT_DISPLAY_LIMIT\n        commits_to_truncate = commits.take(no_of_commits_to_truncate)\n        remaining_commits = commits.drop(no_of_commits_to_truncate)\n\n        [truncated_new_commits(commits_to_truncate, start_rev)] + new_commits_list(remaining_commits)\n      else\n        new_commits_list(commits)\n      end",
    "comment": "Builds an Array of lines describing each commit and truncate them based on the limit to avoid creating a note with a large number of commits.  commits - Array of Commit objects  Returns an Array of Strings  rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4791"
  },
  {
    "raw_code": "def truncated_new_commits(commits, start_rev)\n      count = commits.size\n\n      commit_ids = if count == 1\n                     commits.first.short_id\n                   elsif start_rev && !Gitlab::Git.blank_ref?(start_rev)\n                     \"#{Commit.truncate_sha(start_rev)}...#{commits.last.short_id}\"\n                   else\n                     # This two-dots notation seems to be not functioning as expected, but we should\n                     # fallback to it as start_rev can be empty.\n                     #\n                     # For more information, please see https://gitlab.com/gitlab-org/gitlab/-/issues/391809\n                     \"#{commits.first.short_id}..#{commits.last.short_id}\"\n                   end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord Builds a summary line that describes given truncated commits.  commits - Array of Commit objects start_rev - String SHA of a Commit that will be used as the starting SHA of the range  Returns a String wrapped in 'li' tag.",
    "label": "",
    "id": "4792"
  },
  {
    "raw_code": "def commits_list(noteable, new_commits, existing_commits, oldrev)\n      existing_commit_summary = existing_commit_summary(noteable, existing_commits, oldrev)\n      start_rev = existing_commits.empty? ? oldrev : existing_commits.last.id\n      new_commit_summary = new_commit_summary(new_commits, start_rev).join\n\n      content_tag('ul', \"#{existing_commit_summary}#{new_commit_summary}\".html_safe)\n    end",
    "comment": "Builds a list of existing and new commits according to existing_commits and new_commits methods. Returns a String wrapped in `ul` and `li` tags.",
    "label": "",
    "id": "4793"
  },
  {
    "raw_code": "def existing_commit_summary(noteable, existing_commits, oldrev = nil)\n      return '' if existing_commits.empty?\n\n      count = existing_commits.size\n\n      commit_ids = if count == 1\n                     existing_commits.first.short_id\n                   elsif oldrev && !Gitlab::Git.blank_ref?(oldrev)\n                     \"#{Commit.truncate_sha(oldrev)}...#{existing_commits.last.short_id}\"\n                   else\n                     \"#{existing_commits.first.short_id}..#{existing_commits.last.short_id}\"\n                   end",
    "comment": "Build a single line summarizing existing commits being added in a merge request  existing_commits - Array of existing Commit objects oldrev           - Optional String SHA of a previous Commit  Examples:  \"* ea0f8418...2f4426b7 - 24 commits from branch `master`\"  \"* ea0f8418..4188f0ea - 15 commits from branch `fork:master`\"  \"* ea0f8418 - 1 commit from branch `feature`\"  Returns a newline-terminated String",
    "label": "",
    "id": "4794"
  },
  {
    "raw_code": "def relate_issuable(noteable_ref)\n      body = \"marked this #{noteable_name} as related to #{extract_issuable_reference(noteable_ref)}\"\n\n      track_issue_event(:track_issue_related_action)\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'relate'))\n    end",
    "comment": " noteable_ref - Referenced noteable object, or array of objects  Example Note text:  \"marked this issue as related to gitlab-foss#9001\" \"marked this issue as related to gitlab-foss#9001, gitlab-foss#9002, and gitlab-foss#9003\"  Returns the created Note object",
    "label": "",
    "id": "4795"
  },
  {
    "raw_code": "def unrelate_issuable(noteable_ref)\n      body = \"removed the relation with #{noteable_ref.to_reference(noteable.resource_parent)}\"\n\n      track_issue_event(:track_issue_unrelated_action)\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'unrelate'))\n    end",
    "comment": " noteable_ref - Referenced noteable object  Example Note text:  \"removed the relation with gitlab-foss#9001\"  Returns the created Note object",
    "label": "",
    "id": "4796"
  },
  {
    "raw_code": "def change_assignee(assignee)\n      body = assignee.nil? ? 'removed assignee' : \"assigned to #{assignee.to_reference}\"\n\n      track_issue_event(:track_issue_assignee_changed_action)\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'assignee'))\n    end",
    "comment": "Called when the assignee of a Noteable is changed or removed  assignee - User being assigned, or nil  Example Note text:  \"removed assignee\"  \"assigned to @rspeicher\"  Returns the created Note object",
    "label": "",
    "id": "4797"
  },
  {
    "raw_code": "def change_issuable_assignees(old_assignees)\n      unassigned_users = old_assignees - noteable.assignees\n      added_users = noteable.assignees.to_a - old_assignees\n      text_parts = []\n\n      Gitlab::I18n.with_default_locale do\n        text_parts << \"#{self.class.issuable_events[:assigned]} #{added_users.map(&:to_reference).to_sentence}\" if added_users.any?\n        text_parts << \"#{self.class.issuable_events[:unassigned]} #{unassigned_users.map(&:to_reference).to_sentence}\" if unassigned_users.any?\n      end",
    "comment": "Called when the assignees of an issuable is changed or removed  assignees - Users being assigned, or nil  Example Note text:  \"removed all assignees\"  \"assigned to @user1 additionally to @user2\"  \"assigned to @user1, @user2 and @user3 and unassigned @user4 and @user5\"  \"assigned to @user1 and @user2\"  Returns the created Note object",
    "label": "",
    "id": "4798"
  },
  {
    "raw_code": "def change_issuable_reviewers(old_reviewers)\n      unassigned_users = old_reviewers - noteable.reviewers\n      added_users = noteable.reviewers - old_reviewers\n      text_parts = []\n\n      Gitlab::I18n.with_default_locale do\n        text_parts << \"#{self.class.issuable_events[:review_requested]} #{added_users.map(&:to_reference).to_sentence}\" if added_users.any?\n        text_parts << \"#{self.class.issuable_events[:review_request_removed]} #{unassigned_users.map(&:to_reference).to_sentence}\" if unassigned_users.any?\n      end",
    "comment": "Called when the reviewers of an issuable is changed or removed  reviewers - Users being requested to review, or nil  Example Note text:  \"requested review from @user1 and @user2\"  \"requested review from @user1, @user2 and @user3 and removed review request for @user4 and @user5\"  Returns the created Note object",
    "label": "",
    "id": "4799"
  },
  {
    "raw_code": "def change_issuable_contacts(added_count, removed_count)\n      text_parts = []\n\n      Gitlab::I18n.with_default_locale do\n        text_parts << \"added #{added_count} #{'contact'.pluralize(added_count)}\" if added_count > 0\n        text_parts << \"removed #{removed_count} #{'contact'.pluralize(removed_count)}\" if removed_count > 0\n      end",
    "comment": "Called when the contacts of an issuable are changed or removed We intend to reference the contacts but for security we are just going to state how many were added/removed for now. See discussion: https://gitlab.com/gitlab-org/gitlab/-/merge_requests/77816#note_806114273  added_count - number of contacts added, or 0 removed_count - number of contacts  removed, or 0  Example Note text:  \"added 2 contacts\"  \"added 3 contacts and removed one contact\"  Returns the created Note object",
    "label": "",
    "id": "4800"
  },
  {
    "raw_code": "def change_title(old_title)\n      new_title = noteable.title\n\n      old_diffs, new_diffs = Gitlab::Diff::InlineDiff.new(old_title, new_title).inline_diffs\n\n      marked_old_title = Gitlab::Diff::InlineDiffMarker.new(old_title).mark(old_diffs)\n      marked_new_title = Gitlab::Diff::InlineDiffMarker.new(new_title).mark(new_diffs)\n\n      body = \"<p>changed title from <code class=\\\"idiff\\\">#{marked_old_title}</code> to <code class=\\\"idiff\\\">#{marked_new_title}</code></p>\"\n\n      track_issue_event(:track_issue_title_changed_action)\n      work_item_activity_counter.track_work_item_title_changed_action(author: author) if noteable.is_a?(WorkItem)\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'title'))\n    end",
    "comment": "Called when the title of a Noteable is changed  old_title - Previous String title  Example Note text:  \"changed title from **Old** to **New**\"  Returns the created Note object",
    "label": "",
    "id": "4801"
  },
  {
    "raw_code": "def hierarchy_changed(work_item, action)\n      params = hierarchy_note_params(action, noteable, work_item)\n\n      create_note(NoteSummary.new(noteable, project, author, params[:parent_note_body], action: params[:parent_action]))\n      create_note(NoteSummary.new(work_item, work_item.project, author, params[:child_note_body], action: params[:child_action]))\n    end",
    "comment": "Called when the hierarchy of a work item is changed  noteable  - Noteable object that responds to `work_item_parent` and `work_item_children` project   - Project owning noteable author    - User performing the change  Example Note text:  \"added #1 as child Task\"  Returns the created Note object",
    "label": "",
    "id": "4802"
  },
  {
    "raw_code": "def change_description\n      body = 'changed the description'\n\n      track_issue_event(:track_issue_description_changed_action)\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'description'))\n    end",
    "comment": "Called when the description of a Noteable is changed  noteable  - Noteable object that responds to `description` project   - Project owning noteable author    - User performing the change  Example Note text:  \"changed the description\"  Returns the created Note object",
    "label": "",
    "id": "4803"
  },
  {
    "raw_code": "def cross_reference(mentioned_in)\n      return if cross_reference_disallowed?(mentioned_in)\n\n      from = noteable.project || noteable.try(:group) || noteable.try(:namespace)\n\n      gfm_reference = mentioned_in.gfm_reference(from)\n      body = cross_reference_note_content(gfm_reference)\n\n      if noteable.is_a?(ExternalIssue)\n        Integrations::CreateExternalCrossReferenceWorker.perform_async(\n          noteable.project_id,\n          noteable.id,\n          mentioned_in.class.name,\n          mentioned_in.id,\n          author.id\n        )\n      else\n        track_cross_reference_action\n\n        created_at = mentioned_in.created_at if USE_COMMIT_DATE_FOR_CROSS_REFERENCE_NOTE && mentioned_in.is_a?(Commit)\n        create_note(NoteSummary.new(noteable, noteable.project, author, body, action: 'cross_reference', created_at: created_at), skip_touch_noteable: true)\n      end",
    "comment": "Called when a Mentionable (the `mentioned_in`) references another Mentionable (the `mentioned`, passed to this service as `noteable`).  Example Note text:  \"mentioned in #1\"  \"mentioned in !2\"  \"mentioned in 54f7727c\"  See cross_reference_note_content.  @param mentioned_in [Mentionable] @return [Note]",
    "label": "",
    "id": "4804"
  },
  {
    "raw_code": "def cross_reference_disallowed?(mentioned_in)\n      return true if noteable.is_a?(ExternalIssue) && !noteable.project&.external_references_supported?\n      return false unless mentioned_in.is_a?(MergeRequest)\n      return false unless noteable.is_a?(Commit)\n\n      mentioned_in.commits.include?(noteable)\n    end",
    "comment": "Check if a cross-reference is disallowed  This method prevents adding a \"mentioned in !1\" note on every single commit in a merge request. Additionally, it prevents the creation of references to external issues (which would fail).  @param mentioned_in [Mentionable] @return [Boolean]",
    "label": "",
    "id": "4805"
  },
  {
    "raw_code": "def change_task_status(new_task)\n      status_label = new_task.complete? ? Taskable::COMPLETED : Taskable::INCOMPLETE\n      body = \"marked the checklist item **#{new_task.source}** as #{status_label}\"\n\n      track_issue_event(:track_issue_description_changed_action)\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'task'))\n    end",
    "comment": "Called when the status of a Task has changed  new_task  - TaskList::Item object.  Example Note text:  \"marked the checklist item Whatever as completed.\"  Returns the created Note object",
    "label": "",
    "id": "4806"
  },
  {
    "raw_code": "def noteable_moved(noteable_ref, direction)\n      unless [:to, :from].include?(direction)\n        raise ArgumentError, \"Invalid direction `#{direction}`\"\n      end",
    "comment": "Called when noteable has been moved to another project  noteable_ref - Referenced noteable direction    - symbol, :to or :from  Example Note text:  \"moved to some_namespace/project_new#11\"  Returns the created Note object",
    "label": "",
    "id": "4807"
  },
  {
    "raw_code": "def noteable_cloned(noteable_ref, direction, created_at: nil)\n      unless [:to, :from].include?(direction)\n        raise ArgumentError, \"Invalid direction `#{direction}`\"\n      end",
    "comment": "Called when noteable has been cloned  noteable_ref - Referenced noteable direction    - symbol, :to or :from created_at   - timestamp for the system note, defaults to current time  Example Note text:  \"cloned to some_namespace/project_new#11\"  Returns the created Note object",
    "label": "",
    "id": "4808"
  },
  {
    "raw_code": "def change_issue_confidentiality\n      if noteable.confidential\n        body = \"made the #{noteable_name} confidential\"\n        action = 'confidential'\n\n        track_issue_event(:track_issue_made_confidential_action)\n      else\n        body = \"made the #{noteable_name} visible to everyone\"\n        action = 'visible'\n\n        track_issue_event(:track_issue_made_visible_action)\n      end",
    "comment": "Called when the confidentiality changes  Example Note text:  \"made the issue confidential\"  Returns the created Note object",
    "label": "",
    "id": "4809"
  },
  {
    "raw_code": "def change_status(status, source = nil)\n      create_resource_state_event(status: status, mentionable_source: source)\n    end",
    "comment": "Called when the status of a Noteable is changed  status   - String status source   - Mentionable performing the change, or nil  Example Note text:  \"merged\"  \"closed via bc17db76\"  Returns the created Note object",
    "label": "",
    "id": "4810"
  },
  {
    "raw_code": "def cross_reference_exists?(mentioned_in)\n      notes = noteable.notes.system\n      existing_mentions_for(mentioned_in, noteable, notes).exists?\n    end",
    "comment": "Check if a cross reference to a Mentionable from the `mentioned_in` Mentionable already exists.  This method is used to prevent multiple notes being created for a mention when a issue is updated, for example. The method also calls `existing_mentions_for` to check if the mention is in a commit, and return matches only on commit hash instead of project + commit, to avoid repeated mentions from forks.  @param mentioned_in [Mentionable] @return [Boolean]",
    "label": "",
    "id": "4811"
  },
  {
    "raw_code": "def mark_canonical_issue_of_duplicate(duplicate_issue)\n      body = \"marked #{duplicate_issue.to_reference(project)} as a duplicate of this issue\"\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'duplicate'))\n    end",
    "comment": "Called when a Noteable has been marked as the canonical Issue of a duplicate  duplicate_issue - Issue that was a duplicate of this  Example Note text:  \"marked #1234 as a duplicate of this issue\"  \"marked other_project#5678 as a duplicate of this issue\"  Returns the created Note object",
    "label": "",
    "id": "4812"
  },
  {
    "raw_code": "def mark_duplicate_issue(canonical_issue)\n      body = \"marked this issue as a duplicate of #{canonical_issue.to_reference(project)}\"\n\n      track_issue_event(:track_issue_marked_as_duplicate_action)\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'duplicate'))\n    end",
    "comment": "Called when a Noteable has been marked as a duplicate of another Issue  canonical_issue - Issue that this is a duplicate of  Example Note text:  \"marked this issue as a duplicate of #1234\"  \"marked this issue as a duplicate of other_project#5678\"  Returns the created Note object",
    "label": "",
    "id": "4813"
  },
  {
    "raw_code": "def create_new_alert(monitoring_tool)\n      body = \"logged an alert from **#{monitoring_tool}**\"\n\n      create_note(NoteSummary.new(noteable, project, Users::Internal.alert_bot, body, action: 'new_alert_added'))\n    end",
    "comment": "Called when the a new AlertManagement::Alert has been created  alert - AlertManagement::Alert object.  Example Note text:  \"GitLab Alert Bot logged an alert from Prometheus\"  Returns the created Note object",
    "label": "",
    "id": "4814"
  },
  {
    "raw_code": "def change_alert_status(reason)\n      status = noteable.state.to_s.titleize\n      body = \"changed the status to **#{status}**#{reason}\"\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'status'))\n    end",
    "comment": "Called when the status of an AlertManagement::Alert has changed  alert - AlertManagement::Alert object.  Example Note text:  \"changed the status to Acknowledged\" \"changed the status to Acknowledged by changing the incident status of #540\"  Returns the created Note object",
    "label": "",
    "id": "4815"
  },
  {
    "raw_code": "def new_alert_issue(issue)\n      body = \"created incident #{issue.to_reference(project)} for this alert\"\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'alert_issue_added'))\n    end",
    "comment": "Called when an issue is created based on an AlertManagement::Alert  issue - Issue object.  Example Note text:  \"created incident #17 for this alert\"  Returns the created Note object",
    "label": "",
    "id": "4816"
  },
  {
    "raw_code": "def log_resolving_alert(monitoring_tool)\n      body = \"logged a recovery alert from **#{monitoring_tool}**\"\n\n      create_note(NoteSummary.new(noteable, project, Users::Internal.alert_bot, body, action: 'new_alert_added'))\n    end",
    "comment": "Called when an alert is resolved due to received resolving alert payload  alert - AlertManagement::Alert object.  Example Note text:  \"changed the status to Resolved by closing issue #17\"  Returns the created Note object",
    "label": "",
    "id": "4817"
  },
  {
    "raw_code": "def merge_when_checks_pass(sha)\n      body = \"enabled an automatic merge when all merge checks for #{sha} pass\"\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'merge'))\n    end",
    "comment": "Called when the auto merge is executed",
    "label": "",
    "id": "4818"
  },
  {
    "raw_code": "def cancel_auto_merge\n      body = 'canceled the automatic merge'\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'merge'))\n    end",
    "comment": "Called when the auto merge is canceled",
    "label": "",
    "id": "4819"
  },
  {
    "raw_code": "def abort_auto_merge(reason)\n      body = \"aborted the automatic merge because #{format_reason(reason)}\"\n\n      ##\n      # TODO: Abort message should be sent by the system, not a particular user.\n      # See https://gitlab.com/gitlab-org/gitlab-foss/issues/63187.\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'merge'))\n    end",
    "comment": "Called when the auto merge is aborted",
    "label": "",
    "id": "4820"
  },
  {
    "raw_code": "def change_branch(branch_type, event_type, old_branch, new_branch)\n      body =\n        case event_type.to_s\n        when 'delete'\n          \"deleted the `#{old_branch}` branch. This merge request now targets the `#{new_branch}` branch\"\n        when 'update'\n          \"changed #{branch_type} branch from `#{old_branch}` to `#{new_branch}`\"\n        else\n          raise ArgumentError, \"invalid value for event_type: #{event_type}\"\n        end",
    "comment": "Called when a branch in Noteable is changed  branch_type - 'source' or 'target' event_type  - the source of event: 'update' or 'delete' old_branch  - old branch name new_branch  - new branch name Example Note text is based on event_type:  update: \"changed target branch from `Old` to `New`\" delete: \"deleted the `Old` branch. This merge request now targets the `New` branch\"  Returns the created Note object",
    "label": "",
    "id": "4821"
  },
  {
    "raw_code": "def change_branch_presence(branch_type, branch, presence)\n      verb =\n        if presence == :add\n          'restored'\n        else\n          'deleted'\n        end",
    "comment": "Called when a branch in Noteable is added or deleted  branch_type - :source or :target branch      - branch name presence    - :add or :delete  Example Note text:  \"restored target branch `feature`\"  Returns the created Note object",
    "label": "",
    "id": "4822"
  },
  {
    "raw_code": "def new_issue_branch(branch, branch_project: nil)\n      branch_project ||= project\n      link = url_helpers.project_compare_path(branch_project, from: branch_project.default_branch, to: branch)\n\n      body = \"created branch [`#{branch}`](#{link}) to address this issue\"\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'branch'))\n    end",
    "comment": "Called when a branch is created from the 'new branch' button on a issue Example note text:  \"created branch `201-issue-branch-button`\"",
    "label": "",
    "id": "4823"
  },
  {
    "raw_code": "def approve_mr\n      body = \"approved this merge request\"\n\n      create_note(NoteSummary.new(noteable, project, author, body, action: 'approved'))\n    end",
    "comment": "Called when the merge request is approved by user  Example Note text:  \"approved this merge request\"  Returns the created Note object",
    "label": "",
    "id": "4824"
  },
  {
    "raw_code": "def change_start_date_or_due_date(changed_dates = {})\n      return if changed_dates.empty?\n\n      # Using instance_of because WorkItem < Issue. We don't want to track work item updates as issue updates\n      if noteable.instance_of?(Issue) && changed_dates.key?('due_date')\n        issue_activity_counter.track_issue_due_date_changed_action(author: author, project: project)\n      end",
    "comment": "Called when the start_date or due_date of an Issue/WorkItem is changed  start_date  - Start date being assigned, or nil due_date  - Due date being assigned, or nil  Example Note text:  \"removed due date\"  \"changed due date to September 20, 2018\" \"changed start date to September 20, 2018 and changed due date to September 25, 2018\"  Returns the created Note object",
    "label": "",
    "id": "4825"
  },
  {
    "raw_code": "def change_time_estimate\n      if noteable.is_a?(Issue)\n        issue_activity_counter.track_issue_time_estimate_changed_action(author: author, project: project)\n      end",
    "comment": "Called when the estimated time of a Noteable is changed  time_estimate - Estimated time  Example Note text:  \"removed time estimate\"  \"changed time estimate to 3d 5h\"  Returns the created Note object",
    "label": "",
    "id": "4826"
  },
  {
    "raw_code": "def change_time_spent\n      update_activity_counter\n      time_spent = noteable.time_spent\n\n      if time_spent == :reset\n        body = \"removed time spent\"\n        create_note(NoteSummary.new(noteable, project, author, body, action: 'time_tracking'))\n      else\n        spent_at = noteable.spent_at\n        time_spent_note(time_spent, spent_at)\n      end",
    "comment": "Called when the spent time of a Noteable is changed  time_spent - Spent time  Example Note text:  \"removed time spent\"  \"added 2h 30m of time spent\"  Returns the created Note object",
    "label": "",
    "id": "4827"
  },
  {
    "raw_code": "def created_timelog(timelog)\n      time_spent = timelog.time_spent\n      spent_at = timelog.spent_at\n\n      update_activity_counter\n      time_spent_note(time_spent, spent_at)\n    end",
    "comment": "Called when a timelog is added to an issuable  timelog - Added timelog  Example Note text:  \"subtracted 1h 15m of time spent\"  \"added 2h 30m of time spent\"  Returns the created Note object",
    "label": "",
    "id": "4828"
  },
  {
    "raw_code": "def nullify_signatures(key)\n      key.gpg_signatures.each_batch(of: BATCH_SIZE) do |batch|\n        batch.update_all(gpg_key_id: nil)\n      end",
    "comment": "When a GPG key is deleted, the related signatures have their gpg_key_id column nullified However, when the number of signatures is large, then a timeout may happen The signatures are processed in batches before GPG key delete is attempted in order to avoid timeouts",
    "label": "",
    "id": "4829"
  },
  {
    "raw_code": "def initialize(\n      author:, scope:, target:, message:,\n      created_at: DateTime.current, additional_details: {}, ip_address: nil, target_details: nil)\n      raise MissingAttributeError, \"author\" if author.blank?\n      raise MissingAttributeError, \"target\" if target.blank?\n      raise MissingAttributeError, \"message\" if message.blank?\n\n      validate_scope!(scope)\n\n      @author = build_author(author)\n      @scope = scope\n      @target = build_target(target)\n      @ip_address = ip_address || build_ip_address\n      @message = build_message(message)\n      @created_at = created_at\n      @additional_details = additional_details\n      @target_details = target_details\n    end",
    "comment": "@raise [MissingAttributeError] when required attributes are blank  @return [BuildService]",
    "label": "",
    "id": "4830"
  },
  {
    "raw_code": "def execute\n      AuditEvent.new(payload)\n    end",
    "comment": "Create an instance of AuditEvent  @return [AuditEvent]",
    "label": "",
    "id": "4831"
  },
  {
    "raw_code": "def metadata(required_fields = [:issue_count, :total_issue_weight])\n      fields = metadata_fields(required_fields)\n      keys = fields.keys\n      columns = fields.values_at(*keys)\n\n      results = item_model\n        .where(id: collection_ids)\n        .pluck(*columns)\n        .flatten\n\n      Hash[keys.zip(results)]\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4832"
  },
  {
    "raw_code": "def init_collection\n      strong_memoize(:init_collection) do\n        filter(finder.execute).without_order\n      end",
    "comment": "We memoize the query here since the finder methods we use are quite complex. This does not memoize the result of the query.",
    "label": "",
    "id": "4833"
  },
  {
    "raw_code": "def board_label_ids\n      @board_label_ids ||= board.lists.movable.pluck(:label_id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4834"
  },
  {
    "raw_code": "def without_board_labels(items)\n      return items unless board_label_ids.any?\n\n      items.where(label_links(items, board_label_ids.compact).arel.exists.not)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4835"
  },
  {
    "raw_code": "def label_links(items, label_ids)\n      labels_filter.label_link_query(items, label_ids: label_ids)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4836"
  },
  {
    "raw_code": "def with_list_label(items)\n      items.where(label_links(items, [list.label_id]).arel.exists)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4837"
  },
  {
    "raw_code": "def labels_filter\n      Issuables::LabelFilter.new(params: {}, parent: parent)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4838"
  },
  {
    "raw_code": "def self.initialize_relative_positions(board, current_user, issues)\n        if Gitlab::Database.read_write? && !board.disabled_for?(current_user)\n          Issue.move_nulls_to_end(issues)\n        end",
    "comment": "It is a class method because we cannot apply it prior to knowing how many items should be fetched for a list.",
    "label": "",
    "id": "4839"
  },
  {
    "raw_code": "def reorder_subsequent_lists!(board, insert_position)\n        lists = board.lists.movable.ordered.positioned_at_or_after(insert_position)\n\n        return if lists.empty?\n\n        mapping = lists.map.with_index do |list, i|\n          [list, { position: insert_position + i + 1 }]\n        end.to_h\n\n        ::Gitlab::Database::BulkUpdate.execute(%i[position], mapping)\n      end",
    "comment": "Shift each list that is after the new list's position so that they are in the correct order.",
    "label": "",
    "id": "4840"
  },
  {
    "raw_code": "def decrement_higher_lists(list)\n        list.board.lists.movable.where('position > ?', list.position)\n            .update_all('position = position - 1')\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4841"
  },
  {
    "raw_code": "def decrement_intermediate_lists\n        board.lists.movable.where('position > ?',  old_position)\n                           .where('position <= ?', new_position)\n                           .update_all('position = position - 1')\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4842"
  },
  {
    "raw_code": "def increment_intermediate_lists\n        board.lists.movable.where('position >= ?', new_position)\n                           .where('position < ?',  old_position)\n                           .update_all('position = position + 1')\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4843"
  },
  {
    "raw_code": "def update_list_position(list)\n        list.update_attribute(:position, new_position)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4844"
  },
  {
    "raw_code": "def ensure_target_is_dirty\n      msg = \"Target instance of #{target.class.name} must be dirty (must have changes to save)\"\n      raise(msg) unless target.has_changes_to_save?\n    end",
    "comment": " In order to be proceed to the spam check process, the target must be a dirty instance, which means it should be already assigned with the new attribute values.",
    "label": "",
    "id": "4845"
  },
  {
    "raw_code": "def perform_spam_service_check\n      ensure_target_is_dirty\n\n      # since we can check for spam, and recaptcha is not verified,\n      # ask the SpamVerdictService what to do with the target.\n      spam_verdict_service.execute.tap do |result|\n        case result\n        when BLOCK_USER\n          target.spam!\n          create_spam_log\n          create_spam_abuse_event(result)\n          ban_user\n        when DISALLOW\n          target.spam!\n          create_spam_log\n          create_spam_abuse_event(result)\n        when CONDITIONAL_ALLOW\n          # This means \"require a CAPTCHA to be solved\"\n          target.needs_recaptcha!\n          create_spam_log\n          create_spam_abuse_event(result)\n        when OVERRIDE_VIA_ALLOW_POSSIBLE_SPAM\n          create_spam_log\n        when ALLOW\n          target.clear_spam_flags!\n        when NOOP\n          # spamcheck is not explicitly rendering a verdict & therefore can't make a decision\n          target.clear_spam_flags!\n        end",
    "comment": " Performs the spam check using the spam verdict service, and modifies the target model accordingly based on the result.",
    "label": "",
    "id": "4846"
  },
  {
    "raw_code": "def execute\n        return if user_id && authorized_users.where(user_id: user_id).exists?\n\n        related_todos.each_batch(of: BATCH_SIZE) do |batch|\n          pending_delete = without_authorized(batch).includes(:target, :user).reject do |todo|\n            Ability.allowed?(todo.user, :read_todo, todo, scope: :user)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4847"
  },
  {
    "raw_code": "def project_ids\n        [project_id]\n      end",
    "comment": "Compatibility for #authorized_users in this class we always work with 1 project for queries efficiency",
    "label": "",
    "id": "4848"
  },
  {
    "raw_code": "def todos\n        Todo.joins_issue_and_assignees\n          .for_target(issues)\n          .merge(Issue.confidential_only)\n          .where('todos.user_id != issues.author_id')\n          .where('todos.user_id != issue_assignees.user_id')\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4849"
  },
  {
    "raw_code": "def todos_to_remove?\n        issues&.any?(&:confidential?)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4850"
  },
  {
    "raw_code": "def unauthorized_private_groups\n        return [] unless entity.is_a?(Namespace)\n\n        groups = entity.self_and_descendants.private_only\n\n        groups.select(:id)\n          .id_not_in(GroupsFinder.new(user, all_available: false).execute.select(:id).reorder(nil))\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4851"
  },
  {
    "raw_code": "def non_authorized_planner_groups\n        entity.self_and_descendants.select(:id)\n          .id_not_in(authorized_planner_groups)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4852"
  },
  {
    "raw_code": "def validate_webauthn_credential(webauthn_credential)\n      webauthn_credential.type == WebAuthn::TYPE_PUBLIC_KEY &&\n        webauthn_credential.raw_id && webauthn_credential.id &&\n        webauthn_credential.raw_id == WebAuthn.standard_encoder.decode(webauthn_credential.id)\n    end",
    "comment": " Validates that webauthn_credential is syntactically valid  duplicated from WebAuthn::PublicKeyCredential#verify which can't be used here as we need to call WebAuthn::AuthenticatorAssertionResponse#verify instead (which is done in #verify_webauthn_credential)",
    "label": "",
    "id": "4853"
  },
  {
    "raw_code": "def verify_webauthn_credential(webauthn_credential, stored_credential, challenge, encoder)\n      # We need to adjust the relaying party id (RP id) we verify against if the registration in question\n      # is a migrated U2F registration. This is because the appid of U2F and the rp id of WebAuthn differ.\n      rp_id = webauthn_credential.client_extension_outputs['appid'] ? WebAuthn.configuration.origin : URI(WebAuthn.configuration.origin).host\n      webauthn_credential.response.verify(\n        encoder.decode(challenge),\n        public_key: encoder.decode(stored_credential.public_key),\n        sign_count: stored_credential.counter,\n        rp_id: rp_id\n      )\n    end",
    "comment": " Verifies that webauthn_credential matches stored_credential with the given challenge ",
    "label": "",
    "id": "4854"
  },
  {
    "raw_code": "def project_group_id; end\n\n    def audit(release, action:)\n      # overridden in EE\n    end\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "4855"
  },
  {
    "raw_code": "def snippet_counts\n      @snippets_finder.execute\n        .without_order\n        .select(<<~SQL)\n          count(case when snippets.visibility_level=#{Snippet::PUBLIC}\n            and snippets.secret is FALSE then 1 else null end) as are_public,\n          count(case when snippets.visibility_level=#{Snippet::INTERNAL}\n            then 1 else null end) as are_internal,\n          count(case when snippets.visibility_level=#{Snippet::PRIVATE}\n            then 1 else null end) as are_private,\n          count(case when visibility_level=#{Snippet::PUBLIC}\n            OR visibility_level=#{Snippet::INTERNAL}\n            then 1 else null end) as are_public_or_internal,\n          count(*) as total\n        SQL\n        .take\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4856"
  },
  {
    "raw_code": "def build_from_params\n      if project\n        project.snippets.build(\n          create_params.merge(organization_id: nil)\n        )\n      else\n        PersonalSnippet.new(\n          create_params.merge(organization_id: organization_id)\n        )\n      end",
    "comment": "If the snippet is a \"project snippet\", specifically set it to nil to override the default database value of 1. We only want organization_id on the PersonalSnippet subclass.  See https://gitlab.com/gitlab-org/gitlab/-/issues/460827",
    "label": "",
    "id": "4857"
  },
  {
    "raw_code": "def create_params\n      return params if snippet_actions.empty?\n\n      params.merge(content: snippet_actions[0].content, file_name: snippet_actions[0].file_path)\n    end",
    "comment": "If the snippet_actions param is present we need to fill content and file_name from the model",
    "label": "",
    "id": "4858"
  },
  {
    "raw_code": "def create_first_commit_using_db_data(snippet)\n      return if snippet_actions.empty?\n\n      attrs = commit_attrs(snippet, INITIAL_COMMIT_MSG)\n      actions = [{ file_path: snippet.file_name, content: snippet.content }]\n\n      snippet.snippet_repository.multi_files_action(current_user, actions, **attrs)\n    end",
    "comment": "If the user provides `snippet_actions` and the repository does not exist, we need to commit first the snippet info stored in the database.  Mostly because the content inside `snippet_actions` would assume that the file is already in the repository.",
    "label": "",
    "id": "4859"
  },
  {
    "raw_code": "def repository_empty?(snippet)\n      snippet.repository._uncached_exists? && !snippet.repository._uncached_has_visible_content?\n    end",
    "comment": "Because we are removing repositories we don't want to remove any existing repository with data. Therefore, we cannot rely on cached methods for that check in order to avoid losing data.",
    "label": "",
    "id": "4860"
  },
  {
    "raw_code": "def create_jira_cloud_integration!\n      integration = Integration.find_or_initialize_non_project_specific_integration(\n        'jira_cloud_app',\n        group_id: namespace.id\n      )\n\n      return unless integration\n\n      Integrations::JiraCloudApp.transaction do\n        integration.inherit_from_id = nil\n        integration.activate!\n\n        Integration.descendants_from_self_or_ancestors_from(integration).each_batch(of: BATCH_SIZE) do |records|\n          records.update!(active: true)\n        end",
    "comment": "We must make all GitLab for Jira app integrations active (or inactive in the DestroyService) regardless of whether those integration inherit, or have defined their own custom settings. Unless the group namespace is linked in Jira, the project integrations do not work, even if they are non-inheriting.  Using Integration.descendants_from_self_or_ancestors_from we update all integrations of all subgroups and sub projects to be active.  We keep their inherit_from_id intact, as they might have custom service_ids fields. We also still queue a PropagateIntegrationWorker in order to create integrations (the Integration.descendants_from_self_or_ancestors_from only updates existing ones).",
    "label": "",
    "id": "4861"
  },
  {
    "raw_code": "def batching_scope\n        return model_class.none unless last_id_in_postgresql\n\n        table = model_class.arel_table\n\n        model_class\n          .where(table[:id].gt(context.last_record_id))\n          .where(table[:id].lteq(last_id_in_postgresql))\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- because model here is dynamic and is passed by child class",
    "label": "",
    "id": "4862"
  },
  {
    "raw_code": "def projections\n        raise NotImplementedError, \"Subclasses must implement `projections`\"\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4863"
  },
  {
    "raw_code": "def primary_key\n        :id\n      end",
    "comment": "UInt type primary key used for cursor management, override if necessary.",
    "label": "",
    "id": "4864"
  },
  {
    "raw_code": "def initialize(alert, user)\n      @alert = alert\n      @user = user\n    end",
    "comment": "@param alert [AlertManagement::Alert] @param user [User]",
    "label": "",
    "id": "4865"
  },
  {
    "raw_code": "def initialize(alert, current_user, params)\n        @alert = alert\n        @param_errors = []\n        @status = params.delete(:status)\n\n        super(project: alert.project, current_user: current_user, params: params)\n      end",
    "comment": "@param alert [AlertManagement::Alert] @param current_user [User] @param params [Hash] Attributes of the alert",
    "label": "",
    "id": "4866"
  },
  {
    "raw_code": "def filter_assignees\n        return if params[:assignees].nil?\n\n        # Always take first assignee while multiple are not currently supported\n        params[:assignees] = Array(params[:assignees].first)\n\n        param_errors << _('Assignee has no permissions') if unauthorized_assignees?\n      end",
    "comment": "----- Assignee-related behavior ------",
    "label": "",
    "id": "4867"
  },
  {
    "raw_code": "def filter_status\n        return unless status\n\n        status_event = alert.status_event_for(status)\n\n        unless status_event\n          param_errors << _('Invalid status')\n          return\n        end",
    "comment": "------ Status-related behavior -------",
    "label": "",
    "id": "4868"
  },
  {
    "raw_code": "def initialize(alert, current_user)\n          @alert = alert\n          @current_user = current_user\n        end",
    "comment": "@param alert [AlertManagement::Alert] @param current_user [User]",
    "label": "",
    "id": "4869"
  },
  {
    "raw_code": "def initialize(integration, current_user)\n        @integration = integration\n        @current_user = current_user\n      end",
    "comment": "@param integration [AlertManagement::HttpIntegration] @param current_user [User]",
    "label": "",
    "id": "4870"
  },
  {
    "raw_code": "def initialize(project, current_user, params)\n        @response = nil\n\n        super(project: project, current_user: current_user, params: params.with_indifferent_access)\n      end",
    "comment": "@param project [Project] @param current_user [User] @param params [Hash]",
    "label": "",
    "id": "4871"
  },
  {
    "raw_code": "def permitted_params_keys\n        %i[name active type_identifier]\n      end",
    "comment": "overriden in EE",
    "label": "",
    "id": "4872"
  },
  {
    "raw_code": "def initialize(integration, current_user, params)\n        @integration = integration\n\n        super(integration.project, current_user, params)\n      end",
    "comment": "@param integration [AlertManagement::HttpIntegration] @param current_user [User] @param params [Hash]",
    "label": "",
    "id": "4873"
  },
  {
    "raw_code": "def initialize(project_or_group, current_user = nil, params = {})\n      @project_or_group = project_or_group\n      @current_user = current_user\n      @params = params\n    end",
    "comment": "current_user - The user that performs the action params - A hash of parameters",
    "label": "",
    "id": "4874"
  },
  {
    "raw_code": "def move_project_topics\n      project_ids_for_projects_currently_using_source_and_target = ::Projects::ProjectTopic\n        .where(topic_id: target_topic).select(:project_id)\n      # Only update for projects that exclusively use the source topic\n      ::Projects::ProjectTopic.where(topic_id: source_topic.id)\n        .where.not(project_id: project_ids_for_projects_currently_using_source_and_target)\n        .update_all(topic_id: target_topic.id)\n\n      # Delete source topic for projects that were using source and target\n      ::Projects::ProjectTopic.where(topic_id: source_topic.id).delete_all\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4875"
  },
  {
    "raw_code": "def execute\n        return error_no_permissions unless allowed?\n\n        organization = Organization.create(params.merge(group_id: group.id))\n\n        return error_creating(organization) unless organization.persisted?\n\n        ServiceResponse.success(payload: organization)\n      end",
    "comment": "returns the created organization",
    "label": "",
    "id": "4876"
  },
  {
    "raw_code": "def data\n      strong_memoize(:data) do\n        case trigger\n        when 'push_events'\n          push_events_data\n        when 'tag_push_events'\n          tag_push_events_data\n        when 'note_events'\n          note_events_data\n        when 'issues_events', 'confidential_issues_events'\n          issues_events_data\n        when 'merge_requests_events'\n          merge_requests_events_data\n        when 'job_events'\n          job_events_data\n        when 'pipeline_events'\n          pipeline_events_data\n        when 'wiki_page_events'\n          wiki_page_events_data\n        when 'releases_events'\n          releases_events_data\n        when 'milestone_events'\n          milestone_events_data\n        when 'emoji_events'\n          emoji_events_data\n        when 'resource_access_token_events'\n          access_tokens_events_data\n        when 'project_events'\n          project_events_data\n        when 'vulnerability_events'\n          vulnerability_events_data\n        end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity -- despite a high count, this isn't that complex",
    "label": "",
    "id": "4877"
  },
  {
    "raw_code": "def fetch_and_update_cache!\n        parsed_response = fetch_google_ip_list\n\n        parse_google_prefixes(parsed_response).tap do |subnets|\n          ::ObjectStorage::CDN::GoogleIpCache.update!(subnets)\n        end",
    "comment": "Attempts to retrieve and parse the list of IPs from Google. Updates the internal cache so that the data is accessible.  Returns an array of IPAddr objects consisting of subnets.",
    "label": "",
    "id": "4878"
  },
  {
    "raw_code": "def find_for_project\n        group_vars_by_environment(GCP_KEYS).map do |environment_scope, value|\n          {\n            ref: environment_scope,\n            gcp_project: value['GCP_PROJECT_ID'],\n            service_account_exists: value['GCP_SERVICE_ACCOUNT'].present?,\n            service_account_key_exists: value['GCP_SERVICE_ACCOUNT_KEY'].present?\n          }\n        end",
    "comment": " Find GCP Service Accounts in a GitLab project  This method looks up GitLab project's CI vars and returns Google Cloud Service Accounts combinations aligning GitLab project and ref to GCP projects",
    "label": "",
    "id": "4879"
  },
  {
    "raw_code": "def projects\n      @projects ||= ::ProjectsFinder.new(current_user: current_user).execute.preload(:topics, :project_topics, :route)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4880"
  },
  {
    "raw_code": "def execute\n      return false unless spam_params.captcha_response\n\n      @request = RequestStruct.new(spam_params.ip_address)\n\n      Gitlab::Recaptcha.load_configurations!\n\n      # NOTE: We could pass the model and let the recaptcha gem automatically add errors to it,\n      # but we do not, for two reasons:\n      #\n      # 1. We want control over when the errors are added\n      # 2. We want control over the wording and i18n of the message\n      # 3. We want a consistent interface and behavior when adding support for other captcha\n      #    libraries which may not support automatically adding errors to the model.\n      verify_recaptcha(response: spam_params.captcha_response)\n    end",
    "comment": " Performs verification of a captcha response.  NOTE: Currently only supports reCAPTCHA, and is not yet used in all places of the app in which captchas are verified, but these can be addressed in future MRs.  See: https://gitlab.com/gitlab-org/gitlab/-/issues/273480",
    "label": "",
    "id": "4881"
  },
  {
    "raw_code": "def transform_move_actions(actions)\n      actions.map do |action|\n        action[:infer_content] = true if action[:content].nil?\n\n        action\n      end",
    "comment": "When moving a file, `content: nil` means \"use the contents of the previous file\", while `content: ''` means \"move the file and set it to empty\"",
    "label": "",
    "id": "4882"
  },
  {
    "raw_code": "def initialize(current_user:, personal_access_token_plaintext:, request:)\n      @current_user = current_user\n      @personal_access_token_plaintext = personal_access_token_plaintext\n      @request = request\n    end",
    "comment": "Demonstrating Proof of Possession (DPoP) blueprint: https://gitlab.com/gitlab-com/gl-security/product-security/appsec/security-feature-blueprints/-/blob/main/sender_constraining_access_tokens/index.md",
    "label": "",
    "id": "4883"
  },
  {
    "raw_code": "def ensure_container_repository!(path, actions)\n      return if path.has_repository?\n      return unless actions.include?('push')\n\n      find_or_create_repository_from_path(path)\n    end",
    "comment": " Because we do not have two way communication with registry yet, we create a container repository image resource when push to the registry is successfully authorized. ",
    "label": "",
    "id": "4884"
  },
  {
    "raw_code": "def find_or_create_repository_from_path(path)\n      ContainerRepository.find_or_create_from_path!(path)\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4885"
  },
  {
    "raw_code": "def can_access?(requested_project, requested_action)\n      return false unless requested_project.container_registry_enabled?\n\n      case requested_action\n      when 'pull'\n        build_can_pull?(requested_project) || user_can_pull?(requested_project) || deploy_token_can_pull?(requested_project)\n      when 'push'\n        build_can_push?(requested_project) || user_can_push?(requested_project) || deploy_token_can_push?(requested_project)\n      when 'delete'\n        build_can_delete?(requested_project) || user_can_admin?(requested_project)\n      when '*'\n        user_can_admin?(requested_project)\n      else\n        false\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4886"
  },
  {
    "raw_code": "def build_can_push?(requested_project)\n      # Build can push only to the project from which it originates\n      has_authentication_ability?(:build_create_container_image) &&\n        requested_project == project\n    end",
    "comment": " We still support legacy pipeline triggers which do not have associated actor. New permissions model and new triggers are always associated with an actor. So this should be improved once https://gitlab.com/gitlab-org/gitlab-foss/issues/37452 is resolved. ",
    "label": "",
    "id": "4887"
  },
  {
    "raw_code": "def extra_info\n      {}\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "4888"
  },
  {
    "raw_code": "def extra_attributes_for_measurement\n    defined?(super) ? super : {}\n  end",
    "comment": "You can set extra attributes for performance measurement log.",
    "label": "",
    "id": "4889"
  },
  {
    "raw_code": "def base_log_data\n    extra_attributes_for_measurement.merge({ class: self.class.name })\n  end",
    "comment": "These attributes are always present in log.",
    "label": "",
    "id": "4890"
  },
  {
    "raw_code": "def notification_service\n    NotificationService.new\n  end",
    "comment": "Convenience service methods",
    "label": "",
    "id": "4891"
  },
  {
    "raw_code": "def log_info(message)\n    Gitlab::AppLogger.info message\n  end",
    "comment": "Logging",
    "label": "",
    "id": "4892"
  },
  {
    "raw_code": "def deny_visibility_level(model, denied_visibility_level = nil)\n    denied_visibility_level ||= model.visibility_level\n\n    level_name = Gitlab::VisibilityLevel.level_name(denied_visibility_level).downcase\n\n    model.errors.add(:visibility_level, \"#{level_name} has been restricted by your GitLab administrator\")\n  end",
    "comment": "Add an error to the specified model for restricted visibility levels",
    "label": "",
    "id": "4893"
  },
  {
    "raw_code": "def error(message, http_status = nil, status: :error, pass_back: {})\n    result = {\n      message: message,\n      status: status\n    }.reverse_merge(pass_back)\n\n    result[:http_status] = http_status if http_status\n    result\n  end",
    "comment": "Return a Hash with an `error` status  message     - Error message to include in the Hash http_status - Optional HTTP status code override (default: nil) pass_back   - Additional attributes to be included in the resulting Hash",
    "label": "",
    "id": "4894"
  },
  {
    "raw_code": "def success(pass_back = {})\n    pass_back[:status] = :success\n    pass_back\n  end",
    "comment": "Return a Hash with a `success` status  pass_back - Additional attributes to be included in the resulting Hash",
    "label": "",
    "id": "4895"
  },
  {
    "raw_code": "def valid_visibility_level_change?(target, new_visibility)\n    return true unless new_visibility\n\n    new_visibility_level = Gitlab::VisibilityLevel.level_value(new_visibility, fallback_value: nil)\n\n    if new_visibility_level != target.visibility_level_value\n      unless can?(current_user, :change_visibility_level, target) &&\n          Gitlab::VisibilityLevel.allowed_for?(current_user, new_visibility_level)\n\n        deny_visibility_level(target, new_visibility_level)\n        return false\n      end",
    "comment": "check that user is allowed to set specified visibility_level",
    "label": "",
    "id": "4896"
  },
  {
    "raw_code": "def process_alert\n      alert.persisted? ? process_existing_alert : process_new_alert\n    end",
    "comment": "Updates or creates alert from payload for project including system notes",
    "label": "",
    "id": "4897"
  },
  {
    "raw_code": "def complete_post_processing_tasks\n      process_incident_issues if process_issues?\n      send_alert_email if send_email? && notifying_alert?\n    end",
    "comment": "Creates or closes issue for alert and notifies stakeholders",
    "label": "",
    "id": "4898"
  },
  {
    "raw_code": "def create_issue_type_allowed?(object, issue_type)\n      WorkItems::Type.base_types.key?(issue_type.to_s) &&\n        can?(current_user, :\"create_#{issue_type}\", object)\n    end",
    "comment": "@param object [Issue, Project] @param issue_type [String, Symbol]",
    "label": "",
    "id": "4899"
  },
  {
    "raw_code": "def filter_resolve_discussion_params\n      @merge_request_to_resolve_discussions_object ||= params.delete(:merge_request_to_resolve_discussions_object)\n      @merge_request_to_resolve_discussions_of_iid ||= params.delete(:merge_request_to_resolve_discussions_of)\n      @discussion_to_resolve_id ||= params.delete(:discussion_to_resolve)\n    end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "4900"
  },
  {
    "raw_code": "def merge_request_to_resolve_discussions_of\n      strong_memoize(:merge_request_to_resolve_discussions_of) do\n        # sometimes this will be a Group, when work item is created at group level.\n        # Not sure if we will need to handle resolving an MR with an issue at group level?\n        next unless container.is_a?(Project)\n        next merge_request_to_resolve_discussions_object if merge_request_to_resolve_discussions_object.present?\n\n        MergeRequestsFinder.new(current_user, project_id: container.id)\n          .find_by(iid: merge_request_to_resolve_discussions_of_iid)\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4901"
  },
  {
    "raw_code": "def discussions_to_resolve\n      return [] unless merge_request_to_resolve_discussions_of\n\n      @discussions_to_resolve ||= # rubocop:disable Gitlab/ModuleWithInstanceVariables\n        if discussion_to_resolve_id\n          discussion_or_nil = merge_request_to_resolve_discussions_of\n                                .find_discussion(discussion_to_resolve_id)\n          Array(discussion_or_nil)\n        else\n          merge_request_to_resolve_discussions_of\n            .discussions_to_be_resolved\n        end.reject(&:confidential?)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4902"
  },
  {
    "raw_code": "def initialize_callbacks!(work_item)\n      @callbacks = work_item.widgets.filter_map do |widget|\n        callback_class = widget.class.try(:callback_class)\n        next if callback_class.nil?\n\n        callback_params = widget_params[widget.class.api_symbol] || {}\n        callback_params[:excluded_in_new_type] = true if new_type_excludes_widget?(widget, work_item.resource_parent)\n\n        if callback_class.const_defined?(:ALLOWED_PARAMS)\n          callback_params.reverse_merge!(params.slice(*callback_class::ALLOWED_PARAMS))\n        end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "4903"
  },
  {
    "raw_code": "def handle_quick_actions(work_item)\n      # Unify description parameters into widget params for quick action processing\n      if params[:description].present?\n        description_widget_params = widget_params[::WorkItems::Widgets::Description.api_symbol] ||= {}\n        description_widget_params.reverse_merge!(params.slice(:description))\n      end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "4904"
  },
  {
    "raw_code": "def preload_issue_ids\n      index = 0\n      cached_project_id = caching.get_current_project_id\n\n      collection = projects_collection\n      collection = projects_collection.where(Project.arel_table[:id].gteq(cached_project_id.to_i)) if cached_project_id.present?\n\n      collection.each do |project|\n        caching.cache_current_project_id(project.id)\n        index += 1\n        scope = Issue.in_projects(project).order_by_relative_position.with_non_null_relative_position.select(:id, :relative_position)\n\n        with_retry(PREFETCH_ISSUES_BATCH_SIZE, 100) do |batch_size|\n          Gitlab::Pagination::Keyset::Iterator.new(scope: scope).each_batch(of: batch_size) do |batch|\n            caching.cache_issue_ids(batch)\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4905"
  },
  {
    "raw_code": "def assign_indexes(ids, start_index)\n      ids.each_with_index.map do |id, idx|\n        [id, start_index + idx]\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4906"
  },
  {
    "raw_code": "def update_positions_with_retry(pairs_with_index, query_name)\n      retry_batch_size = pairs_with_index.size\n\n      until pairs_with_index.empty?\n        with_retry(retry_batch_size, SMALLEST_BATCH_SIZE) do |batch_size|\n          retry_batch_size = batch_size\n          update_positions(pairs_with_index.first(batch_size), query_name)\n          # pairs_with_index[batch_size - 1] - can be nil for last batch\n          # if last batch is smaller than batch_size, so we just get the last pair.\n          last_pair_in_batch = pairs_with_index[batch_size - 1] || pairs_with_index.last\n          caching.cache_current_index(last_pair_in_batch.last + 1)\n          pairs_with_index = pairs_with_index.drop(batch_size)\n        end",
    "comment": "The method runs in a loop where we try for RETRIES_LIMIT=3 times, to run the update statement on a number of records(batch size). Method gets an array of (id, value) pairs as argument that is used to build the update query matching by id and updating relative_position = value. If we get a statement timeout, we split the batch size in half and try(for 3 times again) to batch update on a smaller number of records. On success, because we know the batch size and we always pick from the beginning of the array param, we can remove first batch_size number of items from array and continue with the successful batch_size for next batches. On failures we continue to split batch size to a SMALLEST_BATCH_SIZE limit, which is now set at 5.  e.g. 0. items | previous batch size|new batch size | comment 1. 100   | 100                | 100           | 3 failures -> split the batch size in half 2. 100   | 100                | 50            | 3 failures -> split the batch size in half again 3. 100   | 50                 | 25            | 3 succeed -> so we drop 25 items 3 times, 4th fails -> split the batch size in half again 5. 25    | 25                 | 12            | 3 failures -> split the batch size in half 6. 25    | 12                 | 6             | 3 failures -> we exit because smallest batch size is 5 and we'll be at 3 if we split again",
    "label": "",
    "id": "4907"
  },
  {
    "raw_code": "def after_create(issue)\n      user_agent_detail_service.create\n      handle_add_related_issue(issue)\n      resolve_discussions_with_issue(issue)\n      handle_escalation_status_change(issue)\n      create_timeline_event(issue)\n      try_to_associate_contacts(issue)\n      publish_event(issue)\n\n      super\n    end",
    "comment": "Add new items to Issues::AfterCreateService if they can be performed in Sidekiq",
    "label": "",
    "id": "4908"
  },
  {
    "raw_code": "def execute(issue, commit: nil, notifications: true, system_note: true, skip_authorization: false, status: nil)\n      unless can_close?(issue, skip_authorization: skip_authorization)\n        log_failed_auth(issue, commit)\n        return issue\n      end",
    "comment": "Closes the supplied issue if the current user is able to do so.",
    "label": "",
    "id": "4909"
  },
  {
    "raw_code": "def close_issue(issue, closed_via: nil, notifications: true, system_note: true, status: nil)\n      if issue.is_a?(ExternalIssue)\n        close_external_issue(issue, closed_via)\n\n        return issue\n      end",
    "comment": "Closes the supplied issue without checking if the user is authorized to do so.  The code calling this method is responsible for ensuring that a user is allowed to close the given issue.",
    "label": "",
    "id": "4910"
  },
  {
    "raw_code": "def handle_closing_issue!(issue, current_user)\n      issue.close(current_user)\n    end",
    "comment": "overriden in EE",
    "label": "",
    "id": "4911"
  },
  {
    "raw_code": "def after_close(issue, _status, closed_via: nil, notifications: true, system_note: true)\n      event_service.close_issue(issue, current_user)\n      create_note(issue, closed_via) if system_note\n\n      if current_user.project_bot?\n        log_audit_event(issue, current_user, \"#{issue.issue_type}_closed_by_project_bot\",\n          \"Closed #{issue.issue_type.humanize(capitalize: false)} #{issue.title}\")\n      end",
    "comment": "overriden in EE",
    "label": "",
    "id": "4912"
  },
  {
    "raw_code": "def closed_by_merge_requests(issue)\n      return [] unless issue.open?\n\n      merge_requests = extract_merge_requests(issue, filter: :system).select(&:open?)\n\n      return [] if merge_requests.empty?\n\n      ids = MergeRequestsClosingIssues.where(\n        merge_request_id: merge_requests.map(&:id),\n        issue_id: issue.id\n      ).pluck(:merge_request_id)\n      merge_requests.select { |mr| mr.id.in?(ids) }\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4913"
  },
  {
    "raw_code": "def issue_notes(issue)\n      @issue_notes ||= {}\n      @issue_notes[issue] ||= issue.notes.includes(:author)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4914"
  },
  {
    "raw_code": "def sort_by_iid(merge_requests)\n      Gitlab::IssuableSorter.sort(project, merge_requests) { |mr| mr.iid.to_s }\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4915"
  },
  {
    "raw_code": "def execute(issue)\n      @issue = issue\n      @errors = []\n\n      return error_no_permissions unless allowed?\n      return error_invalid_params unless valid_params?\n\n      @existing_ids = issue.customer_relations_contact_ids\n      determine_changes if set_present?\n      return error_too_many if too_many?\n\n      @added_count = 0\n      @removed_count = 0\n\n      add if params[:add_ids].present?\n      remove if params[:remove_ids].present?\n\n      add_by_email if params[:add_emails].present?\n      remove_by_email if params[:remove_emails].present?\n\n      if issue.valid?\n        GraphqlTriggers.issue_crm_contacts_updated(issue)\n        issue.touch\n        create_system_note\n        ServiceResponse.success(payload: issue)\n      else\n        # The default error isn't very helpful: \"Issue customer relations contacts is invalid\"\n        issue.errors.delete(:issue_customer_relations_contacts)\n        issue.errors.add(:issue_customer_relations_contacts, errors.to_sentence)\n        ServiceResponse.error(payload: issue, message: issue.errors.full_messages.to_sentence)\n      end",
    "comment": "Replacing contacts by email is not currently supported",
    "label": "",
    "id": "4916"
  },
  {
    "raw_code": "def after_reopen(issue, _status)\n      event_service.reopen_issue(issue, current_user)\n\n      if current_user.project_bot?\n        log_audit_event(issue, current_user, \"#{issue.issue_type}_reopened_by_project_bot\",\n          \"Reopened #{issue.issue_type.humanize(capitalize: false)} #{issue.title}\")\n      end",
    "comment": "overriden in EE",
    "label": "",
    "id": "4917"
  },
  {
    "raw_code": "def reopen_issue(issue)\n      issue.reopen\n    end",
    "comment": "overriden in EE",
    "label": "",
    "id": "4918"
  },
  {
    "raw_code": "def self.constructor_container_arg(value)\n      { container: value }\n    end",
    "comment": "overriding this because IssuableBaseService#constructor_container_arg returns { project: value } Issues::ReopenService constructor signature is different now, it takes container instead of project also IssuableBaseService#change_state dynamically picks one of the `Issues::ReopenService`, MergeRequests::ReopenService, so we need this method to return { }container: value } for Issues::ReopenService",
    "label": "",
    "id": "4919"
  },
  {
    "raw_code": "def execute_incident_hooks(issue, issue_data)\n      issue_data[:object_kind] = 'incident'\n      issue_data[:event_type] = 'incident'\n      issue.namespace.execute_integrations(issue_data, :incident_hooks)\n    end",
    "comment": "We can remove this code after proposal in https://gitlab.com/gitlab-org/gitlab/-/issues/367550#proposal is updated.",
    "label": "",
    "id": "4920"
  },
  {
    "raw_code": "def initialize(container:, current_user: nil, params: {}, perform_spam_check: false)\n      super(container: container, current_user: current_user, params: params)\n      @perform_spam_check = perform_spam_check\n    end",
    "comment": "NOTE: For Issues::UpdateService, we default perform_spam_check to false, because spam_checking is not necessary in many cases, and we don't want to require every caller to explicitly pass it to disable spam checking.",
    "label": "",
    "id": "4921"
  },
  {
    "raw_code": "def initialize(packages:, current_user: nil)\n      @packages = packages\n      @current_user = current_user\n    end",
    "comment": "Initialize this service with the given packages and user.  * `packages`: must be an ActiveRecord relationship. * `current_user`: an User object. Could be nil.",
    "label": "",
    "id": "4922"
  },
  {
    "raw_code": "def write_metadata\n        metadatum.update!(\n          authors: gemspec&.authors,\n          files: gemspec&.files&.to_json,\n          summary: gemspec&.summary,\n          description: gemspec&.description,\n          email: gemspec&.email,\n          homepage: gemspec&.homepage,\n          licenses: gemspec&.licenses&.to_json,\n          metadata: gemspec&.metadata&.to_json,\n          author: gemspec&.author,\n          bindir: gemspec&.bindir,\n          executables: gemspec&.executables&.to_json,\n          extensions: gemspec&.extensions&.to_json,\n          extra_rdoc_files: gemspec&.extra_rdoc_files&.to_json,\n          platform: gemspec&.platform,\n          post_install_message: gemspec&.post_install_message,\n          rdoc_options: gemspec&.rdoc_options&.to_json,\n          require_paths: gemspec&.require_paths&.to_json,\n          required_ruby_version: gemspec&.required_ruby_version&.to_s,\n          required_rubygems_version: gemspec&.required_rubygems_version&.to_s,\n          requirements: gemspec&.requirements&.to_json,\n          rubygems_version: gemspec&.rubygems_version\n        )\n      end",
    "comment": "rubocop:disable Metrics/AbcSize rubocop:disable Metrics/PerceivedComplexity rubocop:disable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "4923"
  },
  {
    "raw_code": "def metadatum\n        # safe_find_or_create_by! was originally called here.\n        # We merely switched to `find_or_create_by!`\n        # rubocop: disable CodeReuse/ActiveRecord\n        Packages::Rubygems::Metadatum.find_or_create_by!(package: package)\n        # rubocop: enable CodeReuse/ActiveRecord\n      end",
    "comment": "rubocop:enable Metrics/AbcSize rubocop:enable Metrics/PerceivedComplexity rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "4924"
  },
  {
    "raw_code": "def lease_key\n        \"packages:rubygems:process_gem_service:package:#{package.id}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4925"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4926"
  },
  {
    "raw_code": "def uniqueness_constraint_columns\n        recipe_revision_id.nil? ? UNIQUENESS_COLUMNS : UNIQUENESS_COLUMNS_WITH_REVISION\n      end",
    "comment": "Two uniqueness constraints are used: - (package_id, reference) when recipe_revision_id is NULL - (package_id, recipe_revision_id, reference) when recipe_revision_id is present This allows having the same package_id/reference pair with different recipe_revision_id values",
    "label": "",
    "id": "4927"
  },
  {
    "raw_code": "def initialize(data)\n          @data = data\n        end",
    "comment": "Expected `data` structure  data = { filelists: { checksum: { type: \"sha256\", value: \"123\" }, location: { href: \"repodata/123-filelists.xml.gz\" }, ... }, ... }",
    "label": "",
    "id": "4928"
  },
  {
    "raw_code": "def lease_key\n        \"packages:helm:process_file_service:package_file:#{package_file.id}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4929"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4930"
  },
  {
    "raw_code": "def lease_key\n        \"packages:helm:create_metadata_cache_service:metadata_caches:#{project.id}_#{channel}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4931"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4932"
  },
  {
    "raw_code": "def lease_key\n        \"packages:debian:process_package_file_service:#{temp_package.project_id}_#{package_name}_#{package_version}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4933"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4934"
  },
  {
    "raw_code": "def lease_key\n        \"packages:debian:generate_distribution_service:#{@distribution.class.container_type}_distribution:#{@distribution.id}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4935"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4936"
  },
  {
    "raw_code": "def calculated_package_file_size\n        # This calculation is based on:\n        # 1. 4 chars in a Base64 encoded string are 3 bytes in the original string. Meaning 1 char is 0.75 bytes.\n        # 2. The encoded string may have 1 or 2 extra '=' chars used for padding. Each padding char means 1 byte less in\n        #    the original string.\n        # Reference:\n        # - https://blog.aaronlenoir.com/2017/11/10/get-original-length-from-base-64-string/\n        # - https://en.wikipedia.org/wiki/Base64#Decoding_Base64_with_padding\n        encoded_data = attachment['data']\n        ((encoded_data.length * 0.75) - encoded_data[-2..].count('=')).to_i\n      end",
    "comment": "TODO (technical debt): Extract the package size calculation to its own component and unit test it separately.",
    "label": "",
    "id": "4937"
  },
  {
    "raw_code": "def lease_key\n        \"packages:npm:create_package_service:packages:#{project.id}_#{name}_#{version}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4938"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4939"
  },
  {
    "raw_code": "def lease_key\n        \"packages:npm:create_metadata_cache_service:metadata_caches:#{project.id}_#{package_name}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4940"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4941"
  },
  {
    "raw_code": "def prefix_from(artifact_id)\n          artifact_id.gsub(/-?maven-?/, '')\n                     .gsub(/-?plugin-?/, '')\n        end",
    "comment": "Maven plugin prefix generation from https://github.com/apache/maven/blob/c3dba0e5ba71ee7cbd62620f669a8c206e71b5e2/maven-plugin-api/src/main/java/org/apache/maven/plugin/descriptor/PluginDescriptor.java#L189",
    "label": "",
    "id": "4942"
  },
  {
    "raw_code": "def lease_key\n        \"#{self.class.name.underscore}:#{project.id}_#{metadata[:package_name]}_#{metadata[:package_version]}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4943"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4944"
  },
  {
    "raw_code": "def lease_key\n        package_id = existing_package ? existing_package.id : @package_file.package_id\n        \"packages:nuget:update_package_from_metadata_service:package:#{package_id}\"\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4945"
  },
  {
    "raw_code": "def lease_timeout\n        DEFAULT_LEASE_TIMEOUT\n      end",
    "comment": "used by ExclusiveLeaseGuard",
    "label": "",
    "id": "4946"
  },
  {
    "raw_code": "def initialize(file)\n          @file = file\n        end",
    "comment": "The extraction of the signature in this service is based on the following documentation: https://github.com/dotnet/symstore/blob/main/docs/specs/SSQP_Key_Conventions.md#portable-pdb-signature",
    "label": "",
    "id": "4947"
  },
  {
    "raw_code": "def checksum\n          sha = OpenSSL::Digest.new('SHA256')\n          count = 0\n          chunk = (+'').force_encoding(Encoding::BINARY)\n          file.rewind\n\n          while file.read(SHA_CHUNK_SIZE, chunk)\n            count += 1\n            chunk[pdb_id] = TWENTY_ZEROED_BYTES if count == 1\n            sha.update(chunk)\n          end",
    "comment": "https://github.com/dotnet/corefx/blob/master/src/System.Reflection.Metadata/specs/PE-COFF.md#portable-pdb-checksum",
    "label": "",
    "id": "4948"
  },
  {
    "raw_code": "def execute_async_for_all(pipelines, failure_reason, context_user)\n      pipelines.cancelable.select(:id).find_in_batches do |pipelines_batch|\n        Ci::DropPipelineWorker.bulk_perform_async_with_contexts(\n          pipelines_batch,\n          arguments_proc: ->(pipeline) { [pipeline.id, failure_reason] },\n          context_proc: ->(_) { { user: context_user } }\n        )\n      end",
    "comment": "execute service asynchronously for each cancelable pipeline",
    "label": "",
    "id": "4949"
  },
  {
    "raw_code": "def preloaded_relations\n      [:project, :pipeline, :metadata, :job_definition, :deployment, :taggings]\n    end",
    "comment": "overridden in EE",
    "label": "",
    "id": "4950"
  },
  {
    "raw_code": "def id\n      \"#{project.id}-#{current_user.id}\"\n    end",
    "comment": "Required for ReactiveCaching, it is also used in `reactive_cache_worker_finder`",
    "label": "",
    "id": "4951"
  },
  {
    "raw_code": "def each_build(params, &blk)\n      queue = Ci::Queue::BuildQueueService.new(runner)\n      builds = queue.build_candidates\n\n      build_and_partition_ids = retrieve_queue(-> { queue.execute(builds) })\n      queue_size = build_and_partition_ids.size\n\n      @metrics.observe_queue_size(-> { queue_size }, @runner.runner_type)\n\n      build_and_partition_ids.each do |build_id, partition_id|\n        yield Ci::Build.find_by!(partition_id: partition_id, id: build_id), queue_size\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4952"
  },
  {
    "raw_code": "def retrieve_queue(queue_query_proc)\n      ##\n      # We want to reset a load balancing session to discard the side\n      # effects of writes that could have happened prior to this moment.\n      #\n      ::Gitlab::Database::LoadBalancing::SessionMap.clear_session\n\n      @metrics.observe_queue_time(:retrieve, @runner.runner_type) do\n        @logger.instrument(:retrieve_queue, once: true) do\n          queue_query_proc.call\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4953"
  },
  {
    "raw_code": "def present_build!(build, queue_size:, queue_depth:)\n      # We need to use the presenter here because Gitaly calls in the presenter\n      # may fail, and we need to ensure the response has been generated.\n      presented_build = ::Ci::BuildRunnerPresenter.new(build) # rubocop:disable CodeReuse/Presenter -- old code\n      presented_build.set_queue_metrics(size: queue_size, depth: queue_depth)\n\n      @logger.instrument(:present_build_logs) do\n        log_artifacts_context(build)\n        log_build_dependencies_size(presented_build)\n      end",
    "comment": "Force variables evaluation to occur now",
    "label": "",
    "id": "4954"
  },
  {
    "raw_code": "def batch_execute(worker_name:)\n      start_time = Time.current\n      in_lock(EXCLUSIVE_LOCK_KEY, ttl: LOCK_TIMEOUT, retries: 1) do\n        Ci::Build.with_stale_live_trace.find_each(batch_size: BATCH_SIZE).with_index do |build, index|\n          break if Time.current - start_time > LOOP_TIMEOUT\n\n          if index > LOOP_LIMIT\n            Sidekiq.logger.warn(class: worker_name, message: 'Loop limit reached.', job_id: build.id)\n            break\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4955"
  },
  {
    "raw_code": "def execute(job, worker_name:)\n      unless job.trace.archival_attempts_available?\n        Sidekiq.logger.warn(class: worker_name, message: 'The job is out of archival attempts.', job_id: job.id)\n\n        job.trace.attempt_archive_cleanup!\n        return\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4956"
  },
  {
    "raw_code": "def unsafe_execute(pipelines, skip_cancel: false)\n      expire_cache(pipelines)\n      cancel_jobs(pipelines) unless skip_cancel\n      reload_and_destroy(pipelines)\n\n      ServiceResponse.success(message: 'Pipeline not found')\n    end",
    "comment": "In this we're intentionally grouping the operations in batches, starting with the read queries, because we want to use the database replica for as long as possible. It is unsafe because the caller needs to ensure proper permissions check.  @param [<Array>Ci::Pipeline] pipelines @param [Boolean] skip_cancel - skips canceling the pipeline before destroy. Only to be used when deleting old pipelines and  we don't care about triggering side effects like, counting CI minutes, email notifications, etc. It should not be used with `execute` because for user-triggered deletions we always want side effects to be triggered.",
    "label": "",
    "id": "4957"
  },
  {
    "raw_code": "def cancel_jobs(pipelines)\n      Array.wrap(pipelines).each do |pipeline|\n        ::Ci::CancelPipelineService.new(\n          pipeline: pipeline,\n          current_user: current_user,\n          cascade_to_children: true,\n          execute_async: false).force_execute\n      end",
    "comment": "Ensure cancellation happens sync so we accumulate compute minutes successfully before deleting the pipeline.",
    "label": "",
    "id": "4958"
  },
  {
    "raw_code": "def destroy_all_records(pipeline)\n      pipeline.destroy!\n    rescue ActiveRecord::StaleObjectError\n      force_destroy_all_records(pipeline)\n    end",
    "comment": "The pipeline, the builds, job and pipeline artifacts all get destroyed here. Ci::Pipeline#destroy triggers fast destroy on job_artifacts and build_trace_chunks to remove the records and data stored in object storage. ci_builds records are deleted using ON DELETE CASCADE from ci_pipelines ",
    "label": "",
    "id": "4959"
  },
  {
    "raw_code": "def first_2_metadata_entries_for_artifacts_exposed_paths(job)\n      return [] unless job.artifacts_metadata\n\n      job.artifacts_exposed_paths\n        .lazy\n        .map { |path| job.artifacts_metadata_entry(path, recursive: true) }\n        .select { |entry| entry.exists? }\n        .first(2)\n    end",
    "comment": "we don't need to fetch all artifacts entries for a job because it could contain many. We only need to know whether it has 1 or more artifacts, so fetching the first 2 would be sufficient.",
    "label": "",
    "id": "4960"
  },
  {
    "raw_code": "def unlock_job_artifacts_query(pipeline_ids)\n      ci_job_artifacts = ::Ci::JobArtifact.arel_table\n\n      build_ids = ::Ci::Build.select(:id).where(commit_id: pipeline_ids)\n\n      returning = Arel::Nodes::Grouping.new(ci_job_artifacts[:id])\n\n      Arel::UpdateManager.new\n        .table(ci_job_artifacts)\n        .where(ci_job_artifacts[:job_id].in(Arel.sql(build_ids.to_sql)))\n        .set([[ci_job_artifacts[:locked], ::Ci::JobArtifact.lockeds[:unlocked]]])\n        .to_sql + \" RETURNING #{returning.to_sql}\"\n    end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4961"
  },
  {
    "raw_code": "def unlock_pipelines_query(ci_ref, before_pipeline)\n      ci_pipelines = ::Ci::Pipeline.arel_table\n\n      pipelines_scope = ci_ref.pipelines.artifacts_locked\n      pipelines_scope = pipelines_scope.before_pipeline(before_pipeline) if before_pipeline\n      pipelines_scope = pipelines_scope.select(:id).limit(BATCH_SIZE).lock('FOR UPDATE SKIP LOCKED')\n\n      returning = Arel::Nodes::Grouping.new(ci_pipelines[:id])\n\n      Arel::UpdateManager.new\n        .table(ci_pipelines)\n        .where(ci_pipelines[:id].in(Arel.sql(pipelines_scope.to_sql)))\n        .set([[ci_pipelines[:locked], ::Ci::Pipeline.lockeds[:unlocked]]])\n        .to_sql + \" RETURNING #{returning.to_sql}\"\n    end",
    "comment": "rubocop:enable CodeReuse/ActiveRecord rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4962"
  },
  {
    "raw_code": "def unlock_pipeline_artifacts(pipelines)\n      return 0 if pipelines.empty?\n\n      ::Ci::PipelineArtifact.where(pipeline_id: pipelines.rows.flatten).update_all(locked: :unlocked)\n    end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4963"
  },
  {
    "raw_code": "def unlock_pipelines(ci_ref, before_pipeline)\n      ::Ci::Pipeline.connection.exec_query(unlock_pipelines_query(ci_ref, before_pipeline))\n    end",
    "comment": "rubocop:enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4964"
  },
  {
    "raw_code": "def load_next_batch\n      # `find_by_sql` performs a write in this case and we need to wrap it in\n      # a transaction to stick to the primary database.\n      Ci::DeletedObject.transaction do\n        Ci::DeletedObject.find_by_sql([next_batch_sql, { new_pick_up_at: RETRY_IN.from_now }])\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4965"
  },
  {
    "raw_code": "def next_batch_sql\n      <<~SQL.squish\n      UPDATE \"ci_deleted_objects\"\n        SET \"pick_up_at\" = :new_pick_up_at\n        WHERE \"ci_deleted_objects\".\"id\" IN (#{locked_object_ids_sql})\n        RETURNING *\n      SQL\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4966"
  },
  {
    "raw_code": "def dependent_jobs\n      ordered_by_dag(\n        @pipeline.processables\n          .from_union(needs_dependent_jobs, stage_dependent_jobs)\n          .skipped\n          .ordered_by_stage\n          .preload(:needs)\n      )\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4967"
  },
  {
    "raw_code": "def initialize(\n      pipeline:,\n      current_user:,\n      cascade_to_children: true,\n      auto_canceled_by_pipeline: nil,\n      execute_async: true,\n      safe_cancellation: false)\n      @pipeline = pipeline\n      @current_user = current_user\n      @cascade_to_children = cascade_to_children\n      @auto_canceled_by_pipeline = auto_canceled_by_pipeline\n      @execute_async = execute_async\n      @safe_cancellation = safe_cancellation\n    end",
    "comment": " @cascade_to_children - if true cancels all related child pipelines for parent child pipelines @auto_canceled_by_pipeline - store the pipeline_id of the pipeline that triggered cancellation @execute_async - if true cancel the children asyncronously @safe_cancellation - if true only cancel interruptible:true jobs",
    "label": "",
    "id": "4968"
  },
  {
    "raw_code": "def force_execute\n      return ServiceResponse.error(message: 'No pipeline provided', reason: :no_pipeline) unless pipeline\n\n      unless pipeline.cancelable?\n        return ServiceResponse.error(message: 'Pipeline is not cancelable', reason: :pipeline_not_cancelable)\n      end",
    "comment": "This method should be used only when we want to always cancel the pipeline without checking whether the current_user has permissions to do so, or when we don't have a current_user available in the context.",
    "label": "",
    "id": "4969"
  },
  {
    "raw_code": "def cancel_children\n      cancel_jobs(pipeline.bridges_in_self_and_project_descendants.cancelable)\n\n      # For parent child-pipelines only (not multi-project)\n      pipeline.all_child_pipelines.each do |child_pipeline|\n        if execute_async?\n          ::Ci::CancelPipelineWorker.perform_async(\n            child_pipeline.id,\n            @auto_canceled_by_pipeline&.id\n          )\n        else\n          # cascade_to_children is false because we iterate through children\n          # we also cancel bridges prior to prevent more children\n          self.class.new(\n            pipeline: child_pipeline.reset,\n            current_user: nil,\n            cascade_to_children: false,\n            execute_async: execute_async?,\n            auto_canceled_by_pipeline: @auto_canceled_by_pipeline\n          ).force_execute\n        end",
    "comment": "We don't handle the case when `cascade_to_children` is `true` and `safe_cancellation` is `true` because `safe_cancellation` is passed as `true` only when `cascade_to_children` is `false` from `CancelRedundantPipelinesService`. In the future, when \"safe cancellation\" is implemented as a regular cancellation feature, we need to handle this case.",
    "label": "",
    "id": "4970"
  },
  {
    "raw_code": "def execute(pipelines, failure_reason)\n      batch_size = 100\n      processed_count = 0\n\n      loop do\n        processed_count += 1\n        if processed_count > ABORT_PIPELINE_BATCHING_LIMIT\n          raise PipelinesAbortLimitExceededError, \"Exceeded the maximum batching limit to abort pipelines\"\n        end",
    "comment": "NOTE: This call fails pipelines in bulk without running callbacks. Only for pipeline abandonment scenarios (examples: project delete)",
    "label": "",
    "id": "4971"
  },
  {
    "raw_code": "def upsert_records(batch)\n      keys = %i[build_id partition_id name project_id]\n\n      builds_upsert_data =\n        batch\n          .pluck(:id, :partition_id, :name, :project_id)\n          .map { |values| Hash[keys.zip(values)] }\n\n      return unless builds_upsert_data.any?\n\n      Ci::BuildName.upsert_all(builds_upsert_data, unique_by: [:build_id, :partition_id])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- plucking attributes is more efficient than loading the records rubocop: disable Database/AvoidUsingPluckWithoutLimit -- plucking on batch",
    "label": "",
    "id": "4972"
  },
  {
    "raw_code": "def audit_change(instance_variable); end\n\n    def persist_records\n      changes = []\n      success = false\n\n      Ci::InstanceVariable.transaction do\n        changes = @records.map do |record|\n          if record.marked_for_destruction?\n            { action: record.destroy, record: record }\n          else\n            { action: record.save, record: record }\n          end\n        end",
    "comment": "overridden in EE",
    "label": "",
    "id": "4973"
  },
  {
    "raw_code": "def update_etag_cache(pipeline, store)\n      project = pipeline.project\n\n      etag_paths = [\n        project_pipelines_path(project),\n        new_merge_request_pipelines_path(project),\n        graphql_project_on_demand_scan_counts_path(project)\n      ]\n\n      etag_paths << commit_pipelines_path(project, pipeline.commit) if pipeline.sha && pipeline.commit.present?\n\n      each_pipelines_merge_request_path(pipeline) do |path|\n        etag_paths << path\n      end",
    "comment": "Updates ETag caches of a pipeline.  This logic resides in a separate method so that EE can more easily extend it.  @param [Ci::Pipeline] pipeline @param [Gitlab::EtagCaching::Store] store",
    "label": "",
    "id": "4974"
  },
  {
    "raw_code": "def in_build_trace_lock(&block)\n      build.trace.lock do |_, lease| # rubocop:disable CodeReuse/ActiveRecord\n        build.run_on_status_commit { lease.cancel }\n\n        yield\n      end",
    "comment": " This method is releasing an exclusive lock on a build trace the moment we conclude that build status has been written and the build state update has been committed to the database.  Because a build state machine schedules a bunch of workers to run after build status transition to complete, we do not want to keep the lease until all the workers are scheduled because it opens a possibility of race conditions happening.  Instead of keeping the lease until the transition is fully done and workers are scheduled, we immediately release the lock after the database commit happens. ",
    "label": "",
    "id": "4975"
  },
  {
    "raw_code": "def push(build, transition)\n      raise InvalidQueueTransition unless transition.to == 'pending'\n\n      transition.within_transaction do\n        result = build.create_queuing_entry!\n\n        unless result.empty?\n          metrics.increment_queue_operation(:build_queue_push)\n\n          result.rows.dig(0, 0)\n        end",
    "comment": " Add a build to the pending builds queue ",
    "label": "",
    "id": "4976"
  },
  {
    "raw_code": "def pop(build, transition)\n      raise InvalidQueueTransition unless transition.from == 'pending'\n\n      transition.within_transaction do\n        remove!(build).tap { build.cancel_wait_for_runner_ack }\n      end",
    "comment": " Remove a build from the pending builds queue ",
    "label": "",
    "id": "4977"
  },
  {
    "raw_code": "def remove!(build)\n      removed = build.all_queuing_entries.delete_all\n\n      if removed > 0\n        metrics.increment_queue_operation(:build_queue_pop)\n\n        build.id\n      end",
    "comment": " Force remove build from the queue, without checking a transition state ",
    "label": "",
    "id": "4978"
  },
  {
    "raw_code": "def track(build, transition)\n      return if build.runner.nil?\n\n      raise InvalidQueueTransition unless transition.to == 'running'\n\n      transition.within_transaction do\n        result = ::Ci::RunningBuild.upsert_build!(build)\n\n        unless result.empty?\n          metrics.increment_queue_operation(:shared_runner_build_new) if build.shared_runner_build?\n\n          result.rows.dig(0, 0)\n        end",
    "comment": " Add runner build tracking entry (used for queuing and for runner fleet dashboard). ",
    "label": "",
    "id": "4979"
  },
  {
    "raw_code": "def untrack(build, transition)\n      return if build.runner.nil?\n\n      raise InvalidQueueTransition unless transition.from == 'running'\n\n      transition.within_transaction do\n        removed = build.all_runtime_metadata.delete_all\n\n        if removed > 0\n          metrics.increment_queue_operation(:shared_runner_build_done) if build.shared_runner_build?\n\n          build.id\n        end",
    "comment": " Remove a runtime build tracking entry for a runner build (used for queuing and for runner fleet dashboard). ",
    "label": "",
    "id": "4980"
  },
  {
    "raw_code": "def tick(build)\n      tick_for(build, build.project.all_available_runners)\n    end",
    "comment": " Unblock runner associated with given project / build ",
    "label": "",
    "id": "4981"
  },
  {
    "raw_code": "def delete_batch!(klass)\n      deleted = 0\n\n      klass.transaction do\n        ids = klass.deletable.lock('FOR UPDATE SKIP LOCKED').limit(BATCH_SIZE).pluck(:id)\n        deleted = klass.where(id: ids).delete_all if ids.any?\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4982"
  },
  {
    "raw_code": "def builds_relation\n      if Feature.enabled?(:disable_ci_partition_pruning, pipeline.project)\n        # CTE is used to force the query to use btree index starting with (commit_id, type)\n        # e.g. p_ci_builds_commit_id_type_ref_idx\n        # rubocop: disable CodeReuse/ActiveRecord -- custom CTE query\n        cte = Gitlab::SQL::CTE.new(\n          :cte_builds,\n          # Do not include partition_id in the select expression of this CTE query!\n          # Because when new partition of p_ci_builds starts to receive writes,\n          # the query plan will flip and choose an unexpected index e.g. primary key index\n          Ci::Build.in_pipelines(pipeline).select(:id)\n        )\n\n        Ci::Build\n          .with(cte.to_arel)\n          .from('\"cte_builds\" AS \"p_ci_builds\"')\n          .unscope(where: :type)\n        # rubocop: enable CodeReuse/ActiveRecord\n      else\n        pipeline.builds\n      end",
    "comment": "Removes the partition_id filter from the query until we get more data in the second partition.",
    "label": "",
    "id": "4983"
  },
  {
    "raw_code": "def partition_id\n      pipeline.partition_id\n    end",
    "comment": "All the partitionable entities connected to a pipeline belong to the same partition where the pipeline is.",
    "label": "",
    "id": "4984"
  },
  {
    "raw_code": "def execute(\n      source,\n      ignore_skip_ci: false, save_on_errors: true, schedule: nil, merge_request: nil,\n      external_pull_request: nil, bridge: nil, inputs: {},\n      **options, &block\n    )\n      @logger = build_logger\n      @command_logger = Gitlab::Ci::Pipeline::CommandLogger.new\n      @pipeline = Ci::Pipeline.new\n\n      validate_options!(options)\n\n      @command = Gitlab::Ci::Pipeline::Chain::Command.new(\n        source: source,\n        origin_ref: params[:ref],\n        checkout_sha: params[:checkout_sha],\n        after_sha: params[:after],\n        before_sha: params[:before],          # The base SHA of the source branch (i.e merge_request.diff_base_sha).\n        source_sha: params[:source_sha],      # The HEAD SHA of the source branch (i.e merge_request.diff_head_sha).\n        target_sha: params[:target_sha],      # The HEAD SHA of the target branch.\n        schedule: schedule,\n        merge_request: merge_request,\n        external_pull_request: external_pull_request,\n        ignore_skip_ci: ignore_skip_ci,\n        save_incompleted: save_on_errors,\n        seeds_block: block,\n        variables_attributes: params[:variables_attributes],\n        project: project,\n        current_user: current_user,\n        push_options: ::Ci::PipelineCreation::PushOptions.fabricate(params[:push_options]),\n        chat_data: params[:chat_data],\n        bridge: bridge,\n        logger: @logger,\n        partition_id: params[:partition_id],\n        inputs: inputs,\n        gitaly_context: params[:gitaly_context],\n        **extra_options(**options))\n\n      @pipeline.readonly! if @command.readonly?\n\n      Gitlab::Ci::Pipeline::Chain::Sequence\n        .new(pipeline, @command, SEQUENCE)\n        .build!\n\n      if pipeline.persisted?\n        Gitlab::EventStore.publish(\n          Ci::PipelineCreatedEvent.new(data: { pipeline_id: pipeline.id })\n        )\n\n        after_successful_creation_hook\n      else\n        # If pipeline is not persisted, try to recover IID\n        pipeline.reset_project_iid\n      end",
    "comment": "Create a new pipeline in the specified project.  @param [Symbol] source                                  What event (Ci::Pipeline.sources) triggers the pipeline creation. @param [Boolean] ignore_skip_ci                         Whether skipping a pipeline creation when `[skip ci]` comment is present in the commit body @param [Boolean] save_on_errors                         Whether persisting an invalid pipeline when it encounters an error during creation (e.g. invalid yaml) @param [Ci::PipelineSchedule] schedule                  The pipeline schedule triggers the pipeline creation. @param [MergeRequest] merge_request                     The merge request triggers the pipeline creation. @param [Ci::ExternalPullRequest] external_pull_request  The external pull request triggers the pipeline creation. @param [Ci::Bridge] bridge                              The bridge job that triggers the downstream pipeline creation. @param [String] content                                 The content of .gitlab-ci.yml to override the default config contents (e.g. .gitlab-ci.yml in repostiry). Mainly used for generating a dangling pipeline.  @return [Ci::Pipeline]                                  The created Ci::Pipeline object. rubocop: disable Metrics/ParameterLists, Metrics/AbcSize",
    "label": "",
    "id": "4985"
  },
  {
    "raw_code": "def execute_async(source, options)\n      pipeline_creation_request = ::Ci::PipelineCreation::Requests.start_for_project(project)\n      creation_params = params.merge(pipeline_creation_request: pipeline_creation_request)\n\n      ::CreatePipelineWorker.perform_async(\n        project.id, current_user.id, params[:ref], source.to_s,\n        options.stringify_keys, creation_params.except(:ref).stringify_keys\n      )\n\n      ServiceResponse.success(payload: pipeline_creation_request['id'])\n    end",
    "comment": "rubocop: enable Metrics/ParameterLists, Metrics/AbcSize",
    "label": "",
    "id": "4986"
  },
  {
    "raw_code": "def validate_options!(_)\n      raise ArgumentError, \"Param `partition_id` is not allowed\" if params[:partition_id]\n    end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment :nocov: Tested in FOSS and fully overridden and tested in EE",
    "label": "",
    "id": "4987"
  },
  {
    "raw_code": "def extra_options(content: nil, dry_run: false, linting: false)\n      { content: content, dry_run: dry_run, linting: linting }\n    end",
    "comment": ":nocov: rubocop:enable Gitlab/NoCodeCoverageComment",
    "label": "",
    "id": "4988"
  },
  {
    "raw_code": "def pending_builds(timeout)\n        Ci::Build\n          .pending\n          .created_at_before(timeout)\n          .updated_at_before(timeout)\n          .order(created_at: :asc, project_id: :asc)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord We're adding the ordering clause by `created_at` and `project_id` because we want to force the query planner to use the `ci_builds_gitlab_monitor_metrics` index all the time.",
    "label": "",
    "id": "4989"
  },
  {
    "raw_code": "def fetch(builds)\n        loop do\n          jobs = builds.includes(:tags, :runner, project: [:namespace, :route])\n            .limit(100)\n            .to_a\n\n          break if jobs.empty?\n\n          jobs.each do |job|\n            Gitlab::ApplicationContext.with_context(project: job.project) { yield(job) }\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4990"
  },
  {
    "raw_code": "def drop_build(type, build, reason)\n        log_dropping_message(type, build, reason)\n        Gitlab::OptimisticLocking.retry_lock(build, 3, name: 'stuck_ci_jobs_worker_drop_build') do |b|\n          b.drop!(reason)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4991"
  },
  {
    "raw_code": "def builds_for_shared_runner\n        shared_builds = builds_available_for_shared_runners\n\n        builds_ordered_for_shared_runners(shared_builds)\n      end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4992"
  },
  {
    "raw_code": "def builds_for_shared_runner\n        strategy.builds_for_shared_runner\n      end",
    "comment": " This is overridden in EE ",
    "label": "",
    "id": "4993"
  },
  {
    "raw_code": "def builds_for_group_runner\n        strategy.builds_for_group_runner\n      end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4994"
  },
  {
    "raw_code": "def create_included_ci_variables(workload)\n        @ci_variables_included.each do |var|\n          workload.variable_inclusions.create!(variable_name: var, project: workload.project)\n        end",
    "comment": "By default a Workload will not get any of the CI variables configured at the project/group/instance level. Setting ci_included_variables option ensures these named variables will later be made available from the CI variables configured at the project/group/instance level.",
    "label": "",
    "id": "4995"
  },
  {
    "raw_code": "def execute\n        scope = Ci::JobArtifact.select(:id).for_project(project_id).order(:id)\n        file_type_values = Ci::JobArtifact.erasable_file_types.map { |file_type| [Ci::JobArtifact.file_types[file_type]] }\n        from_sql = Arel::Nodes::Grouping.new(Arel::Nodes::ValuesList.new(file_type_values)).as('file_types (file_type)').to_sql\n        array_scope = Ci::JobArtifact.from(from_sql).select(:file_type)\n        array_mapping_scope = ->(file_type_expression) { Ci::JobArtifact.where(Ci::JobArtifact.arel_table[:file_type].eq(file_type_expression)) }\n\n        Gitlab::Pagination::Keyset::Iterator\n          .new(scope: scope, in_operator_optimization_options: { array_scope: array_scope, array_mapping_scope: array_mapping_scope })\n          .each_batch(of: BATCH_SIZE) do |batch|\n          ids = batch.to_a.map(&:id)\n          Ci::JobArtifact.unlocked.where(id: ids).update_all(locked: Ci::JobArtifact.lockeds[:unlocked], expire_at: expiry_time)\n        end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "4996"
  },
  {
    "raw_code": "def initialize(job_artifacts, pick_up_at: nil, skip_projects_on_refresh: false)\n        @job_artifacts = job_artifacts.with_destroy_preloads.to_a\n        @pick_up_at = pick_up_at\n        @skip_projects_on_refresh = skip_projects_on_refresh\n        @destroyed_ids = []\n      end",
    "comment": "Danger: Private - Should only be called in Ci Services that pass a batch of job artifacts Not for use outside of the Ci:: namespace Adds the passed batch of job artifacts to the `ci_deleted_objects` table for asyncronous destruction of the objects in Object Storage via the `Ci::DeleteObjectsService` and then deletes the batch of related `ci_job_artifacts` records. Params: +job_artifacts+:: A relation of job artifacts to destroy (fewer than MAX_JOB_ARTIFACT_BATCH_SIZE) +pick_up_at+:: When to pick up for deletion of files Returns: +Hash+:: A hash with status and destroyed_artifacts_count keys",
    "label": "",
    "id": "4997"
  },
  {
    "raw_code": "def destroy_around_hook(artifacts)\n        yield\n      end",
    "comment": "Overriden in EE :nocov:",
    "label": "",
    "id": "4998"
  },
  {
    "raw_code": "def destroy_related_records(artifacts); end\n\n      # Overriden in EE\n      def after_batch_destroy_hook(artifacts); end\n\n      # using ! here since this can't be called inside a transaction\n      def update_project_statistics!\n        statistics_updates_per_project.each do |project, increments|\n          ProjectStatistics.bulk_increment_statistic(project, Ci::JobArtifact.project_statistics_name, increments)\n        end\n      end\n\n      def statistics_updates_per_project\n        strong_memoize(:statistics_updates_per_project) do\n          result = Hash.new { |updates, project| updates[project] = [] }\n\n          @job_artifacts.each_with_object(result) do |job_artifact, result|\n            next unless job_artifact.project\n\n            increment = Gitlab::Counters::Increment.new(amount: -job_artifact.size.to_i, ref: job_artifact.id)\n            result[job_artifact.project] << increment\n          end\n        end",
    "comment": ":nocov: Overriden in EE",
    "label": "",
    "id": "4999"
  },
  {
    "raw_code": "def execute\n        in_lock(EXCLUSIVE_LOCK_KEY, ttl: LOCK_TIMEOUT, retries: 1) do\n          destroy_unlocked_job_artifacts\n        end",
    "comment": " Destroy expired job artifacts on GitLab instance  This destroy process cannot run for more than 6 minutes. This is for preventing multiple `ExpireBuildArtifactsWorker` CRON jobs run concurrently, which is scheduled every 7 minutes.",
    "label": "",
    "id": "5000"
  },
  {
    "raw_code": "def service_disabled?\n        Feature.enabled?(:disable_cancel_redundant_pipelines_service, project, type: :ops)\n      end",
    "comment": "Finding the pipelines to cancel is an expensive task that is not well covered by indexes for all project use-cases and sometimes it might harm other services. See https://gitlab.com/gitlab-com/gl-infra/production/-/issues/14758 This feature flag is in place to disable this feature for rogue projects. ",
    "label": "",
    "id": "5001"
  },
  {
    "raw_code": "def initialize(current_user:, project:, ref:, pipeline_source: :web)\n        @current_user = current_user\n        @project = project\n        @ref = ref\n        @pipeline_source = pipeline_source\n      end",
    "comment": "This service is used by the frontend to display inputs as an HTML form when creating a pipeline as a web request. For the reason we are defaulting `pipeline_source` to be `web`.",
    "label": "",
    "id": "5002"
  },
  {
    "raw_code": "def yaml_result_of_internal_include(content)\n        locations = content[:include]\n        return if locations.blank?\n\n        files = ::Gitlab::Ci::Config::External::Mapper::Matcher.new(context).process(locations)\n\n        ::Gitlab::Ci::Config::External::Mapper::Verifier.new(context).skip_load_content!.process(files)\n\n        files.first&.load_uninterpolated_yaml\n      end",
    "comment": "TODO: temporary technical debt until https://gitlab.com/gitlab-org/gitlab/-/issues/520828",
    "label": "",
    "id": "5003"
  },
  {
    "raw_code": "def enqueue_upcoming_processables(free_resources, resource_group)\n        resource_group.upcoming_processables.take(free_resources).each do |upcoming|\n          Gitlab::OptimisticLocking.retry_lock(upcoming, name: 'enqueue_waiting_for_resource') do |processable|\n            if processable.has_outdated_deployment?\n              processable.drop!(:failed_outdated_deployment_job)\n            else\n              processable.enqueue_waiting_for_resource\n\n              track_internal_event(\n                \"job_enqueued_by_resource_group\",\n                user: processable.user,\n                project: resource_group.project,\n                additional_properties: {\n                  label: resource_group.process_mode,\n                  property: processable.id.to_s,\n                  resource_group_id: resource_group.id\n                }\n              )\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5004"
  },
  {
    "raw_code": "def release_resource_from_stale_jobs(resource_group)\n        resource_group.resources.stale_processables.find_each do |processable|\n          resource_group.release_resource_from(processable)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5005"
  },
  {
    "raw_code": "def new_alive_jobs\n        initial_stopped_job_names = @collection.stopped_job_names\n\n        return [] if initial_stopped_job_names.empty?\n\n        new_collection = AtomicProcessingService::StatusCollection.new(pipeline)\n        new_alive_job_names = initial_stopped_job_names - new_collection.stopped_job_names\n\n        return [] if new_alive_job_names.empty?\n\n        pipeline\n          .current_jobs\n          .by_name(new_alive_job_names)\n          .preload(:user) # rubocop: disable CodeReuse/ActiveRecord\n          .to_a\n      end",
    "comment": "Gets the jobs that changed from stopped to alive status since the initial status collection was evaluated. We determine this by checking if their current status is no longer stopped.",
    "label": "",
    "id": "5006"
  },
  {
    "raw_code": "def set_job_status(id, status, lock_version)\n          job = all_jobs_by_id[id]\n          return unless job\n\n          job[:status] = status\n          job[:lock_version] = lock_version\n        end",
    "comment": "This method updates internal status for given ID",
    "label": "",
    "id": "5007"
  },
  {
    "raw_code": "def status_of_all\n          status_for_array(all_jobs)\n        end",
    "comment": "This methods gets composite status of all jobs",
    "label": "",
    "id": "5008"
  },
  {
    "raw_code": "def status_of_stage(stage_position)\n          strong_memoize(\"status_of_stage_#{stage_position}\") do\n            stage_jobs = all_jobs_grouped_by_stage_position[stage_position].to_a\n\n            status_for_array(stage_jobs.flatten)\n          end",
    "comment": "This methods gets composite status for jobs at a given stage",
    "label": "",
    "id": "5009"
  },
  {
    "raw_code": "def status_of_jobs(names)\n          jobs = all_jobs_by_name.slice(*names)\n\n          status_for_array(jobs.values, dag: true)\n        end",
    "comment": "This methods gets composite status for jobs with given names",
    "label": "",
    "id": "5010"
  },
  {
    "raw_code": "def status_of_jobs_prior_to_stage(stage_position)\n          strong_memoize(\"status_of_jobs_prior_to_stage_#{stage_position}\") do\n            stage_jobs = all_jobs_grouped_by_stage_position\n              .select { |position, _| position < stage_position }\n\n            status_for_array(stage_jobs.values.flatten)\n          end",
    "comment": "This methods gets composite status for jobs before given stage",
    "label": "",
    "id": "5011"
  },
  {
    "raw_code": "def created_job_ids_in_stage(stage_position)\n          all_jobs_grouped_by_stage_position[stage_position]\n            .to_a\n            .select { |job| job[:status] == 'created' }\n            .map { |job| job[:id] }\n        end",
    "comment": "This methods gets a list of jobs for a given stage",
    "label": "",
    "id": "5012"
  },
  {
    "raw_code": "def processing_jobs\n          all_jobs.lazy.reject { |job| job[:processed] }\n        end",
    "comment": "This method returns a list of all job, that are to be processed",
    "label": "",
    "id": "5013"
  },
  {
    "raw_code": "def stopped_job_names\n          all_jobs.select { |job| job[:status].in?(Ci::HasStatus::STOPPED_STATUSES) }.pluck(:name) # rubocop: disable CodeReuse/ActiveRecord\n        end",
    "comment": "This method returns the names of jobs that have a stopped status",
    "label": "",
    "id": "5014"
  },
  {
    "raw_code": "def all_jobs\n          # We fetch all relevant data in one go.\n          #\n          # This is more efficient than relying on PostgreSQL to calculate composite status for us\n          #\n          # Since we need to reprocess everything we can fetch all of them and do processing ourselves.\n          strong_memoize(:all_jobs) do\n            raw_jobs = pipeline\n              .current_jobs\n              .ordered_by_stage\n              .pluck(*JOB_ATTRS)\n\n            raw_jobs.map do |row|\n              JOB_ATTRS.zip(row).to_h\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5015"
  },
  {
    "raw_code": "def initialize(runners:, current_user:, paused:)\n        @runners = runners\n        @current_user = current_user\n        @paused = paused\n      end",
    "comment": "@param runners [Array<Ci::Runner>] the runners to pause or unpause @param current_user [User] the user performing the operation @param paused [Boolean] action of pausing or unpausing",
    "label": "",
    "id": "5016"
  },
  {
    "raw_code": "def initialize(runner, project, user, quiet: false)\n        @runner = runner\n        @project = project\n        @user = user\n        @quiet = quiet\n      end",
    "comment": "@param [Ci::Runner] runner: the runner to assign to a project @param [Project] project: the new project to assign the runner to @param [User] user: the user performing the operation @param [Boolean] quiet: true if service should avoid side effects, such as logging (e.g. when used by another service)",
    "label": "",
    "id": "5017"
  },
  {
    "raw_code": "def insert_runner_versions\n        versions_from_runners = Set[]\n        new_record_count = 0\n        Ci::RunnerManager.distinct_each_batch(column: :version, of: VERSION_BATCH_SIZE) do |version_batch|\n          batch_versions = version_batch.pluck(:version).to_set\n          versions_from_runners += batch_versions\n\n          # Avoid hitting primary DB\n          already_existing_versions = Ci::RunnerVersion.where(version: batch_versions).pluck(:version)\n          new_versions = batch_versions - already_existing_versions\n\n          if new_versions.any?\n            new_record_count += Ci::RunnerVersion.insert_all(\n              new_versions.map { |v| { version: v } },\n              returning: :version,\n              unique_by: :version).count\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5018"
  },
  {
    "raw_code": "def outdated_runner_versions\n        Ci::RunnerVersion.potentially_outdated\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5019"
  },
  {
    "raw_code": "def initialize(runner, author, system_id:)\n        @runner = runner\n        @author = author\n        @system_id = system_id\n      end",
    "comment": "@param [Ci::Runner] runner the runner to unregister/destroy @param [User, authentication token String] author the user or the authentication token authorizing the removal @param [String] system_id ID of the system being unregistered",
    "label": "",
    "id": "5020"
  },
  {
    "raw_code": "def initialize(runner, author)\n        @runner = runner\n        @author = author\n      end",
    "comment": "@param [Ci::Runner] runner the runner to unregister/destroy @param [User, authentication token String] author the user or the authentication token that authorizes the removal",
    "label": "",
    "id": "5021"
  },
  {
    "raw_code": "def initialize(runners:, current_user:)\n        @runners = runners\n        @current_user = current_user\n      end",
    "comment": "@param runners [Array<Ci::Runner>] the runners to unregister/destroy @param current_user [User] the user performing the operation",
    "label": "",
    "id": "5022"
  },
  {
    "raw_code": "def create_hosted_runner!(runner, should_mark_hosted); end\n\n      def track_runner_events(runner)\n        kwargs = { user: user }\n\n        case runner.runner_type\n        when 'group_type'\n          kwargs[:namespace] = @scope\n        when 'project_type'\n          kwargs[:project] = @scope\n        end\n\n        track_internal_event(\n          'create_ci_runner',\n          **kwargs,\n          additional_properties: {\n            label: runner.runner_type,\n            property: 'authenticated_user'\n          }\n        )\n\n        return if params[:maintenance_note].blank?\n\n        track_internal_event(\n          'set_runner_maintenance_note',\n          **kwargs,\n          additional_properties: {\n            label: runner.runner_type\n          }\n        )\n      end",
    "comment": "CE implementation - no-op",
    "label": "",
    "id": "5023"
  },
  {
    "raw_code": "def initialize(runner:, current_user:, project_ids:)\n        @runner = runner\n        @current_user = current_user\n        @project_ids = project_ids || []\n      end",
    "comment": "@param [Ci::Runner] runner: the project runner to assign/unassign projects from @param [User] current_user: the user performing the operation @param [Array<Integer>] project_ids: the IDs of the associated projects to assign the runner to",
    "label": "",
    "id": "5024"
  },
  {
    "raw_code": "def initialize(scope, user)\n        @scope = scope\n        @user = user\n      end",
    "comment": "@param [ApplicationSetting, Project, Group] scope: the scope of the reset operation @param [User] user: the user performing the operation",
    "label": "",
    "id": "5025"
  },
  {
    "raw_code": "def initialize(runner_project, user)\n        @runner_project = runner_project\n        @runner = runner_project.runner\n        @project = runner_project.project\n        @user = user\n      end",
    "comment": "@param [Ci::RunnerProject] runner_project the runner/project association to destroy @param [User] user the user performing the operation",
    "label": "",
    "id": "5026"
  },
  {
    "raw_code": "def initialize(project, options = {})\n        raise ArgumentError, 'project must be a valid Project instance' unless project.is_a?(Project)\n\n        @project = project\n        @select_fields = options[:select_fields] || []\n        @aggregations = options[:aggregations] || []\n        @sort = options[:sort]\n        @source = options[:source]\n        @ref = options[:ref]\n        @from_time = options[:from_time] || 1.week.ago.utc\n        @to_time = options[:to_time]\n        @name_search = options[:name_search]\n      end",
    "comment": "@param project [Project] The project to find jobs for @param options [Hash] Options for filtering and configuring the query builder @option options [Array] :select_fields Fields to select @option options [Array] :aggregations Aggregations to perform @option options [String] :sort Sort order (ex. rate_of_success_asc) @option options [String] :source Pipeline source @option options [String] :ref Git reference @option options [Time] :from_time Start time for filtering (defaults to 7 days ago) @option options [Time] :to_time End time for filtering @option options [String] :name_search Search by name of the pipeline jobs.",
    "label": "",
    "id": "5027"
  },
  {
    "raw_code": "def execute(source)\n      badge = Badges::BuildService.new(params).execute(source)\n\n      badge.tap { |b| b.save }\n    end",
    "comment": "returns the created badge",
    "label": "",
    "id": "5028"
  },
  {
    "raw_code": "def execute(source)\n      if source.is_a?(Group)\n        GroupBadge.new(params.merge(group: source))\n      else\n        ProjectBadge.new(params.merge(project: source))\n      end",
    "comment": "returns the created badge",
    "label": "",
    "id": "5029"
  },
  {
    "raw_code": "def execute(badge)\n      if params.present?\n        badge.update(params)\n      end",
    "comment": "returns the updated badge",
    "label": "",
    "id": "5030"
  },
  {
    "raw_code": "def to_resource(job)\n      environment = job.project.environments\n                       .safe_find_or_create_by(name: job.expanded_environment_name) do |environment|\n        # Initialize the attributes at creation\n        environment.auto_stop_in = expanded_auto_stop_in(job)\n        environment.tier = job.expanded_deployment_tier\n        environment.merge_request = job.pipeline.merge_request\n      end",
    "comment": "rubocop: disable Performance/ActiveRecordSubtransactionMethods",
    "label": "",
    "id": "5031"
  },
  {
    "raw_code": "def expanded_auto_stop_in(job)\n      return unless job.environment_auto_stop_in\n\n      ExpandVariables.expand(job.environment_auto_stop_in, -> { job.simple_variables.sort_and_expand_all })\n    end",
    "comment": "rubocop: enable Performance/ActiveRecordSubtransactionMethods",
    "label": "",
    "id": "5032"
  },
  {
    "raw_code": "def execute\n      in_lock(EXCLUSIVE_LOCK_KEY, ttl: LOCK_TIMEOUT, retries: 1) do\n        loop_until(timeout: LOOP_TIMEOUT, limit: LOOP_LIMIT) do\n          stop_in_batch\n        end",
    "comment": " Stop expired environments on GitLab instance  This auto stop process cannot run for more than 45 minutes. This is for preventing multiple `AutoStopCronWorker` CRON jobs run concurrently, which is scheduled at every hour.",
    "label": "",
    "id": "5033"
  },
  {
    "raw_code": "def can_set_auto_stop?\n      deployable.verifies_environment? || can_reset_timer?\n    end",
    "comment": "Jobs that start an environment (using `action: start`) can also specify a stop time, however this is handled by the deployment process. Actions other than `start` do not create deployments, so these must be processed separately.",
    "label": "",
    "id": "5034"
  },
  {
    "raw_code": "def execute\n      in_lock(EXCLUSIVE_LOCK_KEY, ttl: LOCK_TIMEOUT, retries: 1) do\n        loop_until(timeout: LOOP_TIMEOUT, limit: LOOP_LIMIT) do\n          recover_in_batch\n        end",
    "comment": " Recover environments that are stuck stopping on a GitLab instance  This auto stop process cannot run for more than 45 minutes. This is for preventing multiple `AutoStopCronWorker` CRON jobs run concurrently, which is scheduled at every hour.",
    "label": "",
    "id": "5035"
  },
  {
    "raw_code": "def unsafe_execute!(environment)\n      if params[:force]\n        actions = []\n\n        environment.stop_complete!\n      else\n        actions = environment.stop_with_actions!\n      end",
    "comment": " Stops the environment without checking user permissions. This should only be used if initiated by a system action and a user cannot be specified.",
    "label": "",
    "id": "5036"
  },
  {
    "raw_code": "def execute(environment)\n        canary_ingress = environment.ingresses&.find(&:canary?)\n\n        unless canary_ingress.present?\n          return error(_('Canary Ingress does not exist in the environment.'))\n        end",
    "comment": "This method actually executes the PATCH request to Kubernetes, that is used by internal processes i.e. sidekiq worker. You should always use `execute_async` to properly validate user's requests.",
    "label": "",
    "id": "5037"
  },
  {
    "raw_code": "def threshold_commits\n      strong_memoize(:threshold_commits) do\n        if creating_default_branch?\n          # The most recent PROCESS_COMMIT_LIMIT commits in the default branch.\n          # They are returned newest-to-oldest, but we need to present them oldest-to-newest\n          project.repository.commits(newrev, limit: PROCESS_COMMIT_LIMIT + 1).reverse!\n        elsif creating_branch?\n          # Use the pushed commits that aren't reachable by the default branch\n          # as a heuristic. This may include more commits than are actually\n          # pushed, but that shouldn't matter because we check for existing\n          # cross-references later.\n          project.repository.commits_between(project.default_branch, newrev, limit: PROCESS_COMMIT_LIMIT + 1)\n        elsif updating_branch?\n          project.repository.commits_between(oldrev, newrev, limit: PROCESS_COMMIT_LIMIT + 1)\n        else # removing branch\n          []\n        end",
    "comment": "Taking limit+1 commits allows us to detect when the limit is in effect",
    "label": "",
    "id": "5038"
  },
  {
    "raw_code": "def enqueue_process_commit_messages\n      referencing_commits = limited_commits.select(&:matches_cross_reference_regex?)\n\n      upstream_commit_ids = upstream_commit_ids(referencing_commits)\n\n      referencing_commits.each do |commit|\n        # Avoid reprocessing commits that already exist upstream if the project\n        # is a fork. This will prevent duplicated/superfluous system notes on\n        # mentionables referenced by a commit that is pushed to the upstream,\n        # that is then also pushed to forks when these get synced by users.\n        next if upstream_commit_ids.include?(commit.id)\n\n        ProcessCommitWorker.perform_in(\n          process_commit_worker_delay,\n          project.id,\n          current_user.id,\n          commit.to_hash,\n          default_branch?\n        )\n      end",
    "comment": "Schedules processing of commit messages",
    "label": "",
    "id": "5039"
  },
  {
    "raw_code": "def creating_branch?\n      strong_memoize(:creating_branch) do\n        Gitlab::Git.blank_ref?(oldrev) ||\n          !project.repository.branch_exists?(branch_name)\n      end",
    "comment": "It's not sufficient to just check for a blank SHA as it's possible for the branch to be pushed, but for the `post-receive` hook to never run: https://gitlab.com/gitlab-org/gitlab-foss/issues/59257",
    "label": "",
    "id": "5040"
  },
  {
    "raw_code": "def changes\n      params[:changes] || []\n    end",
    "comment": "See: [Gitlab::GitPostReceive#changes]",
    "label": "",
    "id": "5041"
  },
  {
    "raw_code": "def execute\n      return unless Gitlab::Git.branch_ref?(ref)\n\n      enqueue_update_mrs\n      enqueue_detect_repository_languages\n      enqueue_record_project_target_platforms\n\n      execute_related_hooks\n\n      stop_environments\n      unlock_artifacts\n\n      true\n    end",
    "comment": "This method will be called after each git update and only if the provided user and project are present in GitLab.  All callbacks for post receive action should be placed here.  Next, this method: 1. Creates the push event 2. Updates merge requests 3. Recognizes cross-references from commit messages 4. Executes the project's webhooks 5. Executes the project's services 6. Checks if the project's main language has changed ",
    "label": "",
    "id": "5042"
  },
  {
    "raw_code": "def enqueue_update_mrs\n      return if params[:merge_request_branches]&.exclude?(branch_name)\n\n      UpdateMergeRequestsWorker.perform_async(\n        project.id,\n        current_user.id,\n        oldrev,\n        newrev,\n        ref,\n        params.slice(:push_options).deep_stringify_keys\n      )\n    end",
    "comment": "Update merge requests that may be affected by this push. A new branch could cause the last commit of a merge request to change.",
    "label": "",
    "id": "5043"
  },
  {
    "raw_code": "def stop_environments\n      return unless removing_branch?\n\n      Environments::StopService.new(project, current_user).execute_for_branch(branch_name)\n    end",
    "comment": "Only stop environments if the ref is a branch that is being deleted",
    "label": "",
    "id": "5044"
  },
  {
    "raw_code": "def limited_commits\n      raise NotImplementedError, \"Please implement #{self.class}##{__method__}\"\n    end",
    "comment": "This should return PROCESS_COMMIT_LIMIT commits, ordered with newest last",
    "label": "",
    "id": "5045"
  },
  {
    "raw_code": "def create_events\n      return unless params.fetch(:create_push_event, true)\n\n      EventCreateService.new.push(project, current_user, event_push_data)\n    end",
    "comment": "Push events in the activity feed only show information for the last commit.",
    "label": "",
    "id": "5046"
  },
  {
    "raw_code": "def pipeline_options\n      {\n        inputs: ci_push_options.inputs\n      }\n    end",
    "comment": "merges with EE override",
    "label": "",
    "id": "5047"
  },
  {
    "raw_code": "def initialize(wiki, change, raw_change)\n        @wiki = wiki\n        @raw_change = raw_change\n        @change = change\n      end",
    "comment": "@param [Wiki] wiki @param [Hash] change - must have keys `:oldrev` and `:newrev` @param [Gitlab::Git::RawDiffChange] raw_change",
    "label": "",
    "id": "5048"
  },
  {
    "raw_code": "def event_action\n        case raw_change.operation\n        when :added\n          :created\n        when :deleted\n          :destroyed\n        else\n          :updated\n        end",
    "comment": "See [Gitlab::Git::RawDiffChange#extract_operation] for the definition of the full range of operation values.",
    "label": "",
    "id": "5049"
  },
  {
    "raw_code": "def ensure_export_file_exists!\n      FileUtils.touch(exported_filepath)\n    end",
    "comment": "Create empty file on disk if relation is empty and nothing was exported",
    "label": "",
    "id": "5050"
  },
  {
    "raw_code": "def batch_size\n      key = self.class.batch_size_cache_key(export.id)\n\n      Gitlab::Cache::Import::Caching.read_integer(key) ||\n        Gitlab::Cache::Import::Caching.write(\n          key,\n          Gitlab::CurrentSettings.relation_export_batch_size,\n          timeout: CACHE_DURATION\n        )\n    end",
    "comment": "Returns the batch size for processing relation exports.  The batch size determines how many records are processed together in each batch during the export operation. We cache the batch size so that any retried workers for the same relation export use the same batch size.  @return [Integer] The number of records to process per batch",
    "label": "",
    "id": "5051"
  },
  {
    "raw_code": "def enqueue_batch_exports\n      batch_number = 0\n\n      resolved_relation.in_batches(of: batch_size) do |batch|\n        batch_number += 1\n\n        batch_id = find_or_create_batch(batch_number).id\n        ids = batch.pluck(batch.model.primary_key)\n\n        Gitlab::Cache::Import::Caching.set_add(self.class.cache_key(export.id, batch_id), ids, timeout: CACHE_DURATION)\n\n        RelationBatchExportWorker.perform_async(user.id, batch_id)\n      end",
    "comment": "rubocop:disable Cop/InBatches rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5052"
  },
  {
    "raw_code": "def find_or_create_batch(batch_number)\n      export.batches.find_or_create_by!(batch_number: batch_number)\n    end",
    "comment": "rubocop:enable Cop/InBatches",
    "label": "",
    "id": "5053"
  },
  {
    "raw_code": "def initialize(portable:, user:, batched: false)\n      @portable = portable\n      @current_user = user\n      @batched = batched\n    end",
    "comment": "@param portable [Project|Group] A project or a group to export. @param user [User] A user performing the export. @param batched [Boolean] Whether to export the data in batches.",
    "label": "",
    "id": "5054"
  },
  {
    "raw_code": "def last_chunk_context\n      @last_chunk_context.to_s.truncate(LAST_CHUNK_CONTEXT_CHAR_LIMIT).force_encoding('utf-8').scrub\n    end",
    "comment": "Before logging, we truncate the context to a reasonable length and scrub any non-printable characters.",
    "label": "",
    "id": "5055"
  },
  {
    "raw_code": "def serializer\n      @serializer ||= ::Gitlab::ImportExport::Json::StreamingSerializer.new(\n        portable,\n        config.portable_tree,\n        ::Gitlab::ImportExport::Json::NdjsonWriter.new(export_path),\n        exportable_path: '',\n        current_user: user\n      )\n    end",
    "comment": "rubocop: disable CodeReuse/Serializer",
    "label": "",
    "id": "5056"
  },
  {
    "raw_code": "def extension\n      return 'json' if self_relation?(relation)\n\n      'ndjson'\n    end",
    "comment": "rubocop: enable CodeReuse/Serializer",
    "label": "",
    "id": "5057"
  },
  {
    "raw_code": "def re_enqueue\n      # Touch entities with created status so they are not marked as stale by\n      # BulkImports::StaleImportWorker if it takes more 24 hours to start them.\n      touch_created_entities\n\n      BulkImportWorker.perform_in(PERFORM_DELAY, bulk_import.id)\n    end",
    "comment": "A new BulkImportWorker job is enqueued to either - Process the new BulkImports::Entity created during import (e.g. for the subgroups) - Or to mark the `bulk_import` as finished",
    "label": "",
    "id": "5058"
  },
  {
    "raw_code": "def append_lfs_json_for_batch(lfs_objects_batch)\n      lfs_objects_projects = LfsObjectsProject\n        .select('lfs_objects.oid, array_agg(distinct lfs_objects_projects.repository_type) as repository_types')\n        .joins(:lfs_object)\n        .where(project: portable, lfs_object: lfs_objects_batch)\n        .group('lfs_objects.oid')\n\n      lfs_objects_projects.each do |group|\n        oid = group.oid\n\n        lfs_json[oid] ||= []\n        lfs_json[oid] += group.repository_types\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5059"
  },
  {
    "raw_code": "def write_lfs_json\n      filepath = File.join(export_path, \"#{BulkImports::FileTransfer::ProjectConfig::LFS_OBJECTS_RELATION}.json\")\n\n      File.write(filepath, Gitlab::Json.dump(lfs_json))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5060"
  },
  {
    "raw_code": "def base_url\n      Rails.env.production? ? PRODUCTION_BASE_URL : STAGING_BASE_URL\n    end",
    "comment": "See https://gitlab.com/gitlab-org/gitlab/-/issues/233615 for details",
    "label": "",
    "id": "5061"
  },
  {
    "raw_code": "def find_chat_name\n      ChatName.find_by(\n        team_id: team_id,\n        chat_id: user_id\n      )\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5062"
  },
  {
    "raw_code": "def record_chat_activity(chat_name)\n      chat_name.update_last_used_at\n      Users::ActivityService.new(author: chat_name.user).execute\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5063"
  },
  {
    "raw_code": "def initialize(current_user = nil)\n      @current_user = current_user\n    end",
    "comment": "current_user - The User to generate the data for.",
    "label": "",
    "id": "5064"
  },
  {
    "raw_code": "def error(errors, award: nil, status: nil)\n      errors = Array.wrap(errors)\n\n      super(errors.to_sentence.presence, status).merge({\n        award: award,\n        errors: errors\n      })\n    end",
    "comment": "Provide more error state data than what BaseService allows. - An array of errors - The `AwardEmoji` if present",
    "label": "",
    "id": "5065"
  },
  {
    "raw_code": "def organization_user_details_for_participants(participants)\n      return participants unless Feature.enabled?(:organization_users_internal, organization)\n\n      participants.map do |participant|\n        next participant unless participant.is_a?(User)\n\n        detail = participant.organization_user_details.to_a.find do |det|\n          det.organization == organization\n        end",
    "comment": "for users that have an OrganizationUserDetail for the current organization, use this instead of the User model for rendering username, display_name, and other details details should be pre-loaded to avoid N+1 queries",
    "label": "",
    "id": "5066"
  },
  {
    "raw_code": "def move_fork_network_members\n      ForkNetworkMember.where(project: source_project).update_all(project_id: @project.id)\n      ForkNetworkMember.where(forked_from_project: source_project).update_all(forked_from_project_id: @project.id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5067"
  },
  {
    "raw_code": "def update_root_project\n      # Update root network project\n      ForkNetwork.where(root_project: source_project).update_all(root_project_id: @project.id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5068"
  },
  {
    "raw_code": "def refresh_forks_count\n      Projects::ForksCountService.new(@project).refresh_cache\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5069"
  },
  {
    "raw_code": "def async?\n      has_importer? && !!importer_class.try(:async?)\n    end",
    "comment": "Returns true if this importer is supposed to perform its work in the background.  This method will only return `true` if async importing is explicitly supported by an importer class (`Gitlab::GithubImport::ParallelImporter` for example).",
    "label": "",
    "id": "5070"
  },
  {
    "raw_code": "def global_count\n      @global_count ||= count_service.query(project_ids)\n                     .group(:forked_from_project_id)\n                     .count\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5071"
  },
  {
    "raw_code": "def count_service\n      ::Projects::ForksCountService\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5072"
  },
  {
    "raw_code": "def global_count\n      @global_count ||= count_service.query(project_ids).group(:project_id).count\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5073"
  },
  {
    "raw_code": "def count_service\n      ::Projects::OpenIssuesCountService\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5074"
  },
  {
    "raw_code": "def execute\n      repository_languages = project.repository_languages\n      detection = Gitlab::LanguageDetection.new(repository, repository_languages)\n\n      matching_programming_languages = ensure_programming_languages(detection)\n\n      RepositoryLanguage.transaction do\n        RepositoryLanguage.where(project_id: project.id, programming_language_id: detection.deletions).delete_all\n\n        detection.updates.each do |update|\n          RepositoryLanguage\n            .where(project_id: project.id)\n            .where(programming_language_id: update[:programming_language_id])\n            .update_all(share: update[:share])\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5075"
  },
  {
    "raw_code": "def ensure_programming_languages(detection)\n      existing_languages = ProgrammingLanguage.where(name: detection.languages)\n      return existing_languages if detection.languages.size == existing_languages.size\n\n      missing_languages = detection.languages - existing_languages.map(&:name)\n      created_languages = missing_languages.map do |name|\n        create_language(name, detection.language_color(name))\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5076"
  },
  {
    "raw_code": "def create_language(name, color)\n      ProgrammingLanguage.transaction do\n        ProgrammingLanguage.where(name: name).first_or_create(color: color)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5077"
  },
  {
    "raw_code": "def set_detected_repository_languages\n      return if project.detected_repository_languages?\n\n      project.update_column(:detected_repository_languages, true)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5078"
  },
  {
    "raw_code": "def transfer(project)\n      @old_path = project.full_path\n      @old_group = project.group\n      @new_path = File.join(@new_namespace.try(:full_path) || '', project.path)\n      @old_namespace = project.namespace\n\n      if Project.where(namespace_id: @new_namespace.try(:id)).where('path = ? or name = ?', project.path, project.name).exists?\n        raise TransferError, s_(\"TransferProject|Project with same name or path in target namespace already exists\")\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5079"
  },
  {
    "raw_code": "def verify_if_container_registry_tags_can_be_handled(project)\n      return unless project.has_container_registry_tags?\n\n      raise_error_due_to_tags_if_transfer_is_not_allowed\n      raise_error_due_to_tags_if_not_in_same_root(project)\n      raise_error_due_to_tags_if_transfer_dry_run_fails(project)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5080"
  },
  {
    "raw_code": "def post_update_hooks(project, _old_group)\n      ensure_personal_project_owner_membership(project)\n      invalidate_personal_projects_counts\n\n      publish_event\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5081"
  },
  {
    "raw_code": "def remove_paid_features; end\n\n    def invalidate_personal_projects_counts\n      # If the project was moved out of a personal namespace,\n      # the cache of the namespace owner, before the transfer, should be cleared.\n      if @owner_of_personal_project_before_transfer.present?\n        @owner_of_personal_project_before_transfer.invalidate_personal_projects_count\n      end\n\n      # If the project has now moved into a personal namespace,\n      # the cache of the target namespace owner should be cleared.\n      project.invalidate_personal_projects_count_of_owner\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5082"
  },
  {
    "raw_code": "def initialize(project)\n      @project = project\n\n      @default_branch_protection = Gitlab::Access::DefaultBranchProtection.new(\n        project.namespace.default_branch_protection_settings\n      )\n    end",
    "comment": "@param [Project] project",
    "label": "",
    "id": "5083"
  },
  {
    "raw_code": "def code_owner_approval_required?\n      false\n    end",
    "comment": "overriden in EE",
    "label": "",
    "id": "5084"
  },
  {
    "raw_code": "def current_namespace\n      strong_memoize(:current_namespace) do\n        Namespace.find_by(id: params[:namespace_id]) || current_user.namespace\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5085"
  },
  {
    "raw_code": "def project_path\n      \"#{current_namespace.full_path}/#{params[:path]}\"\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5086"
  },
  {
    "raw_code": "def remove_remaining_project_group_links\n      source_project.reset.project_group_links.destroy_all # rubocop: disable Cop/DestroyAll\n    end",
    "comment": "Remove remaining project group links from source_project",
    "label": "",
    "id": "5087"
  },
  {
    "raw_code": "def non_existent_group_links\n      source_project.project_group_links\n                    .where.not(group_id: group_links_in_target_project)\n    end",
    "comment": "Look for groups in source_project that are not in the target project rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5088"
  },
  {
    "raw_code": "def self.query(project_ids)\n      ForkNetworkMember.where(forked_from_project: project_ids)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5089"
  },
  {
    "raw_code": "def setup_authorizations\n      if @project.group\n        group_access_level = @project.group.max_member_access_for_user(\n          current_user,\n          only_concrete_membership: true\n        )\n\n        if group_access_level > GroupMember::NO_ACCESS\n          ProjectAuthorization.find_or_create_authorization_for(current_user.id, @project.id, group_access_level)\n        end",
    "comment": "Add an authorization for the current user authorizations inline (so they can access the project immediately after this request completes), and any other affected users in the background",
    "label": "",
    "id": "5090"
  },
  {
    "raw_code": "def initialize(project, path_before:, full_path_before:)\n      @project = project\n      @path_before = path_before\n      @full_path_before = full_path_before\n      @full_path_after = project.full_path\n    end",
    "comment": "@param [Project] project The Project being renamed. @param [String] path_before The path slug the project was using, before the rename took place.",
    "label": "",
    "id": "5091"
  },
  {
    "raw_code": "def global_count\n      @global_count ||= count_service.query(project_ids).group(:project_id).count\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5092"
  },
  {
    "raw_code": "def count_service\n      ::Projects::OpenMergeRequestsCountService\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5093"
  },
  {
    "raw_code": "def config_files_containing_sdk_setting(sdk)\n      BUILD_CONFIG_FILENAMES.flat_map do |filename|\n        file_finder.find(\"SDKROOT = #{sdk} filename:#{filename}\")\n      end",
    "comment": "Return array of project.pbxproj and/or *.xcconfig files (Gitlab::Search::FoundBlob) that contain the setting definition string \"SDKROOT = <sdk_name>\"",
    "label": "",
    "id": "5094"
  },
  {
    "raw_code": "def remove_remaining_notification_settings\n      source_project.notification_settings.destroy_all # rubocop: disable Cop/DestroyAll\n    end",
    "comment": "Remove remaining notification settings from source_project",
    "label": "",
    "id": "5095"
  },
  {
    "raw_code": "def users_in_target_project\n      @project.notification_settings.select(:user_id)\n    end",
    "comment": "Get users of current notification_settings",
    "label": "",
    "id": "5096"
  },
  {
    "raw_code": "def non_existent_notifications\n      source_project.notification_settings\n        .select(:id)\n        .where.not(user_id: users_in_target_project)\n    end",
    "comment": "Look for notification_settings in source_project that are not in the target project rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5097"
  },
  {
    "raw_code": "def destroy_mr_diff_relations!\n      delete_batch_size = 1000\n\n      project.merge_requests.each_batch(column: :iid, of: BATCH_SIZE) do |relation_ids|\n        [MergeRequestDiffCommit, MergeRequestDiffFile].each do |model|\n          loop do\n            inner_query = model\n              .select(:merge_request_diff_id, :relative_order)\n              .where(merge_request_diff_id: MergeRequestDiff.where(merge_request_id: relation_ids).select(:id))\n              .limit(delete_batch_size)\n\n            deleted_rows = model\n              .where(\"(#{model.table_name}.merge_request_diff_id, #{model.table_name}.relative_order) IN (?)\", inner_query) # rubocop:disable GitlabSecurity/SqlInjection\n              .delete_all\n\n            break if deleted_rows == 0\n          end",
    "comment": "Projects will have at least one merge_request_diff_commit for every commit contained in every MR, which deleting via `project.destroy!` and cascading deletes may exceed statement timeouts, causing failures. (see https://gitlab.com/gitlab-org/gitlab/-/issues/346166)  Removing merge_request_diff_files records may also cause timeouts, so they can be deleted in batches as well.  rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5098"
  },
  {
    "raw_code": "def destroy_merge_request_diffs!\n      delete_batch_size = 1000\n\n      project.merge_requests.each_batch(column: :iid, of: BATCH_SIZE) do |relation|\n        loop do\n          deleted_rows = MergeRequestDiff\n            .where(merge_request: relation)\n            .limit(delete_batch_size)\n            .delete_all\n\n          break if deleted_rows == 0\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5099"
  },
  {
    "raw_code": "def destroy_ci_records!\n      # Make sure to destroy this first just in case the project is undergoing stats refresh.\n      # This is to avoid logging the artifact deletion in Ci::JobArtifacts::DestroyBatchService.\n      project.build_artifacts_size_refresh&.destroy\n\n      all_pipelines.find_each(batch_size: BATCH_SIZE) do |pipeline| # rubocop: disable CodeReuse/ActiveRecord\n        # Destroy artifacts, then builds, then pipelines\n        # All builds have already been dropped by Ci::AbortPipelinesService,\n        # so no Ci::Build-instantiating cancellations happen here.\n        # https://gitlab.com/gitlab-org/gitlab/-/merge_requests/71342#note_691523196\n\n        # We already have done the permission check for `remove_project` so we can use `unsafe_execute` here.\n        # Using `#execute` causes issues with removing pipeline when `ProjectPolicy#repository_disabled` is true.\n        ::Ci::DestroyPipelineService.new(project, current_user).unsafe_execute(pipeline)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5100"
  },
  {
    "raw_code": "def destroy_orphaned_ci_job_artifacts!\n      orphaned_job_artifacts = ::Ci::JobArtifact.for_project(project)\n      return if orphaned_job_artifacts.none?\n\n      service = orphaned_job_artifacts.begin_fast_destroy\n      orphaned_job_artifacts.finalize_fast_destroy(service)\n\n      Gitlab::AppLogger.info(\n        class: self.class.name,\n        project_id: project.id,\n        message: 'Orphaned CI job artifacts deleted'\n      )\n    end",
    "comment": "This method will delete all orphaned CI Job artifacts for the project, which are job artifacts whose jobs do not exist anymore. The reason these artifacts might still exist is because of https://gitlab.com/gitlab-org/gitlab/-/issues/508672. TODO: remove this method after we have a working & valid FK.",
    "label": "",
    "id": "5101"
  },
  {
    "raw_code": "def destroy_project_bots!\n      members = project.members\n        .allow_cross_joins_across_databases(url: 'https://gitlab.com/gitlab-org/gitlab/-/issues/422405')\n        .includes(:user).references(:user).merge(User.project_bot)\n\n      members.each do |member|\n        Users::DestroyService.new(current_user).execute(member.user, skip_authorization: true)\n      end",
    "comment": "The project can have multiple project bots with personal access tokens generated. We need to remove them when a project is deleted rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5102"
  },
  {
    "raw_code": "def remove_registry_tags\n      return true unless Gitlab.config.registry.enabled\n      return false if protected_for_delete?(project:, current_user:)\n      return false unless remove_legacy_registry_tags\n\n      results = []\n      project.container_repositories.find_each do |container_repository|\n        results << destroy_repository(project, container_repository)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5103"
  },
  {
    "raw_code": "def remove_legacy_registry_tags\n      return true unless Gitlab.config.registry.enabled\n\n      root_repository = ::ContainerRepository.build_root_repository(project)\n      root_repository.has_tags? ? destroy_repository(project, root_repository) : true\n    end",
    "comment": " This method makes sure that we correctly remove registry tags for legacy image repository (when repository path equals project path). ",
    "label": "",
    "id": "5104"
  },
  {
    "raw_code": "def execute(refresh_statistics: true)\n      fork_network = @project.fork_network\n      forked_from = @project.forked_from_project\n\n      return unless fork_network\n\n      log_info(message: \"UnlinkForkService: Unlinking fork network\", fork_network_id: fork_network.id)\n\n      merge_requests = fork_network\n                         .merge_requests\n                         .opened\n                         .from_and_to_forks(@project)\n\n      merge_requests.find_each do |mr|\n        ::MergeRequests::CloseService.new(\n          project: @project,\n          current_user: @current_user,\n          params: { skip_authorization: true }\n        ).execute(mr)\n        log_info(message: \"UnlinkForkService: Closed merge request\", merge_request_id: mr.id)\n      end",
    "comment": "Close existing MRs coming from the project and remove it from the fork network",
    "label": "",
    "id": "5105"
  },
  {
    "raw_code": "def execute\n      apply_bfg_object_map!\n\n      # Remove older objects that are no longer referenced\n      Projects::GitGarbageCollectWorker.new.perform(project.id, :prune, \"project_cleanup:gc:#{project.id}\")\n\n      # The cache may now be inaccurate, and holding onto it could prevent\n      # bugs assuming the presence of some object from manifesting for some\n      # time. Better to feel the pain immediately.\n      project.repository.expire_all_method_caches\n\n      self.class.cleanup_after(project)\n    end",
    "comment": "Attempt to clean up the project following the push. Warning: this is destructive!  path is the path of an upload of a BFG object map file. It contains a line per rewritten object, with the old and new SHAs space-separated. It can be used to update or remove content that references the objects that BFG has altered",
    "label": "",
    "id": "5106"
  },
  {
    "raw_code": "def commit_status\n      GenericCommitStatus.new(\n        user: build.user,\n        ci_stage: stage,\n        name: 'pages:deploy',\n        stage_idx: stage.position\n      )\n    end",
    "comment": "Create status notifying the deployment of pages",
    "label": "",
    "id": "5107"
  },
  {
    "raw_code": "def stage\n      build.pipeline.stages.safe_find_or_create_by(name: 'deploy', pipeline_id: build.pipeline.id) do |stage|\n        stage.position = GenericCommitStatus::EXTERNAL_STAGE_IDX\n        stage.project = build.project\n      end",
    "comment": "rubocop: disable Performance/ActiveRecordSubtransactionMethods",
    "label": "",
    "id": "5108"
  },
  {
    "raw_code": "def create_pages_deployment(file, build)\n      attributes = pages_deployment_attributes(file, build)\n      deployment = project.pages_deployments.build(**attributes)\n\n      return if deployment.file.size != file.size\n\n      deployment.tap(&:save!)\n    end",
    "comment": "rubocop: enable Performance/ActiveRecordSubtransactionMethods",
    "label": "",
    "id": "5109"
  },
  {
    "raw_code": "def pages_deployment_attributes(file, build)\n      {\n        file: file,\n        file_count: deployment_validations.entries_count,\n        file_sha256: build.job_artifacts_archive.file_sha256,\n        ci_build_id: build.id,\n        root_directory: build.pages[:publish]\n      }\n    end",
    "comment": "overridden on EE",
    "label": "",
    "id": "5110"
  },
  {
    "raw_code": "def non_existent_members\n      source_project.members\n                    .select(:id)\n                    .where.not(user_id: @project.project_members.select(:user_id))\n    end",
    "comment": "Look for members in source_project that are not in the target project rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5111"
  },
  {
    "raw_code": "def refresh_cache(&block)\n      count_grouped_by_confidential = self.class.query(@project, public_only: false).group(:confidential).count\n      public_count = count_grouped_by_confidential[false] || 0\n      total_count = public_count + (count_grouped_by_confidential[true] || 0)\n\n      update_cache_for_key(public_count_cache_key) do\n        public_count\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5112"
  },
  {
    "raw_code": "def self.query(projects, public_only: true)\n      open_issues = Issue.opened.without_hidden\n\n      if public_only\n        open_issues.public_only.where(project: projects)\n      else\n        open_issues.where(project: projects)\n      end",
    "comment": "We only show issues count including confidential for planners, who are allowed to view confidential issues. This will still show a discrepancy on issues number but should be less than before. Check https://gitlab.com/gitlab-org/gitlab-foss/issues/38418 description.",
    "label": "",
    "id": "5113"
  },
  {
    "raw_code": "def non_existent_lfs_objects_projects\n      source_project.lfs_objects_projects.where.not(lfs_object: @project.lfs_objects)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5114"
  },
  {
    "raw_code": "def non_existent_deploy_keys_projects\n      source_project.deploy_keys_projects\n                    .joins(:deploy_key)\n                    .where.not(keys: { fingerprint_sha256: @project.deploy_keys.select(:fingerprint_sha256) })\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5115"
  },
  {
    "raw_code": "def remove_remaining_deploy_keys_projects\n      source_project.deploy_keys_projects.destroy_all # rubocop: disable Cop/DestroyAll\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5116"
  },
  {
    "raw_code": "def non_existent_authorization\n      source_project.project_authorizations\n                    .select(:user_id)\n                    .where.not(user: @project.authorized_users)\n    end",
    "comment": "Look for authorizations in source_project that are not in the target project rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5117"
  },
  {
    "raw_code": "def has_custom_head_branch?\n      project.repository.branch_names.any? { |name| name.casecmp('head') == 0 }\n    end",
    "comment": "See issue: https://gitlab.com/gitlab-org/gitlab/-/issues/381731",
    "label": "",
    "id": "5118"
  },
  {
    "raw_code": "def audit_topic_change(from:); end\n\n    # overridden by EE module\n    def remove_unallowed_params\n      params.delete(:emails_enabled) unless can?(current_user, :set_emails_disabled, project)\n\n      params.delete(:runner_registration_enabled) if Gitlab::CurrentSettings.valid_runner_registrars.exclude?('project')\n\n      params.delete(:max_artifacts_size) unless can?(current_user, :update_max_artifacts_size, project)\n    end\n\n    def after_update\n      track_job_token_scope_setting_changes(project.ci_cd_settings, current_user)\n\n      todos_features_changes = %w[\n        issues_access_level\n        merge_requests_access_level\n        repository_access_level\n      ]\n      project_changed_feature_keys = project.project_feature.previous_changes.keys\n\n      if project.visibility_level_previous_changes && project.private?\n        # don't enqueue immediately to prevent todos removal in case of a mistake\n        TodosDestroyer::ConfidentialIssueWorker.perform_in(Todo::WAIT_FOR_DELETE, nil, project.id)\n        TodosDestroyer::ProjectPrivateWorker.perform_in(Todo::WAIT_FOR_DELETE, project.id)\n      elsif (project_changed_feature_keys & todos_features_changes).present?\n        TodosDestroyer::PrivateFeaturesWorker.perform_in(Todo::WAIT_FOR_DELETE, project.id)\n      end\n\n      if project.previous_changes.include?('path')\n        after_rename_service(project).execute\n      else\n        system_hook_service.execute_hooks_for(project, :update)\n      end",
    "comment": "overridden by EE module",
    "label": "",
    "id": "5119"
  },
  {
    "raw_code": "def target_path_discardable?(new_path)\n        return false unless File.directory?(new_path)\n\n        found = Dir.glob(File.join(new_path, '**', '**'))\n\n        (found - discardable_paths(new_path)).empty?\n      end",
    "comment": "Check if target path has discardable content  @param [String] new_path @return [Boolean] whether we can discard the target path or not",
    "label": "",
    "id": "5120"
  },
  {
    "raw_code": "def skipped?\n        @skipped\n      end",
    "comment": "Return whether this operation was skipped or not  @return [Boolean] true if skipped of false otherwise",
    "label": "",
    "id": "5121"
  },
  {
    "raw_code": "def target_path_discardable?(new_path)\n        false\n      end",
    "comment": "Check if target path has discardable content  @param [String] new_path @return [Boolean] whether we can discard the target path or not",
    "label": "",
    "id": "5122"
  },
  {
    "raw_code": "def discard_path!(new_path)\n        discarded_path = \"#{new_path}-#{Time.current.utc.to_i}\"\n\n        logger.info(\"Moving existing empty attachments folder from '#{new_path}' to '#{discarded_path}', (PROJECT_ID=#{project.id})\")\n        FileUtils.mv(new_path, discarded_path)\n      end",
    "comment": "Rename a path adding a suffix in order to prevent data-loss.  @param [String] new_path",
    "label": "",
    "id": "5123"
  },
  {
    "raw_code": "def execute\n        return {} unless project&.lfs_enabled?\n\n        lfs_changes = if Feature.enabled?(:mirroring_lfs_optimization,\n          project) && params[:updated_revisions] != ['--all']\n                        Gitlab::Git::LfsChanges.new(project.repository,\n                          params[:updated_revisions]).new_pointers(not_in: [])\n                      else\n                        Gitlab::Git::LfsChanges.new(project.repository).all_pointers\n                      end",
    "comment": "Retrieve all lfs blob pointers and returns a hash with the structure { lfs_file_oid => lfs_file_size }",
    "label": "",
    "id": "5124"
  },
  {
    "raw_code": "def each_link(oids, &block)\n        return unless project&.lfs_enabled? && remote_uri && oids.present?\n\n        download_links_in_batches(oids, &block)\n      end",
    "comment": "- oids: hash of oids to query. The structure is { lfs_file_oid => lfs_file_size } Yields operation for each link batch-by-batch",
    "label": "",
    "id": "5125"
  },
  {
    "raw_code": "def should_add_credentials?(link_uri)\n        url_credentials? && link_uri.host == remote_uri.host\n      end",
    "comment": "The download link can be a local url or an object storage url If the download link has the some host as the import url then we add the same credentials because we may need them",
    "label": "",
    "id": "5126"
  },
  {
    "raw_code": "def execute(oids)\n        return [] unless project&.lfs_enabled?\n\n        validate!(oids)\n\n        yield if block_given?\n\n        # Search and link existing LFS Object\n        link_existing_lfs_objects(oids)\n      end",
    "comment": "Accept an array of oids to link  Returns an array with the oid of the existent lfs objects",
    "label": "",
    "id": "5127"
  },
  {
    "raw_code": "def lfs_pointers_in_repository\n        @lfs_pointers_in_repository ||=\n          LfsListService.new(project, current_user,\n            { updated_revisions: params[:updated_revisions] }).execute\n      end",
    "comment": "Retrieves all lfs pointers in the repository",
    "label": "",
    "id": "5128"
  },
  {
    "raw_code": "def default_endpoint_uri\n        @default_endpoint_uri ||= import_uri.dup.tap do |uri|\n          path = uri.path.gsub(%r{/$}, '')\n          path += '.git' unless path.ends_with?('.git')\n          uri.path = path + LFS_BATCH_API_ENDPOINT\n        end",
    "comment": "The import url must end with '.git' here we ensure it is",
    "label": "",
    "id": "5129"
  },
  {
    "raw_code": "def partition_by_keep_n(tags)\n        return [tags, []] unless keep_n\n\n        tags = order_by_date_desc(tags)\n\n        tags.partition.with_index { |_, index| index >= keep_n_as_integer }\n      end",
    "comment": "Should return [tags_to_delete, tags_to_keep]",
    "label": "",
    "id": "5130"
  },
  {
    "raw_code": "def partition_by_older_than(tags)\n        return [tags, []] unless older_than\n\n        older_than_timestamp = older_than_in_seconds.ago\n\n        tags.partition do |tag|\n          timestamp = pushed_at(tag)\n\n          timestamp && timestamp < older_than_timestamp\n        end",
    "comment": "Should return [tags_to_delete, tags_to_keep]",
    "label": "",
    "id": "5131"
  },
  {
    "raw_code": "def execute\n          return success(deleted: []) if tag_names.empty?\n\n          filter_out_protected!\n          return error(PROTECTED_TAGS_ERROR_MESSAGE, pass_back: { deleted: [] }) if tag_names.empty?\n\n          delete_tags\n        rescue TimeoutError, ::Faraday::Error => e\n          ::Gitlab::ErrorTracking.track_exception(e, tags_count: tag_names&.size, container_repository_id: container_repository&.id)\n          error('error while deleting tags', nil, pass_back: { deleted: deleted_tags, exception_class_name: e.class.name })\n        end",
    "comment": "Delete tags by name with a single DELETE request. This is only supported by the GitLab Container Registry fork. See https://gitlab.com/gitlab-org/gitlab/-/merge_requests/23325 for details.",
    "label": "",
    "id": "5132"
  },
  {
    "raw_code": "def execute\n          return success(deleted: []) if @tag_names.empty?\n\n          # generates the blobs for the dummy image\n          dummy_manifest = @container_repository.client.generate_empty_manifest(@container_repository.path)\n          return error('could not generate manifest') if dummy_manifest.nil?\n\n          deleted_tags = replace_tag_manifests(dummy_manifest)\n\n          # Deletes the dummy image\n          # All created tag digests are the same since they all have the same dummy image.\n          # a single delete is sufficient to remove all tags with it\n          if deleted_tags.any? && @container_repository.delete_tag(deleted_tags.each_value.first)\n            success(deleted: deleted_tags.keys)\n          else\n            error(\"could not delete tags: #{@tag_names.join(', ')}\".truncate(1000))\n          end",
    "comment": "Replace a tag on the registry with a dummy tag. This is a hack as the registry doesn't support deleting individual tags. This code effectively pushes a dummy image and assigns the tag to it. This way when the tag is deleted only the dummy image is affected. This is used to preserve compatibility with third-party registries that don't support fast delete. See https://gitlab.com/gitlab-org/gitlab/issues/15737 for a discussion",
    "label": "",
    "id": "5133"
  },
  {
    "raw_code": "def replace_tag_manifests(dummy_manifest)\n          deleted_tags = @tag_names.map do |name|\n            digest = @container_repository.client.put_tag(@container_repository.path, name, dummy_manifest)\n            next unless digest\n\n            [name, digest]\n          end.compact.to_h\n\n          # make sure the digests are the same (it should always be)\n          digests = deleted_tags.values.uniq\n\n          # rubocop: disable CodeReuse/ActiveRecord\n          Gitlab::ErrorTracking.track_and_raise_for_dev_exception(ArgumentError.new('multiple tag digests')) if digests.many?\n\n          deleted_tags\n        end",
    "comment": "update the manifests of the tags with the new dummy image",
    "label": "",
    "id": "5134"
  },
  {
    "raw_code": "def execute_service\n        counts = []\n        source_sha = source_project.commit.sha\n\n        Gitlab::Git::CrossRepo.new(repository, source_project.repository)\n          .execute(source_sha) do |cross_repo_source_sha|\n            counts = repository.diverging_commit_count(head_sha, cross_repo_source_sha)\n            ahead, behind = counts\n            next if behind == 0\n\n            execute_with_fetched_source(cross_repo_source_sha, ahead)\n          end",
    "comment": "The method executes multiple steps:  1. Gitlab::Git::CrossRepo fetches upstream default branch into a temporary ref and returns new source sha. 2. New divergence counts are calculated using the source sha. 3. If the fork is not behind, there is nothing to merge -> exit. 4. Otherwise, continue with the new source sha. 5. If Gitlab::Git::CommandError is raised it means that merge couldn't happen due to a merge conflict. The details are updated to transfer this error to the user.",
    "label": "",
    "id": "5135"
  },
  {
    "raw_code": "def perform_merge(cross_repo_source_sha, ahead)\n        if ahead > 0\n          message = \"Merge branch #{source_project.path}:#{source_project.default_branch} into #{target_branch}\"\n\n          repository.merge_to_branch(current_user,\n            source_sha: cross_repo_source_sha,\n            target_branch: target_branch,\n            target_sha: head_sha,\n            message: message)\n        else\n          repository.ff_merge(current_user, cross_repo_source_sha, target_branch, target_sha: head_sha)\n        end",
    "comment": "This method merges the upstream default branch to the fork specified branch. Depending on whether the fork branch is ahead of upstream or not, a different type of merge is performed.  If the fork's branch is not ahead of the upstream (only behind), fast-forward merge is performed. However, if the fork's branch contains commits that don't exist upstream, a merge commit is created. In this case, a conflict may happen, which interrupts the merge and returns a message to the user.",
    "label": "",
    "id": "5136"
  },
  {
    "raw_code": "def with_linked_lfs_pointers(newrev, &block)\n        return yield unless project.lfs_enabled?\n\n        oldrev = head_sha\n        new_lfs_oids =\n          Gitlab::Git::LfsChanges\n            .new(repository, newrev)\n            .new_pointers(not_in: [oldrev])\n            .map(&:lfs_oid)\n\n        Projects::LfsPointers::LfsLinkService.new(project).execute(new_lfs_oids, &block)\n      rescue Projects::LfsPointers::LfsLinkService::TooManyOidsError => e\n        raise MergeError, e.message\n      end",
    "comment": "This method links the newly merged lfs objects (if any) with the existing ones upstream. The LfsLinkService service has a limit and may raise an error if there are too many lfs objects to link. This is the reason why the block is passed:  1. Verify that there are not too many lfs objects to link 2. Execute the block (which basically performs the merge) 3. Link lfs objects",
    "label": "",
    "id": "5137"
  },
  {
    "raw_code": "def initialize(current_user:, params:)\n        @current_user = current_user\n        @params = params\n      end",
    "comment": "Creates a new RelationImportService.  @param [User] current_user @param [Hash] params @option params [String] path The full path of the project @option params [String] relation The relation to import. See IMPORTABLE_RELATIONS for permitted values. @option params [UploadedFile] file The export archive containing the data to import",
    "label": "",
    "id": "5138"
  },
  {
    "raw_code": "def execute\n        return error(_('Project not found'), :not_found) unless project\n\n        unless relation_valid?\n          return error(\n            format(\n              _('Imported relation must be one of %{relations}'),\n              relations: IMPORTABLE_RELATIONS.to_sentence(last_word_connector: ', or ')\n            ),\n            :bad_request\n          )\n        end",
    "comment": "Checks the validity of the chosen project and triggers the re-import of the chosen relation.  @return [Services::ServiceResponse]",
    "label": "",
    "id": "5139"
  },
  {
    "raw_code": "def first_pipeline_failure?\n        auto_devops_pipelines.success.limit(1).count == 0 &&\n          auto_devops_pipelines.failed.limit(1).count.nonzero?\n      end",
    "comment": "We're using `limit` to optimize `auto_devops pipeline` query, since we only care about the first element, and using only `.count` is an expensive operation. See https://gitlab.com/gitlab-org/gitlab-foss/merge_requests/21172#note_99037378 for more context.",
    "label": "",
    "id": "5140"
  },
  {
    "raw_code": "def initialize(*args)\n      super\n\n      @patches = Gitlab::Git::Patches::Collection.new(Array(params[:patches]))\n    end",
    "comment": "Requires: - project: `Project` to be committed into - user: `User` that will be the committer - params: - branch_name: `String` the branch that will be committed into - start_branch: `String` the branch that will be started from - patches: `Gitlab::Git::Patches::Collection` that contains the patches",
    "label": "",
    "id": "5141"
  },
  {
    "raw_code": "def validate!\n      validate_patches!\n      validate_new_branch_name! if new_branch?\n      validate_permissions!\n    end",
    "comment": "Overridden from the Commits::CreateService, to skip some validations we don't need: - validate_on_branch! Not needed, the patches are applied on top of HEAD if the branch did not exist - validate_branch_existence! Not needed because we continue applying patches on the branch if it already existed, and create it if it did not exist.",
    "label": "",
    "id": "5142"
  },
  {
    "raw_code": "def destroy_possible?(key)\n      true\n    end",
    "comment": "overridden in EE::Keys::DestroyService",
    "label": "",
    "id": "5143"
  },
  {
    "raw_code": "def initialize(key)\n      @key = key\n    end",
    "comment": "key - The Key for which to update the last used timestamp.",
    "label": "",
    "id": "5144"
  },
  {
    "raw_code": "def initialize(container:, current_user: nil, params: {})\n      super(container: container, current_user: current_user, params: params)\n    end",
    "comment": "TODO: this is to be removed once we get to rename the IssuableBaseService project param to container",
    "label": "",
    "id": "5145"
  },
  {
    "raw_code": "def update_class(type)\n      type.classify.pluralize.constantize::UpdateService\n    end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5146"
  },
  {
    "raw_code": "def dup_params\n      dup = HashWithIndifferentAccess.new\n\n      params.each do |key, value|\n        dup[key] = value.is_a?(ActiveRecord::Base) ? value : value.dup\n      end",
    "comment": "Duplicates params and its top-level values We cannot use deep_dup because ActiveRecord objects will result to new records with no id assigned",
    "label": "",
    "id": "5147"
  },
  {
    "raw_code": "def restore_removed_locked_labels(new_label_ids)\n        return new_label_ids unless issuable.supports_lock_on_merge?\n        return new_label_ids unless issuable.label_ids.present?\n\n        removed_label_ids = issuable.label_ids - new_label_ids\n        removed_locked_label_ids = available_labels_service.filter_locked_label_ids(removed_label_ids)\n\n        new_label_ids + removed_locked_label_ids\n      end",
    "comment": "Restore any locked labels that the user is attempting to remove",
    "label": "",
    "id": "5148"
  },
  {
    "raw_code": "def update_pat_ip\n      @personal_access_token.last_used_ips << Authn::PersonalAccessTokenLastUsedIp.new(\n        organization: @personal_access_token.organization,\n        ip_address: Gitlab::IpAddressState.current)\n\n      ip_count = @personal_access_token.last_used_ips.where(\n        personal_access_token_id: @personal_access_token.id).count\n\n      return unless ip_count > NUM_IPS_TO_STORE\n\n      @personal_access_token\n        .last_used_ips\n        .order(created_at: :asc)\n        .limit(ip_count - NUM_IPS_TO_STORE)\n        .delete_all\n    end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord  -- this is specific to this service",
    "label": "",
    "id": "5149"
  },
  {
    "raw_code": "def update_project_bot_to_inherit_current_user_external_status\n      return unless target_user.project_bot?\n\n      target_user.update(external: current_user.external?)\n    end",
    "comment": "See https://gitlab.com/gitlab-org/gitlab/-/issues/509324",
    "label": "",
    "id": "5150"
  },
  {
    "raw_code": "def route_in_request?\n      slack_event == URL_VERIFICATION_EVENT\n    end",
    "comment": "The `url_verification` slack_event response must be returned to Slack in-request, so for this event we call the service directly instead of through a worker.  All other events must be handled asynchronously in order to return a 2xx response immediately to Slack in the request. See https://api.slack.com/apis/connections/events-api.",
    "label": "",
    "id": "5151"
  },
  {
    "raw_code": "def route_event\n      return SlackEvents::UrlVerificationService.new(params).execute if route_in_request?\n\n      SlackEventWorker.perform_async(slack_event: slack_event, params: params)\n\n      {}\n    end",
    "comment": "Returns a payload for the service response.",
    "label": "",
    "id": "5152"
  },
  {
    "raw_code": "def execute\n        Integration.transaction do\n          Integration.where(id: batch_ids).update_all(integration_hash(:update))\n\n          if integration.data_fields_present?\n            integration.data_fields.class.where(data_fields_foreign_key => batch_ids)\n              .update_all(\n                data_fields_hash(:update)\n              )\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5153"
  },
  {
    "raw_code": "def data_fields_foreign_key\n        integration.data_fields.class.reflections['integration'].foreign_key\n      end",
    "comment": "service_id or integration_id",
    "label": "",
    "id": "5154"
  },
  {
    "raw_code": "def update_other_installations!(installation)\n        updatable_attributes = installation.attributes.slice(\n          'user_id',\n          'bot_user_id',\n          'encrypted_bot_access_token',\n          'encrypted_bot_access_token_iv',\n          'updated_at'\n        )\n\n        SlackIntegration.by_team(installation.team_id).id_not_in(installation.id).each_batch do |batch|\n          batch_ids = batch.pluck_primary_key\n          batch.update_all(updatable_attributes)\n\n          Integrations::SlackWorkspace::IntegrationApiScope.update_scopes(batch_ids, installation.slack_api_scopes)\n        end",
    "comment": "Due to our modelling (mentioned in epic 9418) we create a SlackIntegration record for a Slack workspace (team_id) for every GitLab for Slack integration. The repetition is redundant, and we should more correctly only create a single record per workspace.  Records that share a team_id (Slack workspace ID) should have identical bot token and permission scope data. We currently paper-over the modelling problem by mass-updating all records that share a team_id so they always reflect the same state. for this data. This means if we release a new version of the GitLab for Slack app that has a new required permission scope, the first time the workspace authorizes the new scope all other records for their workspace will be updated with the latest authorization data for that workspace.",
    "label": "",
    "id": "5155"
  },
  {
    "raw_code": "def filtered_groups\n        group_ids = groups.map(&:id) + ancestor_integration_group_ids\n        groups.reject do |g|\n          g.ancestor_ids.intersect?(group_ids)\n        end",
    "comment": "Exclusions for groups should propagate to subgroup children Skip creating integrations for subgroups and projects that would already be deactivated by an ancestor integration. Also skip for projects and groups that would be deactivated by creating an integration for another group in the same call to #execute.",
    "label": "",
    "id": "5156"
  },
  {
    "raw_code": "def initialize(integration, current_user, event = nil)\n        @integration = integration\n        @current_user = current_user\n        @event = event\n      end",
    "comment": "@param integration [Service] The external service that will be called @param current_user [User] The user calling the service @param event [String/nil] The event that triggered this",
    "label": "",
    "id": "5157"
  },
  {
    "raw_code": "def external_action\n      raise NotImplementedError\n    end",
    "comment": "Passed to web-hooks, and send to external consumers.",
    "label": "",
    "id": "5158"
  },
  {
    "raw_code": "def internal_event_name\n      raise NotImplementedError\n    end",
    "comment": "Should return a valid event name to be used with Gitlab::InternalEvents",
    "label": "",
    "id": "5159"
  },
  {
    "raw_code": "def event_action\n      raise NotImplementedError\n    end",
    "comment": "Used to create `Event` records. Must be a valid value for `Event#action`",
    "label": "",
    "id": "5160"
  },
  {
    "raw_code": "def increment_usage(page)\n      track_event(page, internal_event_name)\n    end",
    "comment": "This method throws an error if internal_event_name returns an unknown event name",
    "label": "",
    "id": "5161"
  },
  {
    "raw_code": "def initialize(author)\n      raise ArgumentError, 'author must not be nil' unless author\n\n      @author = author\n    end",
    "comment": "@param [User] author The event author",
    "label": "",
    "id": "5162"
  },
  {
    "raw_code": "def execute(notes, options = {})\n      Banzai::ObjectRenderer\n        .new(user: current_user, redaction_context: options)\n        .render(notes, :note)\n    end",
    "comment": "Renders a collection of Note instances.  notes - The notes to render.  Possible options:  requested_path - The request path. project_wiki - The project's wiki. ref - The current Git reference. only_path - flag to turn relative paths into absolute ones. xhtml - flag to save the html in XHTML",
    "label": "",
    "id": "5163"
  },
  {
    "raw_code": "def sanitized_note_params(note)\n      MarkdownContentRewriterService\n        .new(current_user, note, :note, from_project, to_noteable.resource_parent)\n        .execute\n    end",
    "comment": "Skip copying cached markdown HTML if text does not contain references or uploads.",
    "label": "",
    "id": "5164"
  },
  {
    "raw_code": "def apply_updates(update_params, note)\n      return if update_params.empty?\n      return unless supported?(note)\n\n      # We need the `id` after the note is persisted\n      if update_params[:spend_time]\n        update_params[:spend_time][:note_id] = note.id\n      end",
    "comment": "Applies updates extracted to note#noteable The update parameters are extracted on self#execute",
    "label": "",
    "id": "5165"
  },
  {
    "raw_code": "def update_hook_failure_state\n      return unless hook.auto_disabling_enabled?\n\n      in_lock(lock_name, ttl: LOCK_TTL, sleep_sec: LOCK_SLEEP, retries: LOCK_RETRY) do |_retried|\n        hook.reset # Reload within the lock so properties are guaranteed to be current.\n\n        case response_category\n        when :ok\n          hook.enable!\n        when :error\n          hook.backoff!\n        end",
    "comment": "Perform this operation within an `Gitlab::ExclusiveLease` lock to make it safe to be called concurrently from different workers.",
    "label": "",
    "id": "5166"
  },
  {
    "raw_code": "def after_destroy(_web_hook)\n      success({ async: false })\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5167"
  },
  {
    "raw_code": "def initialize(user)\n      @user = user\n    end",
    "comment": "user - The User for which to get the number of SSH keys.",
    "label": "",
    "id": "5168"
  },
  {
    "raw_code": "def can_be_deactivated?(user)\n      user.can_be_deactivated?\n    end",
    "comment": "Wrapped in a method to allow overriding in subclasses",
    "label": "",
    "id": "5169"
  },
  {
    "raw_code": "def initialize(user, source: nil, incorrect_auth_found_callback: nil, missing_auth_found_callback: nil)\n      @user = user\n      @source = source\n      @incorrect_auth_found_callback = incorrect_auth_found_callback\n      @missing_auth_found_callback = missing_auth_found_callback\n\n      @start_time = current_monotonic_time\n      @duration_statistics = {}\n    end",
    "comment": "user - The User for which to refresh the authorized projects.",
    "label": "",
    "id": "5170"
  },
  {
    "raw_code": "def execute_without_lease\n      remove, add = AuthorizedProjectUpdate::FindRecordsDueForRefreshService.new(\n        user,\n        source: source,\n        incorrect_auth_found_callback: incorrect_auth_found_callback,\n        missing_auth_found_callback: missing_auth_found_callback\n      ).execute\n\n      reset_timer_and_store_duration(:find_records_due_for_refresh)\n\n      update_authorizations(remove, add)\n    end",
    "comment": "This method returns the updated User object.",
    "label": "",
    "id": "5171"
  },
  {
    "raw_code": "def update_authorizations(remove = [], add = [])\n      ProjectAuthorizations::Changes.new do |changes|\n        changes.add(add)\n        changes.remove_projects_for_user(user, remove)\n      end.apply!\n\n      user.update!(project_authorizations_recalculated_at: Time.zone.now) if remove.any? || add.any?\n\n      reset_timer_and_store_duration(:update_authorizations)\n\n      log_refresh_details(remove, add)\n\n      # Since we batch insert authorization rows, Rails' associations may get\n      # out of sync. As such we force a reload of the User object.\n      user.reset\n    end",
    "comment": "Updates the list of authorizations for the current user.  remove - The project IDs of the authorization rows to remove. add - Rows to insert in the form `[{ user_id: user_id, project_id: project_id, access_level: access_level}, ...]`",
    "label": "",
    "id": "5172"
  },
  {
    "raw_code": "def execute(accepted:)\n      agreement = @user.term_agreements.find_or_initialize_by(term: @term)\n      agreement.accepted = accepted\n\n      if agreement.save\n        store_accepted_term(accepted)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5173"
  },
  {
    "raw_code": "def self.execute(batch_size: BATCH_SIZE)\n      scope = UserStatus\n        .select(:user_id)\n        .scheduled_for_cleanup\n        .lock('FOR UPDATE SKIP LOCKED')\n        .limit(batch_size)\n\n      deleted_rows = UserStatus.where(user_id: scope).delete_all\n\n      { deleted_rows: deleted_rows }\n    end",
    "comment": "Cleanup BATCH_SIZE user_statuses records rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5174"
  },
  {
    "raw_code": "def execute(user, options = {})\n      delete_solo_owned_groups = options.fetch(:delete_solo_owned_groups, options[:hard_delete])\n\n      unless Ability.allowed?(current_user, :destroy_user, user) || options[:skip_authorization]\n        raise Gitlab::Access::AccessDeniedError, \"#{current_user} tried to destroy user #{user}!\"\n      end",
    "comment": "Asynchronously destroys +user+ Migrating the associated user records, and post-migration cleanup is handled by the Users::MigrateRecordsToGhostUserInBatchesWorker cron worker.  The operation will fail if the user is the sole owner of any groups. To force the groups to be destroyed, pass `delete_solo_owned_groups: true` in +options+.  The user's contributions will be migrated to a global ghost user. To force the contributions to be destroyed, pass `hard_delete: true` in +options+.  `hard_delete: true` implies `delete_solo_owned_groups: true`.  To perform a hard deletion without destroying solo-owned groups, pass `delete_solo_owned_groups: false, hard_delete: true` in +options+. ",
    "label": "",
    "id": "5175"
  },
  {
    "raw_code": "def track_event(_); end\n\n    def state_error(user)\n      error(_(\"You cannot %{action} %{state} users.\" % { action: action.to_s, state: user.state }), :forbidden)\n    end\n\n    def allowed?\n      can?(current_user, :admin_all_resources)\n    end\n\n    def permission_error\n      error(_(\"You are not allowed to %{action} a user\" % { action: action.to_s }), :forbidden)\n    end\n\n    def log_event(user)\n      Gitlab::AppLogger.info(\n        message: \"User #{action}\",\n        username: user.username.to_s,\n        user_id: user.id,\n        email: user.email.to_s,\n        \"#{action}_by\": current_user.username.to_s,\n        ip_address: current_user.current_sign_in_ip.to_s\n      )\n    end\n  end",
    "comment": "Overridden in Users::BanService",
    "label": "",
    "id": "5176"
  },
  {
    "raw_code": "def cache_last_push_event(event)\n      keys = [\n        project_cache_key(event.project),\n        user_cache_key\n      ]\n\n      if forked_from = event.project.forked_from_project\n        keys << project_cache_key(forked_from)\n      end",
    "comment": "Caches the given push event for the current user in the Rails cache.  event - An instance of PushEvent to cache.",
    "label": "",
    "id": "5177"
  },
  {
    "raw_code": "def last_event_for_user\n      find_cached_event(user_cache_key)\n    end",
    "comment": "Returns the last PushEvent for the current user.  This method will return nil if no event was found.",
    "label": "",
    "id": "5178"
  },
  {
    "raw_code": "def last_event_for_project(project)\n      find_cached_event(project_cache_key(project))\n    end",
    "comment": "Returns the last PushEvent for the current user and the given project.  project - An instance of Project for which to retrieve the PushEvent.  This method will return nil if no event was found.",
    "label": "",
    "id": "5179"
  },
  {
    "raw_code": "def find_event_in_database(id)\n      PushEvent\n        .without_existing_merge_requests\n        .find_by(id: id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5180"
  },
  {
    "raw_code": "def user_cache_key\n      \"last-push-event/#{@user.id}\"\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5181"
  },
  {
    "raw_code": "def initialize(user_ids)\n      @user_ids = user_ids\n    end",
    "comment": "user_ids - An array of User IDs",
    "label": "",
    "id": "5182"
  },
  {
    "raw_code": "def batched_migrate(base_scope, column, batch_size: 50)\n      loop do\n        update_count = base_scope.where(column => user.id).limit(batch_size).update_all(column => ghost_user.id)\n        break if update_count == 0\n        raise Gitlab::Utils::ExecutionTracker::ExecutionTimeOutError if execution_tracker.over_limit?\n      end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5183"
  },
  {
    "raw_code": "def admin_create_params\n      [\n        :access_level,\n        :admin,\n        :avatar,\n        :bio,\n        :bot_namespace,\n        :can_create_group,\n        :color_mode_id,\n        :color_scheme_id,\n        :discord,\n        :email,\n        :external,\n        :force_random_password,\n        :hide_no_password,\n        :hide_no_ssh_key,\n        :linkedin,\n        :location,\n        :name,\n        :note,\n        :user_detail_organization,\n        :password,\n        :password_automatically_set,\n        :password_expires_at,\n        :private_profile,\n        :projects_limit,\n        :public_email,\n        :remember_me,\n        :skip_confirmation,\n        :theme_id,\n        :twitter,\n        :user_type,\n        :username,\n        :view_diffs_file_by_file,\n        :website_url,\n        :github\n      ]\n    end",
    "comment": "Allowed params for creating a user (admins only)",
    "label": "",
    "id": "5184"
  },
  {
    "raw_code": "def signup_params\n      [\n        :email,\n        :name,\n        :password,\n        :password_automatically_set,\n        :preferred_language,\n        :username,\n        :user_type,\n        :first_name,\n        :last_name\n      ]\n    end",
    "comment": "Allowed params for user signup",
    "label": "",
    "id": "5185"
  },
  {
    "raw_code": "def after_block_hook(user)\n      custom_attribute = {\n        user_id: user.id,\n        key: UserCustomAttribute::BLOCKED_BY,\n        value: \"#{current_user.username}/#{current_user.id}+#{Time.current}\"\n      }\n      UserCustomAttribute.upsert_custom_attributes([custom_attribute])\n    end",
    "comment": "overridden by EE module",
    "label": "",
    "id": "5186"
  },
  {
    "raw_code": "def create_lfs_object!(lfs_pointer_file, file_content, detect_content_type)\n      LfsObject.find_or_create_by(oid: lfs_pointer_file.sha256, size: lfs_pointer_file.size) do |lfs_object|\n        lfs_object.file = if detect_content_type && (content_type = ::Gitlab::Utils::MimeType.from_string(file_content))\n                            CarrierWaveStringFile.new_file(\n                              file_content: file_content,\n                              filename: '',\n                              content_type: content_type\n                            )\n                          else\n                            CarrierWaveStringFile.new(file_content)\n                          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5187"
  },
  {
    "raw_code": "def link_lfs_object!(lfs_object)\n      LfsObjectsProject.safe_find_or_create_by!(\n        project: project,\n        lfs_object: lfs_object,\n        repository_type: repository_type\n      )\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5188"
  },
  {
    "raw_code": "def lfs_objects_relation\n      project.lfs_objects_for_repository_types(nil, :project)\n    end",
    "comment": "Currently we only set repository_type for design repository objects, so push mirroring must send objects with a `nil` repository type - but if the wiki repository uses LFS, its objects will also be sent. This will be addressed by https://gitlab.com/gitlab-org/gitlab/-/issues/250346",
    "label": "",
    "id": "5189"
  },
  {
    "raw_code": "def current_lock\n      project.lfs_file_locks.find_by(path: params[:path])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5190"
  },
  {
    "raw_code": "def create_lock!\n      lock = project.lfs_file_locks.create!(user: current_user, path: params[:path])\n\n      project.refresh_lfs_file_locks_changed_epoch\n\n      success(http_status: 201, lock: lock)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5191"
  },
  {
    "raw_code": "def find_locks\n      options = params.slice(:id, :path).to_h.compact.symbolize_keys\n\n      project.lfs_file_locks.where(options)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5192"
  },
  {
    "raw_code": "def execute(**args)\n      preload_reviewers_for_merge_requests(args[:merge_requests]) if args.key?(:merge_requests)\n\n      JiraConnectInstallation.for_project(project).flat_map do |installation|\n        client = Atlassian::JiraConnect::Client.new(installation.base_url, installation.shared_secret)\n\n        responses = client.send_info(project: project, **args)\n\n        responses.each { |r| log_response(r) }\n      end",
    "comment": "Parameters: see Atlassian::JiraConnect::Client#send_info Includes: update_sequence_id, commits, branches, merge_requests, pipelines",
    "label": "",
    "id": "5193"
  },
  {
    "raw_code": "def self.user_or_deploy_token_from_jwt(raw_jwt)\n      token_payload = self.new(raw_jwt).execute\n\n      if token_payload['user_id']\n        User.find(token_payload['user_id'])\n      elsif token_payload['deploy_token']\n        DeployToken.active.find_by_token(token_payload['deploy_token'])\n      end",
    "comment": "TODO: Rename to make it obvious how it's used in Gitlab::Auth::RequestAuthenticator which is to return an <object>.<id> that is used as a rack-attack discriminator that way it cannot be confused with `.user_or_token_from_jwt` https://gitlab.com/gitlab-org/gitlab/-/issues/454518",
    "label": "",
    "id": "5194"
  },
  {
    "raw_code": "def initialize(\n      project,\n      user,\n      version:,\n      branch: project.default_branch_or_main,\n      from: nil,\n      to: branch,\n      date: DateTime.now,\n      trailer: DEFAULT_TRAILER,\n      config_file: Gitlab::Changelog::Config::DEFAULT_FILE_PATH,\n      config_file_ref: Gitlab::Changelog::Config::DEFAULT_CONFIG_FILE_REFERENCE,\n      file: DEFAULT_FILE,\n      message: \"Add changelog for version #{version}\"\n    )\n      @project = project\n      @user = user\n      @version = version\n      @from = from\n      @to = to\n      @date = date\n      @branch = branch\n      @trailer = trailer\n      @config_file = config_file\n      @config_file_ref = config_file_ref\n      @file = file\n      @message = message\n    end",
    "comment": "The `project` specifies the `Project` to generate the changelog section for.  The `user` argument specifies a `User` to use for committing the changes to the Git repository.  The `version` arguments must be a version `String` using semantic versioning as the format.  The arguments `from` and `to` must specify a Git ref or SHA to use for fetching the commits to include in the changelog. The SHA/ref set in the `from` argument isn't included in the list.  The `date` argument specifies the date of the release, and defaults to the current time/date.  The `branch` argument specifies the branch to commit the changes to. The branch must already exist.  The `trailer` argument is the Git trailer to use for determining what commits to include in the changelog.  The `config_file` arguments specifies the path to the configuration file as stored in the project's Git repository.  The `config_file_ref` argument specifies the ref where the configuration file is stored. By default, it's a default repository branch.  The `file` arguments specifies the name/path of the file to commit the changes to. If the file doesn't exist, it's created automatically.  The `message` argument specifies the commit message to use when committing the changelog changes.  rubocop: disable Metrics/ParameterLists",
    "label": "",
    "id": "5195"
  },
  {
    "raw_code": "def execute(commit_to_changelog: true)\n      config = Gitlab::Changelog::Config.from_git(@project, @user, @config_file, @config_file_ref)\n      from = start_of_commit_range(config)\n\n      # For every entry we want to only include the merge request that\n      # originally introduced the commit, which is the oldest merge request that\n      # contains the commit. We fetch there merge requests in batches, reducing\n      # the number of SQL queries needed to get this data.\n      mrs_finder = MergeRequests::OldestPerCommitFinder.new(@project)\n      release = Gitlab::Changelog::Release\n        .new(version: @version, date: @date, config: config)\n\n      commits =\n        ChangelogCommitsFinder.new(project: @project, from: from, to: @to)\n\n      verify_commit_range!(from, @to)\n\n      commits.each_page(@trailer) do |page|\n        mrs = mrs_finder.execute(page)\n\n        # Preload the authors. This ensures we only need a single SQL query per\n        # batch of commits, instead of needing a query for every commit.\n        page.each(&:lazy_author)\n\n        # Preload author permissions\n        @project.team.max_member_access_for_user_ids(page.map(&:author).compact.map(&:id))\n\n        page.each do |commit|\n          release.add_entry(\n            title: commit.title,\n            commit: commit,\n            category: commit.trailers.fetch(@trailer),\n            author: commit.author,\n            merge_request: mrs[commit.id]\n          )\n        end",
    "comment": "rubocop: enable Metrics/ParameterLists",
    "label": "",
    "id": "5196"
  },
  {
    "raw_code": "def ignore_git_errors(&block)\n    yield\n  rescue Gitlab::Git::CommandError => e\n    Gitlab::GitLogger.warn(class: self.class.name, container_id: container.id, disk_path: disk_path, message: e.to_s)\n  end",
    "comment": "If we get a Gitaly error, the repository may be corrupted. We can ignore these errors since we're going to trash the repositories anyway.",
    "label": "",
    "id": "5197"
  },
  {
    "raw_code": "def available_work_item_types\n      WorkItems::Type.all.reject { |wit| wit.base_type == :epic }\n        .index_by(&:name).with_indifferent_access.transform_keys(&:strip).transform_keys(&:downcase)\n    end",
    "comment": "todo: This should be updated once we can determine available work item types based on namespace, see https://gitlab.com/gitlab-org/gitlab/-/issues/524828",
    "label": "",
    "id": "5198"
  },
  {
    "raw_code": "def build_dates_source_attributes\n        attributes = { due_date_is_fixed: true, start_date_is_fixed: true }\n\n        if params.key?(:start_date)\n          attributes[:start_date] = params[:start_date]\n          attributes[:start_date_fixed] = params[:start_date]\n        end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5199"
  },
  {
    "raw_code": "def perform_destroy_link(link, linked_item)\n        link.destroy!\n        removed_ids << linked_item.id\n        true\n      end",
    "comment": "Overriden on EE to sync deletion with related epic links records",
    "label": "",
    "id": "5200"
  },
  {
    "raw_code": "def can_add_to_parent?(parent_work_item, _child_work_item = nil)\n        can_admin_link?(parent_work_item)\n      end",
    "comment": "Overriden in EE",
    "label": "",
    "id": "5201"
  },
  {
    "raw_code": "def move_link(link, adjacent_work_item, relative_position)\n        if relative_position\n          link.move_before(adjacent_work_item.parent_link) if relative_position == 'BEFORE'\n          link.move_after(adjacent_work_item.parent_link) if relative_position == 'AFTER'\n        elsif link.changes.include?(:work_item_parent_id)\n          # position item at the start of the list if parent changed and relative_position is not provided\n          link.move_to_start\n        end",
    "comment": "overriden in EE",
    "label": "",
    "id": "5202"
  },
  {
    "raw_code": "def transaction_create(new_work_item)\n        super.tap do |save_result|\n          break save_result unless save_result\n\n          if operation == :move\n            ::WorkItems::DataSync::MoveService.transaction_callback(new_work_item, original_work_item, current_user)\n          elsif operation == :promote\n            ::WorkItems::LegacyEpics::IssuePromoteService.transaction_callback(new_work_item, original_work_item,\n              current_user)\n          else\n            ::WorkItems::DataSync::CloneService.transaction_callback(new_work_item, original_work_item, current_user)\n          end",
    "comment": "In legacy Issues::MoveService and Issues::CloneService, system notes are created within the work item move transaction, so we replicate the behaviour for now. This is to be changed in MVC2: https://gitlab.com/groups/gitlab-org/-/epics/15476",
    "label": "",
    "id": "5203"
  },
  {
    "raw_code": "def initialize(work_item:, target_namespace:, current_user: nil, params: {})\n        # this helps reuse this service with Issue instances in legacy code, as well as WorkItem instances\n        @work_item = ensure_work_item(work_item)\n        @target_namespace = handle_target_namespace_type(target_namespace)\n\n        super(container: work_item.namespace, current_user: current_user, params: params)\n      end",
    "comment": "work_item - original work item target_namespace - ProjectNamespace, Group or Project. When Project is passed it is translated into `Namespaces::ProjectNamespace` afterwards. current_user - user performing the move/clone action",
    "label": "",
    "id": "5204"
  },
  {
    "raw_code": "def cleanup_work_item; end\n      end",
    "comment": "this will handle work item deletion",
    "label": "",
    "id": "5205"
  },
  {
    "raw_code": "def initialize(work_item:, target_namespace:, target_work_item_type:, current_user: nil, params: {}, overwritten_params: {})\n          @work_item = work_item\n          @target_namespace = target_namespace\n          @target_work_item_type = target_work_item_type\n          @current_user = current_user\n          @params = params\n          @operation = params.delete(:operation)\n\n          @create_params = {\n            id: nil,\n            iid: nil,\n            created_at: work_item.created_at,\n            updated_at: work_item.updated_at,\n            updated_by_id: work_item.updated_by_id,\n            state_id: work_item.state_id,\n            closed_at: work_item.closed_at,\n            closed_by_id: work_item.closed_by_id,\n            duplicated_to_id: work_item.duplicated_to_id,\n            moved_to_id: work_item.moved_to_id,\n            promoted_to_epic_id: work_item.promoted_to_epic_id,\n            upvotes_count: work_item.upvotes_count,\n            blocking_issues_count: work_item.blocking_issues_count,\n            work_item_type: target_work_item_type,\n            project_id: project&.id,\n            namespace_id: target_namespace.id,\n            title: work_item.title,\n            author_id: work_item.author_id,\n            relative_position: relative_position,\n            confidential: work_item.confidential,\n            cached_markdown_version: work_item.cached_markdown_version,\n            lock_version: work_item.lock_version,\n            service_desk_reply_to: service_desk_reply_to,\n            imported_from: :none\n          }.merge(overwritten_params)\n        end",
    "comment": "rubocop:disable Layout/LineLength -- Keyword arguments are making the line a bit longer",
    "label": "",
    "id": "5206"
  },
  {
    "raw_code": "def execute\n          # create the new work item\n          ::WorkItems::DataSync::BaseCreateService.new(\n            original_work_item: work_item,\n            operation: operation,\n            container: target_namespace,\n            current_user: current_user,\n            params: create_params.merge(params)\n          ).execute(skip_system_notes: skip_system_notes?)\n        end",
    "comment": "rubocop:enable Layout/LineLength",
    "label": "",
    "id": "5207"
  },
  {
    "raw_code": "def new_notes(notes_batch, notes_ids_map)\n            notes_batch.map do |note|\n              new_discussion_ids[note.discussion_id] ||= Note.new(\n                noteable_id: target_noteable.id,\n                noteable_type: target_noteable.class.base_class\n              ).discussion_id\n\n              note.attributes.tap do |attrs|\n                attrs['id'] = notes_ids_map[note.id]\n                attrs['noteable_id'] = target_noteable.id\n                # we want this if we want to use this also to copy notes when promoting issue to epic\n                attrs['noteable_type'] = target_noteable.class.base_class\n                attrs['discussion_id'] = new_discussion_ids[note.discussion_id]\n                # need to use `try` to be able to handle Issue model and legacy Epic model instances\n                attrs['project_id'] = target_noteable.try(:project_id)\n                attrs['namespace_id'] = target_noteable.try(:namespace_id) || target_noteable.try(:group_id)\n                attrs['imported_from'] = 'none' # maintaining current copy notes implementation\n\n                # this data is not changed, but it is being serialized, and we need it deserialized for bulk inserts\n                attrs['position'] = note.attributes_before_type_cast['position']\n                attrs['original_position'] = note.attributes_before_type_cast['original_position']\n                attrs['change_position'] = note.attributes_before_type_cast['change_position']\n                attrs['st_diff'] = note.attributes_before_type_cast['st_diff']\n                attrs['cached_markdown_version'] = note.cached_markdown_version\n\n                sanitized_note_params = sanitized_note_params(note)\n                attrs['note'] = sanitized_note_params['note']\n                attrs['note_html'] = sanitized_note_params['note_html']\n              end",
    "comment": "rubocop: disable Metrics/AbcSize -- Despite being long, this method is straightforward.",
    "label": "",
    "id": "5208"
  },
  {
    "raw_code": "def new_notes_emoji(notes_emoji, notes_ids_map)\n            notes_emoji.map do |note_emoji|\n              note_emoji.attributes.except('id').tap do |attrs|\n                attrs['awardable_id'] = notes_ids_map[note_emoji.awardable_id]\n              end",
    "comment": "rubocop: enable Metrics/AbcSize",
    "label": "",
    "id": "5209"
  },
  {
    "raw_code": "def build_description_version_attributes(description_version, description_version_ids_map)\n            description_version.attributes.tap do |attrs|\n              attrs['id'] = description_version_ids_map[description_version.id]\n              attrs['issue_id'] = target_noteable.id\n              attrs['namespace_id'] = target_noteable.namespace_id\n            end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5210"
  },
  {
    "raw_code": "def post_move_cleanup\n          # The update is only done here for testing purposes, these attributes will be removed upon original work item\n          # cleanup.\n          work_item.update(start_date: nil, due_date: nil)\n          work_item.dates_source&.destroy\n        end",
    "comment": "NOTE: This method is for cleanup simulation and testing purposes, it is not actually called within the application yet.  In the product, the post move cleanup of widget data is going to be implemented later.",
    "label": "",
    "id": "5211"
  },
  {
    "raw_code": "def post_move_cleanup; end\n      end",
    "comment": "IMPORTANT: This is a callback that is called by `BaseCleanupDataService` from `DataSync::MoveService` after the work item is moved to the target namespace to delete the original work item data. That is because we have to implement `MoveService` as `copy` to destination & `delete` from source.  Has to be implemented in the specific widget class or it can be an empty implementation if it does not need to cleanup any data on the original work item",
    "label": "",
    "id": "5212"
  },
  {
    "raw_code": "def post_move_cleanup\n          work_item.label_links.each_batch(of: BATCH_SIZE) do |label_links_batch|\n            label_links_batch.by_targets([work_item]).delete_all\n          end",
    "comment": "overwritten in EE",
    "label": "",
    "id": "5213"
  },
  {
    "raw_code": "def cleanup_designs\n          work_item.designs.each_batch(of: BATCH_SIZE) do |designs|\n            designs.destroy_all # rubocop:disable Cop/DestroyAll -- need to destroy all designs with associated records\n          end",
    "comment": "cleanup all designs for the work item, we use destroy as there are the notes, user_mentions and events associations that have `dependent: delete_all` and they need to be deleted too, after they are being copied to the target work item",
    "label": "",
    "id": "5214"
  },
  {
    "raw_code": "def cleanup_design_versions\n          work_item.design_versions.each_batch(of: BATCH_SIZE) do |design_versions|\n            design_versions.delete_all\n          end",
    "comment": "cleanup all design versions for the work item, we can safely use delete_all as there are no associated records or callbacks",
    "label": "",
    "id": "5215"
  },
  {
    "raw_code": "def post_move_cleanup\n          IssueLink.for_source(work_item).each_batch(of: BATCH_SIZE, column: :target_id) do |links_batch|\n            links_batch.delete_all\n          end",
    "comment": "NOTE: No need to override this in EE for legacy ::Epic::RelatedEpicLink records, because ::Epic::RelatedEpicLink records are built to contain `issue_link_id` FK with ON DELETE CASCADE, which would delete ::Epic::RelatedEpicLink when corresponding IssueLink records for a given Epic Work Item are deleted in this method.",
    "label": "",
    "id": "5216"
  },
  {
    "raw_code": "def after_save_commit\n          return unless target_work_item.get_widget(:hierarchy)\n\n          handle_parent\n\n          # we only handle child items for `move` functionality, `clone` does not copy child items.\n          return unless params[:operation] == :move\n\n          handle_children\n        end",
    "comment": "overriden in EE",
    "label": "",
    "id": "5217"
  },
  {
    "raw_code": "def csv_builder\n      @csv_builder ||= begin\n        data_hash = MapExportFieldsService.new(fields, header_to_value_hash).execute\n\n        if preload_associations_in_batches?\n          CsvBuilder.new(objects, data_hash, associations_to_preload)\n        else\n          CsvBuilder.new(objects.preload(associations_to_preload), data_hash, [])\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5218"
  },
  {
    "raw_code": "def associations_to_preload\n      []\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5219"
  },
  {
    "raw_code": "def base_api_url\n        \"#{context_path}/rest/api/#{api_version}\"\n      end",
    "comment": "We have to add the context_path here because the Jira client is not taking it into account",
    "label": "",
    "id": "5220"
  },
  {
    "raw_code": "def api_version\n        JIRA_API_VERSION\n      end",
    "comment": "override this method in the specific request class implementation if a differnt API version is required",
    "label": "",
    "id": "5221"
  },
  {
    "raw_code": "def reportable_error_message(error)\n        case error\n        when ERRORS[:jira_ruby]\n          reportable_jira_ruby_error_message(error)\n        when ERRORS[:ssl]\n          s_('JiraRequest|An SSL error occurred while connecting to Jira: %{message}. Try your request again.') % { message: error.message }\n        when *ERRORS[:uri]\n          s_('JiraRequest|The Jira API URL for connecting to Jira is not valid. Check your Jira integration API URL and try again.')\n        when *ERRORS[:timeout]\n          s_('JiraRequest|A timeout error occurred while connecting to Jira. Try your request again.')\n        when *ERRORS[:connection]\n          s_('JiraRequest|A connection error occurred while connecting to Jira. Try your request again.')\n        when ERRORS[:url_blocked]\n          safe_format(s_('JiraRequest|Unable to connect to the Jira URL. Please verify your %{config_link_start}Jira integration URL%{config_link_end} and attempt the connection again.'), config_link_start: config_integration_link_start, config_link_end: '</a>'.html_safe)\n        end",
    "comment": "Returns a user-facing error message if possible, otherwise `nil`.",
    "label": "",
    "id": "5222"
  },
  {
    "raw_code": "def reportable_jira_ruby_error_message(error)\n        case error.message\n        when 'Unauthorized'\n          safe_format(s_('JiraRequest|The credentials for accessing Jira are not valid. Check your %{docs_link_start}Jira integration credentials%{docs_link_end} and try again.'), docs_link_start: auth_docs_link_start, docs_link_end: '</a>'.html_safe)\n        when 'Forbidden'\n          safe_format(s_('JiraRequest|The credentials for accessing Jira are not allowed to access the data. Check your %{docs_link_start}Jira integration credentials%{docs_link_end} and try again.'), docs_link_start: auth_docs_link_start, docs_link_end: '</a>'.html_safe)\n        when 'Bad Request'\n          jira_ruby_json_error_message(error.response.body) || safe_format(s_('JiraRequest|An error occurred while requesting data from Jira. Check your %{docs_link_start}Jira integration configuration%{docs_link_end} and try again.'), docs_link_start: config_docs_link_start, docs_link_end: '</a>'.html_safe)\n        end",
    "comment": "Returns a user-facing error message for a `JIRA::HTTPError` if possible, otherwise `nil`.",
    "label": "",
    "id": "5223"
  },
  {
    "raw_code": "def handle_access(access_as)\n        access_as_agent if access_as.key?('agent')\n      end",
    "comment": "Override in EE",
    "label": "",
    "id": "5224"
  },
  {
    "raw_code": "def execution_interval\n        DEFAULT_EXECUTION_INTERVAL\n      end",
    "comment": "Override this method to customize the execution interval",
    "label": "",
    "id": "5225"
  },
  {
    "raw_code": "def group_labels_applied_to_issues\n      @labels_applied_to_issues ||= Label.joins(:issues)\n        .joins(\"INNER JOIN namespaces on namespaces.id = labels.group_id AND namespaces.type = 'Group'\")\n        .where(issues: { project_id: project.id }).without_order\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5226"
  },
  {
    "raw_code": "def group_labels_applied_to_merge_requests\n      @labels_applied_to_mrs ||= Label.joins(:merge_requests)\n        .joins(\"INNER JOIN namespaces on namespaces.id = labels.group_id AND namespaces.type = 'Group'\")\n        .where(merge_requests: { target_project_id: project.id }).without_order\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5227"
  },
  {
    "raw_code": "def find_or_create_label!(label)\n      params    = label.attributes.slice('title', 'description', 'color')\n      new_label = FindOrCreateService.new(\n        current_user,\n        project,\n        params.merge(include_ancestor_groups: true)\n      ).execute(skip_authorization: true)\n\n      new_label.id\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5228"
  },
  {
    "raw_code": "def update_label_links(link_ids, old_label_id:, new_label_id:)\n      LabelLink.where(id: link_ids, label_id: old_label_id)\n        .update_all(label_id: new_label_id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5229"
  },
  {
    "raw_code": "def update_label_priorities(old_label_id:, new_label_id:)\n      LabelPriority.where(project_id: project.id, label_id: old_label_id)\n        .update_all(label_id: new_label_id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5230"
  },
  {
    "raw_code": "def execute(target_params)\n      params[:color] = convert_color_name_to_hex if params[:color].present?\n\n      project_or_group = target_params[:project] || target_params[:group]\n\n      if project_or_group.present?\n        params.delete(:lock_on_merge) unless project_or_group.supports_lock_on_merge?\n\n        project_or_group.labels.create(params)\n      elsif target_params[:template]\n        label = Label.new(params)\n        label.organization_id = target_params[:organization_id]\n        label.template = true\n        label.save\n        label\n      else\n        Gitlab::AppLogger.warn(\"target_params should contain :project or :group or :template, actual value: #{target_params}\")\n      end",
    "comment": "returns the created label",
    "label": "",
    "id": "5231"
  },
  {
    "raw_code": "def execute(label)\n      return unless project.group &&\n        label.is_a?(ProjectLabel)\n\n      ProjectLabel.transaction do\n        # use the existing group label if it exists\n        group_label = find_or_create_group_label(label)\n\n        label_ids_for_merge(group_label).find_in_batches(batch_size: BATCH_SIZE) do |batched_ids|\n          update_old_label_relations(group_label, batched_ids)\n          destroy_project_labels(batched_ids)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5232"
  },
  {
    "raw_code": "def subscribe_users(group_label, label_ids)\n      # users can be subscribed to multiple labels that will be merged into the group one\n      # we want to keep only one subscription / user\n      ids_to_update = Subscription.where(subscribable_id: label_ids, subscribable_type: 'Label')\n        .group(:user_id)\n        .pluck('MAX(id)')\n      Subscription.where(id: ids_to_update).update_all(subscribable_id: group_label.id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5233"
  },
  {
    "raw_code": "def label_ids_for_merge(group_label)\n      LabelsFinder\n        .new(current_user, title: group_label.title, group_id: project.group.id)\n        .execute(skip_authorization: true)\n        .id_not_in(group_label)\n        .select(:id, :project_id, :group_id, :type) # Can't use pluck() to avoid object-creation because of the batching\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5234"
  },
  {
    "raw_code": "def update_issuables(group_label, label_ids)\n      LabelLink\n        .where(label: label_ids)\n        .update_all(label_id: group_label.id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5235"
  },
  {
    "raw_code": "def update_resource_label_events(group_label, label_ids)\n      ResourceLabelEvent\n        .where(label: label_ids)\n        .update_all(label_id: group_label.id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5236"
  },
  {
    "raw_code": "def update_issue_board_lists(group_label, label_ids)\n      List\n        .where(label: label_ids)\n        .update_all(label_id: group_label.id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5237"
  },
  {
    "raw_code": "def update_priorities(group_label, label_ids)\n      LabelPriority\n        .where(label: label_ids)\n        .update_all(label_id: group_label.id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5238"
  },
  {
    "raw_code": "def destroy_project_labels(label_ids)\n      Label.where(id: label_ids).destroy_all # rubocop: disable Cop/DestroyAll\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5239"
  },
  {
    "raw_code": "def find_or_create_label(find_only: false)\n      new_label = find_existing_label(title)\n\n      return new_label if find_only\n\n      if new_label.nil? && (skip_authorization || Ability.allowed?(current_user, :admin_label, parent))\n        create_params = params.except(:include_ancestor_groups)\n        new_label = Labels::CreateService.new(create_params).execute(parent_type.to_sym => parent)\n      end",
    "comment": "Only creates the label if current_user can do so, if the label does not exist and the user can not create the label, nil is returned",
    "label": "",
    "id": "5240"
  },
  {
    "raw_code": "def find_existing_label(title)\n      return existing_labels_by_title[title] if existing_labels_by_title\n\n      available_labels.find_by(title: title)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5241"
  },
  {
    "raw_code": "def title\n      params[:title] || params[:name]\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5242"
  },
  {
    "raw_code": "def execute(label)\n      params[:name] = params.delete(:new_name) if params.key?(:new_name)\n      params[:color] = convert_color_name_to_hex if params[:color].present?\n      params.delete(:lock_on_merge) unless allow_lock_on_merge?(label)\n\n      label.update(params)\n      label\n    end",
    "comment": "returns the updated label",
    "label": "",
    "id": "5243"
  },
  {
    "raw_code": "def project\n        note.project\n      end",
    "comment": "NOTE: may be nil, in the case of a PersonalSnippet  (this is okay because NotificationRecipient is written to handle nil projects)",
    "label": "",
    "id": "5244"
  },
  {
    "raw_code": "def recipients_target\n        target\n      end",
    "comment": "override if needed",
    "label": "",
    "id": "5245"
  },
  {
    "raw_code": "def add_recipients(users, type, reason)\n        if users.is_a?(ActiveRecord::Relation)\n          users = users.includes(:notification_settings)\n            .allow_cross_joins_across_databases(url: 'https://gitlab.com/gitlab-org/gitlab/-/issues/421821')\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5246"
  },
  {
    "raw_code": "def user_scope\n        User.includes(:notification_settings)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5247"
  },
  {
    "raw_code": "def make_recipient(user, type, reason)\n        NotificationRecipient.new(\n          user, type,\n          reason: reason,\n          project: project,\n          group: group,\n          custom_action: custom_action,\n          target: recipients_target,\n          acting_user: acting_user\n        )\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5248"
  },
  {
    "raw_code": "def add_custom_notifications\n        notification_by_sources = related_notification_settings_sources(:custom)\n\n        return if notification_by_sources.blank?\n\n        user_ids = NotificationSetting.from_union(notification_by_sources).select(:user_id)\n\n        add_recipients(user_scope.where(id: user_ids), :custom, nil)\n      end",
    "comment": "Get project/group users with CUSTOM notification level rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5249"
  },
  {
    "raw_code": "def add_project_watchers\n        add_recipients(project_watchers, :watch, nil) if project\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5250"
  },
  {
    "raw_code": "def project_watchers\n        notification_by_sources = related_notification_settings_sources(:watch)\n\n        return if notification_by_sources.blank?\n\n        user_ids = NotificationSetting.from_union(notification_by_sources).select(:user_id)\n\n        user_scope.where(id: user_ids)\n      end",
    "comment": "Get project users with WATCH notification level rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5251"
  },
  {
    "raw_code": "def group_watchers\n        return [] unless group\n\n        user_ids = group\n          .notification_settings\n          .where(source_or_global_setting_by_level_query(:watch)).select(:user_id)\n\n        user_scope.where(id: user_ids)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5252"
  },
  {
    "raw_code": "def add_subscribed_users\n        return unless target.respond_to? :subscribers\n\n        add_recipients(target.subscribers(project), :subscription, NotificationReason::SUBSCRIBED)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5253"
  },
  {
    "raw_code": "def custom_action\n        :new_note\n      end",
    "comment": "A new review is a batch of new notes therefore new_note subscribers should also receive incoming new reviews",
    "label": "",
    "id": "5254"
  },
  {
    "raw_code": "def custom_action\n        return @custom_action unless @custom_action.nil?\n\n        target_name = if target.respond_to?(:custom_notification_target_name)\n                        target.custom_notification_target_name\n                      else\n                        target.class.model_name.name.underscore\n                      end",
    "comment": "Build event key to search on custom notification level Check NotificationSetting.email_events",
    "label": "",
    "id": "5255"
  },
  {
    "raw_code": "def execute(draft = nil)\n      drafts = draft || draft_notes\n\n      clear_highlight_diffs_cache(Array.wrap(drafts))\n\n      drafts.is_a?(DraftNote) ? drafts.destroy! : drafts.delete_all\n\n      after_execute\n    end",
    "comment": "If no `draft` is given it fallsback to all draft notes of the given merge request and user.",
    "label": "",
    "id": "5256"
  },
  {
    "raw_code": "def run_actions(actions, skip_system_notes: false)\n      raise NoActions if actions.empty?\n\n      sha = repository.commit_files(\n        current_user,\n        branch_name: target_branch,\n        message: commit_message,\n        actions: actions.map(&:gitaly_action)\n      )\n\n      DesignManagement::Version\n        .create_for_designs(actions, sha, current_user)\n        .tap { |version| post_process(version, skip_system_notes) }\n    end",
    "comment": "This concern requires the following methods to be implemented: current_user, target_branch, repository, commit_message  Before calling `run_actions`, you should ensure the repository exists, by calling `repository.create_if_not_exists`.  @raise [NoActions] if actions are empty @return [DesignManagement::Version]",
    "label": "",
    "id": "5257"
  },
  {
    "raw_code": "def get_raw_file(action)\n      raw_files_by_path[action.design.full_path]\n    end",
    "comment": "Returns the `CarrierWave::SanitizedFile` of the original design file",
    "label": "",
    "id": "5258"
  },
  {
    "raw_code": "def raw_files_by_path\n      @raw_files_by_path ||= LfsObject.for_oids(blobs_by_oid.keys).each_with_object({}) do |lfs_object, h|\n        blob = blobs_by_oid[lfs_object.oid]\n        file = lfs_object.file.file\n        # The `CarrierWave::SanitizedFile` is loaded without knowing the `content_type`\n        # of the file, due to the file not having an extension.\n        #\n        # Set the content_type from the `Blob`.\n        file.content_type = blob.content_type\n        h[blob.path] = file\n      end",
    "comment": "Returns the `Carrierwave:SanitizedFile` instances for all of the original design files, mapping to { design.filename => `Carrierwave::SanitizedFile` }.  As design files are stored in Git LFS, the only way to retrieve their original files is to first fetch the LFS pointer file data from the Git design repository. The LFS pointer file data contains an \"OID\" that lets us retrieve `LfsObject` records, which have an Uploader (`LfsObjectUploader`) for the original design file.",
    "label": "",
    "id": "5259"
  },
  {
    "raw_code": "def blobs_by_oid\n      @blobs ||= begin\n        items = version.designs.map { |design| [version.sha, design.full_path] }\n        blobs = repository.blobs_at(items)\n        blobs.reject! { |blob| blob.lfs_size > MAX_DESIGN_SIZE }\n        blobs.index_by(&:lfs_oid)\n      end",
    "comment": "Returns the `Blob`s that correspond to the design files in the repository.  All design `Blob`s are LFS Pointer files, and are therefore small amounts of data to load.  `Blob`s whose size are above a certain threshold: `MAX_DESIGN_SIZE` are filtered out.",
    "label": "",
    "id": "5260"
  },
  {
    "raw_code": "def initialize(user, params)\n      super(nil, user, params.merge(issue: nil))\n    end",
    "comment": "@param user [User] The current user @param [Hash] params @option params [DesignManagement::Design] :current_design @option params [DesignManagement::Design] :previous_design (nil) @option params [DesignManagement::Design] :next_design (nil)",
    "label": "",
    "id": "5261"
  },
  {
    "raw_code": "def designs\n      @designs ||= files.map do |file|\n        collection.find_or_create_design!(filename: file.original_filename)\n      end",
    "comment": "Returns `Design` instances that correspond with `files`. New `Design`s will be created where a file name does not match an existing `Design`",
    "label": "",
    "id": "5262"
  },
  {
    "raw_code": "def design_unchanged?(design, content)\n      content == existing_blobs[design]&.data\n    end",
    "comment": "Returns true if the design file is the same as its latest version",
    "label": "",
    "id": "5263"
  },
  {
    "raw_code": "def existing_blobs\n      @existing_blobs ||= begin\n        items = designs.map { |d| [target_branch, d.full_path] }\n\n        repository.blobs_at(items).each_with_object({}) do |blob, h|\n          design = designs.find { |d| d.full_path == blob.path }\n\n          h[design] = blob\n        end",
    "comment": "Returns the latest blobs for the designs as a Hash of `{ Design => Blob }`",
    "label": "",
    "id": "5264"
  },
  {
    "raw_code": "def initialize(project, user, params = {})\n        super\n\n        @target_issue = params.fetch(:target_issue)\n        @target_project = @target_issue.project\n        @target_repository = @target_project.design_repository\n        @target_design_collection = @target_issue.design_collection\n        @temporary_branch = \"CopyDesignCollectionService_#{SecureRandom.hex}\"\n        # The user who triggered the copy may not have permissions to push\n        # to the design repository.\n        @git_user = @target_project.first_owner\n\n        @designs = DesignManagement::Design.unscoped.where(issue: issue).order(:id).load\n        @versions = DesignManagement::Version.unscoped.where(issue: issue).order(:id).includes(:designs).load\n\n        @sha_attribute = Gitlab::Database::ShaAttribute.new\n        @shas = []\n        @event_enum_map = DesignManagement::DesignAction::EVENT_FOR_GITALY_ACTION.invert\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5265"
  },
  {
    "raw_code": "def execute\n        return error('User cannot copy design collection to issue') unless user_can_copy?\n        return error('Target design collection must first be queued') unless target_design_collection.copy_in_progress?\n        return error('Design collection has no designs') if designs.empty?\n        return error('Target design collection already has designs') unless target_design_collection.empty?\n\n        with_temporary_branch do\n          copy_commits!\n\n          ApplicationRecord.transaction do\n            design_ids = copy_designs!\n            version_ids = copy_versions!\n            copy_actions!(design_ids, version_ids)\n            link_lfs_files!\n            copy_notes!(design_ids)\n            finalize!\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5266"
  },
  {
    "raw_code": "def target_branch\n        target_repository.root_ref || Gitlab::DefaultBranch.value(object: target_project)\n      end",
    "comment": "Designs are being copied over to a target repository. Target repository's default branch can be different from source repository. E.g. we have some old projects where `master` is still the default branch, whereas newer projects have `main` as repository default branch.  So we need to make sure we lookup the correct merge branch based on target repository",
    "label": "",
    "id": "5267"
  },
  {
    "raw_code": "def create_default_branch!\n        target_repository.create_file(\n          git_user,\n          \".CopyDesignCollectionService_#{Time.now.to_i}\",\n          '.gitlab',\n          message: \"Commit to create #{merge_branch} branch in CopyDesignCollectionService\",\n          branch_name: merge_branch\n        )\n      end",
    "comment": "A project that does not have any designs will have a blank design repository. To create a temporary branch from default branch we need to create default branch first by adding a file to it.",
    "label": "",
    "id": "5268"
  },
  {
    "raw_code": "def finalize!\n        source_sha = shas.last\n\n        target_repository.raw.merge(\n          git_user,\n          source_sha: source_sha,\n          target_branch: merge_branch,\n          message: 'CopyDesignCollectionService finalize merge'\n        ) { nil }\n\n        target_design_collection.end_copy!\n      end",
    "comment": "Merge the temporary branch containing the commits to default branch and update the state of the target_design_collection.",
    "label": "",
    "id": "5269"
  },
  {
    "raw_code": "def copy_commits!\n        # Execute another query to include actions and their designs\n        DesignManagement::Version.unscoped.where(id: versions).order(:id).includes(actions: :design).find_each(batch_size: 100) do |version|\n          gitaly_actions = version.actions.map do |action|\n            design = action.design\n            # Map the raw Action#event enum value to a Gitaly \"action\" for the\n            # `Repository#commit_files` call.\n            gitaly_action_name = @event_enum_map[action.event_before_type_cast]\n            # `content` will be the LfsPointer file and not the design file,\n            # and can be nil for deletions.\n            content = blobs.dig(version.sha, design.filename)&.data\n            file_path = DesignManagement::Design.build_full_path(target_issue, design)\n\n            {\n              action: gitaly_action_name,\n              file_path: file_path,\n              content: content\n            }.compact\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5270"
  },
  {
    "raw_code": "def copy_designs!\n        design_attributes = attributes_config[:design_attributes]\n\n        DesignManagement::Design.with_project_iid_supply(target_project) do |supply|\n          new_rows = designs.each_with_index.map do |design, i|\n            design.attributes.slice(*design_attributes).merge(\n              issue_id: target_issue.id,\n              project_id: target_project.id,\n              iid: supply.next_value\n            )\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5271"
  },
  {
    "raw_code": "def copy_actions!(new_design_ids, new_version_ids)\n        # Create a map of <Old design id> => <New design id>\n        design_id_map = new_design_ids.each_with_index.to_h do |design_id, i|\n          [designs[i].id, design_id]\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5272"
  },
  {
    "raw_code": "def commit_message(version)\n        \"Copy commit #{version.sha} from issue #{issue.to_reference(full: true)}\"\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5273"
  },
  {
    "raw_code": "def copy_notes!(design_ids)\n        new_designs = DesignManagement::Design.unscoped.find(design_ids)\n\n        # Execute another query to filter only designs with notes\n        DesignManagement::Design.unscoped.where(id: designs).joins(:notes).distinct.find_each(batch_size: 100) do |old_design|\n          new_design = new_designs.find { |d| d.filename == old_design.filename }\n\n          Notes::CopyService.new(current_user, old_design, new_design).execute\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5274"
  },
  {
    "raw_code": "def link_lfs_files!\n        oids = blobs.values.flat_map(&:values).map(&:lfs_oid)\n        repository_type = LfsObjectsProject.repository_types[:design]\n\n        lfs_objects = oids.each_slice(1000).flat_map do |oids_batch|\n          LfsObject.for_oids(oids_batch).not_linked_to_project(target_project, repository_type: repository_type)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5275"
  },
  {
    "raw_code": "def blobs\n        @blobs ||= begin\n          items = versions.flat_map { |v| v.designs.map { |d| [v.sha, DesignManagement::Design.build_full_path(issue, d)] } }\n\n          repository.blobs_at(items).each_with_object({}) do |blob, h|\n            design = designs.find { |d| DesignManagement::Design.build_full_path(issue, d) == blob.path }\n\n            h[blob.commit_id] ||= {}\n            h[blob.commit_id][design.filename] = blob\n          end",
    "comment": "Blob data is used to find the oids for LfsObjects and to copy to Git. Blobs are reasonably small in memory, as their data are LFS Pointer files.  Returns all blobs for the designs as a Hash of `{ Blob#commit_id => { Design#filename => Blob } }`",
    "label": "",
    "id": "5276"
  },
  {
    "raw_code": "def validate_membership_status\n      ServiceResponse.success\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5277"
  },
  {
    "raw_code": "def initialize(current_user, params)\n      @current_user = current_user\n      @params = params\n    end",
    "comment": "@param [User] current_user @param [Hash] params @option params [String] bitbucket_username - Bitbucket Cloud username @option params [String] bitbucket_app_password - Bitbucket Cloud user app password",
    "label": "",
    "id": "5278"
  },
  {
    "raw_code": "def execute\n      unless authorized?\n        return log_and_return_error(\"You don't have permissions to import this project\", :unauthorized)\n      end",
    "comment": "rubocop:disable Style/IfUnlessModifier -- line becomes too long",
    "label": "",
    "id": "5279"
  },
  {
    "raw_code": "def initialize(current_user, params:, file_acquisition_strategy: FileAcquisitionStrategies::FileUpload)\n        @current_user = current_user\n        @params = params.dup\n        @strategy = file_acquisition_strategy.new(current_user: current_user, params: params)\n      end",
    "comment": "Creates a new CreateProjectService.  @param [User] current_user @param [Hash] :params @param [Import::GitlabProjects::FileAcquisitionStrategies::*] :file_acquisition_strategy",
    "label": "",
    "id": "5280"
  },
  {
    "raw_code": "def execute\n        return error(errors.full_messages) unless valid?\n        return error(project.errors.full_messages) unless project.saved?\n\n        success(project)\n      rescue StandardError => e\n        error(e.message)\n      end",
    "comment": "Creates a project with the strategy parameters  @return [Services::ServiceResponse]",
    "label": "",
    "id": "5281"
  },
  {
    "raw_code": "def valid?\n        super && strategy.valid?\n      end",
    "comment": "Cascade the validation to strategy",
    "label": "",
    "id": "5282"
  },
  {
    "raw_code": "def errors\n        super.tap { _1.merge!(strategy.errors) }\n      end",
    "comment": "Merge with strategy's errors",
    "label": "",
    "id": "5283"
  },
  {
    "raw_code": "def read_attribute_for_validation(key)\n          return file_url if key == :file_url\n\n          params[key]\n        end",
    "comment": "Make the validated params/methods accessible",
    "label": "",
    "id": "5284"
  },
  {
    "raw_code": "def quick_actions_supported?(_note)\n          false\n        end",
    "comment": "Github does not have support to quick actions in notes (like /assign) Therefore, when importing notes we skip the quick actions processing",
    "label": "",
    "id": "5285"
  },
  {
    "raw_code": "def initialize(namespace, current_user:)\n        @namespace = namespace\n        @current_user = current_user\n      end",
    "comment": "@param namespace [Namespace, Group] The namespace where the import source users are associated @param current_user [User] The user performing the CSV export",
    "label": "",
    "id": "5286"
  },
  {
    "raw_code": "def async_execute\n        return ServiceResponse.error(message: csv_validator.formatted_errors) unless csv_validator.valid?\n\n        Import::UserMapping::AssignmentFromCsvWorker.perform_async(\n          current_user.id,\n          namespace.id,\n          upload.id\n        )\n\n        ServiceResponse.success\n      end",
    "comment": "@return [ServiceResponse]",
    "label": "",
    "id": "5287"
  },
  {
    "raw_code": "def execute\n        return ServiceResponse.error(message: :invalid_csv_format) unless csv_validator.valid?\n\n        process_csv\n\n        ServiceResponse.success(payload: {\n          stats: reassignment_stats,\n          failures_csv_data: failure_csv\n        })\n      end",
    "comment": "@return [ServiceResponse]",
    "label": "",
    "id": "5288"
  },
  {
    "raw_code": "def run_validations\n        return error_invalid_permissions unless current_user.can?(:admin_import_source_user, import_source_user)\n        return error_namespace_type if root_namespace.user_namespace?\n\n        error_invalid_assignee unless valid_assignee?\n      end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5289"
  },
  {
    "raw_code": "def enterprise_skip_reassignment_confirmation?\n        false\n      end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- method is tested in EE :nocov: Overridden in EE",
    "label": "",
    "id": "5290"
  },
  {
    "raw_code": "def initialize(data)\n        @data = data\n      end",
    "comment": "@param data [Array<Hash>] An array of hashes to be converted into CSV rows. Hash keys must match the values of COLUMN_MAPPING.",
    "label": "",
    "id": "5291"
  },
  {
    "raw_code": "def update_params\n        placeholder_creator = Gitlab::Import::PlaceholderUserCreator.new(import_source_user)\n\n        update_params = {}\n        update_params[:name] = placeholder_creator.placeholder_name if params[:source_name]\n\n        if params[:source_username]\n          update_params[:username] = placeholder_creator.send(:username_and_email_generator).username # rubocop:disable GitlabSecurity/PublicSend -- Safe to call, we don't want to publically expose this method.\n        end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5292"
  },
  {
    "raw_code": "def add_error_for_member(member, existing_errors)\n      prefix = \"#{member.user.username}: \" if member.user.present?\n\n      errors << \"#{prefix}#{all_member_errors(member, existing_errors).to_sentence}\"\n    end",
    "comment": "overridden",
    "label": "",
    "id": "5293"
  },
  {
    "raw_code": "def add_members(sources, invitees, access_level, **args)\n        return [] unless invitees.present?\n\n        sources = Array.wrap(sources) if sources.is_a?(ApplicationRecord) # For single source\n\n        Gitlab::Database::QueryAnalyzers::PreventCrossDatabaseModification.temporary_ignore_tables_in_transaction(\n          %w[users user_preferences user_details emails identities], url: 'https://gitlab.com/gitlab-org/gitlab/-/issues/424276'\n        ) do\n          Member.transaction do\n            sources.flat_map do |source|\n              # If this user is attempting to manage Owner members and doesn't have permission, do not allow\n              current_user = args[:current_user]\n              next [] if managing_owners?(current_user, access_level) && cannot_manage_owners?(source, current_user)\n\n              emails, users, existing_members, users_by_emails = parse_users_list(source, invitees)\n\n              common_arguments = {\n                source: source,\n                access_level: access_level,\n                existing_members: existing_members,\n                users_by_emails: users_by_emails\n              }.merge(parsed_args(args))\n\n              build_members(emails, users, common_arguments)\n            end",
    "comment": "Add members to sources with passed access option  access can be an integer representing a access code or symbol like :maintainer representing role  Ex. add_members( sources, user_ids, Member::MAINTAINER )  add_members( sources, user_ids, :maintainer )  @param sources [Group, Project, Array<Group>, Array<Project>, Group::ActiveRecord_Relation, Project::ActiveRecord_Relation] - Can't be an array of source ids because we don't know the type of source. @return Array<Member>",
    "label": "",
    "id": "5294"
  },
  {
    "raw_code": "def member_attributes\n      {\n        created_by: member.created_by || current_user,\n        access_level: access_level,\n        expires_at: args[:expires_at],\n        importing: skip_authorization? && source&.importing?\n      }\n    end",
    "comment": "Populates the attributes of a member.  This logic resides in a separate method so that EE can extend this logic, without having to patch the `add_members` method directly.",
    "label": "",
    "id": "5295"
  },
  {
    "raw_code": "def mark_as_recursive_call\n      @recursive_call = true\n    end",
    "comment": "We use this to mark recursive calls made to this service from within the same service. We do this so as to help us run some tasks that needs to be run only once per hierarchy, and not recursively.",
    "label": "",
    "id": "5296"
  },
  {
    "raw_code": "def enqueue_jobs_that_needs_to_be_run_only_once_per_hierarchy(member, unassign_issuables)\n      return if recursive_call?\n\n      enqueue_cleanup_jobs_once_per_hierarchy(member, unassign_issuables)\n    end",
    "comment": "These actions need to be executed only once per hierarchy because the underlying services apply these actions to the entire hierarchy anyway, so there is no need to execute them recursively.",
    "label": "",
    "id": "5297"
  },
  {
    "raw_code": "def destroy_group_member_permission(_member)\n      :destroy_group_member\n    end",
    "comment": "overridden in EE::Members::DestroyService",
    "label": "",
    "id": "5298"
  },
  {
    "raw_code": "def initialize(current_user = nil, params = {})\n      @current_user = current_user\n      @params = params\n\n      # could be a string, force to an integer, part of fix\n      # https://gitlab.com/gitlab-org/gitlab/-/issues/219496\n      # Allow the ArgumentError to be raised if it can't be converted to an integer.\n      @params[:access_level] = Integer(@params[:access_level]) if @params[:access_level]\n    end",
    "comment": "current_user - The user that performs the action params - A hash of parameters",
    "label": "",
    "id": "5299"
  },
  {
    "raw_code": "def initialize(user, entity, requesting_user)\n      @user = user\n      @requesting_user = requesting_user\n      @entity = entity\n    end",
    "comment": "@param [User] user user whose membership is being deleted from entity @param [Group, Project] entity @param [User] requesting_user user who initiated the membership deletion of `user`",
    "label": "",
    "id": "5300"
  },
  {
    "raw_code": "def execute(members, permission: :update)\n      validate_source_type!\n\n      members = Array.wrap(members)\n\n      old_values_map = members.to_h do |member|\n        [member.id, build_old_values_map(member)]\n      end",
    "comment": "@param members [Member, Array<Member>] returns the updated member(s)",
    "label": "",
    "id": "5301"
  },
  {
    "raw_code": "def available_commands(quick_action_target)\n      @quick_action_target = quick_action_target\n\n      self.class.command_definitions.map do |definition|\n        next unless definition.available?(self)\n\n        definition.to_h(self)\n      end.compact\n    end",
    "comment": "Takes an quick_action_target and returns an array of all the available commands represented with .to_h",
    "label": "",
    "id": "5302"
  },
  {
    "raw_code": "def execute(content, quick_action_target, only: nil)\n      return [content, {}, ''] unless current_user.can?(:use_quick_actions)\n\n      @quick_action_target = quick_action_target\n      @updates = {}\n      @execution_message = {}\n\n      content, commands = extractor.extract_commands(content, only: only)\n      extract_updates(commands)\n\n      [content, @updates, execution_messages_for(commands), command_names(commands)]\n    end",
    "comment": "IMPORTANT: unsafe! Use `execute_with_original_text` instead as it handles cleanup of any residual quick actions left in the original description.  Takes a text and interprets the commands that are extracted from it. Returns the content without commands, a hash of changes to be applied to a record and a string containing the execution_message to show to the user.",
    "label": "",
    "id": "5303"
  },
  {
    "raw_code": "def execute_with_original_text(new_text, quick_action_target, only: nil, original_text: nil)\n      sanitized_new_text, new_command_params, execution_messages, command_names = execute(\n        new_text, quick_action_target, only: only\n      )\n\n      if original_text\n        _, original_command_params = self.class.new(\n          container: container,\n          current_user: current_user,\n          params: params\n        ).execute(original_text, quick_action_target, only: only)\n\n        new_command_params = (new_command_params.to_a - original_command_params.to_a).to_h if original_command_params\n      end",
    "comment": "Similar to `execute` except also tries to extract any quick actions from original_text, and if found removes them from the main list of quick actions.",
    "label": "",
    "id": "5304"
  },
  {
    "raw_code": "def explain(content, quick_action_target, keep_actions: false)\n      return [content, []] unless current_user.can?(:use_quick_actions)\n\n      @quick_action_target = quick_action_target\n\n      content, commands = extractor(keep_actions).extract_commands(content)\n      commands = explain_commands(commands)\n      [content, commands]\n    end",
    "comment": "Takes a text and interprets the commands that are extracted from it. Returns the content without commands, and array of changes explained. `keep_actions: true` will keep the quick actions in the content.",
    "label": "",
    "id": "5305"
  },
  {
    "raw_code": "def extract_users(params)\n      Gitlab::QuickActions::UsersExtractor\n        .new(current_user, project: project, group: group, target: quick_action_target, text: params)\n        .execute\n\n    rescue Gitlab::QuickActions::UsersExtractor::Error => err\n      extract_users_failed(err)\n    end",
    "comment": "Find users for commands like /assign  eg. /assign me and @jane and jack",
    "label": "",
    "id": "5306"
  },
  {
    "raw_code": "def extract_references(arg, type)\n      return [] unless arg\n\n      ext = Gitlab::ReferenceExtractor.new(project, current_user)\n\n      ext.analyze(arg, author: current_user, group: group)\n\n      ext.references(type)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5307"
  },
  {
    "raw_code": "def usage_ping_tracking(quick_action_name, arg)\n      # Need to add this guard clause as `duo_code_review` quick action will fail\n      # if we continue to track its usage. This is because we don't have a metric\n      # for it and this is something that can change soon (e.g. quick action may\n      # be replaced by a UI component).\n      return if quick_action_name == :duo_code_review\n\n      Gitlab::UsageDataCounters::QuickActionActivityUniqueCounter.track_unique_action(\n        quick_action_name.to_s,\n        args: arg&.strip,\n        user: current_user,\n        project: project\n      )\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5308"
  },
  {
    "raw_code": "def work_item(type_iid)\n      if type_iid.blank?\n        parent = group_container? ? { namespace: group } : { project: project, namespace: project.project_namespace }\n        return WorkItem.new(\n          work_item_type_id: params[:work_item_type_id] || WorkItems::Type.default_issue_type.id,\n          **parent\n        )\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5309"
  },
  {
    "raw_code": "def issue(type_iid)\n      return container.issues.build if type_iid.nil?\n\n      IssuesFinder.new(current_user, **parent_params).find_by(iid: type_iid) || container.issues.build\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5310"
  },
  {
    "raw_code": "def merge_request(type_iid)\n      return project.merge_requests.build if type_iid.nil?\n\n      MergeRequestsFinder.new(current_user, project_id: project.id).find_by(iid: type_iid) || project.merge_requests.build\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5311"
  },
  {
    "raw_code": "def commit(type_iid)\n      project.commit(type_iid)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5312"
  },
  {
    "raw_code": "def source_object(id)\n      source_model.find_by(source_sort_key => id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5313"
  },
  {
    "raw_code": "def create_sync_event_for(id)\n      if source_model == Namespace\n        sync_event_class.create!(namespace_id: id)\n      elsif source_model == Project\n        sync_event_class.create!(project_id: id)\n      else\n        raise(\"Unknown Source Model #{source_model.name}\")\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5314"
  },
  {
    "raw_code": "def execute\n      start_id = next_start_id\n\n      return EMPTY_RESULT if start_id.nil?\n\n      result = consistency_checker.execute(start_id: start_id)\n      result[:start_id] = start_id\n\n      save_next_start_id(result[:next_start_id])\n\n      result\n    end",
    "comment": "This class takes two ActiveRecord models, and compares the selected columns of the two models tables, for the purposes of checking the consistency of mirroring of tables. For example Namespace and Ci::NamepaceMirror  It compares up to 25 batches (1000 records / batch), or up to 30 seconds for all the batches in total.  It saves the cursor of the next start_id (cursor) in Redis. If the start_id wasn't saved in Redis, for example, in the first run, it will choose some random start_id  Example: service = Database::ConsistencyCheckService.new( source_model: Namespace, target_model: Ci::NamespaceMirror, source_columns: %w[id traversal_ids], target_columns: %w[namespace_id traversal_ids], ) result = service.execute  result is a hash that has the following fields: - batches: Number of batches checked - matches: The number of matched records - mismatches: The number of mismatched records - mismatches_details: It's an array that contains details about the mismatched records. each record in this array is a hash of format {id: ID, source_table: [...], target_table: [...]} Each record represents the attributes of the records in the two tables. - start_id: The start id cursor of the current batch. <nil> means no records. - next_start_id: The ID that can be used for the next batch iteration check. <nil> means no records",
    "label": "",
    "id": "5315"
  },
  {
    "raw_code": "def random_start_id\n      range_start = min_id\n      range_end = [min_id, max_id - Gitlab::Database::ConsistencyChecker::BATCH_SIZE].max\n      rand(range_start..range_end)\n    end",
    "comment": "This returns some random start_id, so that we don't always start checking from the start of the table, in case we lose the cursor in Redis.",
    "label": "",
    "id": "5316"
  },
  {
    "raw_code": "def relate_issuables(referenced_issuable)\n      link = link_class.find_or_initialize_by(source: issuable, target: referenced_issuable)\n\n      set_link_type(link)\n\n      if link.changed? && link.save\n        new_links << link\n        create_notes(link)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5317"
  },
  {
    "raw_code": "def track_event\n      # no-op\n    end",
    "comment": "Override on child classes to perform actions when the service is executed.",
    "label": "",
    "id": "5318"
  },
  {
    "raw_code": "def after_create_for(_link)\n      # no-op\n    end",
    "comment": "Override on child classes to perform actions for each object created.",
    "label": "",
    "id": "5319"
  },
  {
    "raw_code": "def milestones_to_transfer\n      Milestone.from_union([group_milestones_applied_to_issues, group_milestones_applied_to_merge_requests])\n        .without_order\n        .distinct\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5320"
  },
  {
    "raw_code": "def group_milestones_applied_to_issues\n      Milestone.joins(:issues)\n        .where(\n          issues: { project_id: project.id },\n          group_id: old_group.self_and_ancestors\n        )\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5321"
  },
  {
    "raw_code": "def group_milestones_applied_to_merge_requests\n      Milestone.joins(:merge_requests)\n        .where(\n          merge_requests: { target_project_id: project.id },\n          group_id: old_group.self_and_ancestors\n        )\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5322"
  },
  {
    "raw_code": "def find_or_create_milestone(milestone)\n      params = milestone.attributes.slice('title', 'description', 'start_date', 'due_date', 'state')\n\n      FindOrCreateService.new(project, current_user, params).execute\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5323"
  },
  {
    "raw_code": "def update_issues_milestone(old_milestone, new_milestone)\n      Issue.where(project: project, milestone_id: old_milestone.id)\n        .update_all(milestone_id: new_milestone&.id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5324"
  },
  {
    "raw_code": "def update_merge_requests_milestone(old_milestone_id, new_milestone_id)\n      MergeRequest.where(project_id: project.id, milestone_id: old_milestone_id)\n        .update_all(milestone_id: new_milestone_id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5325"
  },
  {
    "raw_code": "def delete_milestone_counts_caches(milestone)\n      return unless milestone\n\n      Milestones::IssuesCountService.new(milestone).delete_cache\n      Milestones::ClosedIssuesCountService.new(milestone).delete_cache\n      Milestones::MergeRequestsCountService.new(milestone).delete_cache\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5326"
  },
  {
    "raw_code": "def update_children(group_milestone, milestone_ids)\n      issues = Issue.where(project_id: group_project_ids, milestone_id: milestone_ids)\n      merge_requests = MergeRequest.where(source_project_id: group_project_ids, milestone_id: milestone_ids)\n      milestone_events = ResourceMilestoneEvent.where(milestone_id: milestone_ids)\n\n      [issues, merge_requests, milestone_events].each do |collection|\n        collection.each_batch do |batch|\n          batch.update_all(milestone_id: group_milestone.id)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5327"
  },
  {
    "raw_code": "def group\n      @group ||= parent.group || raise_error(s_('PromoteMilestone|Project does not belong to a group.'))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5328"
  },
  {
    "raw_code": "def destroy_old_milestones(milestone)\n      Milestone.where(id: milestone_ids_for_merge(milestone)).destroy_all # rubocop: disable Cop/DestroyAll\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5329"
  },
  {
    "raw_code": "def group_project_ids\n      group.projects.select(:id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5330"
  },
  {
    "raw_code": "def find_milestone\n      groups = project.group&.self_and_ancestors_ids\n      Milestone.for_projects_and_groups([project.id], groups).find_by(title: params[\"title\"])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5331"
  },
  {
    "raw_code": "def create_milestone\n      return unless current_user.can?(:admin_milestone, project)\n\n      new_milestone if new_milestone.persisted?\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5332"
  },
  {
    "raw_code": "def access_levels_attributes(access_levels, access_levels_params)\n      attributes = access_levels.filter_map do |access_level|\n        next if remove_matched_access_level_params!(access_levels_params, access_level)\n\n        # access levels that do not have matching params are marked for deletion\n        { id: access_level.id, _destroy: true }\n      end",
    "comment": "In ProtectedBranch we are using:  `accepts_nested_attributes_for :{type}_access_levels, allow_destroy: true`  This branch rule update service acts like we have defined this `accepts_nested_attributes_for` with `update: true`.  Unfortunately we are unable to modify the `accepts_nested_attributes_for` config as we use this logic in other locations. As we are reusing the ProtectedBranches::UpdateService we also can't custom write the logic to persist the access levels manually.  For now the best solution appears to be matching the params against the existing levels to check which access levels still exist and marking unmatched access levels for destruction.  Given the following: access_levels = [{ id: 1, access_level: 30 }, { id: 2, user_id: 1 }, { id: 3, group_id: 1 }] access_levels_params = [{ access_level: 30 }, { user_id: 1 }, { deploy_key_id: 1 }]  The output should be: [{ id: 3, _destroy: true }, { deploy_key_id: 1 }]  NOTE: :user_id and :group_id are only available in EE. ",
    "label": "",
    "id": "5333"
  },
  {
    "raw_code": "def object_from_id(global_id, ctx = {})\n      gid = parse_gid(global_id, ctx)\n\n      find_by_gid(gid)\n    end",
    "comment": "Find an object by looking it up from its global ID, passed as a string.  This is the composition of 'parse_gid' and 'find_by_gid', see these methods for further documentation.",
    "label": "",
    "id": "5334"
  },
  {
    "raw_code": "def find_by_gid(gid)\n      return unless gid\n\n      if gid.model_class < ApplicationRecord\n        Gitlab::Graphql::Loaders::BatchModelLoader.new(gid.model_class, gid.model_id).find\n      elsif gid.model_class.respond_to?(:lazy_find)\n        gid.model_class.lazy_find(gid.model_id)\n      else\n        begin\n          gid.find\n        # other if conditions return nil when the record is not found\n        rescue ActiveRecord::RecordNotFound\n          nil\n        end",
    "comment": "Find an object by looking it up from its 'GlobalID'.  * For `ApplicationRecord`s, this is equivalent to `global_id.model_class.find(gid.model_id)`, but more efficient. * For classes that implement `.lazy_find(global_id)`, this class method will be called. * All other classes will use `GlobalID#find`",
    "label": "",
    "id": "5335"
  },
  {
    "raw_code": "def parse_gid(global_id, ctx = {})\n      expected_types = Array(ctx[:expected_type])\n      gid = GlobalID.parse(global_id)\n\n      raise Gitlab::Graphql::Errors::ArgumentError, \"#{global_id} is not a valid GitLab ID.\" unless gid\n\n      if expected_types.any? && expected_types.none? { |type| gid.model_class.ancestors.include?(type) }\n        vars = { global_id: global_id, expected_types: expected_types.join(', ') }\n        msg = _('%{global_id} is not a valid ID for %{expected_types}.') % vars\n        raise Gitlab::Graphql::Errors::ArgumentError, msg\n      end",
    "comment": "Parse a string to a GlobalID, raising ArgumentError if there are problems with it.  Problems that may occur: * it may not be syntactically valid * it may not match the expected type (see below)  Options: * :expected_type [Class] - the type of object this GlobalID should refer to. * :expected_type [[Class]] - array of the types of object this GlobalID should refer to.  e.g.  ``` gid = GitlabSchema.parse_gid(my_string, expected_type: ::Project) project_id = gid.model_id gid.model_class == ::Project ```",
    "label": "",
    "id": "5336"
  },
  {
    "raw_code": "def parse_gids(global_ids, ctx = {})\n      global_ids.map { |gid| parse_gid(gid, ctx) }\n    end",
    "comment": "Parse an array of strings to an array of GlobalIDs, raising ArgumentError if there are problems with it. See #parse_gid  ``` gids = GitlabSchema.parse_gids(my_array_of_strings, expected_type: ::Project) project_ids = gids.map(&:model_id) gids.all? { |gid| gid.model_class == ::Project } ```",
    "label": "",
    "id": "5337"
  },
  {
    "raw_code": "def errors_on_object(record)\n      record.errors.full_messages\n    end",
    "comment": "Returns Array of errors on an ActiveRecord object",
    "label": "",
    "id": "5338"
  },
  {
    "raw_code": "def self.authorization\n      @authorization ||= ::Gitlab::Graphql::Authorize::ObjectAuthorization.new(authorize, authorization_scopes)\n    end",
    "comment": "See: AuthorizeResource#authorized_resource?",
    "label": "",
    "id": "5339"
  },
  {
    "raw_code": "def process_args_for_params!(args)\n        convert_blob_actions_to_snippet_actions!(args)\n\n        # We need to rename `uploaded_files` into `files` because\n        # it's the expected key param\n        args[:files] = args.delete(:uploaded_files)\n        args[:organization_id] = Current.organization.id\n        # Return nil to make it explicit that this method is mutating the args parameter, and that\n        # the return value is not relevant and is not to be used.\n        nil\n      end",
    "comment": "process_args_for_params!(args)    -> nil  Modifies/adds/deletes mutation resolve args as necessary to be passed as params to service layer.",
    "label": "",
    "id": "5340"
  },
  {
    "raw_code": "def process_args_for_params!(args)\n        convert_blob_actions_to_snippet_actions!(args)\n\n        # Return nil to make it explicit that this method is mutating the args parameter, and that\n        # the return value is not relevant and is not to be used.\n        nil\n      end",
    "comment": "process_args_for_params!(args)    -> nil  Modifies/adds/deletes mutation resolve args as necessary to be passed as params to service layer.",
    "label": "",
    "id": "5341"
  },
  {
    "raw_code": "def convert_blob_actions_to_snippet_actions!(args)\n        # We need to rename `blob_actions` into `snippet_actions` because\n        # it's the expected key param\n        args[:snippet_actions] = args.delete(:blob_actions)&.map(&:to_h)\n\n        # Return nil to make it explicit that this method is mutating the args parameter\n        nil\n      end",
    "comment": "convert_blob_actions_to_snippet_actions!(args)    -> nil  Converts the blob_actions mutation argument into the snippet_actions hash which the service layer expects",
    "label": "",
    "id": "5342"
  },
  {
    "raw_code": "def http_integration_params(_project, args)\n          args.slice(:name, :active, :type_identifier)\n        end",
    "comment": "overriden in EE",
    "label": "",
    "id": "5343"
  },
  {
    "raw_code": "def authorized_find_discussion!(id:)\n        find_object(id: id).tap do |discussion|\n          raise_resource_not_available_error! unless discussion&.can_resolve?(current_user)\n        end",
    "comment": "`Discussion` permissions are checked through `Discussion#can_resolve?`, so we use this method of checking permissions rather than by defining an `authorize` permission and calling `authorized_find!`.",
    "label": "",
    "id": "5344"
  },
  {
    "raw_code": "def build_create_issue_params(params, _project)\n        params[:milestone_id] &&= params[:milestone_id]&.model_id\n        params[:assignee_ids] &&= params[:assignee_ids].map { |assignee_id| assignee_id&.model_id }\n        params[:label_ids] &&= params[:label_ids].map { |label_id| label_id&.model_id }\n\n        if params[:move_before_id].present? || params[:move_after_id].present?\n          params[:move_between_ids] = [\n            params.delete(:move_before_id)&.model_id,\n            params.delete(:move_after_id)&.model_id\n          ]\n        end",
    "comment": "_project argument is unused here, but it is necessary on the EE version of the method",
    "label": "",
    "id": "5345"
  },
  {
    "raw_code": "def variables_attributes_for(variables)\n          variables.map do |variable|\n            variable.to_h.tap do |hash|\n              hash[:id] = GlobalID::Locator.locate(hash[:id]).id if hash[:id]\n\n              hash[:_destroy] = hash.delete(:destroy)\n            end",
    "comment": "This method transforms the GraphQL argument values for pipeline schedule variables into values that can be understood by ActiveRecord when performing a nested attributes collection update.",
    "label": "",
    "id": "5346"
  },
  {
    "raw_code": "def pre_update_checks!(note, _position)\n        raise_resource_not_available_error! 'Resource is not an ImageDiffNote' unless note.position&.on_image?\n      end",
    "comment": "An ImageDiffNote does not exist as a class itself, but is instead just a `DiffNote` with a particular kind of `Gitlab::Diff::Position`. In addition to accepting a `DiffNote` Global ID we also need to perform this check.",
    "label": "",
    "id": "5347"
  },
  {
    "raw_code": "def raise_feature_not_available_error!(_type)\n        raise Gitlab::Graphql::Errors::ArgumentError, DISABLED_FF_ERROR\n      end",
    "comment": "type is used in overridden EE method",
    "label": "",
    "id": "5348"
  },
  {
    "raw_code": "def resolve_designs(issue, filenames)\n        designs = issue.design_collection.designs_by_filename(filenames)\n\n        validate_all_were_found!(designs, filenames)\n\n        designs\n      end",
    "comment": "Here we check that: * we find exactly as many designs as filenames",
    "label": "",
    "id": "5349"
  },
  {
    "raw_code": "def subscribe(*)\n      nil\n    end",
    "comment": "We override graphql-ruby's default `subscribe` since it returns :no_response instead, which leads to empty hashes rendered out to the caller which has caused problems in the client.  Eventually, we should move to an approach where the caller receives a response here upon subscribing, but we don't need this currently because Vue components also perform an initial fetch query. See https://gitlab.com/gitlab-org/gitlab/-/issues/402614",
    "label": "",
    "id": "5350"
  },
  {
    "raw_code": "def object\n      case super\n      when ::GraphQL::Pagination::Connection\n        super.try(:parent)&.work_item\n      else\n        super\n      end",
    "comment": "We call this resolver from `IssueType` where object is an `Issue` instance, and we also call this resolver from `Widgets::DevelopmentType`, in which case the object is a connection type, so we need to get its respective work item.",
    "label": "",
    "id": "5351"
  },
  {
    "raw_code": "def self.complexity_multiplier(args)\n      0.005\n    end",
    "comment": "https://gitlab.com/gitlab-org/gitlab/-/issues/235681",
    "label": "",
    "id": "5352"
  },
  {
    "raw_code": "def self.calculate_ext_conn_complexity\n      false\n    end",
    "comment": "This is a flag to allow us to use `complexity_multiplier` to compute complexity for connection fields(see BaseField#connection_complexity_multiplier) in resolvers that do external connection pagination, thus disabling the default `connection` option.",
    "label": "",
    "id": "5353"
  },
  {
    "raw_code": "def self.before_connection_authorization_block\n      @before_connection_authorization_block\n    end",
    "comment": "rubocop: disable Style/TrivialAccessors",
    "label": "",
    "id": "5354"
  },
  {
    "raw_code": "def offset_pagination(relation)\n      ::Gitlab::Graphql::Pagination::OffsetPaginatedRelation.new(relation)\n    end",
    "comment": "rubocop: enable Style/TrivialAccessors",
    "label": "",
    "id": "5355"
  },
  {
    "raw_code": "def select_result(results)\n      results\n    end",
    "comment": "Overridden in sub-classes (see .single, .last)",
    "label": "",
    "id": "5356"
  },
  {
    "raw_code": "def resolve(sort:, **filters)\n      return unless packages_available?\n\n      params = filters.merge(SORT_TO_PARAMS_MAP.fetch(sort))\n      params[:preload_pipelines] = false\n\n      ::Packages::PackagesFinder.new(object, params).execute\n    end",
    "comment": "The GraphQL type is defined in the extended class",
    "label": "",
    "id": "5357"
  },
  {
    "raw_code": "def self.single\n      Resolvers::ReleaseResolver\n    end",
    "comment": "This resolver has a custom singular resolver",
    "label": "",
    "id": "5358"
  },
  {
    "raw_code": "def self.resolver_complexity(args, child_complexity:)\n      super + (args[:paths] || []).size\n    end",
    "comment": "We fetch blobs from Gitaly efficiently but it still scales O(N) with the number of paths being fetched, so apply a scaling limit to that.",
    "label": "",
    "id": "5359"
  },
  {
    "raw_code": "def prepare_args(args)\n      args.delete(:project_id)\n      args.delete(:project_path)\n      args[:non_archived] = args.delete(:include_archived) != true\n    end",
    "comment": "These arguments are handled in load_project, and should not be passed to the finder directly.",
    "label": "",
    "id": "5360"
  },
  {
    "raw_code": "def resolve(lookahead:, first: nil, last: nil, after: nil, before: nil)\n      default_value = default_value_for(first: first, last: last, after: after, before: before)\n      BatchLoader::GraphQL.for(package.id)\n                          .batch(default_value: default_value) do |package_ids, loader|\n        build_infos = ::Packages::BuildInfosFinder.new(\n          package_ids,\n          first: first,\n          last: last,\n          after: decode_cursor(after),\n          before: decode_cursor(before),\n          max_page_size: MAX_PAGE_SIZE,\n          support_next_page: lookahead.selects?(:page_info)\n        ).execute\n\n        build_infos.each do |build_info|\n          loader.call(build_info.package_id) do |connection|\n            connection.items << lazy_load_pipeline(build_info.pipeline_id)\n            connection\n          end",
    "comment": "This returns a promise for a connection of promises for pipelines: Lazy[Connection[Lazy[Pipeline]]] structure",
    "label": "",
    "id": "5361"
  },
  {
    "raw_code": "def finder(args)\n      ::WorkItems::WorkItemsFinder.new(current_user, args)\n    end",
    "comment": "When we search on a group level, this finder is being overwritten in app/graphql/resolvers/namespaces/work_items_resolver.rb:32",
    "label": "",
    "id": "5362"
  },
  {
    "raw_code": "def error_message\n        \"Field 'alertManagementAlerts' doesn't exist on type 'Project'.\"\n      end",
    "comment": "This error is raised when the alert feature is disabled via feature flag. Not yet a deprecated field, as the FF is disabled by default (see issue#537182). If the FF is enabled in the future, we may need to consider deprecating this field.",
    "label": "",
    "id": "5363"
  },
  {
    "raw_code": "def allowed?(item)\n    true\n  end",
    "comment": "Override to apply filters on a per-item basis",
    "label": "",
    "id": "5364"
  },
  {
    "raw_code": "def preload\n    nil\n  end",
    "comment": "Override to specify preloads for each query",
    "label": "",
    "id": "5365"
  },
  {
    "raw_code": "def item_found(query_input, item); end\n\n  def max_union_size\n    MAX_UNION_SIZE\n  end\n\n  private\n\n  def primary_key\n    @primary_key ||= (model_class.primary_key || raise(\"No primary key for #{model_class}\"))\n  end\n\n  def batch\n    { key: self.class, default_value: [] }\n  end\n\n  def found(loader, key, value)\n    return unless allowed?(value)\n\n    loader.call(key) do |vs|\n      item_found(key, value)\n      vs << value\n    end\n  end",
    "comment": "Override this to intercept the items once they are found",
    "label": "",
    "id": "5366"
  },
  {
    "raw_code": "def tag(queries)\n    queries.each_with_index.map do |q, i|\n      limit(q.select(all_fields, member_idx(i)))\n    end",
    "comment": "Tag each row returned from each query with a the index of which query in the union it comes from. This lets us map the results back to the cache key.",
    "label": "",
    "id": "5367"
  },
  {
    "raw_code": "def query_limit\n    field&.max_page_size.presence || context.schema.default_max_page_size\n  end",
    "comment": "rubocop: disable Graphql/Descriptions -- false positive",
    "label": "",
    "id": "5368"
  },
  {
    "raw_code": "def member_idx(idx)\n    ::Arel::Nodes::SqlLiteral.new(idx.to_s).as('union_member_idx')\n  end",
    "comment": "rubocop: enable Graphql/Descriptions",
    "label": "",
    "id": "5369"
  },
  {
    "raw_code": "def resolve_groups(**args)\n    raise NotImplementedError\n  end",
    "comment": "The resolver should implement this method.",
    "label": "",
    "id": "5370"
  },
  {
    "raw_code": "def resolve_project(full_path: nil, project_id: nil)\n    unless full_path.present? ^ project_id.present?\n      raise ::Gitlab::Graphql::Errors::ArgumentError, 'Incompatible arguments: projectId, projectPath.'\n    end",
    "comment": "Accepts EITHER one of - full_path: String (see Project#full_path) - project_id: GlobalID. Arguments should be typed as: `::Types::GlobalIDType[Project]`",
    "label": "",
    "id": "5371"
  },
  {
    "raw_code": "def verify_search_rate_limit!(**args)\n      if current_user\n        key = :search_rate_limit\n        scope = [current_user, safe_search_scope(**args)].compact\n        users_allowlist = Gitlab::CurrentSettings.current_application_settings.search_rate_limit_allowlist\n      else\n        key = :search_rate_limit_unauthenticated\n        scope = [context[:request].ip]\n        users_allowlist = nil\n      end",
    "comment": "Implement scope and search_params methods in the class which includes this concern.",
    "label": "",
    "id": "5372"
  },
  {
    "raw_code": "def unconditional_includes\n        []\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5373"
  },
  {
    "raw_code": "def custom_branch_rules(args)\n        [all_branches_rule]\n      end",
    "comment": "BranchRules for 'All branches' i.e. no associated ProtectedBranch",
    "label": "",
    "id": "5374"
  },
  {
    "raw_code": "def page_info_cursors(commits)\n        return [nil, nil] if commits.empty?\n        return Array.new(2, encode_token(commits.first.sha)) if commits.length == 1\n\n        [encode_token(commits.first.sha), encode_token(commits.last.sha)]\n      end",
    "comment": "Each cursor is a base64 encoded SHA value. The previous_cursor is generated from the first commit in the list and the next_cursor is generated from the last commit in the list.  `commits` could be: 1. [] - an empty list. 2. [Commit(#1)] - a list with a single item. 3. [Commit(#1), ..., Commit(#15)] - a list with multiple items.  Returns => [previous_cursor, next_cursor] 1. [nil, nil] 2. [SHA#1, SHA#1] 3. [SHA#1, SHA#15]",
    "label": "",
    "id": "5375"
  },
  {
    "raw_code": "def authorize_template!(template_project, _)\n        authorize!(template_project.project_namespace)\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5376"
  },
  {
    "raw_code": "def fetch_root_templates_project(namespace)\n        if namespace.is_a?(::Namespaces::ProjectNamespace)\n          namespace.project\n        elsif namespace.is_a?(::Group)\n          Project.find(namespace.file_template_project_id)\n        end",
    "comment": "When we are at project level we return the project itself to fetch the description templates. When we are at group level we fetch first found file_template_project_id from the namespace or its ancestors",
    "label": "",
    "id": "5377"
  },
  {
    "raw_code": "def resolve(id: nil)\n          ::Analytics::CycleAnalytics::ValueStreams::ListService\n            .new(**service_params(id: id))\n            .execute\n            .payload[:value_streams]\n        end",
    "comment": "ignore id in FOSS",
    "label": "",
    "id": "5378"
  },
  {
    "raw_code": "def self.[](context = :project)\n          case context\n          when :project\n            self\n          when :group\n            Class.new(self) do\n              argument :project_ids, [GraphQL::Types::ID],\n                required: false,\n                description: 'Project IDs within the group hierarchy.'\n\n              define_method :finder_params do\n                { group_id: object.id, include_subgroups: true }\n              end",
    "comment": ":project level: no customization, returning the original resolver :group level: add the project_ids argument",
    "label": "",
    "id": "5379"
  },
  {
    "raw_code": "def consistent?(dav)\n        issue.nil? || (dav.present? && dav.design&.issue_id == issue.id)\n      end",
    "comment": "If this resolver is mounted on something that has an issue (such as design collection for instance), then we should check that the DesignAtVersion as found by its ID does in fact belong to this issue.",
    "label": "",
    "id": "5380"
  },
  {
    "raw_code": "def self.single\n        ::Resolvers::DesignManagement::VersionInCollectionResolver\n      end",
    "comment": "This resolver has a custom singular resolver",
    "label": "",
    "id": "5381"
  },
  {
    "raw_code": "def cutoff(id, sha)\n        if sha.present? || id.present?\n          specific_version(id, sha)\n        else\n          :unconstrained\n        end",
    "comment": "Find the most recent version that the client will accept",
    "label": "",
    "id": "5382"
  },
  {
    "raw_code": "def consistent?(dav)\n          return false unless dav.present?\n\n          dav.design.issue_id == issue.id &&\n            dav.version.id == version.id &&\n            dav.design.visible_in?(version)\n        end",
    "comment": "Test that the DAV found by ID actually belongs on this version, and that it is visible at this version.",
    "label": "",
    "id": "5383"
  },
  {
    "raw_code": "def authorized?(object, args, ctx)\n      field_authorized?(object, ctx) && resolver_authorized?(object, ctx)\n    end",
    "comment": "By default fields authorize against the current object, but that is not how our resolvers work - they use declarative permissions to authorize fields manually (so we make them opt in). TODO: https://gitlab.com/gitlab-org/gitlab/-/issues/300922 (separate out authorize into permissions on the object, and on the resolved values) We do not support argument authorization in our schema. If/when we do, we should call `super` here, to apply argument authorization checks. See: https://gitlab.com/gitlab-org/gitlab/-/issues/324647",
    "label": "",
    "id": "5384"
  },
  {
    "raw_code": "def complexity_for(child_complexity:, query:, lookahead:)\n      defined_complexity = complexity\n\n      case defined_complexity\n      when Proc\n        arguments = query.arguments_for(lookahead.ast_nodes.first, self)\n\n        if arguments.respond_to?(:keyword_arguments)\n          defined_complexity.call(query.context, arguments.keyword_arguments, child_complexity)\n        else\n          child_complexity\n        end",
    "comment": "This gets called from the gem's `calculate_complexity` method, allowing us to ensure our complexity calculation is used even for connections. This code is actually a copy of the default case in `calculate_complexity` in `lib/graphql/schema/field.rb` (https://github.com/rmosolgo/graphql-ruby/blob/master/lib/graphql/schema/field.rb)",
    "label": "",
    "id": "5385"
  },
  {
    "raw_code": "def resolver_authorized?(object, ctx)\n      if @resolver_class && @resolver_class.try(:authorizes_object?)\n        @resolver_class.authorized?(object, ctx)\n      else\n        true\n      end",
    "comment": "Historically our resolvers have used declarative permission checks only for _what they resolved_, not the _object they resolved these things from_ We preserve these semantics here, and only apply resolver authorization if the resolver has opted in.",
    "label": "",
    "id": "5386"
  },
  {
    "raw_code": "def declarative_enum(enum_mod, use_name: true, use_description: true)\n        graphql_name(enum_mod.name) if use_name\n        description(enum_mod.description) if use_description\n\n        enum_mod.definition.each do |key, content|\n          value(key.to_s.upcase, value: key.to_s, description: content[:description])\n        end",
    "comment": "Registers enum definition by the given DeclarativeEnum module  @param enum_mod [Module] The enum module to be used @param use_name [Boolean] Does not override the name if set `false` @param use_description [Boolean] Does not override the description if set `false`  Example:  class MyEnum < BaseEnum declarative_enum MyDeclarativeEnum end  Disabling descriptions rubocop for a false positive here ",
    "label": "",
    "id": "5387"
  },
  {
    "raw_code": "def from_rails_enum(enum, description:)\n        enum.each_key do |name|\n          value name.to_s.upcase,\n            value: name,\n            description: format(description, name: name)\n        end",
    "comment": "Helper to define an enum member for each element of a Rails AR enum",
    "label": "",
    "id": "5388"
  },
  {
    "raw_code": "def enum\n        @enum_values ||= {}.with_indifferent_access\n      end",
    "comment": "Returns an indifferent access hash with the key being the downcased name of the attribute and the value being the Ruby value (either the explicit `value` passed or the same as the value attr).",
    "label": "",
    "id": "5389"
  },
  {
    "raw_code": "def title\n      BatchLoader::GraphQL.for(object).batch do |lists, callback|\n        ActiveRecord::Associations::Preloader.new(records: lists, associations: :label).call\n\n        # all list titles are preloaded at this point\n        lists.each { |list| callback.call(list, list.title) }\n      end",
    "comment": "board lists have a data dependency on label - so we batch load them here",
    "label": "",
    "id": "5390"
  },
  {
    "raw_code": "def id\n      GitlabSchema.id_from_object(object)\n    end",
    "comment": "All graphql fields exposing an id, should expose a global id.",
    "label": "",
    "id": "5391"
  },
  {
    "raw_code": "def marked_for_deletion_on\n      project.marked_for_deletion_at\n    end",
    "comment": "marked_for_deletion_at is deprecated in our v5 REST API in favor of marked_for_deletion_on https://docs.gitlab.com/ee/api/projects.html#removals-in-api-v5",
    "label": "",
    "id": "5392"
  },
  {
    "raw_code": "def self.coerce_result(value, _ctx)\n      ::Gitlab::GlobalId.as_global_id(value).to_s\n    end",
    "comment": "@param value [GID] @return [String]",
    "label": "",
    "id": "5393"
  },
  {
    "raw_code": "def self.coerce_input(value, _ctx)\n      return if value.nil?\n\n      gid = GlobalID.parse(value)\n      raise GraphQL::CoercionError, \"#{value.inspect} is not a valid Global ID\" if gid.nil?\n      raise GraphQL::CoercionError, \"#{value.inspect} is not a Gitlab Global ID\" unless gid.app == GlobalID.app\n\n      gid\n    end",
    "comment": "@param value [String] @return [GID]",
    "label": "",
    "id": "5394"
  },
  {
    "raw_code": "def self.[](model_class)\n      @id_types ||= {\n        # WorkItem has a special class as we want to allow IssueID\n        # on WorkItemID while we transition into work items\n        ::WorkItem => ::Types::WorkItemIdType\n      }\n\n      @id_types[model_class] ||= Class.new(self) do\n        model_name = model_class.name\n\n        graphql_name model_name_to_graphql_name(model_name)\n        description <<~MD.strip\n          A `#{graphql_name}` is a global ID. It is encoded as a string.\n\n          An example `#{graphql_name}` is: `\"#{::Gitlab::GlobalId.build(model_name: model_name, id: 1)}\"`.\n          #{\n            if deprecation = Gitlab::GlobalId::Deprecations.deprecation_by(model_name)\n              \"The older format `\\\"#{::Gitlab::GlobalId.build(model_name: deprecation.old_name, id: 1)}\\\"` was deprecated in #{deprecation.milestone}.\"\n            end}\n\n        MD\n\n        define_singleton_method(:to_s) do\n          graphql_name\n        end",
    "comment": "Construct a restricted type, that can only be inhabited by an ID of a given model class.",
    "label": "",
    "id": "5395"
  },
  {
    "raw_code": "def committers\n      object.commits.committers\n    end",
    "comment": "This is temporary to fix a bug where `committers` is already loaded and memoized and calling it again with a certain GraphQL query can cause the Rails to to throw a ActiveRecord::UnmodifiableRelation error",
    "label": "",
    "id": "5396"
  },
  {
    "raw_code": "def user_export_email; end\n      end",
    "comment": "Do not expose the export email for user namespaces, since exporting work items on this namespace type is not supported",
    "label": "",
    "id": "5397"
  },
  {
    "raw_code": "def migration_state\n        ''\n      end",
    "comment": "The migration has now completed and we are cleaning up the migration db columns. For backward compatibility, we are keeping this field accessible. This field will be removed in 18.0.",
    "label": "",
    "id": "5398"
  },
  {
    "raw_code": "def file_metadata\n        case object.package.package_type\n        when 'conan'\n          object.conan_file_metadatum\n        when 'helm'\n          object.helm_file_metadatum\n        end",
    "comment": "NOTE: This method must be kept in sync with the union type: `Types::Packages::FileMetadataType`.  `Types::Packages::FileMetadataType.resolve_type(metadata, ctx)` must never raise.",
    "label": "",
    "id": "5399"
  },
  {
    "raw_code": "def metadata\n        case object.package_type\n        when 'composer'\n          object.composer_metadatum\n        when 'conan'\n          object.conan_metadatum\n        when 'maven'\n          object.maven_metadatum\n        when 'nuget'\n          object.nuget_metadatum\n        when 'pypi'\n          object.pypi_metadatum\n        when 'terraform_module'\n          object.terraform_module_metadatum\n        end",
    "comment": "NOTE: This method must be kept in sync with the union type: `Types::Packages::MetadataType`.  `Types::Packages::MetadataType.resolve_type(metadata, ctx)` must never raise. rubocop: disable GraphQL/ResolverMethodLength",
    "label": "",
    "id": "5400"
  },
  {
    "raw_code": "def metadata\n        model_class = case object.package.package_type\n                      when 'nuget'\n                        ::Packages::Nuget::DependencyLinkMetadatum\n                      end",
    "comment": "NOTE: This method must be kept in sync with the union type: `Types::Packages::DependencyLinkMetadata`.  `Types::Packages::DependencyLinkMetadata.resolve_type(metadata, ctx)` must never raise.",
    "label": "",
    "id": "5401"
  },
  {
    "raw_code": "def resolve_alias\n          object['alias']\n        end",
    "comment": "field :alias` conflicts with a built-in method",
    "label": "",
    "id": "5402"
  },
  {
    "raw_code": "def groups(lookahead:)\n        key = ::Gitlab::Graphql::BatchKey.new(object, lookahead, object_name: :stage)\n\n        BatchLoader::GraphQL.for(key).batch(default_value: []) do |keys, loader|\n          by_pipeline = keys.group_by(&:pipeline)\n          include_needs = keys.any? do |k|\n            k.requires?(%i[nodes jobs nodes needs]) ||\n              k.requires?(%i[nodes jobs nodes previousStageJobsOrNeeds])\n          end",
    "comment": "Issues one query per pipeline",
    "label": "",
    "id": "5403"
  },
  {
    "raw_code": "def jobs_for_pipeline(pipeline, stage_ids, include_needs)\n        jobs = pipeline.statuses.latest.where(stage_id: stage_ids)\n\n        preloaded_relations = [\n          :project, :metadata, :job_definition, :job_artifacts,\n          :downstream_pipeline, :error_job_messages\n        ]\n        preloaded_relations << :needs if include_needs\n\n        ::Ci::Preloaders::CommitStatusPreloader.new(jobs).execute(preloaded_relations)\n\n        jobs.group_by(&:stage_id)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5404"
  },
  {
    "raw_code": "def id\n        return unless object.id.present?\n\n        model_name = object.type || ::CommitStatus.name\n        id = object.id\n        Gitlab::GlobalId.build(model_name: model_name, id: id)\n      end",
    "comment": "This class is a secret union! TODO: turn this into an actual union, so that fields can be referenced safely!",
    "label": "",
    "id": "5405"
  },
  {
    "raw_code": "def edit_url(parent:)\n        runner_url(owner: parent.parent, url_type: :edit_url)\n      end",
    "comment": "here parent is a Keyset::Connection",
    "label": "",
    "id": "5406"
  },
  {
    "raw_code": "def topics\n          BatchLoader::GraphQL.for(object).batch do |resources, loader|\n            # rubocop: disable CodeReuse/ActiveRecord -- this is necessary to batch\n            project_ids = resources.pluck(:project_id)\n            project_topics = ::Projects::ProjectTopic.where(project_id: project_ids)\n            topics = ::Projects::Topic.where(id: project_topics.pluck(:topic_id))\n            grouped_project_topics = project_topics.group_by(&:project_id)\n\n            resources.each do |resource|\n              project_topics_ids_for_resource = grouped_project_topics.fetch(resource.project_id,\n                []).pluck(:topic_id)\n              topics_for_resource = topics.select { |topic| project_topics_ids_for_resource.include?(topic.id) }\n\n              loader.call(resource, topics_for_resource.pluck(:name))\n              # rubocop: enable CodeReuse/ActiveRecord\n            end",
    "comment": "rubocop: disable GraphQL/ResolverMethodLength -- this will be refactored: https://gitlab.com/gitlab-org/gitlab/-/issues/510648",
    "label": "",
    "id": "5407"
  },
  {
    "raw_code": "def reply_id\n        ::Gitlab::GlobalId.build(object, id: object.reply_id)\n      end",
    "comment": "DiscussionID.coerce_result is suitable here, but will always mark this as being a 'Discussion'. Using `GlobalId.build` guarantees that we get the correct class, and that it matches `id`.",
    "label": "",
    "id": "5408"
  },
  {
    "raw_code": "def id\n        return super unless object.is_a?(SyntheticNote)\n\n        # object is a presenter, so object.object returns the concrete note object.\n        ::Gitlab::GlobalId.build(object, model_name: object.object.class.to_s, id: object.discussion_id)\n      end",
    "comment": "We now support also SyntheticNote notes as a NoteType, but SyntheticNote does not have a real note ID, as SyntheticNote is generated dynamically from a ResourceEvent instance.",
    "label": "",
    "id": "5409"
  },
  {
    "raw_code": "def has_children?\n          BatchLoader::GraphQL.for(object.work_item.id).batch(default_value: false) do |ids, loader|\n            links_for_parents = ::WorkItems::ParentLink.for_parents(ids)\n                                           .select(:work_item_parent_id)\n                                           .group(:work_item_parent_id)\n                                           .without_order\n\n            links_for_parents.each { |link| loader.call(link.work_item_parent_id, true) }\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5410"
  },
  {
    "raw_code": "def unpresented\n          object.work_item\n        end",
    "comment": "Overriden as `Types::CurrentUserTodos` relies on `unpresented` being the Issuable record.",
    "label": "",
    "id": "5411"
  },
  {
    "raw_code": "def cached_stateful_version(parent_node)\n        version_gid = context[:at_version_argument] # See: DesignsResolver\n\n        # Caching is scoped to an `issue_id` to allow us to cache the\n        # most recent `Version` for an issue\n        Gitlab::SafeRequestStore.fetch([request_cache_base_key, 'stateful_version', object.issue_id, version_gid]) do\n          if version_gid\n            GitlabSchema.object_from_id(version_gid, expected_type: ::DesignManagement::Version)&.sync\n          else\n            object.issue.design_versions.most_recent\n          end",
    "comment": "Returns a `DesignManagement::Version` for this query based on the `atVersion` argument passed to a parent node if present, or otherwise the most recent `Version` for the issue.",
    "label": "",
    "id": "5412"
  },
  {
    "raw_code": "def represent(merge_request, opts = {}, entity = nil)\n    entity ||= identified_entity(opts)\n\n    super(merge_request, opts, entity)\n  end",
    "comment": "This overrided method takes care of which entity should be used to serialize the `merge_request` based on `serializer` key in `opts` param. Hence, `entity` doesn't need to be declared on the class scope.",
    "label": "",
    "id": "5413"
  },
  {
    "raw_code": "def can_admin_runner?\n    current_user&.can_admin_all_resources?\n  end",
    "comment": "can_admin_all_resources? is used here because the path exposed is only available to admins",
    "label": "",
    "id": "5414"
  },
  {
    "raw_code": "def build_metrics(merge_request)\n    # There's no need to query and serialize metrics data for merge requests that are not\n    # merged or closed.\n    return unless merge_request.merged? || merge_request.closed?\n    return merge_request.metrics if merge_request.merged? && merge_request.metrics&.merged_by_id\n    return merge_request.metrics if merge_request.closed? && merge_request.metrics&.latest_closed_by_id\n\n    build_metrics_from_events(merge_request)\n  end",
    "comment": "There are cases where where metrics object doesn't exist and it needs to be rebuilt. TODO: Once https://gitlab.com/gitlab-org/gitlab/-/issues/342508 has been resolved and all merge requests have metrics we can remove this helper method.",
    "label": "",
    "id": "5415"
  },
  {
    "raw_code": "def distance_of_time_as_hash(diff)\n    diff = diff.abs.floor\n\n    return { seconds: 0 } if diff == 0\n\n    mins = (diff / 60).floor\n    seconds = diff % 60\n    hours = (mins / 60).floor\n    mins %= 60\n    days = (hours / 24).floor\n    hours %= 24\n\n    duration_hash = {}\n\n    duration_hash[:days] = days if days > 0\n    duration_hash[:hours] = hours if hours > 0\n    duration_hash[:mins] = mins if mins > 0\n    duration_hash[:seconds] = seconds if seconds > 0\n\n    duration_hash\n  end",
    "comment": "Converts seconds into a hash such as: { days: 1, hours: 3, mins: 42, seconds: 40 }  It returns 0 seconds for zero or negative numbers It rounds to nearest time unit and does not return zero i.e { min: 1 } instead of { mins: 1, seconds: 0 }",
    "label": "",
    "id": "5416"
  },
  {
    "raw_code": "def remaining_days_in_words(due_date, start_date = nil)\n    if due_date&.past?\n      content_tag(:strong, _('Past due'))\n    elsif due_date&.today?\n      content_tag(:strong, _('Today'))\n    elsif start_date&.future?\n      content_tag(:strong, _('Upcoming'))\n    elsif due_date\n      is_upcoming = (due_date - Date.today).to_i > 0\n      time_ago = distance_of_time_in_words(due_date, Date.today)\n\n      # https://gitlab.com/gitlab-org/gitlab-foss/issues/49440\n      #\n      # Need to improve the i18n here and do a full translation\n      # of the string instead of piecewise translations.\n      content = time_ago\n        .gsub(/\\d+/) { |match| \"<strong>#{match}</strong>\" }\n        .remove('about ')\n      remaining_or_ago = is_upcoming ? _('remaining') : _('ago')\n\n      \"#{content} #{remaining_or_ago}\".html_safe\n    elsif start_date&.past?\n      days = (Date.today - start_date).to_i\n      \"#{content_tag(:strong, days)} #{'day'.pluralize(days)} elapsed\".html_safe\n    end",
    "comment": "Generates an HTML-formatted string for remaining dates based on start_date and due_date  It returns \"Past due\" for expired entities It returns \"Upcoming\" for upcoming entities If due date is provided, it returns \"# days|weeks|months remaining|ago\" If start date is provided and elapsed, with no due date, it returns \"# days elapsed\"",
    "label": "",
    "id": "5417"
  },
  {
    "raw_code": "def itemize(resource)\n    items = resource.order('folder ASC')\n      .group('COALESCE(environment_type, id::text)', 'COALESCE(environment_type, name)')\n      .select(\n        'COALESCE(environment_type, id::text), COALESCE(environment_type, name) AS folder',\n        'COUNT(*) AS size', 'MAX(id) AS last_id'\n      )\n\n    # It makes a difference when you call `paginate` method, because\n    # although `page` is effective at the end, it calls counting methods\n    # immediately.\n    items = @paginator.paginate(items) if paginated?\n\n    environments = batch_load(Environment.where(id: items.map(&:last_id)))\n    environments_by_id = environments.index_by(&:id)\n\n    items.map do |item|\n      Item.new(item.folder, item.size, environments_by_id[item.last_id])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5418"
  },
  {
    "raw_code": "def represent(issue, opts = {})\n    entity = choose_entity(opts)\n\n    super(issue, opts, entity)\n  end",
    "comment": "This overrided method takes care of which entity should be used to serialize the `issue` based on `serializer` key in `opts` param. Hence, `entity` doesn't need to be declared on the class scope.",
    "label": "",
    "id": "5419"
  },
  {
    "raw_code": "def initialize(parameters)\n    parameters.each do |key, value|\n      define_singleton_method(key) { value }\n    end",
    "comment": "We use EntityRequest object to collect parameters and variables from the controller. Because options that are being passed to the entity do appear in each entity object  in the chain, we need a way to pass data that is present in the controller (see  #20045). ",
    "label": "",
    "id": "5420"
  },
  {
    "raw_code": "def truncated_discussion_path_for(discussion)\n    project_discussion_path(discussion.project, discussion.noteable_collection_name, discussion.noteable, discussion)\n  end",
    "comment": "overridden on EE",
    "label": "",
    "id": "5421"
  },
  {
    "raw_code": "def membership\n    return unless request.current_user\n\n    @membership ||= request.current_user.members.find_by(source: object)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5422"
  },
  {
    "raw_code": "def project?\n    object.is_a?(Project)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5423"
  },
  {
    "raw_code": "def represent(resource, opts = {})\n    resource = resource.preload(preloaded_relations(**opts)) if resource.is_a?(ActiveRecord::Relation)\n    resource = paginator.paginate(resource) if paginated?\n    resource = Gitlab::Ci::Pipeline::Preloader.preload!(resource) if opts.delete(:preload)\n\n    super(resource, opts)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5424"
  },
  {
    "raw_code": "def represent_status(resource)\n    return {} unless resource.present?\n\n    data = represent(resource, { only: [{ details: [:status] }] })\n    data.dig(:details, :status) || {}\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5425"
  },
  {
    "raw_code": "def represent(resource, opts = {})\n    if paginated? && (resource.respond_to?(:page) || resource.respond_to?(:cursor_for_next_page))\n      super(paginator.paginate(resource), opts)\n    else\n      super(resource, opts)\n    end",
    "comment": "super is `BaseSerializer#represent` here.  we shouldn't try to paginate single resources",
    "label": "",
    "id": "5426"
  },
  {
    "raw_code": "def link_to(...)\n        ActionController::Base.helpers.link_to(...)\n      end",
    "comment": "Avoid including ActionView::Helpers::UrlHelper",
    "label": "",
    "id": "5427"
  },
  {
    "raw_code": "def title(failure)\n      gh_identifiers = failure.external_identifiers\n\n      case gh_identifiers['object_type']\n      when 'pull_request', 'issue', 'label', 'milestone'\n        gh_identifiers['title']\n      when 'pull_request_merged_by'\n        format(s_(\"GithubImporter|Pull request %{pull_request_iid} merger\"), pull_request_iid: gh_identifiers['iid'])\n      when 'pull_request_review_request'\n        format(\n          s_(\"GithubImporter|Pull request %{pull_request_iid} review request\"),\n          pull_request_iid: gh_identifiers['merge_request_iid']\n        )\n      when 'pull_request_review'\n        format(s_(\"GithubImporter|Pull request review %{review_id}\"), review_id: gh_identifiers['review_id'])\n      when 'collaborator'\n        gh_identifiers['login']\n      when 'protected_branch'\n        gh_identifiers['id']\n      when 'issue_event'\n        gh_identifiers['event']\n      when 'release'\n        gh_identifiers['tag']\n      when 'note'\n        format(\n          s_(\"GithubImporter|%{noteable_type} comment %{note_id}\"),\n          noteable_type: gh_identifiers['noteable_type'],\n          note_id: gh_identifiers['note_id']\n        )\n      when 'diff_note'\n        format(s_(\"GithubImporter|Pull request review comment %{note_id}\"), note_id: gh_identifiers['note_id'])\n      when 'issue_attachment'\n        format(s_(\"GithubImporter|Issue %{issue_iid} attachment\"), issue_iid: gh_identifiers['noteable_iid'])\n      when 'merge_request_attachment'\n        format(\n          s_(\"GithubImporter|Merge request %{merge_request_iid} attachment\"),\n          merge_request_iid: gh_identifiers['noteable_iid']\n        )\n      when 'release_attachment'\n        format(s_(\"GithubImporter|Release %{tag} attachment\"), tag: gh_identifiers['tag'])\n      when 'note_attachment'\n        s_('GithubImporter|Note attachment')\n      when 'lfs_object'\n        gh_identifiers['oid'].to_s\n      else\n        ''\n      end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "5428"
  },
  {
    "raw_code": "def host(uri)\n      parsed_uri = URI.parse(uri)\n      \"#{parsed_uri.scheme}://#{parsed_uri.hostname}\"\n    rescue URI::InvalidURIError\n      nil\n    end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "5429"
  },
  {
    "raw_code": "def subject(*extra)\n    subject = []\n\n    subject << @project.name if @project\n    subject << @group.name if @group\n    subject << @namespace.name if @namespace && !@project\n    subject.concat(extra) if extra.present?\n\n    EmailsHelper.subject_with_prefix_and_suffix(subject)\n  end",
    "comment": "Formats arguments into a String suitable for use as an email subject  extra - Extra Strings to be inserted into the subject  Examples  >> subject('Lorem ipsum') => \"Lorem ipsum\"  # Automatically inserts Project name when @project is set >> @project = Project.last => #<Project id: 1, name: \"Ruby on Rails\", path: \"ruby_on_rails\", ...> >> subject('Lorem ipsum') => \"Ruby on Rails | Lorem ipsum \"  # Accepts multiple arguments >> subject('Lorem ipsum', 'Dolor sit amet') => \"Lorem ipsum | Dolor sit amet\"",
    "label": "",
    "id": "5430"
  },
  {
    "raw_code": "def self.allowed_email_domains\n    domain_parts = Gitlab.config.gitlab.host.split(\".\")\n    allowed_domains = []\n    begin\n      allowed_domains << domain_parts.join(\".\")\n      domain_parts.shift\n    end while domain_parts.length > ActionDispatch::Http::URL.tld_length\n\n    allowed_domains\n  end",
    "comment": "Splits \"gitlab.corp.company.com\" up into \"gitlab.corp.company.com\", \"corp.company.com\" and \"company.com\". Respects set tld length so \"company.co.uk\" won't match \"somethingelse.uk\"",
    "label": "",
    "id": "5431"
  },
  {
    "raw_code": "def sender(sender_id, send_from_user_email: false, sender_name: nil, sender_email: nil)\n    return unless sender = User.find(sender_id)\n\n    address = default_sender_address\n    address.display_name = sender_name.presence || \"#{sender.name} (#{sender.to_reference})\"\n\n    if sender_email\n      address.address = sender_email\n    elsif send_from_user_email && can_send_from_user_email?(sender)\n      address.address = sender.email\n    end",
    "comment": "Return an email address that displays the name of the sender. Override sender_email if you want to hard replace the sender address (e.g. custom email for Service Desk)",
    "label": "",
    "id": "5432"
  },
  {
    "raw_code": "def subject(*extra)\n    subject = []\n\n    subject << @project.name if @project\n    subject << @group.name if @group\n    subject << @namespace.name if @namespace && !@project\n    subject.concat(extra) if extra.present?\n\n    subject_with_prefix_and_suffix(subject)\n  end",
    "comment": "Formats arguments into a String suitable for use as an email subject  extra - Extra Strings to be inserted into the subject  Examples  >> subject('Lorem ipsum') => \"Lorem ipsum\"  # Automatically inserts Project name when @project is set >> @project = Project.last => #<Project id: 1, name: \"Ruby on Rails\", path: \"ruby_on_rails\", ...> >> subject('Lorem ipsum') => \"Ruby on Rails | Lorem ipsum \"  # Accepts multiple arguments >> subject('Lorem ipsum', 'Dolor sit amet') => \"Lorem ipsum | Dolor sit amet\"",
    "label": "",
    "id": "5433"
  },
  {
    "raw_code": "def message_id(model)\n    model_name = model.class.model_name.singular_route_key\n    \"<#{model_name}_#{model.id}@#{Gitlab.config.gitlab.host}>\"\n  end",
    "comment": "Return a string suitable for inclusion in the 'Message-Id' mail header.  The message-id is generated from the unique URL to a model object.",
    "label": "",
    "id": "5434"
  },
  {
    "raw_code": "def mail_new_thread(model, headers = {})\n    headers['Message-ID'] = message_id(model)\n\n    mail_thread(model, headers)\n  end",
    "comment": "Send an email that starts a new conversation thread, with headers suitable for grouping by thread in email clients.  See: mail_answer_thread",
    "label": "",
    "id": "5435"
  },
  {
    "raw_code": "def mail_answer_thread(model, headers = {})\n    headers['Message-ID'] = \"<#{SecureRandom.hex}@#{Gitlab.config.gitlab.host}>\"\n    headers['In-Reply-To'] = message_id(model)\n    headers['References'] = [message_id(model)]\n\n    headers[:subject] = \"Re: #{headers[:subject]}\" if headers[:subject]\n\n    mail_thread(model, headers)\n  end",
    "comment": "Send an email that responds to an existing conversation thread, with headers suitable for grouping by thread in email clients.  For grouping emails by thread, email clients heuristics require the answers to:  * have a subject that begin by 'Re: ' * have a 'In-Reply-To' or 'References' header that references the original 'Message-ID' ",
    "label": "",
    "id": "5436"
  },
  {
    "raw_code": "def add_model_headers(object)\n    # Use replacement so we don't strip the module.\n    prefix = \"X-GitLab-#{object.class.name.gsub(/::/, '-')}\"\n\n    headers[\"#{prefix}-ID\"] = object.id\n    headers[\"#{prefix}-IID\"] = object.iid if object.respond_to?(:iid)\n    headers[\"#{prefix}-State\"] = object.state if object.respond_to?(:state)\n  end",
    "comment": "This method applies threading headers to the email to identify the instance we are discussing.  All model instances must have `#id`, and may implement `#iid`.",
    "label": "",
    "id": "5437"
  },
  {
    "raw_code": "def devise_mail(record, action, opts = {}, &block)\n    validate_single_recipient_in_opts!(opts)\n    super\n  end",
    "comment": "Override devise_mail so that all emails, not just those redefined here, can only be sent to a single to: address.",
    "label": "",
    "id": "5438"
  },
  {
    "raw_code": "def member\n      @member ||= Member.find_by(id: @member_id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5439"
  },
  {
    "raw_code": "def member_source\n      @member_source ||= member.source\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5440"
  },
  {
    "raw_code": "def push_to_merge_request_email(\n      recipient_id,\n      merge_request_id,\n      updated_by_user_id,\n      reason = nil,\n      new_commits:,\n      total_new_commits_count:,\n      existing_commits:,\n      total_existing_commits_count:\n    )\n      setup_merge_request_mail(merge_request_id, recipient_id)\n\n      @new_commits = new_commits\n      @total_new_commits_count = total_new_commits_count\n      @total_stripped_new_commits_count = @total_new_commits_count - @new_commits.length\n\n      @existing_commits = existing_commits\n      @total_existing_commits_count = total_existing_commits_count\n\n      @updated_by_user = User.find(updated_by_user_id)\n\n      mail_answer_thread(@merge_request, merge_request_thread_options(updated_by_user_id, reason))\n    end",
    "comment": "existing_commits - an array containing the first and last commits",
    "label": "",
    "id": "5441"
  },
  {
    "raw_code": "def reassigned_merge_request_email(\n      recipient_id,\n      merge_request_id,\n      previous_assignee_ids,\n      updated_by_user_id,\n      reason = nil\n    )\n      setup_merge_request_mail(merge_request_id, recipient_id)\n\n      previous_assignees = []\n      previous_assignees = User.where(id: previous_assignee_ids) if previous_assignee_ids.any?\n      @added_assignees = @merge_request.assignees.map(&:name) - previous_assignees.map(&:name)\n      @removed_assignees = previous_assignees.map(&:name) - @merge_request.assignees.map(&:name)\n\n      mail_answer_thread(@merge_request, merge_request_thread_options(updated_by_user_id, reason))\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5442"
  },
  {
    "raw_code": "def changed_reviewer_of_merge_request_email(\n      recipient_id,\n      merge_request_id,\n      previous_reviewer_ids,\n      updated_by_user_id,\n      reason = nil\n    )\n      setup_merge_request_mail(merge_request_id, recipient_id)\n\n      @previous_reviewers = []\n      @previous_reviewers = User.where(id: previous_reviewer_ids) if previous_reviewer_ids.any?\n      @updated_by_user = User.find(updated_by_user_id)\n\n      mail_answer_thread(@merge_request, merge_request_thread_options(updated_by_user_id, reason))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5443"
  },
  {
    "raw_code": "def relabeled_merge_request_email(recipient_id, merge_request_id, label_names, updated_by_user_id, reason = nil)\n      setup_merge_request_mail(merge_request_id, recipient_id)\n\n      @label_names = label_names\n      @labels_url = project_labels_url(@project, subscribed: true)\n      mail_answer_thread(@merge_request, merge_request_thread_options(updated_by_user_id, reason))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5444"
  },
  {
    "raw_code": "def reassigned_issue_email(recipient_id, issue_id, previous_assignee_ids, updated_by_user_id, reason = nil)\n      setup_issue_mail(issue_id, recipient_id)\n\n      previous_assignees = []\n      previous_assignees = User.where(id: previous_assignee_ids).order(:id) if previous_assignee_ids.any?\n      @added_assignees = @issue.assignees.map(&:name) - previous_assignees.map(&:name)\n      @removed_assignees = previous_assignees.map(&:name) - @issue.assignees.map(&:name)\n\n      mail_answer_thread(\n        @issue,\n        issue_thread_options(\n          updated_by_user_id,\n          reason,\n          confidentiality: @issue.confidential?\n        )\n      )\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5445"
  },
  {
    "raw_code": "def closed_issue_email(recipient_id, issue_id, updated_by_user_id, reason: nil, closed_via: nil)\n      setup_issue_mail(issue_id, recipient_id, closed_via: closed_via)\n\n      @updated_by = User.find(updated_by_user_id)\n\n      mail_answer_thread(\n        @issue,\n        issue_thread_options(\n          updated_by_user_id,\n          reason,\n          confidentiality: @issue.confidential?\n        )\n      )\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5446"
  },
  {
    "raw_code": "def new_ssh_key_email(key_id)\n      @key = Key.find_by(id: key_id)\n\n      return unless @key\n\n      @current_user = @user = @key.user\n      @target_url = user_url(@user)\n      email_with_layout(to: @user.notification_email_or_default,\n        subject: subject(_(\"SSH key was added to your account\")))\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5447"
  },
  {
    "raw_code": "def new_gpg_key_email(gpg_key_id)\n      @gpg_key = GpgKey.find_by(id: gpg_key_id)\n\n      return unless @gpg_key\n\n      @current_user = @user = @gpg_key.user\n      @target_url = user_url(@user)\n      email_with_layout(to: @user.notification_email_or_default,\n        subject: subject(_(\"GPG key was added to your account\")))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5448"
  },
  {
    "raw_code": "def bot_resource_access_token_about_to_expire_email(recipient, resource, token_name, params = {})\n      params = params.with_indifferent_access\n      @user = recipient\n      @token_name = token_name\n      @days_to_expire = params.fetch(:days_to_expire, PersonalAccessToken::DAYS_TO_EXPIRE)\n      @resource = resource\n      if resource.is_a?(Group)\n        @target_url = group_settings_access_tokens_url(resource)\n        @reason_text = _('You are receiving this email because you are an Owner of the Group.')\n      else\n        @target_url = project_settings_access_tokens_url(resource)\n        @reason_text = _('You are receiving this email because you are either an Owner or Maintainer of the project.')\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord resource owners are sent mail about expiring access tokens which belong to a bot user",
    "label": "",
    "id": "5449"
  },
  {
    "raw_code": "def self.available?\n    false\n  end",
    "comment": "We have experiments in ce/foss code even though they will never be available for ce/foss instances. We do that since we currently only experiment on the ee with SaaS instance. However, if the experiment is successful, we may commit the final code to ce/foss if the feature we are experimenting on is not a licensed or SaaS feature.  This follows the https://docs.gitlab.com/ee/development/ee_features.html guidelines and therefore we have hardcoded `false` here.",
    "label": "",
    "id": "5450"
  },
  {
    "raw_code": "def key_for(source, seed = name)\n    # If FIPS is enabled, we simply call the method available in the gem, which\n    # uses SHA2.\n    return super if Gitlab::FIPS.enabled?\n\n    # If FIPS isn't enabled, we use the legacy MD5 logic to keep existing\n    # experiment events working.\n    source = source.keys + source.values if source.is_a?(Hash)\n    Digest::MD5.hexdigest(Array(source).map { |v| identify(v) }.unshift(seed).join('|'))\n  end",
    "comment": "This is deprecated logic as of v0.6.0 and should eventually be removed, but needs to stay intact for actively running experiments. The new strategy utilizes Digest::SHA2, a secret seed, and generates a 64-byte string.  https://gitlab.com/gitlab-org/gitlab/-/issues/334590  @deprecated",
    "label": "",
    "id": "5451"
  },
  {
    "raw_code": "def project_group_requester?\n    return false if @user.nil?\n    return false unless user_is_user?\n\n    project.group && project.group.requesters.exists?(user_id: @user.id)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5452"
  },
  {
    "raw_code": "def team_access_level\n    return -1 if @user.nil?\n    return -1 unless user_is_user?\n\n    @team_access_level ||= lookup_access_level!\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5453"
  },
  {
    "raw_code": "def epics_license_available?\n    false\n  end",
    "comment": "In FOSS there is no license. This method is overridden in EE",
    "label": "",
    "id": "5454"
  },
  {
    "raw_code": "def project_work_item_epics_available?\n    false\n  end",
    "comment": "Not available in FOSS. This method is Overridden in EE",
    "label": "",
    "id": "5455"
  },
  {
    "raw_code": "def agreement\n      strong_memoize(:agreement) do\n        next nil if @user.nil? || @subject.nil?\n\n        @user.term_agreements.find_by(term: @subject)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5456"
  },
  {
    "raw_code": "def job_user_facing_update_abilities\n        all_job_update_abilities - [:update_build]\n      end",
    "comment": "We exclude `update_build` because it's used as an internal abstraction that is used to influence other permissions, like `erase_build`, etc. Preventing `update_build` would cause other permissions like `erase_build` to be prevented.",
    "label": "",
    "id": "5457"
  },
  {
    "raw_code": "def safe_value?(text)\n    pre_encoded_text = text.gsub('&', '&amp;')\n    Rails::Html::FullSanitizer.new.sanitize(pre_encoded_text) == pre_encoded_text\n  end",
    "comment": "The `FullSanitizer` encodes ampersands as the HTML entity name. This isn't particularly necessary for preventing XSS so the ampersand is pre-encoded to avoid it being flagged in the comparison.",
    "label": "",
    "id": "5458"
  },
  {
    "raw_code": "def validate_duplicates(record, attribute, values)\n    child_attributes.each do |child_attribute|\n      duplicates = values.reject(&:marked_for_destruction?).group_by(&:\"#{child_attribute}\").select { |_, v| v.many? }.map(&:first)\n      next unless duplicates.any?\n\n      error_message = \"have duplicate values (#{duplicates.join(', ')})\"\n      error_message << \" for #{values.first.send(options[:scope])} scope\" if options[:scope] # rubocop:disable GitlabSecurity/PublicSend\n      record.errors.add(attribute, error_message)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5459"
  },
  {
    "raw_code": "def child_attributes\n    options[:child_attributes] || %i[key]\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5460"
  },
  {
    "raw_code": "def certificate(record)\n    record.public_send(options[:certificate])\n  end",
    "comment": "rubocop:disable GitlabSecurity/PublicSend  Allowing `#public_send` here because we don't want the validator to really care about the names of the attributes or where they come from.  The credentials are mostly stored encrypted so we need to go through the accessors to get the values, `read_attribute` bypasses those.",
    "label": "",
    "id": "5461"
  },
  {
    "raw_code": "def perform(_uuid = nil)\n    with_redis do |redis|\n      request_ids = fetch_request_ids(redis)\n      stats = Gitlab::PerformanceBar::Stats.new(redis)\n\n      request_ids.each do |id|\n        stats.process(id)\n      end",
    "comment": "_uuid is kept for backward compatibility, but it's not used anymore",
    "label": "",
    "id": "5462"
  },
  {
    "raw_code": "def perform(cursor = nil)\n    @updated_count = 0\n    paginator = paginate(cursor)\n\n    paginator.each { |member| process_member(member) }\n\n    status = paginator.has_next_page? ? :limit_reached : :completed\n    log_extra_metadata_on_done(:result,\n      status: status,\n      updated_rows: @updated_count\n    )\n\n    return unless paginator.has_next_page?\n\n    self.class.perform_in(BATCH_DELAY, paginator.cursor_for_next_page)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5463"
  },
  {
    "raw_code": "def perform(integration_id, min_id, max_id)\n    integration = Integration.find_by_id(integration_id)\n    return unless integration\n\n    batch = Integration.where(id: min_id..max_id).by_type(integration.type).inherit_from_id(integration.id)\n\n    Integrations::Propagation::BulkUpdateService.new(integration, batch).execute\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5464"
  },
  {
    "raw_code": "def perform(integration_id, min_id, max_id)\n    integration = Integration.find_by_id(integration_id)\n    return unless integration\n\n    batch = Integration.inherited_descendants_from_self_or_ancestors_from(integration).where(id: min_id..max_id)\n\n    Integrations::Propagation::BulkUpdateService.new(integration, batch).execute\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5465"
  },
  {
    "raw_code": "def perform\n    if MergeRequest.use_locked_set?\n      MergeRequests::UnstickLockedMergeRequestsService.new.execute\n    else\n      stuck_merge_requests.find_in_batches(batch_size: 100) do |group|\n        # The logic in this block also exists in `MergeRequests::UnstickLockedMergeRequestsService`\n        # since that is intended to replace this once the feature flag is fully rolled out.\n        #\n        # Any changes that needs to be applied here should be applied to the service as well.\n        jids = group.map(&:merge_jid)\n\n        # Find the jobs that aren't currently running or that exceeded the threshold.\n        completed_jids = Gitlab::SidekiqStatus.completed_jids(jids)\n\n        if completed_jids.any?\n          completed_ids = group.select { |merge_request| completed_jids.include?(merge_request.merge_jid) }.map(&:id)\n\n          apply_current_state!(completed_jids, completed_ids)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5466"
  },
  {
    "raw_code": "def apply_current_state!(completed_jids, completed_ids)\n    merge_requests = MergeRequest.where(id: completed_ids)\n\n    merge_requests.where.not(merge_commit_sha: nil).update_all(state_id: MergeRequest.available_states[:merged])\n\n    merge_requests_to_reopen = merge_requests.where(merge_commit_sha: nil)\n\n    # Do not reopen merge requests using direct queries.\n    # We rely on state machine callbacks to update head_pipeline_id\n    errors = Hash.new { |h, k| h[k] = [] }\n\n    merge_requests_to_reopen.each do |mr|\n      mjid = mr.merge_jid\n\n      next if mr.unlock_mr\n\n      mr.errors.full_messages.each do |msg|\n        errors[msg] << [\"#{mjid}|#{mr.id}\"]\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5467"
  },
  {
    "raw_code": "def stuck_merge_requests\n    MergeRequest.select('id, merge_jid').with_state(:locked).where.not(merge_jid: nil).without_order\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5468"
  },
  {
    "raw_code": "def perform(note_id, _params = {})\n    Gitlab::QueryLimiting.disable!('https://gitlab.com/gitlab-org/gitlab/-/issues/497631', new_threshold: 200)\n\n    if note = Note.find_by_id(note_id)\n      NotificationService.new.new_note(note) unless note.skip_notification?\n      Notes::PostProcessService.new(note).execute\n    else\n      Gitlab::AppLogger.error(\"NewNoteWorker: couldn't find note with ID=#{note_id}, skipping job\")\n    end",
    "comment": "Keep extra parameter to preserve backwards compatibility with old `NewNoteWorker` jobs (can remove later)",
    "label": "",
    "id": "5469"
  },
  {
    "raw_code": "def perform(merge_request_diff_id)\n    merge_request_diff = MergeRequestDiff.find(merge_request_diff_id)\n\n    return if merge_request_diff.without_files?\n\n    MergeRequestDiff.transaction do\n      MergeRequestDiffFile\n        .where(merge_request_diff_id: merge_request_diff.id)\n        .delete_all\n\n      merge_request_diff.clean!\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5470"
  },
  {
    "raw_code": "def perform(integration_id, min_id, max_id)\n    integration = Integration.find_by_id(integration_id)\n    return unless integration\n\n    batch = if integration.instance_level?\n              Group.where(id: min_id..max_id).without_integration(integration)\n            else\n              integration.group.descendants.where(id: min_id..max_id).without_integration(integration)\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5471"
  },
  {
    "raw_code": "def perform(integration_id, min_id, max_id)\n    integration = Integration.find_by_id(integration_id)\n    return unless integration\n\n    batch = Project.where(id: min_id..max_id).without_integration(integration)\n    batch = batch.in_namespace(integration.group.self_and_descendants) if integration.group_level?\n\n    return if batch.empty?\n\n    Integrations::Propagation::BulkCreateService.new(integration, batch, 'project').execute\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5472"
  },
  {
    "raw_code": "def perform(project_id, files = [], statistics = [], refresh_statistics = true)\n    project = Project.find_by_id(project_id)\n\n    return unless project\n\n    update_statistics(project, statistics) if refresh_statistics\n\n    return unless project.repository.exists?\n\n    project.repository.refresh_method_caches(files.map(&:to_sym))\n\n    project.cleanup\n  end",
    "comment": "project_id - The ID of the project for which to flush the cache. files - An Array containing extra types of files to refresh such as `:readme` to flush the README and `:changelog` to flush the CHANGELOG. statistics - An Array containing columns from ProjectStatistics to refresh, if empty all columns will be refreshed refresh_statistics - A boolean that determines whether project statistics should be updated.",
    "label": "",
    "id": "5473"
  },
  {
    "raw_code": "def update_statistics(project, statistics = [])\n    return if Gitlab::Database.read_only?\n    return unless try_obtain_lease_for(project.id, statistics)\n\n    Projects::UpdateStatisticsService.new(project, nil, statistics: statistics).execute\n\n    lease_key = project_cache_worker_key(project.id, statistics)\n    UpdateProjectStatisticsWorker.perform_in(LEASE_TIMEOUT, lease_key, project.id, statistics)\n  end",
    "comment": "NOTE: triggering both an immediate update and one in 15 minutes if we successfully obtain the lease. That way, we only need to wait for the statistics to become accurate if they were already updated once in the last 15 minutes.",
    "label": "",
    "id": "5474"
  },
  {
    "raw_code": "def use_replica_if_available(&blk)\n    ::Gitlab::Database::LoadBalancing::SessionMap\n      .current(ContainerRepository.load_balancer)\n      .use_replicas_for_read_queries(&blk)\n  end",
    "comment": "data_consistency :delayed not used as this is a cron job and those jobs are not perfomed with a delay https://gitlab.com/gitlab-org/gitlab/-/merge_requests/63635#note_603771207",
    "label": "",
    "id": "5475"
  },
  {
    "raw_code": "def perform(issue_id, user_id, issuable_class = 'Issue', skip_notifications = false)\n    @issuable_class = issuable_class.constantize\n\n    return unless objects_found?(issue_id, user_id)\n\n    ::EventCreateService.new.open_issue(issuable, user)\n    ::NotificationService.new.new_issue(issuable, user) unless skip_notifications\n\n    issuable.create_cross_references!(user)\n\n    Issues::AfterCreateService\n      .new(container: issuable.project, current_user: user)\n      .execute(issuable)\n\n    log_audit_event if user.project_bot?\n  end",
    "comment": "TODO: Add skip_notifications argument to the invocations of the worker in the next release (16.9)",
    "label": "",
    "id": "5476"
  },
  {
    "raw_code": "def mark_stuck_jobs_as_failed!\n    jids_and_ids = enqueued_exports.pluck(:jid, :id).to_h\n\n    completed_jids = Gitlab::SidekiqStatus.completed_jids(jids_and_ids.keys)\n    return unless completed_jids.any?\n\n    completed_ids = jids_and_ids.values_at(*completed_jids)\n\n    # We select the export states again, because they may have transitioned from\n    # started to finished while we were looking up their Sidekiq status.\n    completed_jobs = enqueued_exports.where(id: completed_ids)\n\n    Sidekiq.logger.info(\n      message: 'Marked stuck export jobs as failed',\n      job_ids: completed_jobs.map(&:jid)\n    )\n\n    completed_jobs.each do |job|\n      # Parallel export job completes and keeps 'started' state because it has\n      # multiple relation exports running in parallel. Don't mark it as failed\n      # until 6 hours mark\n      next if job.relation_exports.any? && job.created_at > EXPORT_JOBS_EXPIRATION.seconds.ago\n\n      job.fail_op\n    end.count\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5477"
  },
  {
    "raw_code": "def enqueued_exports\n    ProjectExportJob.with_status([:started, :queued])\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5478"
  },
  {
    "raw_code": "def perform\n    project_ids = Issue\n      .with_issue_type(:issue)\n      .opened\n      .due_tomorrow\n      .group(:project_id)\n      .pluck(:project_id)\n      .map { |id| [id] }\n\n    MailScheduler::IssueDueWorker.bulk_perform_async(project_ids) # rubocop:disable Scalability/BulkPerformWithContext\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5479"
  },
  {
    "raw_code": "def perform(lease_key, project_id, statistics = [])\n    return unless Gitlab::ExclusiveLease\n      .new(lease_key, timeout: ProjectCacheWorker::LEASE_TIMEOUT)\n      .try_obtain\n\n    project = Project.find_by_id(project_id)\n\n    Projects::UpdateStatisticsService.new(project, nil, statistics: statistics).execute\n  end",
    "comment": "lease_key     - The exclusive lease key to take project_id    - The ID of the project for which to flush the cache. statistics    - An Array containing columns from ProjectStatistics to refresh, if empty all columns will be refreshed",
    "label": "",
    "id": "5480"
  },
  {
    "raw_code": "def perform(project_id, user_id, commit_hash, default = false)\n    project = Project.id_in(project_id).first\n\n    return unless project\n\n    user = User.id_in(user_id).first\n\n    return unless user\n\n    commit = Commit.build_from_sidekiq_hash(project, commit_hash)\n\n    process_commit_message(project, commit, user, default)\n    update_issue_metrics(commit, user)\n  end",
    "comment": "project_id - The ID of the project this commit belongs to. user_id - The ID of the user that pushed the commit. commit_hash - Hash containing commit details to use for constructing a Commit object without having to use the Git repository. default - The data was pushed to the default branch.",
    "label": "",
    "id": "5481"
  },
  {
    "raw_code": "def check_arguments!(args)\n      args.each do |arg|\n        if arg.class.in?(PERMITTED_TYPES)\n          raise(ArgumentError, \"Argument `#{arg}` cannot be deserialized because of its type\")\n        end",
    "comment": "If an argument is in the PERMITTED_TYPES list, it means the argument cannot be deserialized. Which means there's something wrong with our code.",
    "label": "",
    "id": "5482"
  },
  {
    "raw_code": "def perform(project_id)\n      Issue\n        .with_issue_type(:issue)\n        .opened\n        .due_tomorrow\n        .in_projects(project_id)\n        .preload(:project)\n        .find_each do |issue|\n          notification_service.issue_due(issue)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5483"
  },
  {
    "raw_code": "def import(project)\n          importer = importer_class.new(project)\n\n          importer.execute\n\n          ImportPullRequestsWorker.perform_async(project.id)\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5484"
  },
  {
    "raw_code": "def import(project)\n          waiter = importer_class.new(project).execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            :notes\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5485"
  },
  {
    "raw_code": "def import(project)\n          waiter = importer_class.new(project).execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            :lfs_objects\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5486"
  },
  {
    "raw_code": "def import(project)\n          placeholder_reference_store = project.placeholder_reference_store\n\n          if placeholder_reference_store&.any?\n            info(\n              project.id,\n              message: 'Delaying finalization as placeholder references are pending',\n              placeholder_store_count: placeholder_reference_store.count\n            )\n\n            reschedule(project)\n\n            return\n          end",
    "comment": "@param project [Project]",
    "label": "",
    "id": "5487"
  },
  {
    "raw_code": "def import(project)\n          waiter = importer_class.new(project).execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            :finish\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5488"
  },
  {
    "raw_code": "def import(project)\n          waiter = importer_class.new(project).execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            :issues\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5489"
  },
  {
    "raw_code": "def import(project)\n          waiter = importer_class.new(project).execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            :issues_notes\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5490"
  },
  {
    "raw_code": "def import(project)\n          waiter = importer_class.new(project).execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            :lfs_objects\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5491"
  },
  {
    "raw_code": "def import(project)\n          waiter = importer_class.new(project).execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            :pull_requests_notes\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5492"
  },
  {
    "raw_code": "def import(project)\n          waiter = importer_class.new(project).execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            :finish\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5493"
  },
  {
    "raw_code": "def import(client, project)\n          waiters = importers(project).each_with_object({}) do |klass, hash|\n            info(project.id, message: \"starting importer\", importer: klass.name)\n            waiter = klass.new(project, client).execute\n            hash[waiter.key] = waiter.jobs_remaining\n          end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5494"
  },
  {
    "raw_code": "def importers(project)\n          [\n            Importer::IssuesImporter,\n            diff_notes_importer(project)\n          ]\n        end",
    "comment": "The importers to run in this stage. Issues can't be imported earlier on as we also use these to enrich pull requests with assigned labels.",
    "label": "",
    "id": "5495"
  },
  {
    "raw_code": "def import(client, project)\n          info(project.id, message: \"starting importer\", importer: 'Importer::RepositoryImporter')\n\n          # If a user creates an issue while the import is in progress, this can lead to an import failure.\n          # The workaround is to allocate IIDs before starting the importer.\n          allocate_issues_internal_id!(project, client)\n\n          importer = Importer::RepositoryImporter.new(project, client)\n\n          importer.execute\n\n          counter.increment\n\n          ImportBaseDataWorker.perform_async(project.id)\n        end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5496"
  },
  {
    "raw_code": "def import(client, project)\n          info(project.id, message: \"starting importer\", importer: 'Importer::PullRequestsImporter')\n\n          # If a user creates a new merge request while the import is in progress, GitLab can assign an IID\n          # to this merge request that already exists for a GitHub Pull Request.\n          # The workaround is to allocate IIDs before starting the importer.\n          allocate_merge_requests_internal_id!(project, client)\n\n          waiter = Importer::PullRequestsImporter\n            .new(project, client)\n            .execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            'collaborators'\n          )\n        end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5497"
  },
  {
    "raw_code": "def import(client, project)\n          importer = ::Gitlab::GithubImport::Importer::SingleEndpointIssueEventsImporter\n          info(project.id, message: \"starting importer\", importer: importer.name)\n          waiter = importer.new(project, client).execute\n\n          AdvanceStageWorker.perform_async(project.id, { waiter.key => waiter.jobs_remaining }, 'attachments')\n        end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5498"
  },
  {
    "raw_code": "def import(_, project)\n          @project = project\n\n          return self.class.perform_in(30.seconds, project.id) if reference_store_pending?\n\n          project.after_import\n\n          report_import_time\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5499"
  },
  {
    "raw_code": "def import(client, project)\n          return move_to_next_stage(project, {}) unless import_collaborators?(project)\n\n          unless has_push_access?(client, project.import_source)\n            log_no_push_access(project)\n            return move_to_next_stage(project, {})\n          end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5500"
  },
  {
    "raw_code": "def import(client, project)\n          info(project.id, message: \"starting importer\", importer: 'Importer::ProtectedBranchesImporter')\n          waiter = Importer::ProtectedBranchesImporter\n            .new(project, client)\n            .execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            'lfs_objects'\n          )\n        end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5501"
  },
  {
    "raw_code": "def import(_client, project)\n          waiter = Importer::LfsObjectsImporter\n            .new(project, nil)\n            .execute\n\n          AdvanceStageWorker.perform_async(\n            project.id,\n            { waiter.key => waiter.jobs_remaining },\n            'finish'\n          )\n        end",
    "comment": "project - An instance of Project.",
    "label": "",
    "id": "5502"
  },
  {
    "raw_code": "def import(client, project)\n          return skip_to_next_stage(project) if import_settings(project).disabled?(:attachments_import)\n\n          waiters = importers.each_with_object({}) do |importer, hash|\n            waiter = start_importer(project, importer, client)\n            hash[waiter.key] = waiter.jobs_remaining\n          end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5503"
  },
  {
    "raw_code": "def importers\n          [\n            ::Gitlab::GithubImport::Importer::Attachments::ReleasesImporter,\n            ::Gitlab::GithubImport::Importer::Attachments::NotesImporter,\n            ::Gitlab::GithubImport::Importer::Attachments::IssuesImporter,\n            ::Gitlab::GithubImport::Importer::Attachments::MergeRequestsImporter\n          ]\n        end",
    "comment": "For future issue/mr/milestone/etc attachments importers",
    "label": "",
    "id": "5504"
  },
  {
    "raw_code": "def import(client, project)\n          IMPORTERS.each do |klass|\n            info(project.id, message: \"starting importer\", importer: klass.name)\n            klass.new(project, client).execute\n          end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5505"
  },
  {
    "raw_code": "def perform(project_id, check_job_id, _params = {})\n        return unless SidekiqStatus.running?(check_job_id)\n\n        import_state_jid = ProjectImportState.jid_by(project_id: project_id, status: :started)&.jid\n        return unless import_state_jid\n\n        # As long as the worker is running we want to keep refreshing\n        # the worker's JID as well as the import's JID.\n        Gitlab::SidekiqStatus.expire(check_job_id, Gitlab::Import::StuckImportJob::IMPORT_JOBS_EXPIRATION)\n        Gitlab::SidekiqStatus.set(import_state_jid, Gitlab::Import::StuckImportJob::IMPORT_JOBS_EXPIRATION)\n\n        self.class.perform_in_the_future(project_id, check_job_id)\n      end",
    "comment": "project_id - The ID of the project that is being imported. check_job_id - The ID of the job for which to check the status. params - to avoid multiple releases if parameters change",
    "label": "",
    "id": "5506"
  },
  {
    "raw_code": "def perform(project_id, waiters, next_stage, timeout_timer = Time.zone.now.to_s, previous_job_count = nil)\n        import_state_jid = find_import_state_jid(project_id)\n\n        # If the import state is nil the project may have been deleted or the import\n        # may have failed or been canceled. In this case we tidy up the cache data and no\n        # longer attempt to advance to the next stage.\n        if import_state_jid.nil?\n          clear_waiter_caches(waiters)\n          return\n        end",
    "comment": "project_id - The ID of the project being imported. waiters - A Hash mapping Gitlab::JobWaiter keys to the number of remaining jobs. next_stage - The name of the next stage to start when all jobs have been completed. timeout_timer - Time the sidekiq worker was first initiated with the current job_count previous_job_count - Number of jobs remaining on last invocation of this worker",
    "label": "",
    "id": "5507"
  },
  {
    "raw_code": "def perform\n      aggregation_schedules.find_each do |aggregation_schedule|\n        aggregation_schedule.schedule_root_storage_statistics\n      end",
    "comment": "Worker to prune pending rows on Namespace::AggregationSchedule It's scheduled to run once a day at 1:05am.",
    "label": "",
    "id": "5508"
  },
  {
    "raw_code": "def aggregation_schedules_table_exists?\n      return true unless Rails.env.test?\n\n      Namespace::AggregationSchedule.table_exists?\n    end",
    "comment": "On db/post_migrate/20180529152628_schedule_to_archive_legacy_traces.rb traces are archived through build.trace.archive, which in consequence calls UpdateProjectStatistics#schedule_namespace_statistics_worker.  The migration and specs fails since NamespaceAggregationSchedule table does not exist at that point. https://gitlab.com/gitlab-org/gitlab-foss/issues/50712",
    "label": "",
    "id": "5509"
  },
  {
    "raw_code": "def perform\n      limiter = Gitlab::Metrics::RuntimeLimiter.new(MAX_RUNTIME)\n      ids_to_cache = Set.new\n      last_id = get_last_id\n\n      # 1. Iterate over groups.\n      # 2. For each group, start counting the descendants.\n      # 3. When CACHE_THRESHOLD count is reached, stop the counting.\n      Group.where('id > ?', last_id || 0).each_batch(of: GROUP_BATCH_SIZE) do |relation|\n        relation.select(:id).each do |group|\n          cursor = { current_id: group.id, depth: [group.id] }\n          iterator = Gitlab::Database::NamespaceEachBatch.new(namespace_class: Namespace, cursor: cursor)\n          count = 0\n          iterator.each_batch(of: NAMESPACE_BATCH_SIZE) do |ids|\n            count += ids.size\n\n            break if count >= CACHE_THRESHOLD || limiter.over_time?\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- Batching over groups.",
    "label": "",
    "id": "5510"
  },
  {
    "raw_code": "def persist(ids_to_cache)\n      ids_to_cache.each_slice(PERSIST_SLICE_SIZE) do |slice|\n        Namespaces::Descendants.upsert_all(slice.map { |id| { namespace_id: id, outdated_at: Time.current } })\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5511"
  },
  {
    "raw_code": "def perform(group_id, statistics = [])\n      group = Group.find_by_id(group_id)\n\n      return unless group\n\n      Groups::UpdateStatisticsService.new(group, statistics: statistics).execute\n    end",
    "comment": "group_id - The ID of the group for which to flush the cache. statistics - An Array containing columns from NamespaceStatistics to refresh, if empty all columns will be refreshed",
    "label": "",
    "id": "5512"
  },
  {
    "raw_code": "def perform(project_id, user_id, issue_id, merge_request_id, params = {})\n      project = Project.find_by_id(project_id)\n\n      unless project\n        logger.info(structured_payload(message: 'Project not found.', project_id: project_id))\n        return\n      end",
    "comment": "This worker only accepts ID of an Issue. We are intentionally using this worker to close Issues asynchronously as we only experience SQL timeouts when closing an Issue.",
    "label": "",
    "id": "5513"
  },
  {
    "raw_code": "def perform(merge_request_id, only)\n      merge_request = MergeRequest.find_by_id(merge_request_id)\n      return unless merge_request\n\n      serialized_remove_refs(merge_request.target_project_id) do\n        merge_request.cleanup_refs(only: only.to_sym)\n      end",
    "comment": "Even though this worker is de-duplicated we need to acquire lock on a project to avoid running many concurrent refs removals  TODO: Once underlying fix is done we can remove `in_lock`  Related to: - https://gitlab.com/gitlab-org/gitaly/-/issues/5368 - https://gitlab.com/gitlab-org/gitaly/-/issues/5369",
    "label": "",
    "id": "5514"
  },
  {
    "raw_code": "def handle_event(event)\n      merge_request_id = event.data[:merge_request_id]\n      merge_request = MergeRequest.find_by_id(merge_request_id)\n\n      unless merge_request\n        logger.info(structured_payload(message: 'Merge request not found.', merge_request_id: merge_request_id))\n        return\n      end",
    "comment": "The difference with this worker and AutoMergeProcessWorker is that this will handle the execution from the event store code",
    "label": "",
    "id": "5515"
  },
  {
    "raw_code": "def perform(project_id, user_id, oldrev, newrev, ref)\n        project = Project.find_by_id(project_id)\n        return unless project\n\n        user = User.find_by_id(user_id)\n        return unless user\n\n        MergeRequests::Refresh::WebHooksService\n          .new(project: project, current_user: user)\n          .execute(oldrev, newrev, ref)\n      end",
    "comment": "NOTE: This worker will be deprecated once we switch to using events",
    "label": "",
    "id": "5516"
  },
  {
    "raw_code": "def current_connection_name_and_base_model\n      minutes_since_epoch = Time.current.to_i / 60\n      connections_with_name = Gitlab::Database.database_base_models_with_gitlab_shared.to_a # this will never be empty\n      connections_with_name[minutes_since_epoch % connections_with_name.count]\n    end",
    "comment": "Rotate the databases every minute  If one DB is configured: every minute use the configured DB If two DBs are configured (Main, CI): minute 1 -> Main, minute 2 -> CI",
    "label": "",
    "id": "5517"
  },
  {
    "raw_code": "def lease_key\n      \"container_registry_data_repair_detail_worker:#{next_project.id}\"\n    end",
    "comment": "Used by ExclusiveLeaseGuard",
    "label": "",
    "id": "5518"
  },
  {
    "raw_code": "def lease_timeout\n      LEASE_TIMEOUT\n    end",
    "comment": "Used by ExclusiveLeaseGuard",
    "label": "",
    "id": "5519"
  },
  {
    "raw_code": "def perform\n      order = Gitlab::Pagination::Keyset::Order.build(\n        [\n          Gitlab::Pagination::Keyset::ColumnOrderDefinition.new(\n            attribute_name: 'expires_at_utc',\n            order_expression: Arel.sql(\"date(expires_at AT TIME ZONE 'UTC')\").asc,\n            nullable: :not_nullable,\n            add_to_projections: true\n          ),\n          Gitlab::Pagination::Keyset::ColumnOrderDefinition.new(\n            attribute_name: 'id',\n            order_expression: Key.arel_table[:id].asc\n          )\n        ])\n\n      scope = Key.expired_today_and_not_notified.order(order)\n\n      iterator = Gitlab::Pagination::Keyset::Iterator.new(scope: scope, use_union_optimization: true)\n      iterator.each_batch(of: BATCH_SIZE) do |relation|\n        users = User.where(id: relation.map(&:user_id)) # Keyset pagination will load the rows\n\n        users.each do |user|\n          with_context(user: user) do\n            Keys::ExpiryNotificationService.new(user, { keys: user.expired_today_and_unnotified_keys, expiring_soon: false }).execute\n          end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5520"
  },
  {
    "raw_code": "def perform(release_id, pipeline_id = nil)\n      release = Release.find_by_id(release_id)\n\n      return unless release\n\n      pipeline = Ci::Pipeline.find_by_id(pipeline_id)\n\n      ::Releases::CreateEvidenceService.new(release, pipeline: pipeline).execute\n    end",
    "comment": "pipeline_id is optional for backward compatibility with existing jobs caller should always try to provide the pipeline and pass nil only if pipeline is absent",
    "label": "",
    "id": "5521"
  },
  {
    "raw_code": "def execution_limit\n    DEFAULT_EXECUTION_LIMIT\n  end",
    "comment": "Override this method to customize the execution_limit",
    "label": "",
    "id": "5522"
  },
  {
    "raw_code": "def get_feature_category\n      feature_category = get_class_attribute(:feature_category)\n      calling_context_feature_category_preferred = !!get_class_attribute(:prefer_calling_context_feature_category)\n\n      return feature_category unless feature_category == :not_owned || calling_context_feature_category_preferred\n\n      Gitlab::ApplicationContext.current_context_attribute('meta.feature_category') || feature_category\n    end",
    "comment": "Special case: if a worker is not owned, get the feature category (if present) from the calling context.",
    "label": "",
    "id": "5523"
  },
  {
    "raw_code": "def urgency(urgency)\n      raise \"Invalid urgency: #{urgency}\" unless VALID_URGENCIES.include?(urgency)\n\n      set_class_attribute(:urgency, urgency)\n    end",
    "comment": "This should be set to :high for jobs that need to be run immediately, or, if they are delayed, risk creating inconsistencies in the application that could being perceived by the user as incorrect behavior (ie, a bug)  See doc/development/sidekiq_style_guide.md#urgency for details",
    "label": "",
    "id": "5524"
  },
  {
    "raw_code": "def data_consistency(default, overrides: nil, feature_flag: nil)\n      validate_data_consistency(default, overrides)\n      raise ArgumentError, 'Data consistency is already set' if class_attributes[:data_consistency]\n\n      set_class_attribute(:data_consistency_feature_flag, feature_flag) if feature_flag\n      set_class_attribute(:data_consistency, default)\n\n      # only override data consistency when using multiple databases\n      overrides = nil unless Gitlab::Database.database_mode == Gitlab::Database::MODE_MULTIPLE_DATABASES\n      set_class_attribute(:data_consistency_per_database, compute_data_consistency_per_database(default, overrides))\n    end",
    "comment": "Allows configuring worker's data_consistency.  Worker can utilize Sidekiq readonly database replicas capabilities by setting data_consistency attribute. Workers with data_consistency set to :delayed or :sticky, calling #perform_async will be delayed in order to give replication process enough time to complete.  - *default* - The default data_consistency value. Valid values are: - 'always' - The job is required to use the primary database (default). - 'sticky' - The job uses a replica as long as possible. It switches to primary either on write or long replication lag. - 'delayed' - The job would switch to primary only on write. It would use replica always. If there's a long replication lag the job will be delayed, and only if the replica is not up to date on the next retry, it will switch to the primary. - *overrides* - allows you to override data consistency for specific database connections. Only used in multiple database mode. Valid for values in `Gitlab::Database.database_base_models.keys` - *feature_flag* - allows you to toggle a job's `data_consistency, which permits you to safely toggle load balancing capabilities for a specific job. If disabled, job will default to `:always`, which means that the job will always use the primary.",
    "label": "",
    "id": "5525"
  },
  {
    "raw_code": "def utilizes_load_balancing_capabilities?\n      get_data_consistency_per_database.values.any? { |v| LOAD_BALANCED_DATA_CONSISTENCIES.include?(v) }\n    end",
    "comment": "If data_consistency is not set to :always, worker will try to utilize load balancing capabilities and use the replica",
    "label": "",
    "id": "5526"
  },
  {
    "raw_code": "def worker_has_external_dependencies!\n      set_class_attribute(:external_dependencies, true)\n    end",
    "comment": "Set this attribute on a job when it will call to services outside of the application, such as 3rd party applications, other k8s clusters etc See doc/development/sidekiq_style_guide.md#jobs-with-external-dependencies for details",
    "label": "",
    "id": "5527"
  },
  {
    "raw_code": "def worker_has_external_dependencies?\n      !!get_class_attribute(:external_dependencies)\n    end",
    "comment": "Returns true if the worker has external dependencies. See doc/development/sidekiq_style_guide.md#jobs-with-external-dependencies for details",
    "label": "",
    "id": "5528"
  },
  {
    "raw_code": "def get_concurrency_limit\n      return 0 if Feature.disabled?(:sidekiq_concurrency_limit_middleware, Feature.current_request, type: :ops)\n\n      limit = get_class_attribute(:concurrency_limit)&.call\n      return limit.to_i unless limit.nil?\n\n      unless Feature.enabled?(:use_max_concurrency_limit_percentage_as_default_limit, Feature.current_request)\n        return limit.to_i # limit must be nil now, so cast it to 0\n      end",
    "comment": "Returns an integer value where: - positive value is returned to enforce a valid concurrency limit - 0 value is returned for workers without concurrency limits - negative value is returned for paused workers",
    "label": "",
    "id": "5529"
  },
  {
    "raw_code": "def max_concurrency_limit_percentage(value)\n      unless value.is_a?(Numeric) && value.between?(0, 1)\n        raise ArgumentError, \"max_concurrency_limit_percentage must be a number between 0 and 1, got: #{value.inspect}\"\n      end",
    "comment": "If concurrency_limit attribute is not defined, this sets the maximum percentage of fleet that this worker can use. For example, 0.3 means the worker class is allowed to use 30% of the fleet's threads concurrently.",
    "label": "",
    "id": "5530"
  },
  {
    "raw_code": "def minimum_duration\n    5.seconds\n  end",
    "comment": "Override as needed",
    "label": "",
    "id": "5531"
  },
  {
    "raw_code": "def ensure_minimum_duration(minimum_duration)\n      start_time = Time.current\n\n      result = yield\n\n      sleep_if_time_left(minimum_duration, start_time)\n\n      result\n    end",
    "comment": "The block will run, and then sleep until the minimum duration. Returns the block's return value.  Usage:  ensure_minimum_duration(5.seconds) do # do something end ",
    "label": "",
    "id": "5532"
  },
  {
    "raw_code": "def context_for_arguments(_args)\n      return if Gitlab::ApplicationContext.current_context_include?(:caller_id)\n\n      Gitlab::ApplicationContext.new(caller_id: \"Cronjob\")\n    end",
    "comment": "Cronjobs never get scheduled with arguments, so this is safe to override",
    "label": "",
    "id": "5533"
  },
  {
    "raw_code": "def eligible_shard_names\n    healthy_shard_names\n  end",
    "comment": "override when you want to filter out some shards",
    "label": "",
    "id": "5534"
  },
  {
    "raw_code": "def retry_disabled?\n      get_sidekiq_options['retry'] == 0 || get_sidekiq_options['retry'] == false\n    end",
    "comment": "Checks if sidekiq retry support is disabled",
    "label": "",
    "id": "5535"
  },
  {
    "raw_code": "def loggable_arguments(*args)\n      if args.any?\n        @loggable_arguments = args\n      else\n        @loggable_arguments || []\n      end",
    "comment": "Set/get which arguments can be logged and sent to Sentry.  Numeric arguments are logged by default, so there is no need to list those.  Non-numeric arguments must be listed by position, as Sidekiq cannot see argument names. ",
    "label": "",
    "id": "5536"
  },
  {
    "raw_code": "def import(project, hash)\n        info(project.id, message: 'importer started')\n\n        importer_class.new(project, hash).execute\n\n        info(project.id, message: 'importer finished')\n      rescue ActiveRecord::RecordInvalid => e\n        # We do not raise exception to prevent job retry\n        track_exception(project, e)\n      rescue StandardError => e\n        track_and_raise_exception(project, e)\n      end",
    "comment": "project - An instance of `Project` to import the data into. hash - A Hash containing the details of the object to import.",
    "label": "",
    "id": "5537"
  },
  {
    "raw_code": "def importer_class\n        raise NotImplementedError\n      end",
    "comment": "Returns the class to use for importing the object.",
    "label": "",
    "id": "5538"
  },
  {
    "raw_code": "def perform(project_id)\n        info(project_id, message: 'starting stage')\n\n        return unless (project = find_project(project_id))\n\n        Import::RefreshImportJidWorker.perform_in_the_future(project_id, jid)\n\n        import(project)\n\n        info(project_id, message: 'stage finished')\n      rescue StandardError => e\n        Gitlab::Import::ImportFailureService.track(\n          project_id: project_id,\n          exception: e,\n          error_source: self.class.name,\n          fail_import: abort_on_failure\n        )\n\n        raise(e)\n      end",
    "comment": "project_id - The ID of the GitLab project to import the data into.",
    "label": "",
    "id": "5539"
  },
  {
    "raw_code": "def import(project, hash)\n        info(project.id, message: 'importer started')\n\n        importer_class.new(project, hash).execute\n\n        info(project.id, message: 'importer finished')\n      rescue ActiveRecord::RecordInvalid => e\n        # We do not raise exception to prevent job retry\n        track_exception(project, e)\n      rescue StandardError => e\n        track_and_raise_exception(project, e)\n      end",
    "comment": "project - An instance of `Project` to import the data into. hash - A Hash containing the details of the object to import.",
    "label": "",
    "id": "5540"
  },
  {
    "raw_code": "def importer_class\n        raise NotImplementedError\n      end",
    "comment": "Returns the class to use for importing the object.",
    "label": "",
    "id": "5541"
  },
  {
    "raw_code": "def perform(project_id)\n        info(project_id, message: 'starting stage')\n\n        project = find_project(project_id)\n\n        return unless project\n\n        Import::RefreshImportJidWorker.perform_in_the_future(project_id, jid)\n\n        import(project)\n\n        info(project_id, message: 'stage finished')\n      rescue StandardError => e\n        Gitlab::Import::ImportFailureService.track(\n          project_id: project_id,\n          exception: e,\n          error_source: self.class.name,\n          fail_import: abort_on_failure\n        )\n\n        raise(e)\n      end",
    "comment": "project_id - The ID of the GitLab project to import the data into.",
    "label": "",
    "id": "5542"
  },
  {
    "raw_code": "def import(project, client, hash)\n        if project.import_state&.completed?\n          info(\n            project.id,\n            message: 'Project import is no longer running. Stopping worker.',\n            import_status: project.import_state.status\n          )\n\n          return\n        end",
    "comment": "project - An instance of `Project` to import the data into. client - An instance of `Gitlab::GithubImport::Client` hash - A Hash containing the details of the object to import.",
    "label": "",
    "id": "5543"
  },
  {
    "raw_code": "def representation_class\n        raise NotImplementedError\n      end",
    "comment": "Returns the representation class to use for the object. This class must define the class method `from_json_hash`.",
    "label": "",
    "id": "5544"
  },
  {
    "raw_code": "def importer_class\n        raise NotImplementedError\n      end",
    "comment": "Returns the class to use for importing the object.",
    "label": "",
    "id": "5545"
  },
  {
    "raw_code": "def perform(project_id)\n        info(project_id, message: 'starting stage')\n\n        return unless (project = Project.find_by_id(project_id))\n\n        if project.import_state&.completed?\n          info(\n            project_id,\n            message: 'Project import is no longer running. Stopping worker.',\n            import_status: project.import_state.status\n          )\n\n          return\n        end",
    "comment": "project_id - The ID of the GitLab project to import the data into.",
    "label": "",
    "id": "5546"
  },
  {
    "raw_code": "def try_import(client, project)\n        import(client, project)\n\n        info(project.id, message: 'stage finished')\n      rescue RateLimitError, UserFinder::FailedToObtainLockError => e\n        info(project.id, message: \"stage retrying\", exception_class: e.class.name)\n\n        self.class.perform_in(client.rate_limit_resets_in, project.id)\n      end",
    "comment": "client - An instance of Gitlab::GithubImport::Client. project - An instance of Project.",
    "label": "",
    "id": "5547"
  },
  {
    "raw_code": "def perform(project_id, hash, notify_key = nil)\n        @project = Project.find_by_id(project_id) # rubocop:disable Gitlab/ModuleWithInstanceVariables -- GitHub Import\n        # uses modules everywhere. Too big to refactor.\n\n        return notify_waiter(notify_key) unless project\n\n        client = GithubImport.new_client_for(project, parallel: true)\n\n        if try_import(project, client, hash)\n          notify_waiter(notify_key)\n        else\n          reschedule_job(project, client, hash, notify_key)\n        end",
    "comment": "project_id - The ID of the GitLab project to import the note into. hash - A Hash containing the details of the GitHub object to import. notify_key - The Redis key to notify upon completion, if any.",
    "label": "",
    "id": "5548"
  },
  {
    "raw_code": "def perform(issue_id, project_id = nil)\n      issue = find_issue(issue_id, project_id)\n      return unless issue\n\n      # Temporary disable moving null elements because of performance problems\n      # For more information check https://gitlab.com/gitlab-com/gl-infra/production/-/issues/4321\n      return if issue.blocked_for_repositioning?\n\n      # Move the oldest 100 unpositioned items to the end.\n      # This is to deal with out-of-order execution of the worker,\n      # while preserving creation order.\n      to_place = Issue\n                 .relative_positioning_query_base(issue)\n                 .with_null_relative_position\n                 .order({ created_at: :asc }, { id: :asc })\n                 .limit(QUERY_LIMIT + 1)\n                 .to_a\n\n      leftover = to_place.pop if to_place.count > QUERY_LIMIT\n\n      Issue.move_nulls_to_end(to_place)\n      Issues::BaseService.new(container: nil).rebalance_if_needed(to_place.max_by(&:relative_position))\n      Issues::PlacementWorker.perform_async(nil, leftover.project_id) if leftover.present?\n    rescue RelativePositioning::NoSpaceLeft => e\n      Gitlab::ErrorTracking.log_exception(e, issue_id: issue_id, project_id: project_id)\n      Issues::RebalancingWorker.perform_async(nil, *root_namespace_id_to_rebalance(issue, project_id))\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5549"
  },
  {
    "raw_code": "def project_ids\n      never_checked_project_ids(BATCH_SIZE) + old_checked_project_ids(BATCH_SIZE)\n    end",
    "comment": "Project.find_each does not support WHERE clauses and Project.find_in_batches does not support ordering. So we just build an array of ID's. This is OK because we do it only once an hour, because getting ID's from Postgres is not terribly slow, and because no user has to sit and wait for this query to finish.",
    "label": "",
    "id": "5550"
  },
  {
    "raw_code": "def never_checked_project_ids(batch_size)\n      projects_on_shard.where(last_repository_check_at: nil)\n        .where('created_at < ?', 24.hours.ago)\n        .limit(batch_size).pluck(:id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5551"
  },
  {
    "raw_code": "def old_checked_project_ids(batch_size)\n      projects_on_shard.where.not(last_repository_check_at: nil)\n        .where('last_repository_check_at < ?', 1.month.ago)\n        .reorder(last_repository_check_at: :asc)\n        .limit(batch_size).pluck(:id)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5552"
  },
  {
    "raw_code": "def projects_on_shard\n      Project.where(repository_storage: shard_name)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5553"
  },
  {
    "raw_code": "def try_obtain_lease_for_project(id)\n      # Use a 24-hour timeout because on servers/projects where 'git fsck' is\n      # super slow we definitely do not want to run it twice in parallel.\n      Gitlab::ExclusiveLease.new(\n        \"project_repository_check:#{id}\",\n        timeout: 24.hours\n      ).try_obtain\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5554"
  },
  {
    "raw_code": "def perform\n      # Do small batched updates because these updates will be slow and locking\n      Project.select(:id).find_in_batches(batch_size: 100) do |batch|\n        Project.where(id: batch.map(&:id)).update_all(\n          last_repository_check_failed: nil,\n          last_repository_check_at: nil\n        )\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5555"
  },
  {
    "raw_code": "def has_changes?(project)\n      Project.with_push.exists?(project.id)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5556"
  },
  {
    "raw_code": "def has_wiki_changes?(project)\n      return false unless project.wiki_enabled?\n\n      # Historically some projects never had their wiki repos initialized;\n      # this happens on project creation now. Let's initialize an empty repo\n      # if it is not already there.\n      return false unless project.create_wiki\n\n      has_changes?(project)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5557"
  },
  {
    "raw_code": "def perform(pipeline_id)\n      pipeline = Ci::Pipeline.find_by_id(pipeline_id)\n      return unless pipeline\n      return unless pipeline.persistent_ref.should_delete?\n\n      serialized_remove_refs(pipeline.project_id) do\n        pipeline.reset.persistent_ref.delete\n      end",
    "comment": "Even though this worker is de-duplicated we need to acquire lock on a project to avoid running many concurrent refs removals  TODO: Once underlying fix is done we can remove `in_lock`  Related to: - https://gitlab.com/gitlab-org/gitaly/-/issues/5368 - https://gitlab.com/gitlab-org/gitaly/-/issues/5369",
    "label": "",
    "id": "5558"
  },
  {
    "raw_code": "def process_pipeline(pipeline)\n      return unless finished_pipeline_sync_event?(pipeline)\n\n      # Use upsert since this code can be called more than once for the same pipeline\n      ::Ci::FinishedPipelineChSyncEvent.upsert(\n        {\n          pipeline_id: pipeline.id,\n          pipeline_finished_at: pipeline.finished_at,\n          project_namespace_id: pipeline.project.project_namespace_id\n        },\n        unique_by: [:pipeline_id, :partition]\n      )\n    end",
    "comment": "Processes a single CI pipeline that has finished.  @param [Ci::Pipeline] pipeline The pipeline to process.",
    "label": "",
    "id": "5559"
  },
  {
    "raw_code": "def process_build(build)\n      # We execute these in sync to reduce IO.\n      build.update_coverage\n      Ci::BuildReportResultService.new.execute(build)\n\n      build.execute_hooks\n      ChatNotificationWorker.perform_async(build.id) if build.pipeline.chat?\n      build.track_deployment_usage\n      build.track_verify_environment_usage\n      build.remove_token!\n\n      if build.failed? && !build.auto_retry_expected?\n        ::Ci::MergeRequests::AddTodoWhenBuildFailsWorker.perform_async(build.id)\n      end",
    "comment": "Processes a single CI build that has finished.  This logic resides in a separate method so that EE can extend it more easily.  @param [Ci::Build] build The build to process.",
    "label": "",
    "id": "5560"
  },
  {
    "raw_code": "def perform_work(_model)\n          # no-op\n        end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5561"
  },
  {
    "raw_code": "def perform(pipeline_tracker_id, _stage, entity_id)\n      @entity = ::BulkImports::Entity.find(entity_id)\n      @pipeline_tracker = ::BulkImports::Tracker.find(pipeline_tracker_id)\n\n      log_extra_metadata_on_done(:pipeline_class, @pipeline_tracker.pipeline_name)\n\n      try_obtain_lease do\n        if pipeline_tracker.enqueued? || pipeline_tracker.started?\n          logger.info(log_attributes(message: 'Pipeline starting'))\n          run\n        elsif pipeline_tracker.created?\n          Gitlab::ErrorTracking.log_exception(\n            Pipeline::FailedError.new('Pipeline in invalid status'),\n            log_attributes\n          )\n        else\n          logger.warn(log_attributes(message: 'Pipeline in invalid status'))\n        end",
    "comment": "Keep _stage parameter for backwards compatibility.",
    "label": "",
    "id": "5562"
  },
  {
    "raw_code": "def next_batch_count\n      limit = batch_limit - pipeline_tracker.batches.in_progress.limit(batch_limit).count\n      [limit, 0].max\n    end",
    "comment": "Calculate the number of batches, up to `batch_limit`, to process in the next round.",
    "label": "",
    "id": "5563"
  },
  {
    "raw_code": "def perform\n      Gitlab::Pagination::Keyset::Iterator.new(scope: bulk_import_scope).each_batch do |imports|\n        imports.each do |import|\n          logger.error(message: 'BulkImport stale', bulk_import_id: import.id)\n          import.cleanup_stale\n        end",
    "comment": "Using Keyset pagination for scopes that involve timestamp indexes",
    "label": "",
    "id": "5564"
  },
  {
    "raw_code": "def perform(object_ids, klass, tracker_id)\n      @tracker = BulkImports::Tracker.find_by_id(tracker_id)\n\n      return unless tracker\n\n      project = tracker.entity.project\n\n      klass.constantize.where(id: object_ids, project_id: project.id).find_each do |object|\n        transform_and_save(object)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5565"
  },
  {
    "raw_code": "def perform(user_id, project_id, after_export_strategy = {}, params = {})\n        project = Project.find_by_id(project_id)\n        return unless project\n\n        params.symbolize_keys!\n\n        project_export_job = project.export_jobs.find_or_create_by!(jid: jid) do |export_job|\n          export_job.user_id = user_id\n          export_job.exported_by_admin = !!params[:exported_by_admin]\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5566"
  },
  {
    "raw_code": "def self.batch_size\n      BATCH_SIZE\n    end",
    "comment": "allows easier stubbing in specs",
    "label": "",
    "id": "5567"
  },
  {
    "raw_code": "def process_project_bot_tokens(interval = :seven_days)\n      notifications_delivered = 0\n      project_bot_ids_without_resource = []\n      project_bot_ids_with_failed_delivery = []\n      min_expires_at = nil\n      interval_field = \"#{interval}_notification_sent_at\"\n      bot_user_notified_ids = []\n\n      loop do\n        exclude_user_ids = project_bot_ids_without_resource | project_bot_ids_with_failed_delivery\n        tokens = fetch_bot_tokens(interval, min_expires_at, exclude_user_ids)\n        break if tokens.empty?\n\n        token_ids = tokens.pluck(:id)\n        bot_user_ids = tokens.pluck(:user_id).uniq\n\n        bot_users = User.id_in(bot_user_ids).with_personal_access_tokens_and_resources\n\n        bot_users.each do |project_bot|\n          if project_bot.resource_bot_resource.nil?\n            project_bot_ids_without_resource << project_bot.id\n\n            next\n          end",
    "comment": "this code is quite messy, and should be refactored along with the split out https://gitlab.com/gitlab-org/gitlab/-/issues/495766  rubocop: disable CodeReuse/ActiveRecord -- We need to specify batch size to avoid timing out of worker",
    "label": "",
    "id": "5568"
  },
  {
    "raw_code": "def deliver_bot_notifications(bot_user, token_name, days_to_expire: 7)\n      notification_service.bot_resource_access_token_about_to_expire(\n        bot_user,\n        token_name,\n        days_to_expire: days_to_expire\n      )\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5569"
  },
  {
    "raw_code": "def perform(hook_id, log_data, response_category, _unique_by)\n      hook = ::WebHook.find_by_id(hook_id)\n\n      return unless hook # hook has been deleted before we could run.\n\n      ::WebHooks::LogExecutionService\n        .new(hook: hook, log_data: log_data, response_category: response_category.to_sym)\n        .execute\n    end",
    "comment": "This worker accepts an extra argument. This enables us to treat this worker as idempotent. Currently this is set to the Job ID (jid) of the parent worker.",
    "label": "",
    "id": "5570"
  },
  {
    "raw_code": "def merge_requests_to_sync(project)\n      project.merge_requests.with_jira_issue_keys\n        .preload(:author, :approvals, merge_request_reviewers: :reviewer)\n        .limit(MAX_RECORDS_LIMIT)\n        .order(id: :desc)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5571"
  },
  {
    "raw_code": "def branches_to_sync(project)\n      project.repository.branches_sorted_by(SORT_UPDATED_RECENT).filter_map do |branch|\n        branch if branch.name.match(Gitlab::Regex.jira_issue_key_regex)\n      end.first(MAX_RECORDS_LIMIT)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5572"
  },
  {
    "raw_code": "def expire_caches(post_received, repository)\n      repository.expire_status_cache if repository.empty?\n      expire_branch_cache(post_received, repository) if post_received.includes_branches?\n      expire_tag_cache(post_received, repository) if post_received.includes_tags?\n    end",
    "comment": "Expire the repository status, branch, and tag cache once per push.",
    "label": "",
    "id": "5573"
  },
  {
    "raw_code": "def enqueue_project_cache_update(post_received, project)\n      stats_to_invalidate = %w[repository_size]\n      stats_to_invalidate << 'commit_count' if post_received.includes_default_branch?\n\n      ProjectCacheWorker.perform_async(project.id, [], stats_to_invalidate, true)\n    end",
    "comment": "Schedule an update for the repository size and commit count if necessary.",
    "label": "",
    "id": "5574"
  },
  {
    "raw_code": "def self.enqueue!(uploads, to_store)\n      perform_async(uploads.ids, to_store)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5575"
  },
  {
    "raw_code": "def perform(*args)\n      ids, to_store = retrieve_applicable_args!(args)\n\n      @to_store = to_store\n\n      uploads = Upload.preload(:model).where(id: ids)\n\n      results = migrate(uploads)\n\n      report!(results)\n    rescue SanityCheckError => e\n      # do not retry: the job is insane\n      Gitlab::AppLogger.warn \"#{self.class}: Sanity check error (#{e.message})\"\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5576"
  },
  {
    "raw_code": "def perform(_pool_id, project_id)\n      project = Project.find_by_id(project_id)\n      return unless project&.pool_repository&.joinable?\n\n      project.link_pool_repository\n\n      ::Repositories::HousekeepingService.new(project).execute\n    end",
    "comment": "The use of pool id is deprecated. Keeping the argument allows old jobs to still be performed.",
    "label": "",
    "id": "5577"
  },
  {
    "raw_code": "def create_abuse_event(abuse_report_id, params)\n      AntiAbuse::Event.create!(\n        abuse_report_id: abuse_report_id,\n        category: :spam,\n        metadata: { noteable_type: params[:noteable_type],\n                    title: params[:title],\n                    description: params[:description],\n                    source_ip: params[:source_ip],\n                    user_agent: params[:user_agent],\n                    verdict: params[:verdict] },\n        source: :spamcheck,\n        user: user\n      )\n    end",
    "comment": "Associate the abuse report with an abuse event",
    "label": "",
    "id": "5578"
  },
  {
    "raw_code": "def tracking_database\n          raise NotImplementedError, \"#{self.name} does not implement #{__method__}\"\n        end",
    "comment": ":nocov:",
    "label": "",
    "id": "5579"
  },
  {
    "raw_code": "def enabled?\n          return false if Feature.enabled?(:disallow_database_ddl_feature_flags, type: :ops)\n\n          Feature.enabled?(:execute_batched_migrations_on_schedule, type: :ops)\n        end",
    "comment": ":nocov:",
    "label": "",
    "id": "5580"
  },
  {
    "raw_code": "def perform_with_capacity(args)\n          worker = new\n          worker.remove_failed_jobs\n\n          bulk_perform_async(args)\n        end",
    "comment": "We have to override this one, as we want arguments passed as is, and not duplicated",
    "label": "",
    "id": "5581"
  },
  {
    "raw_code": "def ssh_keys\n    keys = user.all_ssh_keys.join(\"\\n\")\n    keys << \"\\n\" unless keys.empty?\n    render plain: keys\n  end",
    "comment": "Get all keys of a user(params[:username]) in a text format Helpful for sysadmins to put in respective servers",
    "label": "",
    "id": "5582"
  },
  {
    "raw_code": "def gpg_keys\n    keys = user.gpg_keys.filter_map { |gpg_key| gpg_key.key if gpg_key.verified? }.join(\"\\n\")\n    keys << \"\\n\" unless keys.empty?\n    render plain: keys\n  end",
    "comment": "Get all gpg keys of a user(params[:username]) in a text format",
    "label": "",
    "id": "5583"
  },
  {
    "raw_code": "def auth_user\n    if user_signed_in?\n      current_user\n    else\n      try(:authenticated_user)\n    end",
    "comment": " Controllers such as GitHttpController may use alternative methods (e.g. tokens) to authenticate the user, whereas Devise sets current_user. ",
    "label": "",
    "id": "5584"
  },
  {
    "raw_code": "def pager_json(partial, count, locals = {})\n    html = render_to_string(\n      partial,\n      locals: locals,\n      layout: false,\n      formats: [:html]\n    )\n\n    render json: {\n      html: html,\n      count: count\n    }\n  end",
    "comment": "JSON for infinite scroll via Pager object",
    "label": "",
    "id": "5585"
  },
  {
    "raw_code": "def context_user\n    auth_user if strong_memoized?(:auth_user)\n  end",
    "comment": "Avoid loading the auth_user again after the request. Otherwise calling `auth_user` again would also trigger the Warden callbacks again",
    "label": "",
    "id": "5586"
  },
  {
    "raw_code": "def organization_params\n    params.permit(\n      :controller, :namespace_id, :group_id, :id, :organization_path\n    )\n  end",
    "comment": "Used by `set_current_organization` in BaseActionController",
    "label": "",
    "id": "5587"
  },
  {
    "raw_code": "def after_successful_create_hook(user)\n    store_duration(:accept_pending_invitations) { accept_pending_invitations }\n    store_duration(:persist_accepted_terms_if_required) { persist_accepted_terms_if_required(user) }\n    store_duration(:execute_system_hooks) { execute_system_hooks(user) }\n    store_duration(:notify_new_instance_access_request) { notify_new_instance_access_request(user) }\n    store_duration(:track_successful_user_creation) { track_successful_user_creation(user) }\n  end",
    "comment": "overridden by EE module",
    "label": "",
    "id": "5588"
  },
  {
    "raw_code": "def onboarding_status_params\n    # Onboarding::StatusPresenter does not use any params in CE, we'll override in EE\n    {}\n  end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- Fully tested in EE and tested in Foss through feature specs in spec/features/invites_spec.rb :nocov:",
    "label": "",
    "id": "5589"
  },
  {
    "raw_code": "def allow_flash_content?(user)\n    user.blocked_pending_approval? || onboarding_status_presenter.single_invite?\n  end",
    "comment": "rubocop:enable Gitlab/NoCodeCoverageComment",
    "label": "",
    "id": "5590"
  },
  {
    "raw_code": "def set_resource_fields\n    return unless set_blocked_pending_approval?\n\n    resource.state = User::BLOCKED_PENDING_APPROVAL_STATE\n  end",
    "comment": "overridden by EE module",
    "label": "",
    "id": "5591"
  },
  {
    "raw_code": "def track_error(new_user)\n    track_weak_password_error(new_user, self.class.name, 'create')\n  end",
    "comment": "overridden by EE module",
    "label": "",
    "id": "5592"
  },
  {
    "raw_code": "def openid_connect\n    omniauth_flow(Gitlab::Auth::OAuth)\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5593"
  },
  {
    "raw_code": "def failure\n    update_login_counter_metric(failed_strategy.name, 'failed')\n    log_saml_response if params['SAMLResponse']\n\n    username = params[:username].to_s\n    if username.present? && AuthHelper.form_based_provider?(failed_strategy.name)\n      user = User.find_by_login(username)\n\n      user&.increment_failed_attempts!\n      log_failed_login(username, failed_strategy.name)\n    end",
    "comment": "Extend the standard implementation to also increment the number of failed sign in attempts",
    "label": "",
    "id": "5594"
  },
  {
    "raw_code": "def failure_message\n    exception = request.env[\"omniauth.error\"]\n    error = exception.error_reason if exception.respond_to?(:error_reason)\n    error ||= exception.error        if exception.respond_to?(:error)\n    error ||= exception.message      if exception.respond_to?(:message)\n    error ||= request.env[\"omniauth.error.type\"].to_s\n\n    error.to_s.humanize if error\n  end",
    "comment": "Extend the standard message generation to accept our custom exception",
    "label": "",
    "id": "5595"
  },
  {
    "raw_code": "def build_auth_user_params\n    { organization_id: Current.organization.id }\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5596"
  },
  {
    "raw_code": "def set_session_active_since(id); end\n\n  def sign_in_user_flow(auth_user_class)\n    auth_user = build_auth_user(auth_user_class)\n    new_user = allowed_new_user?(auth_user)\n    @user = auth_user.find_and_update!\n\n    if auth_user.valid_sign_in?\n      # In this case the `#current_user` would not be set. So we can't fetch it\n      # from that in `#context_user`. Pushing it manually here makes the information\n      # available in the logs for this request.\n      Gitlab::ApplicationContext.push(user: @user)\n      track_event(@user, oauth['provider'], 'succeeded')\n      Gitlab::Tracking.event(self.class.name, \"#{oauth['provider']}_sso\", user: @user) if new_user\n\n      set_remember_me(@user, auth_user)\n      set_session_active_since(oauth['provider']) if ::AuthHelper.saml_providers.include?(oauth['provider'].to_sym)\n\n      if @user.two_factor_enabled? && !auth_user.bypass_two_factor?\n        prompt_for_two_factor(@user)\n        store_idp_two_factor_status(false)\n      else\n        if @user.deactivated?\n          @user.activate\n          flash[:notice] =\n            _('Welcome back! Your account had been deactivated due to inactivity but is now reactivated.')\n        end\n\n        # session variable for storing bypass two-factor request from IDP\n        store_idp_two_factor_status(auth_user.bypass_two_factor?)\n\n        accept_pending_invitations(user: @user) if new_user\n        synchronize_broadcast_message_dismissals(@user) unless new_user\n        persist_accepted_terms_if_required(@user) if new_user\n\n        perform_registration_tasks(@user, oauth['provider']) if new_user\n\n        enqueue_after_sign_in_workers(@user, auth_user)\n\n        sign_in_and_redirect_or_verify_identity(@user, auth_user, new_user)\n      end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5597"
  },
  {
    "raw_code": "def sign_in_and_redirect_or_verify_identity(user, _, _)\n    sign_in_and_redirect(user, event: :authentication)\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5598"
  },
  {
    "raw_code": "def enqueue_after_sign_in_workers(_user, _auth_user)\n    true\n  end",
    "comment": "overridden in specific EE class",
    "label": "",
    "id": "5599"
  },
  {
    "raw_code": "def allowed_new_user?(auth_user)\n    auth_user.new?\n  end",
    "comment": "overridden in specific EE class",
    "label": "",
    "id": "5600"
  },
  {
    "raw_code": "def projects\n    projects = Autocomplete::MoveToProjectFinder\n      .new(current_user, params)\n      .execute\n\n    render json: MoveToProjectSerializer.new.represent(projects)\n  end",
    "comment": "Displays projects to use for the dropdown when moving a resource from one project to another.",
    "label": "",
    "id": "5601"
  },
  {
    "raw_code": "def presented_suggested_users\n    []\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5602"
  },
  {
    "raw_code": "def documentation_base_url_from_db\n    Gitlab::CurrentSettings.current_application_settings.help_page_documentation_base_url.presence\n  end",
    "comment": "DEPRECATED",
    "label": "",
    "id": "5603"
  },
  {
    "raw_code": "def default_sort\n    'created_desc'\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5604"
  },
  {
    "raw_code": "def set_user\n    @user = User.find_by(id: params.permit(:user_id)[:user_id])\n\n    if @user.nil?\n      redirect_to root_path, alert: _(\"Cannot create the abuse report. The user has been deleted.\")\n    elsif @user.banned?\n      redirect_to @user, alert: _(\"Cannot create the abuse report. This user has been banned.\")\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5605"
  },
  {
    "raw_code": "def check_captcha\n    return unless user_params[:password].present?\n    return unless captcha_enabled? || captcha_on_login_required?\n    return unless Gitlab::Recaptcha.load_configurations!\n\n    if verify_recaptcha\n      increment_successful_login_captcha_counter\n    else\n      increment_failed_login_captcha_counter\n\n      self.resource = resource_class.new\n      flash[:alert] = _('There was an error with the reCAPTCHA. Please solve the reCAPTCHA again.')\n      flash.delete :recaptcha_error\n\n      add_gon_variables\n\n      respond_with_navigational(resource) { render :new }\n    end",
    "comment": "From https://github.com/plataformatec/devise/wiki/How-To:-Use-Recaptcha-with-Devise#devisepasswordscontroller",
    "label": "",
    "id": "5606"
  },
  {
    "raw_code": "def login_counter\n    @login_counter ||= Gitlab::Metrics.counter(:user_session_logins_total, 'User sign in count')\n  end",
    "comment": " We do have some duplication between lib/gitlab/auth/activity.rb here, but leaving this method here because of backwards compatibility. ",
    "label": "",
    "id": "5607"
  },
  {
    "raw_code": "def store_unauthenticated_sessions\n    return if current_user\n\n    Gitlab::AnonymousSession.new(request.remote_ip).count_session_ip\n  end",
    "comment": "counting sessions per IP lets us check if there are associated multiple anonymous sessions with one IP and prevent situations when there are multiple attempts of logging in",
    "label": "",
    "id": "5608"
  },
  {
    "raw_code": "def check_initial_setup\n    return unless in_initial_setup_state?\n\n    redirect_to new_admin_initial_setup_path\n  end",
    "comment": "Handle an \"initial setup\" state, where there's only one user, it's an admin, and they require a password change.",
    "label": "",
    "id": "5609"
  },
  {
    "raw_code": "def onboarding_status_tracking_label; end\nend",
    "comment": "overridden by EE module",
    "label": "",
    "id": "5610"
  },
  {
    "raw_code": "def disable_query_limiting\n    return unless Gitlab::QueryLimiting.enabled_for_env?\n\n    disable_reference = request.headers[DISABLE_SQL_QUERY_LIMIT_HEADER]\n    return unless disable_reference\n\n    first, second = disable_reference.split(',')\n\n    if first.match?(/^\\d+$/)\n      Gitlab::QueryLimiting.disable!(second, new_threshold: first&.to_i)\n    else\n      Gitlab::QueryLimiting.disable!(first)\n    end",
    "comment": "Tests may mark some GraphQL queries as exempt from SQL query limits",
    "label": "",
    "id": "5611"
  },
  {
    "raw_code": "def context\n    api_user = !!sessionless_user?\n    @context ||= {\n      current_user: current_user,\n      is_sessionless_user: api_user,\n      current_organization: Current.organization,\n      request: request,\n      scope_validator: ::Gitlab::Auth::ScopeValidator.new(api_user, request_authenticator),\n      remove_deprecated: Gitlab::Utils.to_boolean(permitted_params[:remove_deprecated], default: false)\n    }\n  end",
    "comment": "When modifying the context, also update GraphqlChannel#context if needed so that we have similar context when executing queries, mutations, and subscriptions",
    "label": "",
    "id": "5612"
  },
  {
    "raw_code": "def multiplex?\n    params[:_json].is_a?(Array)\n  end",
    "comment": "We support Apollo-style query batching where an array of queries will be in the `_json:` key. https://graphql-ruby.org/queries/multiplex.html#apollo-query-batching",
    "label": "",
    "id": "5613"
  },
  {
    "raw_code": "def render_404\n    render_error(\"Not found!\", status: :not_found)\n  end",
    "comment": "Overridden from the ApplicationController to make the response look like a GraphQL response. That is nicely picked up in Graphiql.",
    "label": "",
    "id": "5614"
  },
  {
    "raw_code": "def new\n    @namespace = Namespace.find_by(id: params[:namespace_id]) if params[:namespace_id]\n    return access_denied! if @namespace && !can?(current_user, :create_projects, @namespace)\n\n    @parent_group = Group.find_by(id: params[:namespace_id])\n\n    manageable_groups = ::Groups::AcceptingProjectCreationsFinder.new(current_user).execute.limit(2)\n\n    return access_denied! if manageable_groups.empty? && !can?(current_user, :create_projects, current_user.namespace)\n\n    @current_user_group = manageable_groups.first if manageable_groups.one?\n\n    @project = Project.new(namespace_id: @namespace&.id)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5615"
  },
  {
    "raw_code": "def edit\n    @badge_api_endpoint = expose_path(api_v4_projects_badges_path(id: @project.id))\n    render_edit\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5616"
  },
  {
    "raw_code": "def transfer\n    return access_denied! unless can?(current_user, :change_namespace, @project)\n\n    namespace = Namespace.find_by(id: params[:new_namespace_id])\n    ::Projects::TransferService.new(project, current_user).execute(namespace)\n\n    if @project.errors[:new_namespace].present?\n      flash[:alert] = @project.errors[:new_namespace].first\n      return redirect_to edit_project_path(@project)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5617"
  },
  {
    "raw_code": "def remove_fork\n    return access_denied! unless can?(current_user, :remove_fork_project, @project)\n\n    if ::Projects::UnlinkForkService.new(@project, current_user).execute\n      flash[:notice] = _('The fork relationship has been removed.')\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5618"
  },
  {
    "raw_code": "def refs\n    find_refs = refs_params['find']\n\n    find_branches = true\n    find_tags = true\n    find_commits = true\n\n    unless find_refs.nil?\n      find_branches = find_refs.include?('branches')\n      find_tags = find_refs.include?('tags')\n      find_commits = find_refs.include?('commits')\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5619"
  },
  {
    "raw_code": "def unfoldered_environment_names\n    respond_to do |format|\n      format.json do\n        render json: Environments::EnvironmentNamesFinder.new(@project, current_user).execute\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5620"
  },
  {
    "raw_code": "def render_landing_page\n    Gitlab::Tracking.event('project_overview', 'render', user: current_user, project: @project.project)\n\n    if can?(current_user, :read_code, @project)\n      return render 'projects/no_repo' unless @project.repository_exists?\n      return render 'projects/missing_default_branch', status: :service_unavailable if @ref == ''\n\n      render 'projects/empty' if @project.empty_repo?\n    else\n      if can?(current_user, :read_wiki, @project)\n        @wiki = @project.wiki\n        @wiki_home = @wiki.find_page('home', params[:version_id])\n      elsif @project.feature_available?(:issues, current_user)\n        @issues = issuables_collection.page(params[:page])\n        @issuable_meta_data = Gitlab::IssuableMetadata.new(current_user, @issues).data\n      end",
    "comment": "Render project landing depending of which features are available So if page is not available in the list it renders the next page  pages list order: repository readme, wiki home, issues list, customize workflow",
    "label": "",
    "id": "5621"
  },
  {
    "raw_code": "def load_events\n    projects = Project.where(id: @project.id)\n\n    @events = EventCollection\n      .new(projects, offset: params[:offset].to_i, filter: event_filter)\n      .to_a\n      .map(&:present)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5622"
  },
  {
    "raw_code": "def project_params(attributes: [])\n    params.require(:project)\n      .permit(project_params_attributes + attributes)\n      .merge(import_url_params)\n      .merge(object_format_params)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5623"
  },
  {
    "raw_code": "def redirect_git_extension\n    return unless params[:format] == 'git'\n\n    git_extension_regex = %r{\\.git/?\\Z}\n    return unless request.path.match?(git_extension_regex)\n\n    # `project` calls `find_routable!`, so this will trigger the usual not-found\n    # behaviour when the user isn't authorized to see the project\n    return if project.nil? || performed?\n\n    uri = URI(request.original_url)\n    # Strip the '.git' part from the path\n    uri.path = uri.path.sub(git_extension_regex, '')\n\n    redirect_to(uri.to_s)\n  end",
    "comment": "Redirect from localhost/group/project.git to localhost/group/project",
    "label": "",
    "id": "5624"
  },
  {
    "raw_code": "def edit\n    super\n    reset_password_token = Devise.token_generator.digest(\n      User,\n      :reset_password_token,\n      resource.reset_password_token\n    )\n\n    unless reset_password_token.nil?\n      user = User.where(\n        reset_password_token: reset_password_token\n      ).first_or_initialize\n\n      unless user.reset_password_period_valid?\n        flash[:alert] = _('Your password reset token has expired.')\n        redirect_to(new_user_password_url(user_email: user['email']))\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5625"
  },
  {
    "raw_code": "def update\n    super do |resource|\n      if resource.valid?\n        resource.password_automatically_set = false\n        resource.password_expires_at = nil\n        resource.save(validate: false) if resource.changed?\n      else\n        log_audit_reset_failure(@user)\n        track_weak_password_error(@user, self.class.name, 'create')\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5626"
  },
  {
    "raw_code": "def log_audit_reset_failure(_user); end\n\n  def resource_from_email\n    self.resource = resource_class.find_by_email(resource_params[:email].to_s)\n  end\n\n  def check_password_authentication_available\n    return if Gitlab::CurrentSettings.password_authentication_enabled?\n\n    redirect_to after_sending_reset_password_instructions_path_for(resource_name),\n      alert: _(\"Password authentication is unavailable.\")\n  end\n\n  def check_recaptcha\n    return unless resource_params[:email].present?\n\n    super\n  end\n\n  def throttle_reset\n    return unless resource && resource.recently_sent_password_reset?\n\n    # Throttle reset attempts, but return a normal message to\n    # avoid user enumeration attack.\n    redirect_to new_user_session_path,\n      notice: I18n.t('devise.passwords.send_paranoid_instructions')\n  end\n\n  def context_user\n    resource\n  end\n\n  def resource_params\n    super.permit(:email, :reset_password_token, :password, :password_confirmation)\n  end\nend",
    "comment": "overriden in EE",
    "label": "",
    "id": "5627"
  },
  {
    "raw_code": "def transfer\n    parent_group = Group.find_by(id: params[:new_parent_group_id])\n    service = ::Groups::TransferService.new(@group, current_user)\n\n    if service.execute(parent_group)\n      flash[:notice] = \"Group '#{@group.name}' was successfully transferred.\"\n      redirect_to group_path(@group)\n    else\n      flash[:alert] = service.error.html_safe\n      redirect_to edit_group_path(@group)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5628"
  },
  {
    "raw_code": "def export\n    export_service = Groups::ImportExport::ExportService.new(\n      group: @group,\n      user: current_user,\n      exported_by_admin: current_user.can_admin_all_resources?\n    )\n\n    if export_service.async_execute\n      redirect_to edit_group_path(@group),\n        notice: _('Group export started. A download link will be sent by email and made available on this page.')\n    else\n      redirect_to edit_group_path(@group), alert: _('Group export could not be started.')\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5629"
  },
  {
    "raw_code": "def authorize_create_group!\n    allowed = if params[:parent_id].present?\n                parent = Group.find_by(id: params[:parent_id])\n                can?(current_user, :create_subgroup, parent)\n              else\n                can?(current_user, :create_group)\n              end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5630"
  },
  {
    "raw_code": "def determine_layout\n    if [:new, :create].include?(action_name.to_sym)\n      'dashboard'\n    elsif [:edit, :update].include?(action_name.to_sym)\n      'group_settings'\n    else\n      'group'\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5631"
  },
  {
    "raw_code": "def load_events\n    params[:sort] ||= 'latest_activity_desc'\n\n    options = { include_subgroups: true }\n    projects = GroupProjectsFinder.new(params: params, group: group, options: options, current_user: current_user)\n                                  .execute\n                                  .includes(:namespace)\n\n    @events = EventCollection\n                .new(projects, offset: params[:offset].to_i, filter: event_filter, groups: groups)\n                .to_a\n                .map(&:present)\n\n    Events::RenderService\n      .new(current_user)\n      .execute(@events, atom_request: request.format.atom?)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5632"
  },
  {
    "raw_code": "def user_actions\n    @notification_setting = current_user.notification_settings_for(group) if current_user\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5633"
  },
  {
    "raw_code": "def redirect_if_epic_params; end\nend",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5634"
  },
  {
    "raw_code": "def load_keys\n    [\n      ::Rails.application.credentials.openid_connect_signing_key,\n      ::Gitlab::CurrentSettings.ci_jwt_signing_key\n    ]\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5635"
  },
  {
    "raw_code": "def auth\n    service = SERVICES[params[:service]]\n    return head :not_found unless service\n\n    result = service.new(@authentication_result.project, auth_user, auth_params)\n      .execute(authentication_abilities: @authentication_result.authentication_abilities)\n\n    render json: result, status: result[:http_status]\n  end",
    "comment": "Currently POST requests for this route return a 404 by default and are allowed through in our readonly middleware - ee/lib/ee/gitlab/middleware/read_only/controller.rb If the action here changes to allow POST requests then a check for maintenance mode should be added",
    "label": "",
    "id": "5636"
  },
  {
    "raw_code": "def scopes_param\n    return unless params[:scope].present?\n\n    scopes = Array(Rack::Utils.parse_query(request.query_string)['scope'])\n    scopes.flat_map(&:split)\n  end",
    "comment": "We have to parse scope here, because Docker Client does not send an array of scopes, but rather a flat list and we loose second scope when being processed by Rails: scope=scopeA&scope=scopeB.  Additionally, according to RFC6749 (https://datatracker.ietf.org/doc/html/rfc6749#section-3.3), some clients may use a scope parameter expressed as a list of space-delimited elements. Therefore, we must account for this and split the scope parameter value(s) appropriately.  This method makes to always return an array of scopes",
    "label": "",
    "id": "5637"
  },
  {
    "raw_code": "def unlock_user\n    update_user(&:unlock_access!)\n  end",
    "comment": "method overridden in EE",
    "label": "",
    "id": "5638"
  },
  {
    "raw_code": "def prepare_user_for_update(user)\n    user.skip_reconfirmation!\n    user.send_only_admin_changed_your_password_notification! if admin_making_changes_for_another_user?\n  end",
    "comment": "method overridden in EE",
    "label": "",
    "id": "5639"
  },
  {
    "raw_code": "def after_successful_create_hook(user); end\n\n  # method overridden in EE\n  def after_successful_update_hook(user); end\n\n  def after_successful_create_flash\n    { notice: _('User was successfully created.') }\n  end\n\n  def after_successful_update_flash\n    { notice: _('User was successfully updated.') }\n  end\n\n  def filter_users\n    User.filter_items(params[:filter]).order_name_asc\n  end\n\n  def safe_params\n    params.permit(:personal_projects_page, :projects_page, :groups_page)\n  end\nend\n\nAdmin::UsersController.prepend_mod_with('Admin::UsersController')",
    "comment": "method overridden in EE",
    "label": "",
    "id": "5640"
  },
  {
    "raw_code": "def show\n    @metric = DevOpsReport::Metric.order(:created_at).last&.present\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5641"
  },
  {
    "raw_code": "def show_adoption?\n    false\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5642"
  },
  {
    "raw_code": "def index\n    @spam_logs = SpamLog.preload(user: [:trusted_with_spam_attribute])\n      .order(id: :desc)\n      .page(pagination_params[:page]).without_count\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5643"
  },
  {
    "raw_code": "def destroy\n    spam_log = SpamLog.find(safe_params[:id])\n\n    if safe_params[:remove_user]\n      spam_log.remove_user(deleted_by: current_user)\n      redirect_to admin_spam_logs_path,\n        status: :found,\n        notice: format(_('User %{username} was successfully removed.'), username: spam_log.user.username)\n    else\n      spam_log.destroy\n      head :ok\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5644"
  },
  {
    "raw_code": "def user\n    @user ||= User.find_by!(username: params.permit(:user_id)[:user_id])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5645"
  },
  {
    "raw_code": "def identity\n    @identity ||= user.identities.find(params.permit(:id)[:id])\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5646"
  },
  {
    "raw_code": "def user\n    @user ||= User.find_by!(username: params[:user_id])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5647"
  },
  {
    "raw_code": "def key_params\n    params.require(:user_id, :id)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5648"
  },
  {
    "raw_code": "def user\n    @user ||= User.find_by!(username: params.permit(:user_id)[:user_id])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5649"
  },
  {
    "raw_code": "def verify_impersonation_enabled!\n    access_denied! unless helpers.impersonation_tokens_enabled?\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5650"
  },
  {
    "raw_code": "def show\n    if @group\n      @group_members = present_members(\n        @group.members.order(\"access_level DESC\").page(params[:group_members_page]))\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5651"
  },
  {
    "raw_code": "def destroy\n    ::Projects::DestroyService.new(@project, current_user, {}).async_execute\n    flash[:toast] = format(_(\"Project '%{project_name}' is being deleted.\"), project_name: @project.full_name)\n\n    redirect_to admin_projects_path, status: :found\n  rescue Projects::DestroyService::DestroyError => e\n    redirect_to admin_projects_path, status: :found, alert: e.message\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5652"
  },
  {
    "raw_code": "def transfer\n    namespace = Namespace.find_by(id: params[:new_namespace_id])\n    ::Projects::TransferService.new(@project, current_user, params.dup).execute(namespace)\n\n    flash[:alert] = @project.errors[:new_namespace].first if @project.errors[:new_namespace].present?\n\n    @project.reset\n    redirect_to admin_project_path(@project)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5653"
  },
  {
    "raw_code": "def edit; end\n\n  def update\n    result = ::Projects::UpdateService.new(@project, current_user, project_params).execute\n\n    if result[:status] == :success\n      unless Gitlab::Utils.to_boolean(project_params['runner_registration_enabled'])\n        Ci::Runners::ResetRegistrationTokenService.new(@project, current_user).execute\n      end\n\n      redirect_to [:admin, @project],\n        notice: format(_(\"Project '%{project_name}' was successfully updated.\"), project_name: @project.name)\n    else\n      render \"edit\"\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5654"
  },
  {
    "raw_code": "def clean_up_non_primary_emails(user)\n      user.emails.each do |email|\n        email.destroy unless email.user_primary_email?\n      end",
    "comment": "the initial email generated randomly by fixtures, or from the GITLAB_ROOT_EMAIL env var should be cleaned up if different than the assigned-via-UI initial account email",
    "label": "",
    "id": "5655"
  },
  {
    "raw_code": "def show\n    # Group.with_statistics doesn't behave nicely when including other relations.\n    # Group.find_by_full_path includes the routes relation to avoid a common N+1\n    # (at the expense of this action: there are two queries here to find and retrieve\n    # the Group with statistics).\n    @group = Group.with_statistics.find(group&.id)\n    @members = present_members(\n      group_members.order(\"access_level DESC\").page(safe_params[:members_page]))\n    @requesters = present_members(\n      AccessRequestsFinder.new(@group).execute(current_user))\n    @projects = @group.projects.with_statistics.page(safe_params[:projects_page])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5656"
  },
  {
    "raw_code": "def new\n    @group = Group.new\n    @group.build_admin_note\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5657"
  },
  {
    "raw_code": "def lets_encrypt_terms_of_service\n      redirect_to ::Gitlab::LetsEncrypt.terms_of_service_url\n    end",
    "comment": "Getting ToS url requires `directory` api call to Let's Encrypt which could result in 500 error/slow rendering on settings page Because of that we use separate controller action",
    "label": "",
    "id": "5658"
  },
  {
    "raw_code": "def valid_setting_panels\n      VALID_SETTING_PANELS\n    end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5659"
  },
  {
    "raw_code": "def set_appearance\n    @appearance = Appearance.current || Appearance.new\n  end",
    "comment": "Use callbacks to share common setup or constraints between actions.",
    "label": "",
    "id": "5660"
  },
  {
    "raw_code": "def appearance_params\n    params.require(:appearance).permit(allowed_appearance_params)\n  end",
    "comment": "Only allow a trusted parameter \"allow list\" through.",
    "label": "",
    "id": "5661"
  },
  {
    "raw_code": "def self.needs_authorize_read_runners\n    [:index, :show]\n  end",
    "comment": "overrriden in EE",
    "label": "",
    "id": "5662"
  },
  {
    "raw_code": "def seat_count_data; end\n  end",
    "comment": "To be overridden in ee/app/controllers/ee/groups/usage_quotas_controller.rb",
    "label": "",
    "id": "5663"
  },
  {
    "raw_code": "def disallow_new_uploads!\n    render_404 if upload_version_at_least?(ID_BASED_UPLOAD_PATH_VERSION)\n  end",
    "comment": "Starting with this version, #show is handled by Banzai::UploadsController#show",
    "label": "",
    "id": "5664"
  },
  {
    "raw_code": "def handle_new_work_item_path\n      return unless show_params[:iid] == 'new'\n\n      authenticate_user!\n\n      render :show\n    end",
    "comment": "The work_items/:iid route renders a Vue app that takes care of the show and new pages.",
    "label": "",
    "id": "5665"
  },
  {
    "raw_code": "def push_licensed_features; end\n\n      # Overridden in EE\n      def assign_variables_to_gon; end\n    end\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5666"
  },
  {
    "raw_code": "def show\n        render :index\n      end",
    "comment": "The show action renders index to allow frontend routing to work on page refresh",
    "label": "",
    "id": "5667"
  },
  {
    "raw_code": "def execute\n      start_time = Gitlab::Metrics::System.monotonic_time\n\n      ::Gitlab::Database::LoadBalancing::SessionMap.use_replica_if_available do\n        super\n      end",
    "comment": "Overrides GraphqlController#execute to add rate limiting for GLQL queries. When a query times out (raising ActiveRecord::QueryAborted), we increment the Gitlab::ApplicationRateLimiter counter. If a second failure occurs within a 15-minute window (configured in lib/gitlab/application_rate_limiter.rb), the request is throttled. One failure is allowed, but consecutive failures within the time window trigger throttling.",
    "label": "",
    "id": "5668"
  },
  {
    "raw_code": "def set_namespace_context\n      @project ||= Project.find_by_full_path(permitted_params[:project]) if permitted_params[:project].present?\n      @group ||= Group.find_by_full_path(permitted_params[:group]) if permitted_params[:group].present?\n    end",
    "comment": "When `set_current_context` in app/controllers/application_controller.rb calls `to_lazy_hash` on Gitlab::ApplicationContext, the meta fields (meta.project and meta.root_namespace) will be populated using @group or @project variables.",
    "label": "",
    "id": "5669"
  },
  {
    "raw_code": "def permitted_standalone_query_params\n      params.permit(:query, :operationName, :remove_deprecated, :group, :project, variables: {})\n    end",
    "comment": "Overrides GraphqlController#permitted_params to permit project and group params",
    "label": "",
    "id": "5670"
  },
  {
    "raw_code": "def starred\n    @projects = load_projects.reorder('star_count DESC')\n\n    respond_to do |format|\n      format.html\n      format.json do\n        render json: {\n          html: view_to_html_string(\"explore/projects/_projects\", projects: @projects)\n        }\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5671"
  },
  {
    "raw_code": "def topics\n    load_project_counts\n    load_topics\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5672"
  },
  {
    "raw_code": "def preload_associations(projects)\n    projects.includes(:route, :creator, :group, :project_feature, :topics, namespace: [:route, :owner])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5673"
  },
  {
    "raw_code": "def set_sorting\n    params[:sort] = set_sort_order\n    @sort = params[:sort]\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5674"
  },
  {
    "raw_code": "def ldap\n    return unless Gitlab::Auth::Ldap::Config.sign_in_enabled?\n\n    if Gitlab::CurrentSettings.admin_mode\n      return admin_mode_flow(Gitlab::Auth::Ldap::User) if current_user_mode.admin_mode_requested?\n    end",
    "comment": "We only find ourselves here if the authentication to LDAP was successful.",
    "label": "",
    "id": "5675"
  },
  {
    "raw_code": "def snippet\n    @snippet ||= PersonalSnippet.find_by(id: params[:snippet_id])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5676"
  },
  {
    "raw_code": "def set_application\n    @application = current_user.oauth_applications.find(params[:id])\n  end",
    "comment": "Override Doorkeeper to scope to the current user",
    "label": "",
    "id": "5677"
  },
  {
    "raw_code": "def create\n      client_metadata = Gitlab::Json.parse(request.body.read).symbolize_keys\n\n      allowed_params = [:redirect_uris, :client_name]\n\n      client_metadata = client_metadata.slice(*allowed_params)\n\n      # Validations here are specific to this controller, not the model\n      validation_error = validate_dynamic_fields(client_metadata)\n      if validation_error\n        render json: validation_error, status: :bad_request\n        return\n      end",
    "comment": "POST /oauth/register",
    "label": "",
    "id": "5678"
  },
  {
    "raw_code": "def new\n    if pre_auth.authorizable?\n      if skip_authorization? || (matching_token? && pre_auth.client.application.confidential?)\n        auth = authorization.authorize\n        parsed_redirect_uri = URI.parse(auth.redirect_uri)\n        session.delete(:user_return_to)\n        render \"doorkeeper/authorizations/redirect\", locals: { redirect_uri: parsed_redirect_uri }, layout: false\n      else\n        redirect_uri = URI(authorization.authorize.redirect_uri)\n        allow_redirect_uri_form_action(redirect_uri.scheme)\n\n        render \"doorkeeper/authorizations/new\"\n      end",
    "comment": "Overridden from Doorkeeper::AuthorizationsController to include the call to session.delete",
    "label": "",
    "id": "5679"
  },
  {
    "raw_code": "def allow_redirect_uri_form_action(redirect_uri_scheme)\n    return unless content_security_policy?\n\n    form_action = request.content_security_policy.form_action\n    return unless form_action\n\n    form_action.push(\"#{redirect_uri_scheme}:\")\n    request.content_security_policy.form_action(*form_action)\n  end",
    "comment": "Chrome blocks redirections if the form-action CSP directive is present and the redirect location's scheme isn't allow-listed https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/form-action https://github.com/w3c/webappsec-csp/issues/8",
    "label": "",
    "id": "5680"
  },
  {
    "raw_code": "def downgrade_scopes!\n    auth_type = params.delete('gl_auth_type')\n    return unless auth_type == 'login'\n\n    ensure_read_user_scope!\n\n    params['scope'] = Gitlab::Auth::READ_USER_SCOPE.to_s if application_has_read_user_scope?\n  end",
    "comment": "limit scopes when signing in with GitLab",
    "label": "",
    "id": "5681"
  },
  {
    "raw_code": "def ensure_read_user_scope!\n    return if application_has_read_user_scope?\n    return unless application_has_api_scope?\n\n    add_read_user_scope!\n  end",
    "comment": "Configure the application to support read_user scope, if it already supports scopes with greater levels of privileges.",
    "label": "",
    "id": "5682"
  },
  {
    "raw_code": "def organization_params\n    {}\n  end",
    "comment": "Used by `set_current_organization` in BaseActionController",
    "label": "",
    "id": "5683"
  },
  {
    "raw_code": "def create\n    @note = Notes::CreateService.new(note_project, current_user, create_note_params).execute\n\n    respond_to do |format|\n      format.json do\n        json = {\n          commands_changes: @note.commands_changes&.slice(:emoji_award, :time_estimate, :spend_time)\n        }\n\n        if @note.persisted? && return_discussion?\n          json[:valid] = true\n\n          discussion = @note.discussion\n          prepare_notes_for_rendering(discussion.notes)\n          json[:discussion] = discussion_serializer.represent(discussion, context: self)\n        else\n          prepare_notes_for_rendering([@note])\n\n          json.merge!(note_json(@note))\n        end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5684"
  },
  {
    "raw_code": "def update\n    @note = Notes::UpdateService.new(project, current_user, update_note_params).execute(note)\n    if @note.destroyed?\n      head :gone\n      return\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5685"
  },
  {
    "raw_code": "def destroy\n    Notes::DestroyService.new(project, current_user).execute(note) if note.editable?\n\n    respond_to do |format|\n      format.js { head :ok }\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5686"
  },
  {
    "raw_code": "def prompt_for_two_factor(user)\n    @user = user # rubocop:disable Gitlab/ModuleWithInstanceVariables -- Set @user for Devise views\n\n    return handle_locked_user(user) unless user.can?(:log_in)\n\n    session[:otp_user_id] = user.id\n    session[:user_password_hash] = Digest::SHA256.hexdigest(user.encrypted_password)\n\n    add_gon_variables\n    setup_webauthn_authentication(user)\n\n    render 'devise/sessions/two_factor'\n  end",
    "comment": "Store the user's ID in the session for later retrieval and render the two factor code prompt  The user must have been authenticated with a valid login and password before calling this method!  user - User record  Returns nil",
    "label": "",
    "id": "5687"
  },
  {
    "raw_code": "def setup_webauthn_authentication(user)\n    if user.webauthn_registrations.present?\n\n      webauthn_registration_ids = user.webauthn_registrations.pluck(:credential_xid)\n\n      get_options = WebAuthn::Credential.options_for_get(\n        allow: webauthn_registration_ids,\n        user_verification: 'discouraged',\n        extensions: { appid: WebAuthn.configuration.origin }\n      )\n      session[:challenge] = get_options.challenge\n      gon.push(webauthn: { options: Gitlab::Json.dump(get_options) })\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5688"
  },
  {
    "raw_code": "def handle_two_factor_success(user)\n    # Remove any lingering user data from login\n    clear_two_factor_attempt!\n\n    remember_me(user) if user_params[:remember_me] == '1'\n    sign_in(user, message: :two_factor_authenticated, event: :authentication)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5689"
  },
  {
    "raw_code": "def user_password_changed?(user)\n    return false unless session[:user_password_hash]\n\n    Digest::SHA256.hexdigest(user.encrypted_password) != session[:user_password_hash]\n  end",
    "comment": "If user has been updated since we validated the password, the password might have changed.",
    "label": "",
    "id": "5690"
  },
  {
    "raw_code": "def limit_exceeded?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5691"
  },
  {
    "raw_code": "def set_issuables_index\n    @issuables = issuables_collection\n    set_pagination\n\n    nil if redirect_out_of_range(@issuables, @total_pages)\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5692"
  },
  {
    "raw_code": "def issuables_collection\n    finder.execute.preload(preload_for_collection)\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5693"
  },
  {
    "raw_code": "def issuable_finder_for(finder_class)\n    finder_class.new(current_user, finder_options)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5694"
  },
  {
    "raw_code": "def finder_options\n    params[:state] = default_state if params[:state].blank?\n\n    options = {\n      scope: params[:scope],\n      state: params[:state],\n      confidential: Gitlab::Utils.to_boolean(params[:confidential]),\n      sort: set_sort_order\n    }\n\n    # Used by view to highlight active option\n    @sort = options[:sort]\n\n    # When a user looks for an exact iid, we do not filter by search but only by iid\n    if params[:search] =~ /^#(?<iid>\\d+)\\z/\n      options[:iids] = Regexp.last_match[:iid]\n      params[:search] = nil\n    end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5695"
  },
  {
    "raw_code": "def default_state\n    'opened'\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5696"
  },
  {
    "raw_code": "def preload_for_collection\n    common_attributes = [:author, :assignees, :labels, :milestone]\n    @preload_for_collection ||= case collection_type\n                                when 'Issue'\n                                  common_attributes + [\n                                    :work_item_type,\n                                    :project, { project: :namespace }\n                                  ]\n                                when 'MergeRequest'\n                                  common_attributes + [\n                                    :target_project, :latest_merge_request_diff, :approvals,\n                                    :approved_by_users, :reviewers,\n                                    { source_project: :route, head_pipeline: :project, target_project: :namespace }\n                                  ]\n                                end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5697"
  },
  {
    "raw_code": "def create_commit(service, success_path:, failure_path:, failure_view: nil, success_notice: nil, target_project: nil)\n    target_project ||= @project\n\n    if user_access(target_project).can_push_to_branch?(branch_name_or_ref)\n      @project_to_commit_into = target_project\n      @different_project = false\n      @branch_name ||= @ref\n    else\n      @project_to_commit_into = current_user.fork_of(target_project)\n      @different_project = true\n      @branch_name ||= generated_branch_name(@project_to_commit_into)\n    end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5698"
  },
  {
    "raw_code": "def authorize_edit_tree!\n    return if can_collaborate_with_project?(project, ref: branch_name_or_ref)\n\n    access_denied!\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5699"
  },
  {
    "raw_code": "def new_merge_request_path(target_project)\n    project_new_merge_request_path(\n      @project_to_commit_into,\n      merge_request: {\n        target_project_id: @project_to_commit_into.default_merge_request_target.id,\n        source_branch: @branch_name,\n        target_branch: @start_branch\n      }\n    )\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5700"
  },
  {
    "raw_code": "def existing_merge_request_path\n    project_merge_request_path(@project, @merge_request) # rubocop:disable Gitlab/ModuleWithInstanceVariables\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5701"
  },
  {
    "raw_code": "def merge_request_exists?\n    MergeRequestsFinder.new(current_user, project_id: @project.id)\n        .execute\n        .opened\n        .find_by(\n          source_project_id: @project_to_commit_into,\n          source_branch: @branch_name,\n          target_branch: @start_branch)\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5702"
  },
  {
    "raw_code": "def create_merge_request?\n    # Even if the field is set, if we're checking the same branch\n    # as the target branch in the same project,\n    # we don't want to create a merge request.\n    # FIXME: We should use either 1 or true, not both.\n    ActiveModel::Type::Boolean.new.cast(params[:create_merge_request]) &&\n      (@different_project || @start_branch != @branch_name) # rubocop:disable Gitlab/ModuleWithInstanceVariables\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5703"
  },
  {
    "raw_code": "def render_entity_json\n    if @issuable.valid?\n      render json: serializer.represent(@issuable)\n    else\n      render json: { errors: @issuable.errors.full_messages }, status: :unprocessable_entity\n    end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5704"
  },
  {
    "raw_code": "def serializer\n    raise NotImplementedError\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5705"
  },
  {
    "raw_code": "def render_group_tree(groups)\n    groups = groups.sort_by_attribute(@sort = safe_params[:sort])\n\n    groups = if search_descendants?\n               filtered_groups_with_ancestors(groups)\n             elsif safe_params[:parent_id].present?\n               groups.where(parent_id: safe_params[:parent_id]).page(safe_params[:page])\n             else\n               # If `safe_params[:parent_id]` is `nil`, we will only show root-groups\n               groups.by_parent(nil).page(safe_params[:page])\n             end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5706"
  },
  {
    "raw_code": "def filtered_groups_with_ancestors(groups)\n    filtered_groups = groups.search(safe_params[:filter]).page(safe_params[:page])\n\n    # We find the ancestors by ID of the search results here.\n    # Otherwise the ancestors would also have filters applied,\n    # which would cause them not to be preloaded.\n    #\n    # Pagination needs to be applied before loading the ancestors to\n    # make sure ancestors are not cut off by pagination.\n    ancestors = Group.where(id: filtered_groups.select(:id)).self_and_ancestors\n    ancestors = ancestors.self_or_ancestors_inactive if inactive?\n    ancestors\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5707"
  },
  {
    "raw_code": "def inactive?\n    safe_params[:active] == false\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5708"
  },
  {
    "raw_code": "def user_access(project)\n    @user_access ||= {}\n    @user_access[project] ||= Gitlab::UserAccess.new(current_user, container: project)\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables enabling this so we can easily cache the user access value as it might be used across multiple calls in the view",
    "label": "",
    "id": "5709"
  },
  {
    "raw_code": "def leave\n    member = members_and_requesters.find_by!(user_id: current_user.id)\n    Members::DestroyService.new(current_user).execute(member)\n\n    notice =\n      if member.request?\n        format(_(\"Your access request to the %{source_type} has been withdrawn.\"), source_type: source_type)\n      else\n        format(\n          _(\"You left the \\\"%{membershipable_human_name}\\\" %{source_type}.\"),\n          membershipable_human_name: membershipable.human_name,\n          source_type: source_type\n        )\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5710"
  },
  {
    "raw_code": "def resend_invite\n    member = membershipable_members.find(params[:id])\n\n    if member.invite?\n      member.resend_invite\n\n      redirect_to members_page_url, notice: _('The invitation was successfully resent.')\n    else\n      redirect_to members_page_url, alert: _('The invitation has already been accepted.')\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5711"
  },
  {
    "raw_code": "def set_commits_for_rendering(commits, commits_count: nil)\n    @total_commit_count = commits_count || commits.size\n    limited, @hidden_commit_count = limited_commits(commits, @total_commit_count)\n    prepare_commits_for_rendering(limited)\n  end",
    "comment": "This is used as a helper method in a controller. rubocop: disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5712"
  },
  {
    "raw_code": "def prepare_commits_for_rendering(commits)\n    commits.each(&:lazy_author) # preload commits' authors\n    commits.each(&:lazy_latest_pipeline)\n\n    Banzai::CommitRenderer.render(commits, @project, current_user) # rubocop:disable Gitlab/ModuleWithInstanceVariables\n\n    commits\n  end",
    "comment": "rubocop: enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5713"
  },
  {
    "raw_code": "def snippet\n    snippet_klass.inc_relations_for_view.find_by(snippet_find_params)\n  end",
    "comment": "rubocop:disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5714"
  },
  {
    "raw_code": "def snippet_klass\n    raise NotImplementedError\n  end",
    "comment": "rubocop:enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5715"
  },
  {
    "raw_code": "def show\n    respond_to do |format|\n      format.html do\n        @note = Note.new(noteable: @snippet, project: @snippet.project)\n        @noteable = @snippet\n\n        @discussions = @snippet.discussions\n        @notes = prepare_notes_for_rendering(@discussions.flat_map(&:notes))\n        render 'show'\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5716"
  },
  {
    "raw_code": "def push_licensed_features; end\n\n  def board\n    board_finder.execute.first\n  end\n  strong_memoize_attr :board\n\n  def board_visit_service\n    Boards::Visits::CreateService\n  end\n\n  def parent\n    group? ? group : project\n  end\n  strong_memoize_attr :parent\n\n  def board_path(board)\n    if group?\n      group_board_path(parent, board)\n    else\n      project_board_path(parent, board)\n    end\n  end",
    "comment": "Noop on FOSS",
    "label": "",
    "id": "5717"
  },
  {
    "raw_code": "def check_page_number!(max_page_number)\n    raise PageLimitNotANumberError unless max_page_number.is_a?(Integer)\n    raise PageLimitNotSensibleError unless max_page_number > 0\n\n    return if params[:page].blank?\n    return if params[:page].to_i <= max_page_number\n\n    record_page_limit_interception\n    raise PageOutOfBoundsError, max_page_number\n  end",
    "comment": "If the page exceeds the defined maximum, raise a PageOutOfBoundsError If the page doesn't exceed the limit, it does nothing.",
    "label": "",
    "id": "5718"
  },
  {
    "raw_code": "def default_page_out_of_bounds_response\n    head :bad_request\n  end",
    "comment": "By default just return a HTTP status code and an empty response",
    "label": "",
    "id": "5719"
  },
  {
    "raw_code": "def record_page_limit_interception\n    dd = Gitlab::SafeDeviceDetector.new(request.user_agent)\n\n    Gitlab::Metrics.counter(:gitlab_page_out_of_bounds,\n      controller: params[:controller],\n      action: params[:action],\n      bot: dd.bot?\n    ).increment\n  end",
    "comment": "Record the page limit being hit in Prometheus",
    "label": "",
    "id": "5720"
  },
  {
    "raw_code": "def execute_action_for_2fa_reason(actions)\n    reason = two_factor_verifier.two_factor_authentication_reason\n    groups_enforcing_two_factor = current_user.source_groups_of_two_factor_authentication_requirement\n                                              .reorder(name: :asc)\n    actions[reason].call(groups_enforcing_two_factor)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5721"
  },
  {
    "raw_code": "def two_factor_grace_period\n    two_factor_verifier.two_factor_grace_period\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5722"
  },
  {
    "raw_code": "def peek_enabled?\n    Gitlab::PerformanceBar.enabled_for_request?\n  end",
    "comment": "Needed for Peek's routing to work; Peek::ResultsController#restrict_non_access calls this method.",
    "label": "",
    "id": "5723"
  },
  {
    "raw_code": "def index\n    @resource_access_token = PersonalAccessToken.new\n    set_index_vars\n\n    respond_to do |format|\n      format.html\n      format.json do\n        render json: @active_access_tokens\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5724"
  },
  {
    "raw_code": "def create\n    token_response = ResourceAccessTokens::CreateService.new(current_user, resource, create_params).execute\n\n    if token_response.success?\n      @resource_access_token = token_response.payload[:access_token]\n      tokens, size = active_access_tokens\n      render json: { new_token: @resource_access_token.token,\n                     active_access_tokens: tokens, total: size }, status: :ok\n    else\n      render json: { errors: token_response.errors }, status: :unprocessable_entity\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5725"
  },
  {
    "raw_code": "def revoke\n    @resource_access_token = finder.find(params[:id])\n    revoked_response = ResourceAccessTokens::RevokeService.new(current_user, resource, @resource_access_token).execute\n\n    if revoked_response.success?\n      flash[:notice] =\n        format(_(\"Revoked access token %{access_token_name}!\"), access_token_name: @resource_access_token.name)\n    else\n      flash[:alert] =\n        format(_(\"Could not revoke access token %{access_token_name}.\"), access_token_name: @resource_access_token.name)\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5726"
  },
  {
    "raw_code": "def rotate\n    token = finder.find(rotate_params[:id])\n    result = rotate_service.new(current_user, token, resource, keep_token_lifetime: true).execute\n    resource_access_token = result.payload[:personal_access_token]\n\n    if result.success?\n      tokens, size = active_access_tokens\n      render json: { new_token: resource_access_token.token,\n                     active_access_tokens: tokens, total: size }, status: :ok\n    else\n      render json: { message: result.message }, status: :unprocessable_entity\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5727"
  },
  {
    "raw_code": "def set_index_vars\n    # Loading resource members so that we can fetch access level of the bot\n    # user in the resource without multiple queries.\n    resource.members.load\n\n    @scopes = Gitlab::Auth.available_scopes_for(resource)\n\n    @active_access_tokens, @active_access_tokens_size = active_access_tokens\n    @inactive_access_tokens_size = inactive_access_tokens.size\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5728"
  },
  {
    "raw_code": "def finder(options = {})\n    PersonalAccessTokensFinder.new({ user: bot_users, impersonation: false }.merge(options))\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5729"
  },
  {
    "raw_code": "def issues\n    respond_to do |format|\n      format.html\n      format.atom do\n        @issues = issuables_collection\n                  .non_archived\n                  .page(params[:page])\n\n        @issuable_meta_data = Gitlab::IssuableMetadata.new(current_user, @issues).data\n\n        render layout: 'xml'\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5730"
  },
  {
    "raw_code": "def merge_requests\n    render_merge_requests\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5731"
  },
  {
    "raw_code": "def render_merge_requests\n    @merge_requests = issuables_collection.page(params[:page])\n\n    @issuable_meta_data = Gitlab::IssuableMetadata.new(current_user, @merge_requests).data\n  rescue ActiveRecord::QueryCanceled => exception # rubocop:disable Database/RescueQueryCanceled\n    log_exception(exception)\n\n    @search_timeout_occurred = true\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5732"
  },
  {
    "raw_code": "def authenticate_sessionless_user!(request_format)\n    user = request_authenticator.find_sessionless_user(request_format)\n    sessionless_sign_in(user) if user\n  end",
    "comment": "This filter handles personal access tokens, atom requests with rss tokens, and static object tokens",
    "label": "",
    "id": "5733"
  },
  {
    "raw_code": "def show\n    Gitlab::PathTraversal.check_path_traversal!(params[:filename])\n\n    return render_404 unless uploader&.exists?\n\n    ttl, directives = *cache_settings\n    ttl ||= 0\n    directives ||= { private: true, must_revalidate: true }\n\n    expires_in ttl, directives\n\n    file_uploader = [uploader, *uploader.versions.values].find do |version|\n      version.filename == params[:filename]\n    end",
    "comment": "This should either - send the file directly - or redirect to its URL ",
    "label": "",
    "id": "5734"
  },
  {
    "raw_code": "def set_request_format_from_path_extension\n    path = request.headers['action_dispatch.original_path'] || request.headers['PATH_INFO']\n    match = path&.match(/\\.(\\w+)\\z/)\n    return unless match\n\n    format = Mime[match.captures.first]\n\n    return if format.blank?\n\n    request.format = CUSTOM_REQUEST_FORMAT_MAPPING[format.symbol] || format.symbol\n  end",
    "comment": "Based on ActionDispatch::Http::MimeNegotiation. We have an initializer that monkey-patches this method out (so that repository paths don't guess a format based on extension), but we do want this behavior when serving uploads.",
    "label": "",
    "id": "5735"
  },
  {
    "raw_code": "def build_uploader_from_upload\n    uploader = build_uploader\n    return unless uploader\n\n    upload_paths = uploader.upload_paths(params[:filename])\n    upload = Upload.find_by(model: model, uploader: uploader_class.to_s, path: upload_paths)\n    upload&.retrieve_uploader\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5736"
  },
  {
    "raw_code": "def build_uploader\n    return unless params[:secret] && params[:filename]\n\n    uploader = uploader_class.new(model, secret: params[:secret])\n\n    return unless uploader.model_valid?\n\n    uploader\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5737"
  },
  {
    "raw_code": "def requires_verify_email?(user)\n    treat_as_locked?(user) || !trusted_ip_address?(user) || require_email_based_otp?(user)\n  end",
    "comment": "As this is a prepended controller action, we only want to block log in if the VerifiesWithEmail is required",
    "label": "",
    "id": "5738"
  },
  {
    "raw_code": "def require_email_based_otp?(user)\n    return false unless Feature.enabled?(:email_based_mfa, user)\n\n    password_based_login? &&\n      # Skip on first log in (which occurs for most during account\n      # creation), to avoid double email verification with\n      # Devise::Confirmable\n      user.last_sign_in_at.present? &&\n      user.email_otp_required_after.present? &&\n      (user.email_otp_required_after <= Time.zone.now || in_email_otp_grace_period?(user))\n  end",
    "comment": "Checks whether email-based OTP is required for the current sign-in attempt.  This feature uses a two-part rollout mechanism: - Feature Flag acts as a kill switch that can be quickly disabled via ChatOps - User enrollment is controlled by setting the attribute email_otp_required_after  This allows us to halt or revert the rollout immediately while preserving per-user enrollment dates.  Later, the Feature Flag will be changed to an ApplicationSetting so that self-managed administrators can turn this feature on after validating that they have mail delivery correctly configured.",
    "label": "",
    "id": "5739"
  },
  {
    "raw_code": "def fetch_confirmed_user_secondary_email(user, email)\n    user.emails.confirmed.find_by_email(email)&.email\n  end",
    "comment": "This method must return nil if email is not confirmed and belonging to user",
    "label": "",
    "id": "5740"
  },
  {
    "raw_code": "def prepare_notes_for_rendering(notes)\n    preload_noteable_for_regular_notes(notes)\n    preload_note_namespace(notes)\n    preload_max_access_for_authors(notes, @project)\n    preload_author_status(notes)\n    Notes::RenderService.new(current_user).execute(notes)\n\n    notes\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5741"
  },
  {
    "raw_code": "def sorting_field\n    nil\n  end",
    "comment": "Implement sorting_field method on controllers to choose which column to store the sorting parameter.",
    "label": "",
    "id": "5742"
  },
  {
    "raw_code": "def default_sort_order\n    nil\n  end",
    "comment": "Implement default_sort_order method on controllers to choose which default sort should be applied if sort param is not provided.",
    "label": "",
    "id": "5743"
  },
  {
    "raw_code": "def legacy_sort_cookie_name\n    nil\n  end",
    "comment": "Implement legacy_sort_cookie_name method on controllers to set sort from cookie for backwards compatibility.",
    "label": "",
    "id": "5744"
  },
  {
    "raw_code": "def remember_sorting_key(field = sorting_field)\n    @remember_sorting_key ||= field\n      .to_s\n      .split('_')[0..-2]\n      .map(&:singularize)\n      .join('')\n      .concat('_sort')\n  end",
    "comment": "Convert sorting_field to legacy cookie name for backwards compatibility :merge_requests_sort => 'mergerequest_sort' :issues_sort => 'issue_sort'",
    "label": "",
    "id": "5745"
  },
  {
    "raw_code": "def update_cookie_value(value)\n    case value\n    when 'id_asc'             then sort_value_oldest_created\n    when 'id_desc'            then sort_value_recently_created\n    when 'downvotes_asc'      then sort_value_popularity\n    when 'downvotes_desc'     then sort_value_popularity\n    else value\n    end",
    "comment": "Update old values to the actual ones.",
    "label": "",
    "id": "5746"
  },
  {
    "raw_code": "def include_web_ide_csp\n    return if request.content_security_policy.directives.blank?\n\n    base_uri = URI(request.url)\n    base_uri.path = ::Gitlab.config.gitlab.relative_url_root || '/'\n    # note: `.path +=` handles combining trailing and leading slashes (e.g. `x/` and `/foo`)\n    base_uri.path += '/assets/webpack/'\n    # note: this fixes a browser console warning where CSP included query params\n    base_uri.query = nil\n    webpack_url = base_uri.to_s\n\n    default_src = Array(request.content_security_policy.directives['default-src'] || [])\n    request.content_security_policy.directives['frame-src'] ||= default_src\n    request.content_security_policy.directives['frame-src'].concat([webpack_url, 'https://*.web-ide.gitlab-static.net/',\n      ide_oauth_redirect_url, oauth_authorization_url])\n\n    request.content_security_policy.directives['worker-src'] ||= default_src\n    request.content_security_policy.directives['worker-src'].concat([webpack_url])\n  end",
    "comment": "We want to include frames from `/assets/webpack` of the request's host to support URL flexibility with the Web IDE. https://gitlab.com/gitlab-org/gitlab/-/merge_requests/118875",
    "label": "",
    "id": "5747"
  },
  {
    "raw_code": "def render_issues_calendar(issuables)\n    @issues = issuables\n                  .non_archived\n                  .with_due_date\n                  .limit(100)\n\n    respond_to do |format|\n      format.ics do\n        # NOTE: with text/calendar as Content-Type, the browser always downloads\n        #       the content as a file (even ignoring the Content-Disposition\n        #       header). We want to display the content inline when accessed\n        #       from GitLab, similarly to the RSS feed.\n        response.headers['Content-Type'] = 'text/plain' if request.referer&.start_with?(::Settings.gitlab.base_url)\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5748"
  },
  {
    "raw_code": "def labels\n    respond_to do |format|\n      format.html { redirect_to milestone_redirect_path }\n      format.json do\n        milestone_labels = @milestone.issue_labels_visible_by_user(current_user)\n\n        render json: tabs_json(\"shared/milestones/_labels_tab\", {\n          labels: milestone_labels.map do |label|\n            label.present(issuable_subject: @milestone.resource_parent)\n          end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5749"
  },
  {
    "raw_code": "def pages\n    @wiki_entries = WikiDirectory.group_pages(pages_list)\n\n    render 'shared/wikis/pages'\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5750"
  },
  {
    "raw_code": "def show\n    if page\n      set_encoding_error unless valid_encoding?\n\n      # Assign vars expected by MarkupHelper\n      @ref = params[:version_id]\n      @path = page.path\n      @templates = templates_list\n\n      render 'shared/wikis/show'\n    elsif file_blob\n      # This is needed by [GitLab JH](https://gitlab.com/gitlab-jh/gitlab/-/issues/247)\n      send_wiki_file_blob(wiki, file_blob)\n    else\n      handle_redirection\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables `#show` handles a number of scenarios:  - If `id` matches a WikiPage, then show the wiki page. - If `id` is a file in the wiki repository, then send the file. - If we know the user wants to create a new page with the given `id`, then display a create form. - Otherwise show the empty wiki page and invite the user to create a page.  rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5751"
  },
  {
    "raw_code": "def create\n    response = WikiPages::CreateService.new(container: container, current_user: current_user,\n      params: wiki_params).execute\n    @page = response.payload[:page]\n\n    if response.success?\n      handle_action_success :created, @page\n    else\n      @templates = templates_list\n\n      render 'shared/wikis/edit'\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5752"
  },
  {
    "raw_code": "def history\n    if page\n      @commits_count = page.count_versions\n      @commits = Kaminari.paginate_array(page.versions(page: pagination_params[:page].to_i),\n        total_count: page.count_versions)\n        .page(pagination_params[:page])\n\n      render 'shared/wikis/history'\n    else\n      redirect_to(\n        wiki_path(wiki),\n        notice: _(\"Page not found\")\n      )\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5753"
  },
  {
    "raw_code": "def diff\n    return render_404 unless page\n\n    apply_diff_view_cookie!\n\n    @diffs = page.diffs(diff_options)\n    @diff_notes_disabled = true\n\n    render 'shared/wikis/diff'\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5754"
  },
  {
    "raw_code": "def destroy\n    return render_404 unless page\n\n    response = WikiPages::DestroyService.new(container: container, current_user: current_user).execute(page)\n\n    if response.success?\n      flash[:toast] = _(\"Wiki page was successfully deleted.\")\n\n      redirect_to wiki_path(wiki), status: :found\n    else\n      @error = response.message\n      @templates = templates_list\n\n      render 'shared/wikis/edit'\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5755"
  },
  {
    "raw_code": "def git_access\n    render 'shared/wikis/git_access'\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5756"
  },
  {
    "raw_code": "def load_sidebar\n    @sidebar_page = wiki.find_sidebar(params[:version_id])\n    @wiki_pages_count = pages_list.total_count\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5757"
  },
  {
    "raw_code": "def wiki_params\n    params.require(:wiki).permit(:title, :content, :format, :message, :last_commit_sha)\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5758"
  },
  {
    "raw_code": "def view_file_button(commit_sha, *args)\n    path = wiki_page_path(wiki, page, version_id: page.version.id)\n\n    helpers.link_button_to(path) do\n      helpers.raw(_('View page @ ')) + helpers.content_tag(:span, Commit.truncate_sha(commit_sha), class: 'commit-sha')\n    end",
    "comment": "Override CommitsHelper#view_file_button",
    "label": "",
    "id": "5759"
  },
  {
    "raw_code": "def diff_file_html_data(_project, _diff_file_path, diff_commit_id)\n    {\n      blob_diff_path: wiki_page_path(wiki, page, action: :diff, version_id: diff_commit_id),\n      view: diff_view\n    }\n  end",
    "comment": "Override DiffHelper#diff_file_html_data",
    "label": "",
    "id": "5760"
  },
  {
    "raw_code": "def preload_max_member_access_for_collection(klass, collection)\n    return if !current_user || collection.blank?\n\n    method_name = \"max_member_access_for_#{klass.name.underscore}_ids\"\n\n    collection_ids = collection.try(:map, &:id) || collection.ids\n    current_user.public_send(method_name, collection_ids) # rubocop:disable GitlabSecurity/PublicSend\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5761"
  },
  {
    "raw_code": "def finder_options\n    work_items_collection_params[:state] = (params.permit([:state])[:state].presence || default_state)\n\n    options = {\n      scope: params.permit([:scope])[:scope],\n      state: work_items_collection_params[:state],\n      confidential: Gitlab::Utils.to_boolean(work_items_collection_params[:confidential]),\n      sort: set_sort_order\n    }\n\n    # Used by view to highlight active option\n    @sort = options[:sort]\n\n    # When a user looks for an exact iid, we do not filter by search but only by iid\n    if work_items_collection_params[:search] =~ /^#(?<iid>\\d+)\\z/\n      options[:iids] = Regexp.last_match[:iid]\n      work_items_collection_params[:search] = nil\n    end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables -- we need to use params in the finder",
    "label": "",
    "id": "5762"
  },
  {
    "raw_code": "def convert_html_spam_params_to_headers\n    return unless params['g-recaptcha-response'] || params[:spam_log_id]\n\n    request.headers['X-GitLab-Captcha-Response'] = params['g-recaptcha-response'] if params['g-recaptcha-response']\n    request.headers['X-GitLab-Spam-Log-Id'] = params[:spam_log_id] if params[:spam_log_id]\n\n    # Reset the spam_params on the request context, since they have changed mid-request\n    Gitlab::RequestContext.instance.spam_params = ::Spam::SpamParams.new_from_request(request: request)\n  end",
    "comment": "Convert spam/CAPTCHA values from form field params to headers, because all spam-related services expect these values to be passed as headers.  The 'g-recaptcha-response' field name comes from `Recaptcha::Adapters::ViewMethods#recaptcha_tags` in the recaptcha gem. This is a field which is automatically included by calling the `#recaptcha_tags` method within a HAML template's form.",
    "label": "",
    "id": "5763"
  },
  {
    "raw_code": "def email_format_path\n      nil\n    end",
    "comment": "When overridden this mthod should return a path to view diffs in an email-friendly format.",
    "label": "",
    "id": "5764"
  },
  {
    "raw_code": "def complete_diff_path\n      nil\n    end",
    "comment": "When overridden this method should return a path to view the complete diffs in the UI.",
    "label": "",
    "id": "5765"
  },
  {
    "raw_code": "def invalid_registry_path\n      @invalid_path_error = true\n\n      render :index\n    end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables These instance variables are only read by a view helper to pass them to the frontend See app/views/projects/registry/repositories/index.html.haml app/views/groups/registry/repositories/index.html.haml",
    "label": "",
    "id": "5766"
  },
  {
    "raw_code": "def ping_container_registry\n      ContainerRegistry::Client.registry_info\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "5767"
  },
  {
    "raw_code": "def show\n      render :index\n    end",
    "comment": "The show action renders index to allow frontend routing to work on page refresh",
    "label": "",
    "id": "5768"
  },
  {
    "raw_code": "def authorize_read_dependency_proxy!\n      if auth_user_or_token.is_a?(User)\n        authorize_read_dependency_proxy_for_users!\n      else\n        authorize_read_dependency_proxy_for_tokens!\n      end",
    "comment": "TODO: Split the authorization logic into dedicated methods https://gitlab.com/gitlab-org/gitlab/-/issues/452145",
    "label": "",
    "id": "5769"
  },
  {
    "raw_code": "def fetch_latest_tag\n    allowed_values = ['released_at']\n\n    params.reject! { |key, value| key.to_sym == :order_by && !allowed_values.any?(value) }\n\n    @latest_tag = releases(order_by: params[:order_by]).first&.tag\n  end",
    "comment": "Default order_by is 'released_at', which is set in ReleasesFinder. Also if the passed order_by is invalid, we reject and default to 'released_at'.",
    "label": "",
    "id": "5770"
  },
  {
    "raw_code": "def after_edit_path\n    from_merge_request = MergeRequestsFinder.new(\n      current_user,\n      project_id: @project.id\n    ).find_by(iid: params[:from_merge_request_iid])\n\n    if from_merge_request && @branch_name == @ref\n      diffs_project_merge_request_path(from_merge_request.target_project, from_merge_request) +\n        \"##{hexdigest(@path)}\"\n    else\n      project_blob_path(@project, File.join(@branch_name, @path))\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5771"
  },
  {
    "raw_code": "def after_delete_path\n    branch = BranchesFinder.new(@repository, search: @ref).execute.first\n    if @repository.tree(branch.target, tree_path).entries.empty?\n      project_tree_path(@project, @ref)\n    else\n      project_tree_path(@project, File.join(@ref, tree_path))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5772"
  },
  {
    "raw_code": "def index\n    deployments = environment.deployments.reorder(created_at: :desc)\n    deployments = deployments.where('created_at > ?', params[:after].to_time) if params[:after]&.to_time\n\n    render json: { deployments: DeploymentSerializer.new(project: project)\n                                  .represent_concise(deployments) }\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5773"
  },
  {
    "raw_code": "def show\n    # A deployment belongs to both a project and an environment so either\n    # association could be used to fetch this record. However, because the\n    # IID is defined at the project level, looking up via project is a more\n    # efficient query as it can use the unique index on (project_id, iid).\n    @deployment = project.deployments.find_by_iid!(params[:id])\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5774"
  },
  {
    "raw_code": "def authorize_can_read_issuable!\n    action = [:read_, params[:template_type]].join\n\n    authorize_action!(action)\n  end",
    "comment": "User must have: - `read_merge_request` to see merge request templates, or - `read_issue` to see issue templates  Note params[:template_type] has a route constraint to limit it to `merge_request` or `issue`",
    "label": "",
    "id": "5775"
  },
  {
    "raw_code": "def create\n    branch_name = strip_tags(sanitize(params[:branch_name]))\n    branch_name = Addressable::URI.unescape(branch_name)\n\n    redirect_to_autodeploy = project.empty_repo? && project.deployment_platform.present?\n\n    result = ::Branches::CreateService.new(project, current_user)\n        .execute(branch_name, ref)\n\n    success = (result[:status] == :success)\n\n    if params[:issue_iid] && success\n      target_project = confidential_issue_project || @project\n      issue = IssuesFinder.new(current_user, project_id: target_project.id).find_by(iid: params[:issue_iid])\n\n      if issue\n        SystemNoteService.new_issue_branch(issue, target_project, current_user, branch_name, branch_project: @project)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5776"
  },
  {
    "raw_code": "def destroy\n    result = ::Branches::DeleteService.new(project, current_user).execute(params[:id])\n\n    respond_to do |format|\n      format.html do\n        flash_type = result.error? ? :alert : :notice\n        flash[flash_type] = result.message\n\n        redirect_back_or_default(default: project_branches_path(@project), options: { status: :see_other })\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5777"
  },
  {
    "raw_code": "def limit_diverging_commit_counts!\n    limit = Kaminari.config.default_per_page\n\n    # If we don't have many branches in the repository, then go ahead.\n    return if project.repository.branch_count <= limit\n    return if params[:names].present? && Array(params[:names]).length <= limit\n\n    render json: { error: \"Specify at least one and at most #{limit} branch names\" }, status: :unprocessable_entity\n  end",
    "comment": "It can be expensive to calculate the diverging counts for each branch. Normally the frontend should be specifying a set of branch names, but prior to https://gitlab.com/gitlab-org/gitlab-ce/merge_requests/32496, the frontend could omit this set. To prevent excessive I/O, we require that a list of names be specified.",
    "label": "",
    "id": "5778"
  },
  {
    "raw_code": "def external_file\n    @blob = @entry.blob\n  end",
    "comment": "External files are redirected to Gitlab Pages and might have unsecure content To warn the user about the possible unsecure content, we show a warning page before redirecting the user.",
    "label": "",
    "id": "5779"
  },
  {
    "raw_code": "def folder\n    @folder = params[:id]\n\n    respond_to do |format|\n      format.html\n      format.json do\n        states = SCOPES_TO_STATES.fetch(params[:scope], ACTIVE_STATES)\n        folder_environments = search_environments(type: params[:id])\n\n        @environments = folder_environments.with_state(states)\n          .order(:name)\n\n        render json: {\n          environments: serialize_environments(request, response),\n          available_count: folder_environments.available.count,\n          active_count: folder_environments.active.count,\n          stopped_count: folder_environments.stopped.count\n        }\n      end",
    "comment": "Returns all environments for a given folder rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5780"
  },
  {
    "raw_code": "def show; end\n\n  def new\n    @environment = project.environments.new\n  end\n\n  def edit; end\n\n  def k8s\n    render action: :show\n  end\n\n  def create\n    @environment = project.environments.create(environment_params)\n\n    if @environment.persisted?\n      render json: { environment: @environment, path: project_environment_path(project, @environment) }\n    else\n      render json: { message: @environment.errors.full_messages }, status: :bad_request\n    end\n  end\n\n  def update\n    if @environment.update(environment_params)\n      render json: { environment: @environment, path: project_environment_path(project, @environment) }\n    else\n      render json: { message: @environment.errors.full_messages }, status: :bad_request\n    end\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5781"
  },
  {
    "raw_code": "def terminal_websocket_authorize\n    # Just return the first terminal for now. If the list is in the process of\n    # being looked up, this may result in a 404 response, so the frontend\n    # should retry those errors\n    terminal = environment.terminals.try(:first)\n    if terminal\n      set_workhorse_internal_api_content_type\n      render json: Gitlab::Workhorse.channel_websocket(terminal)\n    else\n      render html: 'Not found', status: :not_found\n    end",
    "comment": "GET .../terminal.ws : implemented in gitlab-workhorse",
    "label": "",
    "id": "5782"
  },
  {
    "raw_code": "def show\n    @tag = @repository.find_tag(params[:id])\n\n    return render_404 unless @tag\n\n    @release = @project.releases.find_by(tag: @tag.name)\n    @commit = @repository.commit(@tag.dereferenced_target)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5783"
  },
  {
    "raw_code": "def create\n    # TODO: remove this with the release creation moved to it's own form https://gitlab.com/gitlab-org/gitlab/-/issues/214245\n    evidence_pipeline = find_evidence_pipeline\n\n    result = ::Tags::CreateService.new(@project, current_user)\n      .execute(params[:tag_name], params[:ref], params[:message])\n\n    if result[:status] == :success\n      # TODO: remove this with the release creation moved to it's own form https://gitlab.com/gitlab-org/gitlab/-/issues/214245\n      if params[:release_description].present?\n        release_params = {\n          tag: params[:tag_name],\n          name: params[:tag_name],\n          description: params[:release_description],\n          evidence_pipeline: evidence_pipeline\n        }\n\n        Releases::CreateService\n          .new(@project, current_user, release_params)\n          .execute\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5784"
  },
  {
    "raw_code": "def find_evidence_pipeline\n    evidence_pipeline_sha = @project.repository.commit(params[:ref])&.sha\n    return unless evidence_pipeline_sha\n\n    @project.ci_pipelines.for_sha(evidence_pipeline_sha).last\n  end",
    "comment": "TODO: remove this with the release creation moved to it's own form https://gitlab.com/gitlab-org/gitlab/-/issues/214245",
    "label": "",
    "id": "5785"
  },
  {
    "raw_code": "def issue\n      @issue ||=\n        IssuesFinder.new(current_user, project_id: @project.id)\n                    .find_by!(iid: params[:issue_id])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5786"
  },
  {
    "raw_code": "def list_service\n      IssueLinks::ListService.new(issue, current_user)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5787"
  },
  {
    "raw_code": "def noteable\n    @noteable ||= noteable_finder_class.new(current_user, project_id: @project.id).find_by!(iid: params[:noteable_id])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5788"
  },
  {
    "raw_code": "def noteable_finder_class\n    case params[:noteable_type]\n    when 'issues'\n      IssuesFinder\n    when 'merge_requests'\n      MergeRequestsFinder\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5789"
  },
  {
    "raw_code": "def show\n    @merge_request = MergeRequestsFinder.new(current_user, project_id: @project.id).execute.opened\n      .find_by(source_project: @project, source_branch: @ref, target_branch: @repository.root_ref)\n\n    @ref_type = ref_type\n\n    respond_to do |format|\n      format.html\n      format.atom { render layout: 'xml' }\n\n      format.json do\n        pager_json(\n          'projects/commits/_commits',\n          @commits.size,\n          project: @project,\n          ref: @ref)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5790"
  },
  {
    "raw_code": "def signatures\n    respond_to do |format|\n      format.json do\n        render json: {\n          signatures: @commits.select(&:has_signature?).map do |commit|\n            {\n              commit_sha: commit.sha,\n              html: view_to_html_string('projects/commit/_signature', signature: commit.signature)\n            }\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5791"
  },
  {
    "raw_code": "def milestone\n    @noteable = @milestone ||= @project.milestones.find_by!(iid: params[:id])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5792"
  },
  {
    "raw_code": "def authorize_admin_milestone!\n    render_404 unless can?(current_user, :admin_milestone, @project)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5793"
  },
  {
    "raw_code": "def codequality_mr_diff_reports\n    reports_response(@merge_request.find_codequality_mr_diff_reports, head_pipeline)\n  end",
    "comment": "documented in doc/development/rails_endpoints/index.md",
    "label": "",
    "id": "5794"
  },
  {
    "raw_code": "def codequality_reports\n    reports_response(@merge_request.compare_codequality_reports)\n  end",
    "comment": "documented in doc/development/rails_endpoints/index.md",
    "label": "",
    "id": "5795"
  },
  {
    "raw_code": "def set_priorities\n    Label.transaction do\n      available_labels_ids = @available_labels.where(id: params[:label_ids]).pluck(:id)\n      label_ids = params[:label_ids].select { |id| available_labels_ids.include?(id.to_i) }\n\n      label_ids.each_with_index do |label_id, index|\n        label = @available_labels.find(label_id)\n        label.prioritize!(project, index)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5796"
  },
  {
    "raw_code": "def promote\n    promote_service = Labels::PromoteService.new(@project, @current_user)\n\n    begin\n      return render_404 unless promote_service.execute(@label)\n\n      flash[:notice] = flash_notice_for(@label, @project.group)\n      respond_to do |format|\n        format.html do\n          redirect_to(project_labels_path(@project), status: :see_other)\n        end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5797"
  },
  {
    "raw_code": "def issue\n    return @issue if defined?(@issue)\n\n    # The Sortable default scope causes performance issues when used with find_by\n    @issuable = @noteable = @issue ||= @project.issues.inc_relations_for_view.iid_in(params[:id]).without_order.take!\n    @note = @project.notes.new(noteable: @issuable)\n\n    return render_404 unless can?(current_user, :read_issue, @issue)\n\n    @issue\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5798"
  },
  {
    "raw_code": "def log_issue_show\n    return unless current_user && @issue\n\n    ::Gitlab::Search::RecentIssues.new(user: current_user).log_view(@issue)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5799"
  },
  {
    "raw_code": "def create_vulnerability_issue_feedback(issue); end\n\n  def redirect_if_work_item\n    return unless use_work_items_path?(issue)\n\n    redirect_to project_work_item_path(project, issue.iid, params: request.query_parameters)\n  end\n\n  def redirect_index_to_work_items\n    return unless index_html_request? && ::Feature.enabled?(:work_item_planning_view, project.group)\n\n    params = request.query_parameters.except(\"type\").merge('type[]' => 'issue')\n    redirect_to project_work_items_path(project, params: params)\n  end\n\n  def require_incident_for_incident_routes\n    return unless params[:incident_tab].present?\n    return if issue.work_item_type&.incident?\n\n    # Redirect instead of 404 to gracefully handle\n    # issue type changes\n    redirect_to project_issue_path(project, issue)\n  end\n\n  # Overridden in EE\n  def redirect_if_epic_params; end\nend\n\nProjects::IssuesController.prepend_mod_with('Projects::IssuesController')",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5800"
  },
  {
    "raw_code": "def require_namespace_project_creation_permission\n    if Gitlab::ImportSources.template?(@project.import_type)\n      render_404 unless can?(current_user, :create_projects, project.namespace)\n    else\n      unless can?(current_user, :admin_project, @project) || can?(current_user, :import_projects, @project.namespace)\n        render_404\n      end",
    "comment": "Project creation by template uses a different permission model to regular imports https://gitlab.com/gitlab-org/gitlab/-/issues/414046#note_1945586449.",
    "label": "",
    "id": "5801"
  },
  {
    "raw_code": "def extract_ref_and_filename(id)\n    path = id.strip\n    data = path.match(%r{(.*)/(.*)})\n\n    if data\n      [data[1], data[2]]\n    else\n      [path, nil]\n    end",
    "comment": "path can be of the form: master master/first.zip master/first/second.tar.gz master/first/second/third.zip  In the archive case, we know that the last value is always the filename, so we do a greedy match to extract the ref. This avoid having to pull all ref names from Redis.",
    "label": "",
    "id": "5802"
  },
  {
    "raw_code": "def project_setting_attributes\n    [:pages_unique_domain_enabled, :pages_primary_domain]\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5803"
  },
  {
    "raw_code": "def pipelines\n    @pipelines = @commit.pipelines.order(id: :desc)\n    @pipelines = @pipelines.where(ref: params[:ref]) if params[:ref]\n    # Capture total count before pagination to ensure accurate count regardless of current page\n    @pipelines_count = @pipelines.count\n    @pipelines = @pipelines.page(params[:page])\n\n    respond_to do |format|\n      format.html\n      format.json do\n        Gitlab::PollingInterval.set_header(response, interval: 10_000)\n\n        render json: {\n          pipelines: PipelineSerializer\n            .new(project: @project, current_user: @current_user)\n            .with_pagination(request, response)\n            .represent(@pipelines),\n          count: {\n            all: @pipelines_count\n          }\n        }\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5804"
  },
  {
    "raw_code": "def merge_requests\n    @merge_requests = MergeRequestsFinder.new(\n      current_user,\n      project_id: @project.id,\n      commit_sha: @commit.sha\n    ).execute.map do |mr|\n      { iid: mr.iid, path: merge_request_path(mr), title: mr.title }\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5805"
  },
  {
    "raw_code": "def define_note_vars\n    @noteable = @commit\n    @note = @project.build_commit_note(commit)\n\n    @new_diff_note_attrs = {\n      noteable_type: 'Commit',\n      commit_id: @commit.id\n    }\n\n    @grouped_diff_discussions = commit.grouped_diff_discussions\n    @discussions = commit.discussions\n\n    if merge_request_iid = params[:merge_request_iid]\n      @merge_request = MergeRequestsFinder.new(current_user, project_id: @project.id).find_by(iid: merge_request_iid)\n\n      if @merge_request\n        @new_diff_note_attrs.merge!(\n          noteable_type: 'MergeRequest',\n          noteable_id: @merge_request.id\n        )\n\n        merge_request_commit_notes = @merge_request.notes.where(commit_id: @commit.id).inc_relations_for_view\n        merge_request_commit_diff_discussions = merge_request_commit_notes.grouped_diff_discussions(@commit.diff_refs)\n        @grouped_diff_discussions.merge!(merge_request_commit_diff_discussions) do |line_code, left, right|\n          left + right\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5806"
  },
  {
    "raw_code": "def define_commit_box_vars\n    @last_pipeline = @commit.last_pipeline\n\n    return unless @commit.last_pipeline\n\n    @last_pipeline_stages = StageSerializer.new(\n      project: @project,\n      current_user: @current_user\n    ).represent(@last_pipeline.stages)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5807"
  },
  {
    "raw_code": "def pipeline\n    return @pipeline if defined?(@pipeline)\n\n    pipelines =\n      if find_latest_pipeline?\n        project.latest_pipelines(ref: params['ref'], limit: 100)\n      else\n        project.all_pipelines.id_in(params[:id])\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5808"
  },
  {
    "raw_code": "def set_pipeline_path\n    @pipeline_path ||= if find_latest_pipeline?\n                         latest_project_pipelines_path(@project, params['ref'])\n                       else\n                         project_pipeline_path(@project, @pipeline)\n                       end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5809"
  },
  {
    "raw_code": "def disallow_new_uploads!\n    render_404 if upload_version_at_least?(ID_BASED_UPLOAD_PATH_VERSION)\n  end",
    "comment": "Starting with this version, #show is handled by Banzai::UploadsController#show",
    "label": "",
    "id": "5810"
  },
  {
    "raw_code": "def target_project\n    strong_memoize(:target_project) do\n      target_project =\n        if !compare_params.key?(:from_project_id)\n          source_project.default_merge_request_target\n        elsif compare_params[:from_project_id].to_i == source_project.id\n          source_project\n        else\n          target_projects(source_project).find_by_id(compare_params[:from_project_id])\n        end",
    "comment": "target == start_ref == from",
    "label": "",
    "id": "5811"
  },
  {
    "raw_code": "def source_project\n    strong_memoize(:source_project) do\n      # Eager load project's avatar url to prevent batch loading\n      # for all forked projects\n      project&.tap(&:avatar_url)\n    end",
    "comment": "source == head_ref == to",
    "label": "",
    "id": "5812"
  },
  {
    "raw_code": "def merge_request\n    @merge_request ||= MergeRequestsFinder.new(current_user, project_id: target_project.id).execute.opened\n      .find_by(source_project: source_project, source_branch: head_ref, target_branch: start_ref)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5813"
  },
  {
    "raw_code": "def compare_params\n    @compare_params ||= params.permit(:from, :to, :from_project_id, :straight, :to_project_id)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5814"
  },
  {
    "raw_code": "def check_merge_requests_available!\n    render_404 if project_policy.merge_requests_disabled?\n  end",
    "comment": "Normally the methods with `check_(\\w+)_available!` pattern are handled by the `method_missing` defined in `ProjectsController::ApplicationController` but that logic does not take the member roles into account, therefore, we handle this case here manually.",
    "label": "",
    "id": "5815"
  },
  {
    "raw_code": "def commit\n    commit_id = params[:commit_id].presence\n    return unless commit_id\n\n    return unless @merge_request.all_commits.exists?(sha: commit_id) ||\n      @merge_request.recent_context_commits.map(&:id).include?(commit_id)\n\n    @commit ||= @project.commit(commit_id)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5816"
  },
  {
    "raw_code": "def build_merge_request\n    params[:merge_request] ||= ActionController::Parameters.new(source_project: @project)\n    new_params = merge_request_params.merge(diff_options: diff_options)\n\n    # Gitaly N+1 issue: https://gitlab.com/gitlab-org/gitlab-foss/issues/58096\n    Gitlab::GitalyClient.allow_n_plus_1_calls do\n      @merge_request = ::MergeRequests::BuildService\n        .new(project: project, current_user: current_user, params: new_params)\n        .execute\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5817"
  },
  {
    "raw_code": "def render_diffs\n    diffs = @compare.diffs(diff_options)\n\n    diffs.unfold_diff_files(note_positions.unfoldable)\n    diffs.write_cache\n\n    request = {\n      current_user: current_user,\n      project: @merge_request.project,\n      render: ->(partial, locals) { view_to_html_string(partial, locals) }\n    }\n\n    options = additional_attributes.merge(\n      diff_view: \"inline\",\n      merge_ref_head_diff: render_merge_ref_head_diff?\n    )\n\n    options[:context_commits] = @merge_request.recent_context_commits\n\n    render json: DiffsSerializer.new(request).represent(diffs, options)\n  end",
    "comment": "Deprecated: https://gitlab.com/gitlab-org/gitlab/issues/37735",
    "label": "",
    "id": "5818"
  },
  {
    "raw_code": "def define_diff_vars\n    @merge_request_diffs = @merge_request.merge_request_diffs.viewable.order_id_desc\n    @compare = commit || find_merge_request_diff_compare\n    render_404 unless @compare\n  end",
    "comment": "Deprecated: https://gitlab.com/gitlab-org/gitlab/issues/37735",
    "label": "",
    "id": "5819"
  },
  {
    "raw_code": "def find_merge_request_diff_compare\n    @merge_request_diff =\n      if params[:diff_id].present?\n        @merge_request.merge_request_diffs.viewable.find_by(id: params[:diff_id])\n      else\n        @merge_request.merge_request_diff\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord  Deprecated: https://gitlab.com/gitlab-org/gitlab/issues/37735",
    "label": "",
    "id": "5820"
  },
  {
    "raw_code": "def additional_attributes\n    {\n      merge_request: @merge_request,\n      merge_request_diff: @merge_request_diff,\n      merge_request_diffs: @merge_request_diffs,\n      start_version: @start_version,\n      start_sha: @start_sha,\n      commit: @commit,\n      latest_diff: @merge_request_diff&.latest?\n    }\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5821"
  },
  {
    "raw_code": "def define_diff_comment_vars\n    @new_diff_note_attrs = {\n      noteable_type: 'MergeRequest',\n      noteable_id: @merge_request.id,\n      commit_id: @commit&.id\n    }\n\n    @diff_notes_disabled = false\n\n    @use_legacy_diff_notes = !@merge_request.has_complete_diff_refs?\n\n    @grouped_diff_discussions = @merge_request.grouped_diff_discussions(@compare.diff_refs)\n    @notes = prepare_notes_for_rendering(@grouped_diff_discussions.values.flatten.flat_map(&:notes))\n  end",
    "comment": "Deprecated: https://gitlab.com/gitlab-org/gitlab/issues/37735",
    "label": "",
    "id": "5822"
  },
  {
    "raw_code": "def merge_request\n    @merge_request ||= MergeRequestsFinder\n      .new(current_user, project_id: @project.id)\n      .find_by!(iid: params[:merge_request_id])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5823"
  },
  {
    "raw_code": "def draft_note_params\n    params.require(:draft_note).permit(\n      :commit_id,\n      :note,\n      :position,\n      :resolve_discussion,\n      :line_code,\n      :internal\n    ).tap do |h|\n      # Old FE version will still be sending `draft_note[commit_id]` as 'undefined'.\n      # That can result to having a note linked to a commit with 'undefined' ID\n      # which is non-existent.\n      h[:commit_id] = nil if h[:commit_id] == 'undefined'\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5824"
  },
  {
    "raw_code": "def selected_target_project\n    return @project unless @project.forked?\n\n    if params[:target_project_id].present?\n      return @project if @project.id.to_s == params[:target_project_id]\n\n      MergeRequestTargetProjectFinder.new(current_user: current_user, source_project: @project)\n        .find_by(id: params[:target_project_id])\n    else\n      @project.default_merge_request_target\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5825"
  },
  {
    "raw_code": "def webide_source?\n    params[:nav_source] == 'webide'\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5826"
  },
  {
    "raw_code": "def authorize_create_snippet!\n    render_404 unless can?(current_user, :create_snippet, project)\n  end",
    "comment": "This overrides the default snippet create authorization because ProjectSnippets are checked against the project rather than the user",
    "label": "",
    "id": "5827"
  },
  {
    "raw_code": "def permitted_project_params\n        [\n          incident_management_setting_attributes: ::Gitlab::Tracking::IncidentManagement.tracking_keys.keys,\n\n          error_tracking_setting_attributes: [\n            :enabled,\n            :integrated,\n            :api_host,\n            :token,\n            { project: [:slug, :name, :organization_slug, :organization_name, :sentry_project_id] }\n          ]\n        ]\n      end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5828"
  },
  {
    "raw_code": "def define_protected_refs\n        @protected_branches = fetch_protected_branches(@project).preload_access_levels\n        @protected_tags = @project.protected_tags.preload_access_levels.order(:name).page(pagination_params[:page])\n        @protected_branch = @project.protected_branches.new\n        @protected_tag = @project.protected_tags.new\n        @protected_tags_count = @protected_tags.reduce(0) { |sum, tag| sum + tag.matching(tag_names).size }\n\n        load_gon_index\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5829"
  },
  {
    "raw_code": "def fetch_protected_branches(project)\n        project.protected_branches.sorted_by_name.page(pagination_params[:page])\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5830"
  },
  {
    "raw_code": "def show\n        render :index\n      end",
    "comment": "The show action renders index to allow frontend routing to work on page refresh",
    "label": "",
    "id": "5831"
  },
  {
    "raw_code": "def ensure_root_container_repository!\n        ::ContainerRegistry::Path.new(@project.full_path).tap do |path|\n          break if path.has_repository?\n\n          ::ContainerRepository.build_from_path(path).tap do |repository|\n            repository.save! if repository.has_tags?\n          end",
    "comment": " Container repository object for root project path.  Needed to maintain a backwards compatibility. ",
    "label": "",
    "id": "5832"
  },
  {
    "raw_code": "def get_user\n      @email = Base64.urlsafe_decode64(params.permit(:email)[:email])\n      User.find_by(email: @email)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5833"
  },
  {
    "raw_code": "def route\n    if current_jira_installation.proxy?\n      redirect_to \"#{current_jira_installation.create_branch_url}?#{request.query_string}\"\n\n      return\n    end",
    "comment": "If the GitLab for Jira Cloud app was installed from the Jira marketplace and points to a self-managed instance, we route the user to the self-managed instance, otherwise we redirect to :new",
    "label": "",
    "id": "5834"
  },
  {
    "raw_code": "def development_tool_module\n    {\n      jiraDevelopmentTool: {\n        actions: actions,\n        key: 'gitlab-development-tool',\n        application: { value: Atlassian::JiraConnect.display_name },\n        name: { value: Atlassian::JiraConnect.display_name },\n        url: HOME_URL,\n        logoUrl: logo_url,\n        capabilities: %w[branch commit pull_request]\n      }\n    }\n  end",
    "comment": "See https://developer.atlassian.com/cloud/jira/software/modules/development-tool/",
    "label": "",
    "id": "5835"
  },
  {
    "raw_code": "def deployment_information_module\n    {\n      jiraDeploymentInfoProvider: common_module_properties.merge(\n        actions: {}, # TODO: list deployments\n        name: { value: \"GitLab Deployments\" },\n        key: \"gitlab-deployments\"\n      )\n    }\n  end",
    "comment": "See: https://developer.atlassian.com/cloud/jira/software/modules/deployment/",
    "label": "",
    "id": "5836"
  },
  {
    "raw_code": "def feature_flag_module\n    {\n      jiraFeatureFlagInfoProvider: common_module_properties.merge(\n        actions: {}, # TODO: create, link and list feature flags https://gitlab.com/gitlab-org/gitlab/-/issues/297386\n        name: { value: 'GitLab Feature Flags' },\n        key: 'gitlab-feature-flags'\n      )\n    }\n  end",
    "comment": "see: https://developer.atlassian.com/cloud/jira/software/modules/feature-flag/",
    "label": "",
    "id": "5837"
  },
  {
    "raw_code": "def build_information_module\n    {\n      jiraBuildInfoProvider: common_module_properties.merge(\n        actions: {},\n        name: { value: \"GitLab CI\" },\n        key: \"gitlab-ci\"\n      )\n    }\n  end",
    "comment": "See: https://developer.atlassian.com/cloud/jira/software/modules/build/",
    "label": "",
    "id": "5838"
  },
  {
    "raw_code": "def proxy_download_actions_download_path(object)\n      \"#{project.http_url_to_repo}/gitlab-lfs/objects/#{object[:oid]}\"\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5839"
  },
  {
    "raw_code": "def upload_http_url_to_repo\n      Gitlab::RepositoryUrlBuilder.build(repository.full_path, protocol: :http)\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5840"
  },
  {
    "raw_code": "def batch_operation_disallowed?\n      upload_request? && Gitlab::Database.read_only?\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5841"
  },
  {
    "raw_code": "def lfs_read_only_message\n      _('You cannot write to this read-only GitLab instance.')\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5842"
  },
  {
    "raw_code": "def info_refs\n      log_user_activity if upload_pack?\n\n      render_ok\n    end",
    "comment": "GET /foo/bar.git/info/refs?service=git-upload-pack (git pull) GET /foo/bar.git/info/refs?service=git-receive-pack (git push)",
    "label": "",
    "id": "5843"
  },
  {
    "raw_code": "def git_upload_pack\n      update_fetch_statistics\n\n      render_ok\n    end",
    "comment": "POST /foo/bar.git/git-upload-pack (git pull)",
    "label": "",
    "id": "5844"
  },
  {
    "raw_code": "def git_receive_pack\n      render_ok\n    end",
    "comment": "POST /foo/bar.git/git-receive-pack\" (git push)",
    "label": "",
    "id": "5845"
  },
  {
    "raw_code": "def ssh_upload_pack\n      render plain: \"Not found\", status: :not_found\n    end",
    "comment": "POST /foo/bar.git/ssh-upload-pack\" (git pull via SSH)",
    "label": "",
    "id": "5846"
  },
  {
    "raw_code": "def ssh_receive_pack\n      render plain: \"Not found\", status: :not_found\n    end",
    "comment": "POST /foo/bar.git/ssh-receive-pack\" (git push via SSH)",
    "label": "",
    "id": "5847"
  },
  {
    "raw_code": "def clusterable\n    raise NotImplementedError\n  end",
    "comment": "For Group/Clusters and Project/Clusters, the clusterable object (group or project) is fetched through `find_routable!`, which calls a `render_404` if the user does not have access to the object The `clusterable` method will need to be in its own before_action call before the `authorize_*` calls so that the call stack will not proceed to the `authorize_*` calls and instead just render a not found page after the `clusterable` call",
    "label": "",
    "id": "5848"
  },
  {
    "raw_code": "def cluster_status\n    respond_to do |format|\n      format.json do\n        Gitlab::PollingInterval.set_header(response, interval: STATUS_POLLING_INTERVAL)\n\n        render json: ClusterSerializer\n          .new(current_user: @current_user)\n          .represent_status(@cluster)\n      end",
    "comment": "Overriding ActionController::Metal#status is NOT a good idea",
    "label": "",
    "id": "5849"
  },
  {
    "raw_code": "def query_params\n    query_params = {\n      top_level_only: true,\n      min_access_level: Gitlab::Access::OWNER\n    }\n\n    query_params[:search] = sanitized_filter_param if sanitized_filter_param\n    query_params\n  end",
    "comment": "Default query string params used to fetch groups from GitLab source instance  top_level_only: fetch only top level groups (subgroups are fetched during import itself) min_access_level: fetch only groups user has maintainer or above permissions search: optional search param to search user's groups by a keyword",
    "label": "",
    "id": "5850"
  },
  {
    "raw_code": "def group\n    @group ||= Group.find_by(id: manifest_import_metadata.group_id)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5851"
  },
  {
    "raw_code": "def manifest_import_metadata\n    @manifest_import_status ||= Gitlab::ManifestImport::Metadata.new(current_user, fallback: session)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5852"
  },
  {
    "raw_code": "def status\n    super\n  end",
    "comment": "We need to re-expose controller's internal method 'status' as action. rubocop:disable Lint/UselessMethodDefinition",
    "label": "",
    "id": "5853"
  },
  {
    "raw_code": "def logged_in_with_provider?\n    current_user.identities.exists?(provider: provider_name)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5854"
  },
  {
    "raw_code": "def rate_limit_threshold_exceeded\n    head :too_many_requests\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5855"
  },
  {
    "raw_code": "def status\n    super\n  end",
    "comment": "We need to re-expose controller's internal method 'status' as action. rubocop:disable Lint/UselessMethodDefinition",
    "label": "",
    "id": "5856"
  },
  {
    "raw_code": "def create\n    bitbucket_client = Bitbucket::Client.new(credentials)\n\n    repo_id = params[:repo_id].to_s\n    name = repo_id.gsub('___', '/')\n    repo = bitbucket_client.repo(name)\n    project_name = params[:new_name].presence || repo.name\n\n    repo_owner = repo.owner\n    repo_owner = current_user.username if repo_owner == bitbucket_client.user.username\n    namespace_path = params[:new_namespace].presence || repo_owner\n    target_namespace = find_or_create_namespace(namespace_path, current_user)\n\n    Gitlab::Tracking.event(\n      self.class.name,\n      'create',\n      label: 'import_access_level',\n      user: current_user,\n      extra: { user_role: user_role(current_user, target_namespace), import_type: 'bitbucket' }\n    )\n\n    if current_user.can?(:import_projects, target_namespace)\n      # The token in a session can be expired, we need to get most recent one because\n      # Bitbucket::Connection class refreshes it.\n      session[:bitbucket_token] = bitbucket_client.connection.token\n\n      project = Gitlab::BitbucketImport::ProjectCreator.new(\n        repo,\n        project_name,\n        target_namespace,\n        current_user,\n        credentials\n      ).execute\n\n      if project.persisted?\n        render json: ProjectSerializer.new.represent(project, serializer: :import)\n      else\n        render json: { errors: project_save_error(project) }, status: :unprocessable_entity\n      end",
    "comment": "rubocop:enable Lint/UselessMethodDefinition",
    "label": "",
    "id": "5857"
  },
  {
    "raw_code": "def find_already_added_projects(import_type)\n    current_user.created_projects.inc_routes.where(import_type: import_type).with_import_state\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "5858"
  },
  {
    "raw_code": "def find_or_create_namespace(names, owner)\n    names = params[:target_namespace].presence || names\n\n    return current_user.namespace if names == owner\n\n    group = Groups::NestedCreateService.new(\n      current_user,\n      organization_id: Current.organization.id,\n      group_path: names\n    ).execute\n\n    group.errors.any? ? current_user.namespace : group\n  rescue StandardError => e\n    Gitlab::AppLogger.error(e)\n\n    current_user.namespace\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord deprecated: being replaced by app/services/import/base_service.rb",
    "label": "",
    "id": "5859"
  },
  {
    "raw_code": "def project_save_error(project)\n    project.errors.full_messages.join(', ')\n  end",
    "comment": "deprecated: being replaced by app/services/import/base_service.rb",
    "label": "",
    "id": "5860"
  },
  {
    "raw_code": "def callback\n      redirect_uri = redirect_uri_from_session\n      ##\n      # when  the user declines authorizations\n      # `error` param is returned\n      if params[:error]\n        flash[:alert] = _('Google Cloud authorizations required')\n        redirect_uri = session[:error_uri]\n      ##\n      # on success, the `code` param is returned\n      elsif params[:code]\n        token, expires_at = GoogleApi::CloudPlatform::Client\n          .new(nil, callback_google_api_auth_url)\n          .get_token(params[:code])\n\n        session[GoogleApi::CloudPlatform::Client.session_key_for_token] = token\n        session[GoogleApi::CloudPlatform::Client.session_key_for_expires_at] = expires_at.to_s\n        redirect_uri = redirect_uri_from_session\n      end",
    "comment": " handle the response from google after the user goes through authentication and authorization process",
    "label": "",
    "id": "5861"
  },
  {
    "raw_code": "def context\n    request_authenticator = Gitlab::Auth::RequestAuthenticator.new(request)\n    scope_validator = ::Gitlab::Auth::ScopeValidator.new(current_user, request_authenticator)\n    {\n      channel: self,\n      current_user: current_user,\n      current_organization: connection.current_organization,\n      is_sessionless_user: false,\n      scope_validator: scope_validator\n    }\n  end",
    "comment": "When modifying the context, also update GraphqlController#context if needed so that we have similar context when executing queries, mutations, and subscriptions  Objects added to the context may also need to be reloaded in `Subscriptions::BaseSubscription` so that they are not stale",
    "label": "",
    "id": "5862"
  },
  {
    "raw_code": "def dashboard_storage_limit_enabled?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5863"
  },
  {
    "raw_code": "def self.last_finished_deployment_group_for_environment(env)\n    return none unless env.latest_finished_jobs.present?\n\n    # this batch loads a collection of deployments associated to `latest_finished_jobs` per `environment`\n    BatchLoader.for(env).batch(key: :latest_finished_jobs, default_value: none) do |environments, loader|\n      job_ids = []\n      environments_hash = {}\n\n      # Preloading the environment's `latest_finished_jobs` avoids N+1 queries.\n      environments.each do |environment|\n        environments_hash[environment.id] = environment\n\n        job_ids << environment.latest_finished_jobs.map(&:id)\n      end",
    "comment": "This method returns the *finished deployments* of the *last finished pipeline* for a given environment e.g., a finished pipeline contains - deploy job A (environment: production, status: success) - deploy job B (environment: production, status: failed) - deploy job C (environment: production, status: canceled) In the above case, `last_finished_deployment_group_for_environment` returns all deployments",
    "label": "",
    "id": "5864"
  },
  {
    "raw_code": "def self.jobs(limit = 1000)\n    deployable_ids = where.not(deployable_id: nil).limit(limit).pluck(:deployable_id)\n\n    Ci::Processable.where(id: deployable_ids)\n  end",
    "comment": "It should be used with caution especially on chaining. Fetching any unbounded or large intermediate dataset could lead to loading too many IDs into memory. See: https://docs.gitlab.com/ee/development/database/multiple_databases.html#use-disable_joins-for-has_one-or-has_many-through-relations For safety we default limit to fetch not more than 1000 records.",
    "label": "",
    "id": "5865"
  },
  {
    "raw_code": "def begin_fast_destroy\n      preload(:project, :environment).find_each.map do |deployment|\n        [deployment.project, deployment.ref_path]\n      end",
    "comment": " FastDestroyAll concerns",
    "label": "",
    "id": "5866"
  },
  {
    "raw_code": "def finalize_fast_destroy(params)\n      by_project = params.group_by(&:shift)\n\n      by_project.each do |project, ref_paths|\n        project.repository.delete_refs(*ref_paths.flatten)\n      rescue Gitlab::Git::Repository::NoRepository\n        next\n      end",
    "comment": " FastDestroyAll concerns",
    "label": "",
    "id": "5867"
  },
  {
    "raw_code": "def update_status(status)\n    update_status!(status)\n  rescue StandardError => e\n    error = StatusUpdateError.new(e.message)\n    error.set_backtrace(caller)\n    Gitlab::ErrorTracking.track_exception(error, deployment_id: id)\n    false\n  end",
    "comment": "Changes the status of a deployment and triggers the corresponding state machine events.",
    "label": "",
    "id": "5868"
  },
  {
    "raw_code": "def tags(limit: 100)\n    strong_memoize_with(:tag, limit) do\n      project.repository.refs_by_oid(oid: sha, limit: limit, ref_patterns: [Gitlab::Git::TAG_REF_PREFIX])\n    end",
    "comment": "default tag limit is 100, 0 means no limit when refs_by_oid is passed an SHA, returns refs for that commit",
    "label": "",
    "id": "5869"
  },
  {
    "raw_code": "def find_by_path_or_name(path)\n      find_by(\"lower(path) = :path OR lower(name) = :path\", path: path.downcase)\n    end",
    "comment": "Case insensitive search for namespace by path or name",
    "label": "",
    "id": "5870"
  },
  {
    "raw_code": "def search(query, include_parents: false, use_minimum_char_limit: true, exact_matches_first: false)\n      if include_parents\n        route_columns = [Route.arel_table[:path], Route.arel_table[:name]]\n        namespaces = without_project_namespaces\n          .where(id: Route.for_routable_type(Namespace.name)\n          .allow_cross_joins_across_databases(url: \"https://gitlab.com/gitlab-org/gitlab/-/issues/420046\")\n            .fuzzy_search(query, route_columns,\n              use_minimum_char_limit: use_minimum_char_limit)\n            .select(:source_id))\n\n        if exact_matches_first\n          namespaces = namespaces\n            .joins(:route)\n            .allow_cross_joins_across_databases(url: \"https://gitlab.com/gitlab-org/gitlab/-/issues/420046\")\n            .order(exact_matches_first_sql(query, route_columns))\n        end",
    "comment": "Searches for namespaces matching the given query.  This method uses ILIKE on PostgreSQL.  query - The search query as a String.  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "5871"
  },
  {
    "raw_code": "def gfm_autocomplete_search(query)\n      namespaces_cte = Gitlab::SQL::CTE.new(table_name, without_order)\n\n      # This scope does not work with `ProjectNamespace` records because they don't have a corresponding `route` association.\n      # We do not chain the `without_project_namespaces` scope because it results in an expensive query plan in certain cases\n      unscoped\n        .with(namespaces_cte.to_arel)\n        .from(namespaces_cte.table)\n        .joins(:route)\n        .where(\n          \"REPLACE(routes.name, ' ', '') ILIKE :pattern OR routes.path ILIKE :pattern\",\n          pattern: \"%#{sanitize_sql_like(query)}%\"\n        )\n        .order(\n          Arel.sql(sanitize_sql(\n            [\n              \"CASE WHEN REPLACE(routes.name, ' ', '') ILIKE :prefix_pattern OR routes.path ILIKE :prefix_pattern THEN 1 ELSE 2 END\",\n              { prefix_pattern: \"#{sanitize_sql_like(query)}%\" }\n            ]\n          )),\n          'routes.path'\n        )\n    end",
    "comment": "This should be kept in sync with the frontend filtering in https://gitlab.com/gitlab-org/gitlab/-/blob/5d34e3488faa3982d30d7207773991c1e0b6368a/app/assets/javascripts/gfm_auto_complete.js#L68 and https://gitlab.com/gitlab-org/gitlab/-/blob/5d34e3488faa3982d30d7207773991c1e0b6368a/app/assets/javascripts/gfm_auto_complete.js#L1053",
    "label": "",
    "id": "5872"
  },
  {
    "raw_code": "def emails_disabled?\n    !emails_enabled?\n  end",
    "comment": "any ancestor can disable emails for all descendants",
    "label": "",
    "id": "5873"
  },
  {
    "raw_code": "def all_projects\n    namespace = user_namespace? ? self : self_and_descendant_ids\n    Project.where(namespace: namespace)\n  end",
    "comment": "Includes projects from this namespace and projects from all subgroups that belongs to this namespace",
    "label": "",
    "id": "5874"
  },
  {
    "raw_code": "def multiple_issue_boards_available?\n    false\n  end",
    "comment": "Overridden on EE module",
    "label": "",
    "id": "5875"
  },
  {
    "raw_code": "def feature_available?(feature, _user = nil)\n    licensed_feature_available?(feature)\n  end",
    "comment": "Deprecated, use #licensed_feature_available? instead. Remove once Namespace#feature_available? isn't used anymore.",
    "label": "",
    "id": "5876"
  },
  {
    "raw_code": "def licensed_feature_available?(_feature)\n    false\n  end",
    "comment": "Overridden in EE::Namespace",
    "label": "",
    "id": "5877"
  },
  {
    "raw_code": "def no_conflict_with_organization_user_details\n    return unless Organizations::OrganizationUserDetail.for_organization(organization).with_usernames(path).any?\n\n    errors.add(:path, _('has already been taken'))\n  end",
    "comment": "route / path global uniqueness is handled by Routeable concern here we are checking only for conflicts with per-organization username aliases",
    "label": "",
    "id": "5878"
  },
  {
    "raw_code": "def cross_project_reference?(from)\n    case from\n    when Project\n      from.project_namespace_id != id\n    else\n      from && self != from\n    end",
    "comment": "Check if a reference is being done cross-project",
    "label": "",
    "id": "5879"
  },
  {
    "raw_code": "def schedule_sync_event_worker\n    run_after_commit do\n      Namespaces::SyncEvent.enqueue_worker\n    end",
    "comment": "SyncEvents are created by PG triggers (with the function `insert_namespaces_sync_event`)",
    "label": "",
    "id": "5880"
  },
  {
    "raw_code": "def duration\n    calculate_duration(started_at, finished_at)\n  end",
    "comment": "Time spent running.",
    "label": "",
    "id": "5881"
  },
  {
    "raw_code": "def queued_duration\n    calculate_duration(queued_at, started_at || finished_at)\n  end",
    "comment": "Time spent in the pending state.",
    "label": "",
    "id": "5882"
  },
  {
    "raw_code": "def exit_code=(value); end\n\n  # For AiAction\n  def to_ability_name\n    'build'\n  end\n\n  def test_suite_name\n    nil\n  end\n\n  # For AiAction\n  def resource_parent\n    project\n  end\n\n  private\n\n  def unrecoverable_failure?\n    script_failure? || missing_dependency_failure? || archived_failure? || scheduler_failure? || data_integrity_failure?\n  end\nend",
    "comment": "Handled only by ci_build",
    "label": "",
    "id": "5883"
  },
  {
    "raw_code": "def initialize(container, commits, ref = nil, page: nil, per_page: nil, count: nil)\n    @container = container\n    @commits = commits\n    @ref = ref\n    @pagination = Gitlab::PaginationDelegate.new(page: page, per_page: per_page, count: count)\n  end",
    "comment": "container - The object the commits belong to. commits - The Commit instances to store. ref - The name of the ref (e.g. \"master\").",
    "label": "",
    "id": "5884"
  },
  {
    "raw_code": "def with_latest_pipeline(ref = nil)\n    return self unless project\n\n    pipelines = project.ci_pipelines.latest_pipeline_per_commit(map(&:id), ref)\n\n    each do |commit|\n      pipeline = pipelines[commit.id]\n      pipeline&.number_of_warnings # preload number of warnings\n\n      commit.set_latest_pipeline_for_ref(ref, pipeline)\n    end",
    "comment": "Returns the collection with the latest pipeline for every commit pre-set.  Setting the pipeline for each commit ahead of time removes the need for running a query for every commit we're displaying.",
    "label": "",
    "id": "5885"
  },
  {
    "raw_code": "def with_markdown_cache\n    Commit.preload_markdown_cache!(commits)\n\n    self\n  end",
    "comment": "Returns the collection with markdown fields preloaded.  Get the markdown cache from redis using pipeline to prevent n+1 requests when rendering the markdown of an attribute (e.g. title, full_title, description).",
    "label": "",
    "id": "5886"
  },
  {
    "raw_code": "def enrich!\n    # A container is needed in order to fetch data from gitaly. Containers\n    # can be absent from commits in certain rare situations (like when\n    # viewing a MR of a deleted fork). In these cases, assume that the\n    # enriched data is not needed.\n    return self if container.blank? || fully_enriched?\n\n    # Batch load full Commits from the repository\n    # and map to a Hash of id => Commit\n    replacements = unenriched.each_with_object({}) do |c, result|\n      result[c.id] = Commit.lazy(container, c.id)\n    end.compact\n\n    # Replace the commits, keeping the same order\n    @commits = @commits.map do |original_commit|\n      # Return the original instance: if it didn't need to be batchloaded, it was\n      # already enriched.\n      batch_loaded_commit = replacements.fetch(original_commit.id, original_commit)\n\n      # If batch loading the commit failed, fall back to the original commit.\n      # We need to explicitly check `.nil?` since otherwise a `BatchLoader` instance\n      # that looks like `nil` is returned.\n      batch_loaded_commit.nil? ? original_commit : batch_loaded_commit\n    end",
    "comment": "Batch load any commits that are not backed by full gitaly data, and replace them in the collection.",
    "label": "",
    "id": "5887"
  },
  {
    "raw_code": "def method_missing(message, *args, &block)\n    commits.public_send(message, *args, &block)\n  end",
    "comment": "rubocop:disable GitlabSecurity/PublicSend",
    "label": "",
    "id": "5888"
  },
  {
    "raw_code": "def self.refresh!\n    # The calculation **must** run in a transaction. If the removal of data and\n    # insertion of new data were to run separately a user might end up with an\n    # empty list of trending projects for a short period of time.\n    transaction do\n      delete_all\n\n      timestamp = connection.quote(MONTHS_TO_INCLUDE.months.ago)\n\n      connection.execute <<-EOF.strip_heredoc\n        INSERT INTO #{table_name} (project_id)\n        SELECT project_id\n        FROM notes\n        INNER JOIN projects ON projects.id = notes.project_id\n        WHERE notes.created_at >= #{timestamp}\n        AND notes.system IS FALSE\n        AND projects.visibility_level = #{Gitlab::VisibilityLevel::PUBLIC}\n        GROUP BY project_id\n        ORDER BY count(*) DESC\n        LIMIT #{PROJECTS_LIMIT};\n      EOF\n    end",
    "comment": "Populates the trending projects table with the current list of trending projects.",
    "label": "",
    "id": "5889"
  },
  {
    "raw_code": "def initialize(projects, limit: 20, offset: 0, filter: nil, groups: nil, preserve_projects_order: false)\n    @projects = projects\n    @limit = limit\n    @offset = offset\n    @filter = filter || EventFilter.new(EventFilter::ALL)\n    @groups = groups\n    @preserve_projects_order = preserve_projects_order\n  end",
    "comment": "projects - An ActiveRecord::Relation object that returns the projects for which to retrieve events. filter - An EventFilter instance to use for filtering events. preserve_projects_order - If true, retains the :order clause for projects.",
    "label": "",
    "id": "5890"
  },
  {
    "raw_code": "def to_a\n    return [] if current_page > MAX_PAGE\n\n    relation = if groups\n                 project_and_group_events\n               else\n                 project_events\n               end",
    "comment": "Returns an Array containing the events.",
    "label": "",
    "id": "5891"
  },
  {
    "raw_code": "def self.build_discussion_id(note)\n    base_discussion_id(note)\n  end",
    "comment": "Returns an array of discussion ID components",
    "label": "",
    "id": "5892"
  },
  {
    "raw_code": "def self.override_discussion_id(note)\n    discussion_id(note)\n  end",
    "comment": "To make sure all out-of-context notes end up grouped as one discussion, we override the discussion ID to be a newly generated but consistent ID.",
    "label": "",
    "id": "5893"
  },
  {
    "raw_code": "def replace_placeholder_action(action, project)\n    return unless project\n\n    action.is_a?(Proc) ? action.call(project) : project.public_send(action) # rubocop:disable GitlabSecurity/PublicSend\n  end",
    "comment": "The action param represents the :symbol or Proc to call in order to retrieve the return value from the project. This method checks if it is a Proc and use the call method, and if it is a symbol just send the action",
    "label": "",
    "id": "5894"
  },
  {
    "raw_code": "def in_progress?\n    scheduled? || started?\n  end",
    "comment": "This method is coupled to the repository mirror domain. Use with caution in the importers domain. As an alternative, use the `#completed?` method. See EE-override and https://gitlab.com/gitlab-org/gitlab/-/merge_requests/4697",
    "label": "",
    "id": "5895"
  },
  {
    "raw_code": "def user_mapping_enabled?\n    user_mapping_enabled || project.import_data&.user_mapping_enabled?\n  end",
    "comment": "Return whether or not user mapping was enabled during the project's import to determine who to send completion emails to. user_mapping_enabled should be set if import_data is removed. This can be removed when all 3rd party project importer user mapping feature flags are removed.",
    "label": "",
    "id": "5896"
  },
  {
    "raw_code": "def self.unique_domain_exists?(domain)\n    where(pages_unique_domain: domain).exists?\n  end",
    "comment": "Checks if a given domain is already assigned to any existing project",
    "label": "",
    "id": "5897"
  },
  {
    "raw_code": "def self.reference_pattern\n    @reference_pattern ||= %r{\n      (?:#{Project.reference_pattern}#{reference_prefix})?\n      (?<commit_range>#{STRICT_PATTERN})\n    }x\n  end",
    "comment": "Pattern used to extract commit range references from text  This pattern supports cross-project references.",
    "label": "",
    "id": "5898"
  },
  {
    "raw_code": "def initialize(range_string, project)\n    @project = project\n\n    range_string = range_string.strip\n\n    unless /\\A#{PATTERN}\\z/o.match?(range_string)\n      raise ArgumentError, \"invalid CommitRange string format: #{range_string}\"\n    end",
    "comment": "Initialize a CommitRange  range_string - The String commit range. project      - The Project model.  Raises ArgumentError if `range_string` does not match `PATTERN`.",
    "label": "",
    "id": "5899"
  },
  {
    "raw_code": "def to_param\n    { from: sha_start, to: sha_to }\n  end",
    "comment": "Return a Hash of parameters for passing to a URL helper  See `namespace_project_compare_url`",
    "label": "",
    "id": "5900"
  },
  {
    "raw_code": "def valid_commits?\n    commit_start.present? && commit_end.present?\n  end",
    "comment": "Check if both the starting and ending commit IDs exist in a project's repository",
    "label": "",
    "id": "5901"
  },
  {
    "raw_code": "def delete_object_pool\n    object_pool.delete\n  end",
    "comment": "The members of the pool should have fetched the missing objects to their own objects directory. If the caller fails to do so, data loss might occur",
    "label": "",
    "id": "5902"
  },
  {
    "raw_code": "def owner_class_attribute_default\n      'initial'\n    end",
    "comment": "Supress warning: both JiraImportState and its :status machine have defined a different default for \"status\". although both have same value but represented in 2 ways: integer(0) and symbol(:initial)",
    "label": "",
    "id": "5903"
  },
  {
    "raw_code": "def from_line_index\n    from_line - 1\n  end",
    "comment": "`from_line_index` and `to_line_index` represents diff/blob line numbers in index-like way (N-1).",
    "label": "",
    "id": "5904"
  },
  {
    "raw_code": "def outdated?(cached: true)\n    return super() if cached\n    return true unless diff_file\n\n    from_content != fetch_from_content\n  end",
    "comment": "Overwrites outdated column",
    "label": "",
    "id": "5905"
  },
  {
    "raw_code": "def self.bulk_insert(*args)\n    ApplicationRecord.legacy_bulk_insert('merge_request_context_commit_diff_files', *args) # rubocop:disable Gitlab/BulkInsert\n  end",
    "comment": "create MergeRequestContextCommitDiffFile by given diff file record(s)",
    "label": "",
    "id": "5906"
  },
  {
    "raw_code": "def self.public_or_visible_to_user(user = nil, min_access_level = nil)\n    min_access_level = nil if user&.can_read_all_resources?\n\n    return public_to_user unless user\n\n    if user.is_a?(DeployToken)\n      where(id: user.accessible_projects)\n    else\n      where(\n        'EXISTS (?) OR projects.visibility_level IN (?)',\n        user.authorizations_for_projects(min_access_level: min_access_level),\n        self.visibility_levels_for_user(user)\n      )\n    end",
    "comment": "Returns a collection of projects that is either public or visible to the logged in user.",
    "label": "",
    "id": "5907"
  },
  {
    "raw_code": "def self.visibility_levels_for_user(user)\n    can_read_all_projects = user&.can_read_all_resources? || user&.can?(:read_admin_projects)\n\n    return ::Gitlab::VisibilityLevel::LEVELS_FOR_ADMINS if can_read_all_projects\n\n    Gitlab::VisibilityLevel.levels_for_user(user)\n  end",
    "comment": "Auditors can :read_all_resources while admins can :read_all_resources and read_admin_projects. In EE, a regular user can read_admin_projects through custom admin roles.",
    "label": "",
    "id": "5908"
  },
  {
    "raw_code": "def self.cascading_with_parent_namespace(attribute)\n    define_method(\"#{attribute}?\") do |inherit_group_setting: false|\n      self.public_send(attribute) # rubocop:disable GitlabSecurity/PublicSend\n    end",
    "comment": "Define two instance methods:  - [attribute]?(inherit_group_setting) Returns the final value after inheriting the parent group - [attribute]_locked?                 Returns true if the value is inherited from the parent group  These functions will be overridden in EE to make sense afterwards",
    "label": "",
    "id": "5909"
  },
  {
    "raw_code": "def self.filter_by_feature_visibility(feature, user)\n    with_feature_available_for_user(feature, user)\n      .public_or_visible_to_user(\n        user,\n        ProjectFeature.required_minimum_access_level_for_private_project(feature)\n      )\n  end",
    "comment": "This scope returns projects where user has access to both the project and the feature.",
    "label": "",
    "id": "5910"
  },
  {
    "raw_code": "def search(query, include_namespace: false, use_minimum_char_limit: true)\n      if include_namespace\n        joins(:route).fuzzy_search(query, [Route.arel_table[:path], Route.arel_table[:name], :description],\n          use_minimum_char_limit: use_minimum_char_limit)\n        .allow_cross_joins_across_databases(url: 'https://gitlab.com/gitlab-org/gitlab/-/issues/421843')\n      else\n        fuzzy_search(query, [:path, :name, :description], use_minimum_char_limit: use_minimum_char_limit)\n      end",
    "comment": "Searches for a list of projects based on the query given in `query`.  On PostgreSQL this method uses \"ILIKE\" to perform a case-insensitive search.  query - The search query as a String.",
    "label": "",
    "id": "5911"
  },
  {
    "raw_code": "def sort_by_attribute(method)\n      case method.to_s\n      when 'storage_size_desc' then sorted_by_storage_size_desc\n      when 'storage_size_asc' then sorted_by_storage_size_asc\n      when 'repository_size_desc' then sorted_by_repository_size_desc\n      when 'repository_size_asc' then sorted_by_repository_size_asc\n      when 'snippets_size_desc'then sorted_by_snippets_size_desc\n      when 'snippets_size_asc'then sorted_by_snippets_size_asc\n      when 'build_artifacts_size_desc' then sorted_by_build_artifacts_size_desc\n      when 'build_artifacts_size_asc'then sorted_by_build_artifacts_size_asc\n      when 'lfs_objects_size_desc'then sorted_by_lfs_objects_size_desc\n      when 'lfs_objects_size_asc' then sorted_by_lfs_objects_size_asc\n      when 'packages_size_desc' then sorted_by_packages_size_desc\n      when 'packages_size_asc' then sorted_by_packages_size_asc\n      when 'wiki_size_desc' then sorted_by_wiki_size_desc\n      when 'wiki_size_asc'then sorted_by_wiki_size_asc\n      when 'container_registry_size_desc' then sorted_by_container_registry_size_desc\n      when 'container_registry_size_asc' then sorted_by_container_registry_size_asc\n      when 'latest_activity_desc' then sorted_by_updated_desc\n      when 'latest_activity_asc' then sorted_by_updated_asc\n      when 'path_desc'then sorted_by_path_desc\n      when 'path_asc' then sorted_by_path_asc\n      when 'full_path_desc'then sorted_by_full_path_desc\n      when 'full_path_asc' then sorted_by_full_path_asc\n      when 'stars_desc' then sorted_by_stars_desc\n      when 'stars_asc' then sorted_by_stars_asc\n      else\n        order_by(method)\n      end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity -- stick to existing implementation for sort params:",
    "label": "",
    "id": "5912"
  },
  {
    "raw_code": "def reference_pattern\n      %r{\n        (?<!#{Gitlab::PathRegex::PATH_START_CHAR})\n        (?<absolute_path>/)?\n        ((?<namespace>#{Gitlab::PathRegex::FULL_NAMESPACE_FORMAT_REGEX})/)?\n        (?<project>#{Gitlab::PathRegex::PROJECT_PATH_FORMAT_REGEX})\n      }xo\n    end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "5913"
  },
  {
    "raw_code": "def markdown_reference_pattern\n      @markdown_reference_pattern ||=\n        %r{\n          #{reference_pattern}\n          (#{reference_postfix}|#{reference_postfix_escaped})\n        }x\n    end",
    "comment": "Pattern used to extract `namespace/project>` project references from text. '>' or its escaped form ('&gt;') are checked for because '>' is sometimes escaped when the reference comes from an external source.",
    "label": "",
    "id": "5914"
  },
  {
    "raw_code": "def ids_with_issuables_available_for(user)\n      with_issues_enabled = with_issues_available_for_user(user).select(:id)\n      with_merge_requests_enabled = with_merge_requests_available_for_user(user).select(:id)\n\n      from_union([with_issues_enabled, with_merge_requests_enabled]).select(:id)\n    end",
    "comment": "Returns ids of projects with issuables available for given user  Used on queries to find milestones or labels which user can see For example: Milestone.where(project_id: ids_with_issuables_available_for(user))",
    "label": "",
    "id": "5915"
  },
  {
    "raw_code": "def set_project_feature_defaults\n    self.class.project_features_defaults.each do |attr, value|\n      # If the deprecated _enabled or the accepted _access_level attribute is specified, we don't need to set the default\n      next unless @init_attributes[:\"#{attr}_enabled\"].nil? && @init_attributes[:\"#{attr}_access_level\"].nil?\n\n      public_send(\"#{attr}_enabled=\", value) # rubocop:disable GitlabSecurity/PublicSend\n    end",
    "comment": "Remove along with ProjectFeaturesCompatibility module",
    "label": "",
    "id": "5916"
  },
  {
    "raw_code": "def ancestors_upto(top = nil, hierarchy_order: nil)\n    Gitlab::ObjectHierarchy.new(Group.where(id: namespace_id))\n      .base_and_ancestors(upto: top, hierarchy_order: hierarchy_order)\n  end",
    "comment": "returns all ancestor-groups upto but excluding the given namespace when no namespace is given, all ancestors upto the top are returned",
    "label": "",
    "id": "5917"
  },
  {
    "raw_code": "def design_management_enabled?\n    lfs_enabled? && hashed_storage?(:repository)\n  end",
    "comment": "LFS and hashed repository storage are required for using Design Management.",
    "label": "",
    "id": "5918"
  },
  {
    "raw_code": "def latest_successful_build_for_ref(job_name, ref = default_branch)\n    return unless ref\n\n    latest_pipeline = ci_pipelines.latest_successful_for_ref(ref)\n    return unless latest_pipeline\n\n    latest_pipeline.build_with_artifacts_in_self_and_project_descendants(job_name)\n  end",
    "comment": "ref can't be HEAD, can only be branch/tag name",
    "label": "",
    "id": "5919"
  },
  {
    "raw_code": "def remove_import_data\n    import_data&.destroy\n  end",
    "comment": "This method is overridden in EE::Project model",
    "label": "",
    "id": "5920"
  },
  {
    "raw_code": "def commit_notes\n    notes.where(noteable_type: \"Commit\")\n  end",
    "comment": "Used by Import/Export to export commit notes",
    "label": "",
    "id": "5921"
  },
  {
    "raw_code": "def safe_import_url(masked: true)\n    url = Gitlab::UrlSanitizer.new(unsafe_import_url)\n    masked ? url.masked_url : url.sanitized_url\n  end",
    "comment": "Returns sanitized import URL.  @param `masked:` [Boolean] Toggles how URL will be sanitized. Defaults to `true`. when `true` the userinfo credentials will be masked, when `false` the userinfo credentials will be stripped.  @example project.safe_import_url #=> \"https://*****:*****@example.com\" @example project.safe_import_url(masked: false) # => \"https://example.com\"  @return [String] Sanitized import URL.",
    "label": "",
    "id": "5922"
  },
  {
    "raw_code": "def unsafe_import_url\n    if import_data && import_url.present?\n      Gitlab::UrlSanitizer.new(import_url, credentials: import_data.credentials).full_url\n    else\n      import_url\n    end",
    "comment": "WARNING - This method returns sensitive userinfo credentials of the import URL. Use `#safe_import_url` instead unless it is necessary to include sensitive credentials.  Builds an import URL including userinfo credentials from the `import_url` attribute and the encrypted `ProjectImportData#credentials`.  @see #safe_import_url  @example project.unsafe_import_url #=> \"https://user:secretpassword@example.com\"  @return [String] Unsanitized import URL.",
    "label": "",
    "id": "5923"
  },
  {
    "raw_code": "def any_import_in_progress?\n    last_relation_import_tracker = relation_import_trackers.last\n\n    (last_relation_import_tracker&.started? && !last_relation_import_tracker.stale?) ||\n      import_started? ||\n      BulkImports::Entity.with_status(:started).where(project_id: id).any?\n  end",
    "comment": "Determine whether any kind of import is in progress. - Full file import - Relation import - Direct Transfer",
    "label": "",
    "id": "5924"
  },
  {
    "raw_code": "def to_reference(from = nil, full: false)\n    base = to_reference_base(from, full: true)\n    \"#{base}#{self.class.reference_postfix}\"\n  end",
    "comment": "Produce a valid reference (see Referable#to_reference)  NB: For projects, all references are 'full' - i.e. they all include the full_path, rather than just the project name. For this reason, we ignore the value of `full:` passed to this method, which is part of the Referable interface.",
    "label": "",
    "id": "5925"
  },
  {
    "raw_code": "def to_reference_base(from = nil, full: false, absolute_path: false)\n    if full || cross_namespace_reference?(from)\n      absolute_path ? \"/#{full_path}\" : full_path\n    elsif cross_project_reference?(from)\n      path\n    end",
    "comment": "`from` argument can be a Namespace or Project.",
    "label": "",
    "id": "5926"
  },
  {
    "raw_code": "def disabled_integrations\n    %w[zentao]\n  end",
    "comment": "Returns a list of integration names that should be disabled at the project-level. Globally disabled integrations should go in Integration.disabled_integration_names.",
    "label": "",
    "id": "5927"
  },
  {
    "raw_code": "def create_labels\n    label_scope = Label.templates\n    if Feature.enabled?(:template_labels_scoped_by_org, :instance)\n      label_scope = label_scope.for_organization(organization)\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "5928"
  },
  {
    "raw_code": "def ci_integrations\n    integrations.where(category: :ci)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "5929"
  },
  {
    "raw_code": "def code\n    path\n  end",
    "comment": "For compatibility with old code",
    "label": "",
    "id": "5930"
  },
  {
    "raw_code": "def send_move_instructions(old_path_with_namespace)\n    # New project path needs to be committed to the DB or notification will\n    # retrieve stale information\n    run_after_commit do\n      NotificationService.new.project_was_moved(self, old_path_with_namespace)\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "5931"
  },
  {
    "raw_code": "def owner\n    # This will be phased out and replaced with `owners` relationship\n    # backed by memberships with direct/inherited Owner access roles\n    # See https://gitlab.com/groups/gitlab-org/-/epics/7405\n    group || namespace.try(:owner)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "5932"
  },
  {
    "raw_code": "def execute_hooks(data, hooks_scope = :push_hooks)\n    run_after_commit_or_now do\n      triggered_hooks(hooks_scope, data).execute\n      SystemHooksService.new.execute_hooks(data, hooks_scope)\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "5933"
  },
  {
    "raw_code": "def triggered_hooks(hooks_scope, data)\n    triggered = ::Projects::TriggeredHooks.new(hooks_scope, data)\n\n    # By default the webhook resource_access_token_hooks will execute for\n    # seven_days interval but we have a setting to allow webhook execution\n    # for thirty_days and sixty_days interval too.\n    if hooks_scope == :resource_access_token_hooks &&\n        data[:interval] != :seven_days &&\n        !self.extended_prat_expiry_webhooks_execute?\n\n      triggered\n    else\n      triggered.add_hooks(hooks)\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "5934"
  },
  {
    "raw_code": "def expire_caches_before_rename(old_path)\n    project_repo = Repository.new(old_path, self, shard: repository_storage)\n    wiki_repo = Repository.new(\"#{old_path}#{Gitlab::GlRepository::WIKI.path_suffix}\", self, shard: repository_storage, repo_type: Gitlab::GlRepository::WIKI)\n    design_repo = Repository.new(\"#{old_path}#{Gitlab::GlRepository::DESIGN.path_suffix}\", self, shard: repository_storage, repo_type: Gitlab::GlRepository::DESIGN)\n\n    [project_repo, wiki_repo, design_repo].each do |repo|\n      repo.before_delete if repo.exists?\n    end",
    "comment": "Expires various caches before a project is renamed.",
    "label": "",
    "id": "5935"
  },
  {
    "raw_code": "def check_repository_path_availability\n    return true if skip_disk_validation\n    return false unless repository_storage\n\n    # Check if repository with same path already exists on disk we can\n    # skip this for the hashed storage because the path does not change\n    if legacy_storage? && repository_with_same_path_already_exists?\n      errors.add(:base, _('There is already a repository with that name on disk'))\n      return false\n    end",
    "comment": "Check if repository already exists on disk",
    "label": "",
    "id": "5936"
  },
  {
    "raw_code": "def membership_locked?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5937"
  },
  {
    "raw_code": "def members_among(users)\n    if users.is_a?(ActiveRecord::Relation) && !users.loaded?\n      authorized_users.merge(users)\n    else\n      return [] if users.empty?\n\n      user_ids = authorized_users.where(users: { id: users.map(&:id) }).pluck(:id)\n      users.select { |user| user_ids.include?(user.id) }\n    end",
    "comment": "Filters `users` to return only authorized users of the project",
    "label": "",
    "id": "5938"
  },
  {
    "raw_code": "def allowed_to_share_with_group?\n    share_with_group_enabled?\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5939"
  },
  {
    "raw_code": "def open_issues_count(current_user = nil)\n    return Projects::OpenIssuesCountService.new(self, current_user).count unless current_user.nil?\n\n    BatchLoader.for(self).batch do |projects, loader|\n      issues_count_per_project = ::Projects::BatchOpenIssuesCountService.new(projects).refresh_cache_and_retrieve_data\n\n      issues_count_per_project.each do |project, count|\n        loader.call(project, count)\n      end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "5940"
  },
  {
    "raw_code": "def open_merge_requests_count(_current_user = nil)\n    BatchLoader.for(self).batch do |projects, loader|\n      ::Projects::BatchOpenMergeRequestsCountService.new(projects)\n        .refresh_cache_and_retrieve_data\n        .each { |project, count| loader.call(project, count) }\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "5941"
  },
  {
    "raw_code": "def visibility_level_allowed_as_fork?(level = self.visibility_level)\n    return true unless forked?\n\n    original_project = fork_source\n    return true unless original_project\n\n    level <= original_project.visibility_level\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "5942"
  },
  {
    "raw_code": "def after_create_default_branch\n    Projects::ProtectDefaultBranchService.new(self).execute\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "5943"
  },
  {
    "raw_code": "def pipeline_status\n    @pipeline_status ||= Gitlab::Cache::Ci::ProjectPipelineStatus.load_for_project(self)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass Lazy loading of the `pipeline_status` attribute",
    "label": "",
    "id": "5944"
  },
  {
    "raw_code": "def predefined_project_variables\n    strong_memoize(:predefined_project_variables) do\n      Gitlab::Ci::Variables::Collection.new\n        .append(key: 'GITLAB_FEATURES', value: licensed_features.join(','))\n        .append(key: 'CI_PROJECT_ID', value: id.to_s)\n        .append(key: 'CI_PROJECT_NAME', value: path)\n        .append(key: 'CI_PROJECT_TITLE', value: title)\n        .append(key: 'CI_PROJECT_DESCRIPTION', value: description)\n        .append(key: 'CI_PROJECT_TOPICS', value: topic_list.first(20).join(',').downcase)\n        .append(key: 'CI_PROJECT_PATH', value: full_path)\n        .append(key: 'CI_PROJECT_PATH_SLUG', value: full_path_slug)\n        .append(key: 'CI_PROJECT_NAMESPACE', value: namespace.full_path)\n        .append(key: 'CI_PROJECT_NAMESPACE_SLUG', value: Gitlab::Utils.slugify(namespace.full_path))\n        .append(key: 'CI_PROJECT_NAMESPACE_ID', value: namespace.id.to_s)\n        .append(key: 'CI_PROJECT_ROOT_NAMESPACE', value: namespace.root_ancestor.path)\n        .append(key: 'CI_PROJECT_URL', value: web_url)\n        .append(key: 'CI_PROJECT_VISIBILITY', value: Gitlab::VisibilityLevel.string_level(visibility_level))\n        .append(key: 'CI_PROJECT_REPOSITORY_LANGUAGES', value: repository_languages.map(&:name).join(',').downcase)\n        .append(key: 'CI_PROJECT_CLASSIFICATION_LABEL', value: external_authorization_classification_label)\n        .append(key: 'CI_DEFAULT_BRANCH', value: default_branch)\n        .append(key: 'CI_DEFAULT_BRANCH_SLUG', value: Gitlab::Utils.slugify(default_branch.to_s))\n        .append(key: 'CI_CONFIG_PATH', value: ci_config_path_or_default)\n    end",
    "comment": "rubocop: disable Metrics/AbcSize -- TODO: Method size will be reduced in follow-up MR, see https://gitlab.com/gitlab-org/gitlab/-/merge_requests/199182#note_2655958320",
    "label": "",
    "id": "5945"
  },
  {
    "raw_code": "def predefined_ci_server_variables\n    Gitlab::Ci::Variables::Collection.new\n      .append(key: 'CI', value: 'true')\n      .append(key: 'GITLAB_CI', value: 'true')\n      .append(key: 'CI_SERVER_FQDN', value: Gitlab.config.gitlab.server_fqdn)\n      .append(key: 'CI_SERVER_URL', value: Gitlab.config.gitlab.url)\n      .append(key: 'CI_SERVER_HOST', value: Gitlab.config.gitlab.host)\n      .append(key: 'CI_SERVER_PORT', value: Gitlab.config.gitlab.port.to_s)\n      .append(key: 'CI_SERVER_PROTOCOL', value: Gitlab.config.gitlab.protocol)\n      .append(key: 'CI_SERVER_SHELL_SSH_HOST', value: Gitlab.config.gitlab_shell.ssh_host.to_s)\n      .append(key: 'CI_SERVER_SHELL_SSH_PORT', value: Gitlab.config.gitlab_shell.ssh_port.to_s)\n      .append(key: 'CI_SERVER_NAME', value: 'GitLab')\n      .append(key: 'CI_SERVER_VERSION', value: Gitlab::VERSION)\n      .append(key: 'CI_SERVER_VERSION_MAJOR', value: Gitlab.version_info.major.to_s)\n      .append(key: 'CI_SERVER_VERSION_MINOR', value: Gitlab.version_info.minor.to_s)\n      .append(key: 'CI_SERVER_VERSION_PATCH', value: Gitlab.version_info.patch.to_s)\n      .append(key: 'CI_SERVER_REVISION', value: Gitlab.revision)\n  end",
    "comment": "rubocop: enable Metrics/AbcSize",
    "label": "",
    "id": "5946"
  },
  {
    "raw_code": "def forks_count\n    BatchLoader.for(self).batch do |projects, loader|\n      fork_count_per_project = ::Projects::BatchForksCountService.new(projects).refresh_cache_and_retrieve_data\n\n      fork_count_per_project.each do |project, count|\n        loader.call(project, count)\n      end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "5947"
  },
  {
    "raw_code": "def legacy_storage?\n    [nil, 0].include?(self.storage_version)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "5948"
  },
  {
    "raw_code": "def hashed_storage?(feature)\n    raise ArgumentError, _(\"Invalid feature\") unless HASHED_STORAGE_FEATURES.include?(feature)\n\n    self.storage_version && self.storage_version >= HASHED_STORAGE_FEATURES[feature]\n  end",
    "comment": "Check if Hashed Storage is enabled for the project with at least informed feature rolled out  @param [Symbol] feature that needs to be rolled out for the project (:repository, :attachments)",
    "label": "",
    "id": "5949"
  },
  {
    "raw_code": "def licensed_feature_available?(_feature)\n    false\n  end",
    "comment": "Overridden in EE::Project",
    "label": "",
    "id": "5950"
  },
  {
    "raw_code": "def git_objects_poolable?\n    hashed_storage?(:repository) &&\n      visibility_level > Gitlab::VisibilityLevel::PRIVATE &&\n      repository_access_level > ProjectFeature::PRIVATE &&\n      repository_exists? &&\n      Gitlab::CurrentSettings.hashed_storage_enabled\n  end",
    "comment": "Git objects are only poolable when the project is or has: - Hashed storage -> The object pool will have a remote to its members, using relative paths. If the repository path changes we would have to update the remote. - not private    -> The visibility level or repository access level has to be greater than private to prevent fetching objects that might not exist - Repository     -> Else the disk path will be empty, and there's nothing to pool",
    "label": "",
    "id": "5951"
  },
  {
    "raw_code": "def swap_pool_repository!\n    return unless repository_exists?\n\n    old_pool_repository = pool_repository\n    return if old_pool_repository.blank?\n    return if pool_repository_shard_matches_repository?(old_pool_repository)\n\n    new_pool_repository = PoolRepository.by_disk_path_and_shard_name(old_pool_repository.disk_path, repository_storage).take!\n    update!(pool_repository: new_pool_repository)\n\n    old_pool_repository.unlink_repository(repository, disconnect: !pending_delete?)\n  end",
    "comment": "After repository is moved from shard to shard, disconnect it from the previous object pool and connect to the new pool",
    "label": "",
    "id": "5952"
  },
  {
    "raw_code": "def self_or_root_group_ids\n    if group\n      root_group = root_namespace\n    else\n      project = self\n    end",
    "comment": "for projects that are part of user namespace, return project.",
    "label": "",
    "id": "5953"
  },
  {
    "raw_code": "def can_suggest_reviewers?\n    false\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5954"
  },
  {
    "raw_code": "def suggested_reviewers_available?\n    false\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5955"
  },
  {
    "raw_code": "def allows_multiple_merge_request_assignees?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5956"
  },
  {
    "raw_code": "def allows_multiple_merge_request_reviewers?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5957"
  },
  {
    "raw_code": "def on_demand_dast_available?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5958"
  },
  {
    "raw_code": "def supports_saved_replies?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5959"
  },
  {
    "raw_code": "def merge_trains_enabled?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "5960"
  },
  {
    "raw_code": "def licensed_ai_features_available?\n    false\n  end",
    "comment": "Overridden for EE",
    "label": "",
    "id": "5961"
  },
  {
    "raw_code": "def ensure_pool_repository\n    pool_repository || create_new_pool_repository\n  end",
    "comment": "Ensures project has a pool repository without exposing private creation logic",
    "label": "",
    "id": "5962"
  },
  {
    "raw_code": "def project_group_links_with_preload\n    project_group_links\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5963"
  },
  {
    "raw_code": "def cross_project_reference?(from)\n    case from\n    when Namespaces::ProjectNamespace\n      project_namespace_id != from.id\n    when Namespace\n      true\n    else\n      from && self != from\n    end",
    "comment": "Check if a reference is being done cross-project",
    "label": "",
    "id": "5964"
  },
  {
    "raw_code": "def has_root_container_repository_tags?\n    return false unless Gitlab.config.registry.enabled\n\n    ContainerRepository.build_root_repository(self).has_tags?\n  end",
    "comment": " This method is here because of support for legacy container repository which has exactly the same path like project does, but which might not be persisted in `container_repositories` table. ",
    "label": "",
    "id": "5965"
  },
  {
    "raw_code": "def schedule_sync_event_worker\n    run_after_commit do\n      Projects::SyncEvent.enqueue_worker\n    end",
    "comment": "SyncEvents are created by PG triggers (with the function `insert_projects_sync_event`)",
    "label": "",
    "id": "5966"
  },
  {
    "raw_code": "def enqueue_catalog_resource_sync_event_worker\n    run_after_commit do\n      ::Ci::Catalog::Resources::SyncEvent.enqueue_worker\n    end",
    "comment": "Catalog resource SyncEvents are created by PG triggers",
    "label": "",
    "id": "5967"
  },
  {
    "raw_code": "def noteable_id\n    merge_request_id\n  end",
    "comment": "noteable_id and noteable_type methods are used to generate discussion_id on Discussion.discussion_id",
    "label": "",
    "id": "5968"
  },
  {
    "raw_code": "def todoable_target_type_name\n    %w[Issue WorkItem]\n  end",
    "comment": "Todo: remove method after target_type cleanup See https://gitlab.com/gitlab-org/gitlab/-/issues/416009",
    "label": "",
    "id": "5969"
  },
  {
    "raw_code": "def get_widget(type)\n    strong_memoize_with(type) do\n      break unless widget_definitions.key?(type.to_sym)\n\n      widget_definitions[type].build_widget(self)\n    end",
    "comment": "Returns widget object if available type parameter can be a symbol, for example, `:description`.",
    "label": "",
    "id": "5970"
  },
  {
    "raw_code": "def transform_quick_action_params(command_params)\n    common_params = command_params.dup\n    widget_params = {}\n\n    work_item_type.widget_classes(resource_parent)\n          .filter { |widget| widget.respond_to?(:quick_action_params) }\n          .each do |widget|\n            widget.quick_action_params\n              .filter { |param_name| common_params.key?(param_name) }\n              .each do |param_name|\n                widget_params[widget.api_symbol] ||= {}\n                param_value = common_params.delete(param_name)\n\n                widget_params[widget.api_symbol].merge!(widget.process_quick_action_param(param_name, param_value))\n              end",
    "comment": "Widgets have a set of quick action params that they must process. Map them to widget_params so they can be picked up by widget services.",
    "label": "",
    "id": "5971"
  },
  {
    "raw_code": "def self.with_fast_read_statement_timeout(timeout_ms = 4500)\n    ::Gitlab::Database::LoadBalancing::SessionMap.current(load_balancer).fallback_to_replicas_for_ambiguous_queries do\n      transaction(requires_new: true) do # rubocop:disable Performance/ActiveRecordSubtransactions\n        connection.exec_query(\"SET LOCAL statement_timeout = #{timeout_ms}\")\n\n        yield\n      end",
    "comment": "Start a new transaction with a shorter-than-usual statement timeout. This is currently one third of the default 15-second timeout with a 500ms buffer to allow callers gracefully handling the errors to still complete within the 5s target duration of a low urgency request.",
    "label": "",
    "id": "5972"
  },
  {
    "raw_code": "def self.===(object)\n    object.is_a?(self)\n  end",
    "comment": "This method has been removed in Rails 7.1 However, application relies on it in case-when usages with objects wrapped in presenters",
    "label": "",
    "id": "5973"
  },
  {
    "raw_code": "def self.priority(reason)\n    REASON_PRIORITY.index(reason) || (REASON_PRIORITY.length + 1)\n  end",
    "comment": "returns the priority of a reason as an integer",
    "label": "",
    "id": "5974"
  },
  {
    "raw_code": "def self.primary_key\n    :id\n  end",
    "comment": "Needed for reactive caching",
    "label": "",
    "id": "5975"
  },
  {
    "raw_code": "def host_keys_changed?\n    cleanup(known_hosts) != cleanup(compare_host_keys)\n  end",
    "comment": "Returns true if the known_hosts data differs from the version passed in at initialization as `compare_host_keys`. Comments, ordering, etc, is ignored",
    "label": "",
    "id": "5976"
  },
  {
    "raw_code": "def cleanup(data)\n    data\n      .to_s\n      .each_line\n      .reject { |line| line.start_with?('#') || line.chomp.empty? }\n      .uniq\n      .sort\n      .join\n  end",
    "comment": "Remove comments and duplicate entries",
    "label": "",
    "id": "5977"
  },
  {
    "raw_code": "def self.protected?(project, ref_name)\n    return true if project.empty_repo? && project.default_branch_protected?\n    return false if ref_name.blank?\n\n    ProtectedBranches::CacheService.new(project).fetch(ref_name) do # rubocop: disable CodeReuse/ServiceClass\n      self.matching(ref_name, protected_refs: protected_refs(project)).present?\n    end",
    "comment": "Check if branch name is marked as protected in the system",
    "label": "",
    "id": "5978"
  },
  {
    "raw_code": "def self.branch_requires_code_owner_approval?(project, branch_name)\n    false\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "5979"
  },
  {
    "raw_code": "def hard_retry!(error_message)\n    update_error_message(error_message)\n    self.update_status = :to_retry\n\n    save!(validate: false)\n  end",
    "comment": "Force the mrror into the retry state",
    "label": "",
    "id": "5980"
  },
  {
    "raw_code": "def hard_fail!(error_message)\n    update_error_message(error_message)\n    self.update_status = :failed\n\n    save!(validate: false)\n\n    send_failure_notifications\n  end",
    "comment": "Force the mirror into the failed state",
    "label": "",
    "id": "5981"
  },
  {
    "raw_code": "def remote_url\n    return url unless ssh_key_auth? && password.present?\n\n    Gitlab::UrlSanitizer.new(read_attribute(:url), credentials: { user: user }).full_url\n  rescue StandardError\n    super\n  end",
    "comment": "The remote URL omits any password if SSH public-key authentication is in use",
    "label": "",
    "id": "5982"
  },
  {
    "raw_code": "def labels_str(label_refs, prefix: '')\n    existing_refs = label_refs.select(&:present?)\n    refs_str = existing_refs.empty? ? nil : existing_refs.join(' ')\n\n    deleted = label_refs.count - existing_refs.count\n    deleted_str = deleted == 0 ? nil : \"#{deleted} deleted\"\n\n    return unless refs_str || deleted_str\n\n    label_list_str = [refs_str, deleted_str].compact.join(' + ')\n    suffix = ' label'.pluralize(deleted > 0 ? deleted : existing_refs.count)\n\n    \"#{prefix} #{label_list_str} #{suffix.squish}\"\n  end",
    "comment": "returns string containing added/removed labels including count of deleted labels:  added ~1 ~2 + 1 deleted label added 3 deleted labels added ~1 ~2 labels",
    "label": "",
    "id": "5983"
  },
  {
    "raw_code": "def begin_fast_destroy\n      {\n        Uploads::Local => Uploads::Local.new.keys(with_files_stored_locally),\n        Uploads::Fog => Uploads::Fog.new.keys(with_files_stored_remotely)\n      }\n    end",
    "comment": " FastDestroyAll concerns",
    "label": "",
    "id": "5984"
  },
  {
    "raw_code": "def finalize_fast_destroy(items_to_remove)\n      items_to_remove.each do |store_class, keys|\n        store_class.new.delete_keys_async(keys)\n      end",
    "comment": " FastDestroyAll concerns",
    "label": "",
    "id": "5985"
  },
  {
    "raw_code": "def build_uploader(mounted_as = nil)\n    uploader_class.new(model, mounted_as || mount_point).tap do |uploader|\n      uploader.upload = self\n    end",
    "comment": "Initialize the associated Uploader class with current model  @param [String] mounted_as @return [GitlabUploader] one of the subclasses, defined at the model's uploader attribute",
    "label": "",
    "id": "5986"
  },
  {
    "raw_code": "def retrieve_uploader(mounted_as = nil)\n    build_uploader(mounted_as).tap do |uploader|\n      uploader.retrieve_from_store!(filename)\n    end",
    "comment": "Initialize the associated Uploader class with current model and retrieve existing file from the store to a local cache  @param [String] mounted_as @return [GitlabUploader] one of the subclasses, defined at the model's uploader attribute",
    "label": "",
    "id": "5987"
  },
  {
    "raw_code": "def exist?\n    exist = if local?\n              File.exist?(absolute_path)\n            else\n              retrieve_uploader.exists?\n            end",
    "comment": "This checks for existence of the upload on storage  @return [Boolean] whether upload exists on storage",
    "label": "",
    "id": "5988"
  },
  {
    "raw_code": "def needs_checksum?\n    checksum.nil? && local? && exist?\n  end",
    "comment": "Returns whether generating checksum is needed  This takes into account whether file exists, if any checksum exists or if the storage has checksum generation code implemented  @return [Boolean] whether generating a checksum is needed",
    "label": "",
    "id": "5989"
  },
  {
    "raw_code": "def update_last_used_at\n    return unless update_last_used_at?\n\n    obtained = Gitlab::ExclusiveLease\n      .new(\"chat_name/last_used_at/#{id}\", timeout: LAST_USED_AT_INTERVAL.to_i)\n      .try_obtain\n\n    touch(:last_used_at) if obtained\n  end",
    "comment": "Updates the \"last_used_timestamp\" but only if it wasn't already updated recently.  The throttling this method uses is put in place to ensure that high chat traffic doesn't result in many UPDATE queries being performed.",
    "label": "",
    "id": "5990"
  },
  {
    "raw_code": "def active?(diff_refs = nil)\n    return @active if defined?(@active)\n    return true if for_commit?\n    return true unless diff_line\n    return false unless noteable\n    return false if diff_refs && diff_refs != noteable.diff_refs\n\n    noteable_diff = find_noteable_diff\n\n    if noteable_diff\n      parsed_lines = Gitlab::Diff::Parser.new.parse(noteable_diff.diff.each_line)\n\n      @active = parsed_lines.any? { |line_obj| line_obj.text == diff_line.text }\n    else\n      @active = false\n    end",
    "comment": "Check if this note is part of an \"active\" discussion  This will always return true for anything except MergeRequest noteables, which have special logic.  If the note's current diff cannot be matched in the MergeRequest's current diff, it's considered inactive.",
    "label": "",
    "id": "5991"
  },
  {
    "raw_code": "def find_noteable_diff\n    diffs = noteable.raw_diffs(Commit.max_diff_options)\n    diffs.find { |d| d.new_path == self.diff.new_path }\n  end",
    "comment": "Find the diff on noteable that matches our own",
    "label": "",
    "id": "5992"
  },
  {
    "raw_code": "def self.alternative_reference_prefix_without_postfix\n    'GL-'\n  end",
    "comment": "Alternative prefix for situations where the standard prefix would be interpreted as a comment, most notably to begin commit messages with (e.g. \"GL-123: My commit\")",
    "label": "",
    "id": "5993"
  },
  {
    "raw_code": "def self.reference_pattern\n    prefix_with_postfix = alternative_reference_prefix_with_postfix\n    if prefix_with_postfix.empty?\n      @reference_pattern ||= %r{\n      (?:\n        (#{Project.reference_pattern})?#{Regexp.escape(reference_prefix)} |\n        #{Regexp.escape(alternative_reference_prefix_without_postfix)}\n      )#{Gitlab::Regex.issue}\n    }x\n    else\n      %r{\n    ((?:\n      (#{Project.reference_pattern})?#{Regexp.escape(reference_prefix)} |\n      #{alternative_reference_prefix_without_postfix}\n    )#{Gitlab::Regex.issue}) |\n    ((?:\n      #{Regexp.escape(prefix_with_postfix)}(#{Project.reference_pattern}/)?\n    )#{Gitlab::Regex.issue(reference_postfix)})\n  }x\n    end",
    "comment": "Pattern used to extract issue references from text This pattern supports cross-project references.",
    "label": "",
    "id": "5994"
  },
  {
    "raw_code": "def check_repositioning_allowed!\n    if blocked_for_repositioning?\n      raise ::Gitlab::RelativePositioning::IssuePositioningDisabled, \"Issue relative position changes temporarily disabled.\"\n    end",
    "comment": "Temporary disable moving null elements because of performance problems For more information check https://gitlab.com/gitlab-com/gl-infra/production/-/issues/4321",
    "label": "",
    "id": "5995"
  },
  {
    "raw_code": "def to_reference(from = nil, full: false)\n    reference = \"#{self.class.reference_prefix}#{iid}\"\n\n    \"#{namespace.to_reference_base(from, full: full)}#{reference}\"\n  end",
    "comment": "`from` argument can be a Namespace or Project.",
    "label": "",
    "id": "5996"
  },
  {
    "raw_code": "def source_project\n    project\n  end",
    "comment": "To allow polymorphism with MergeRequest.",
    "label": "",
    "id": "5997"
  },
  {
    "raw_code": "def allow_possible_spam?(user)\n    return true if Gitlab::CurrentSettings.allow_possible_spam\n    return false if user.support_bot?\n\n    !publicly_visible?\n  end",
    "comment": "Always enforce spam check for support bot but allow for other users when issue is not publicly visible",
    "label": "",
    "id": "5998"
  },
  {
    "raw_code": "def supports_parent?; end\n\n  def as_json(options = {})\n    super(options).tap do |json|\n      if options.key?(:labels)\n        json[:labels] = labels.as_json(\n          project: project,\n          only: [:id, :title, :description, :color, :priority],\n          methods: [:text_color]\n        )\n      end\n    end",
    "comment": "Overriden in EE",
    "label": "",
    "id": "5999"
  },
  {
    "raw_code": "def invalidate_project_counter_caches\n    return unless project\n\n    Projects::OpenIssuesCountService.new(project).delete_cache\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6000"
  },
  {
    "raw_code": "def merge_requests_count(user = nil)\n    ::MergeRequestsClosingIssues.count_for_issue(self.id, user)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6001"
  },
  {
    "raw_code": "def to_work_item_global_id\n    ::Gitlab::GlobalId.as_global_id(id, model_name: WorkItem.name)\n  end",
    "comment": "we want to have subscriptions working on work items only, legacy issues do not support graphql subscriptions, yet so we need sometimes GID of an issue instance to be represented as WorkItem GID. E.g. notes subscriptions.",
    "label": "",
    "id": "6002"
  },
  {
    "raw_code": "def work_item_type_with_default\n    work_item_type || WorkItems::Type.default_by_type(DEFAULT_ISSUE_TYPE)\n  end",
    "comment": "Persisted records will always have a work_item_type. This method is useful in places where we use a non persisted issue to perform feature checks",
    "label": "",
    "id": "6003"
  },
  {
    "raw_code": "def parent_link_confidentiality\n    return unless persisted?\n\n    if confidential? && WorkItems::ParentLink.has_public_children?(id)\n      errors.add(:base, _('A confidential issue must have only confidential children. Make any child items confidential and try again.'))\n    end",
    "comment": "Although parent/child relationship can be set only for WorkItems, we still need to validate it for Issue model too, because both models use same table.",
    "label": "",
    "id": "6004"
  },
  {
    "raw_code": "def publicly_visible?\n    resource_parent.public? && resource_parent.feature_available?(:issues, nil) &&\n      !confidential? && !hidden? && !::Gitlab::ExternalAuthorization.enabled?\n  end",
    "comment": "Returns `true` if this Issue is visible to everybody.",
    "label": "",
    "id": "6005"
  },
  {
    "raw_code": "def emails_disabled?\n    @project ? @project.emails_disabled? : @group&.emails_disabled?\n  end",
    "comment": "They are disabled if the project or group has disallowed it. No need to check the group if there is already a project",
    "label": "",
    "id": "6006"
  },
  {
    "raw_code": "def for_group_ids_and_descendants(group_ids)\n      groups_and_descendants_cte = Gitlab::SQL::CTE.new(\n        :groups_and_descendants_ids,\n        Group.where(id: group_ids).self_and_descendant_ids\n      )\n\n      groups_and_descendants = Namespace.from(groups_and_descendants_cte.table)\n\n      with(groups_and_descendants_cte.to_arel)\n        .from_union([\n          for_project(Project.for_group(groups_and_descendants)),\n          for_group(groups_and_descendants)\n        ], remove_duplicates: false)\n    end",
    "comment": "Returns all todos for the given group ids and their descendants.  group_ids - Group Ids to retrieve todos for.  Returns an `ActiveRecord::Relation`.",
    "label": "",
    "id": "6007"
  },
  {
    "raw_code": "def any_for_target?(target, state = nil)\n      conditions = {}\n\n      if target.respond_to?(:todoable_target_type_name)\n        conditions[:target_type] = target.todoable_target_type_name\n        conditions[:target_id] = target.id\n      else\n        conditions[:target] = target\n      end",
    "comment": "Returns `true` if the current user has any todos for the given target with the optional given state.  target - The value of the `target_type` column, such as `Issue`. state - The value of the `state` column, such as `pending` or `done`.",
    "label": "",
    "id": "6008"
  },
  {
    "raw_code": "def batch_update(**new_attributes)\n      # We force a `WHERE todo.id IN ()` SQL clause to circumvent issues where resolving todos\n      # associated to a specific group would also resolve other todos due to limitations of the\n      # `update_all` which doesn't handle UNIONs well.\n      ids_to_update = select(:id)\n      todos_to_update = where(id: ids_to_update)\n      # Only update those that have different state\n      base = todos_to_update.where.not(state: new_attributes[:state]).except(:order)\n      ids = base.pluck(:id)\n\n      base.update_all(new_attributes.merge(updated_at: Time.current))\n\n      ids\n    end",
    "comment": "Updates attributes of a relation of todos to the new state.  new_attributes - The new attributes of the todos.  Returns an `Array` containing the IDs of the updated todos.",
    "label": "",
    "id": "6009"
  },
  {
    "raw_code": "def sort_by_attribute(method)\n      sorted =\n        case method.to_s\n        when 'snoozed_and_creation_dates_asc' then sort_by_snoozed_and_creation_dates(direction: :asc)\n        when 'snoozed_and_creation_dates_desc' then sort_by_snoozed_and_creation_dates\n        when 'priority', 'label_priority', 'label_priority_asc' then order_by_labels_priority(asc: true)\n        when 'label_priority_desc' then order_by_labels_priority(asc: false)\n        else order_by(method)\n        end",
    "comment": "Priority sorting isn't displayed in the dropdown, because we don't show milestones, but still show something if the user has a URL with that selected.",
    "label": "",
    "id": "6010"
  },
  {
    "raw_code": "def order_by_labels_priority(asc: true)\n      highest_priority = highest_label_priority(\n        target_type_column: \"todos.target_type\",\n        target_column: \"todos.target_id\",\n        project_column: \"todos.project_id\"\n      ).arel.as('highest_priority')\n\n      highest_priority_arel = Arel.sql('highest_priority')\n\n      order = Gitlab::Pagination::Keyset::Order.build([\n        Gitlab::Pagination::Keyset::ColumnOrderDefinition.new(\n          attribute_name: 'highest_priority',\n          column_expression: highest_priority_arel,\n          order_expression: asc ? highest_priority_arel.asc.nulls_last : highest_priority_arel.desc.nulls_first,\n          reversed_order_expression: asc ? highest_priority_arel.desc.nulls_first : highest_priority_arel.asc.nulls_last,\n          nullable: asc ? :nulls_last : :nulls_first,\n          order_direction: asc ? :asc : :desc\n        ),\n        Gitlab::Pagination::Keyset::ColumnOrderDefinition.new(\n          attribute_name: 'created_at',\n          order_expression: asc ? Todo.arel_table[:created_at].asc : Todo.arel_table[:created_at].desc,\n          nullable: :not_nullable\n        ),\n        Gitlab::Pagination::Keyset::ColumnOrderDefinition.new(\n          attribute_name: 'id',\n          order_expression: asc ? Todo.arel_table[:id].asc : Todo.arel_table[:id].desc,\n          nullable: :not_nullable\n        )\n      ])\n\n      select(arel_table[Arel.star], highest_priority).order(order)\n    end",
    "comment": "Order by priority depending on which issue/merge request the Todo belongs to Todos with highest priority first then oldest todos Need to order by created_at last because of differences on Mysql and Postgres when joining by type \"Merge_request/Issue\"",
    "label": "",
    "id": "6011"
  },
  {
    "raw_code": "def pending_count_by_user_id\n      where(state: :pending)\n        .group(:user_id)\n        .count(:id)\n    end",
    "comment": "Count pending todos grouped by user_id and state so we can utilize the index on state / user id.",
    "label": "",
    "id": "6012"
  },
  {
    "raw_code": "def target\n    if for_commit?\n      begin\n        project.commit(commit_id)\n      rescue StandardError\n        nil\n      end",
    "comment": "override to return commits, which are not active record",
    "label": "",
    "id": "6013"
  },
  {
    "raw_code": "def content\n    if @content.respond_to?(:call)\n      @content = @content.call\n    else\n      @content\n    end",
    "comment": "Returns the text of the license",
    "label": "",
    "id": "6014"
  },
  {
    "raw_code": "def resolve!(project_name: nil, fullname: nil, year: Time.current.year.to_s)\n    # Ensure the string isn't shared with any other instance of LicenseTemplate\n    new_content = content.dup\n    new_content.gsub!(YEAR_TEMPLATE_REGEX, year) if year.present?\n    new_content.gsub!(PROJECT_TEMPLATE_REGEX, project_name) if project_name.present?\n    new_content.gsub!(FULLNAME_TEMPLATE_REGEX, fullname) if fullname.present?\n\n    @content = new_content\n\n    self\n  end",
    "comment": "Populate placeholders in the LicenseTemplate content",
    "label": "",
    "id": "6015"
  },
  {
    "raw_code": "def users_that_can_read_project(users, project)\n      DeclarativePolicy.subject_scope do\n        users.select { |u| allowed?(u, :read_project, project) }\n      end",
    "comment": "Given a list of users and a project this method returns the users that can read the given project.",
    "label": "",
    "id": "6016"
  },
  {
    "raw_code": "def users_that_can_read_group(users, group)\n      DeclarativePolicy.subject_scope do\n        users.select { |u| allowed?(u, :read_group, group) }\n      end",
    "comment": "Given a list of users and a group this method returns the users that can read the given group.",
    "label": "",
    "id": "6017"
  },
  {
    "raw_code": "def users_that_can_read_personal_snippet(users, snippet)\n      DeclarativePolicy.subject_scope do\n        users.select { |u| allowed?(u, :read_snippet, snippet) }\n      end",
    "comment": "Given a list of users and a snippet this method returns the users that can read the given snippet.",
    "label": "",
    "id": "6018"
  },
  {
    "raw_code": "def users_that_can_read_internal_notes(users, note_parent)\n      DeclarativePolicy.subject_scope do\n        users.select { |u| allowed?(u, :read_internal_note, note_parent) }\n      end",
    "comment": "A list of users that can read confidential notes in a project",
    "label": "",
    "id": "6019"
  },
  {
    "raw_code": "def issues_readable_by_user(issues, user = nil, filters: {})\n      issues = apply_filters_if_needed(issues, user, filters)\n\n      DeclarativePolicy.user_scope do\n        issues.select { |issue| allowed?(user, :read_issue, issue) }\n      end",
    "comment": "Returns an Array of Issues that can be read by the given user.  issues - The issues to reduce down to those readable by the user. user - The User for which to check the issues filters - A hash of abilities and filters to apply if the user lacks this ability",
    "label": "",
    "id": "6020"
  },
  {
    "raw_code": "def merge_requests_readable_by_user(merge_requests, user = nil, filters: {})\n      merge_requests = apply_filters_if_needed(merge_requests, user, filters)\n\n      DeclarativePolicy.user_scope do\n        merge_requests.select { |mr| allowed?(user, :read_merge_request, mr) }\n      end",
    "comment": "Returns an Array of MergeRequests that can be read by the given user.  merge_requests - MRs out of which to collect MRs readable by the user. user - The User for which to check the merge_requests filters - A hash of abilities and filters to apply if the user lacks this ability",
    "label": "",
    "id": "6021"
  },
  {
    "raw_code": "def before_check(policy, ability, user, subject, opts)\n      # See Support::AbilityCheck and Support::PermissionsCheck.\n    end",
    "comment": "Hook call right before ability check.",
    "label": "",
    "id": "6022"
  },
  {
    "raw_code": "def policy_for(user, subject = :global, cache: true)\n      policy_cache = cache ? ::Gitlab::SafeRequestStore.storage : {}\n\n      DeclarativePolicy.policy_for(user, subject, cache: policy_cache)\n    end",
    "comment": "We cache in the request store by default. This can lead to unexpected results if abilities are re-checked after objects are modified and the check depends on the modified attributes. In such cases, you should pass `cache: false` for the second check to ensure all rules get re-evaluated.",
    "label": "",
    "id": "6023"
  },
  {
    "raw_code": "def forgetting(pattern, &block)\n      was_forgetting = ability_forgetting?\n      ::Gitlab::SafeRequestStore[:ability_forgetting] = true\n      keys_before = ::Gitlab::SafeRequestStore.storage.keys\n\n      yield\n    ensure\n      ::Gitlab::SafeRequestStore[:ability_forgetting] = was_forgetting\n      forget_all_but(keys_before, matching: pattern)\n    end",
    "comment": "This method is something of a band-aid over the problem. The problem is that some conditions may not be re-entrant, if facts change. (`BasePolicy#admin?` is a known offender, due to the effects of `admin_mode`)  To deal with this we need to clear two elements of state: the offending conditions (selected by 'pattern') and the cached ability checks (cached on the `policy#runner(ability)`).  Clearing the conditions (see `forget_all_but`) is fairly robust, provided the pattern is not _under_-selective. Clearing the runners is harder, since there is not good way to know which abilities any given condition may affect. The approach taken here (see `forget_runner_result`) is to discard all runner results generated during a `forgetting` block. This may be _under_-selective if a runner prior to this block cached a state value that might now be invalid.  TODO: add some kind of reverse-dependency mapping in DeclarativePolicy See: https://gitlab.com/gitlab-org/declarative-policy/-/issues/14",
    "label": "",
    "id": "6024"
  },
  {
    "raw_code": "def truncate\n    ProjectMember.truncate_team(project)\n  end",
    "comment": "Remove all users from project team",
    "label": "",
    "id": "6025"
  },
  {
    "raw_code": "def members_in_project_and_ancestors\n    members.where(id: member_user_ids)\n      .allow_cross_joins_across_databases(url: 'https://gitlab.com/gitlab-org/gitlab/-/issues/432606')\n  end",
    "comment": "`members` method uses project_authorizations table which is updated asynchronously, on project move it still contains old members who may not have access to the new location, so we filter out only members of project or project's group",
    "label": "",
    "id": "6026"
  },
  {
    "raw_code": "def member?(user, min_access_level = Gitlab::Access::GUEST)\n    return false unless user\n\n    max_member_access(user.id) >= min_access_level\n  end",
    "comment": "Checks if `user` is authorized for this project, with at least the `min_access_level` (if given).",
    "label": "",
    "id": "6027"
  },
  {
    "raw_code": "def has_user?(user)\n    return false unless user\n\n    project.project_members.non_invite.exists?(user: user)\n  end",
    "comment": "Only for direct and not invited members",
    "label": "",
    "id": "6028"
  },
  {
    "raw_code": "def max_member_access_for_user_ids(user_ids)\n    Gitlab::SafeRequestLoader.execute(\n      resource_key: project.max_member_access_for_resource_key(User),\n      resource_ids: user_ids,\n      default_value: Gitlab::Access::NO_ACCESS\n    ) do |user_ids|\n      project.project_authorizations\n             .where(user: user_ids)\n             .group(:user_id)\n             .maximum(:access_level)\n    end",
    "comment": "Determine the maximum access level for a group of users in bulk.  Returns a Hash mapping user ID -> maximum access level.",
    "label": "",
    "id": "6029"
  },
  {
    "raw_code": "def max_member_access_for_user(user, only_concrete_membership: false)\n    return ProjectMember::NO_ACCESS unless user\n    return ProjectMember::OWNER if project.owner == user\n\n    unless only_concrete_membership\n      return ProjectMember::OWNER if user.can_admin_all_resources?\n      return ProjectMember::OWNER if user.can_admin_organization?(project.organization)\n    end",
    "comment": "Return the highest access level for a user  A special case is handled here when the user is a GitLab admin which implies it has \"OWNER\" access everywhere, but should not officially appear as a member of a project unless specifically added to it  @param user [User] @param only_concrete_membership [Bool] whether require admin concrete membership status",
    "label": "",
    "id": "6030"
  },
  {
    "raw_code": "def update_invalid_gpg_signatures\n    user.update_invalid_gpg_signatures if confirmed?\n  end",
    "comment": "once email is confirmed, update the gpg signatures",
    "label": "",
    "id": "6031"
  },
  {
    "raw_code": "def prevent_concurrent_inserts\n    return if project_id.nil? || group_id.nil?\n\n    lock_key = ['project_group_links', project_id, group_id].join('-')\n    lock_expression = \"hashtext(#{connection.quote(lock_key)})\"\n    connection.execute(\"SELECT pg_advisory_xact_lock(#{lock_expression})\")\n  end",
    "comment": "This method will block while another database transaction attempts to insert the same data. After the lock is released by the other transaction, the uniqueness validation may fail with record not unique validation error. Without this block the uniqueness validation wouldn't be able to detect duplicated records as transactions can't see each other's changes.",
    "label": "",
    "id": "6032"
  },
  {
    "raw_code": "def serializable_hash(options = nil)\n    options = options.try(:dup) || {}\n    options[:except] = Array(options[:except]).dup\n    options[:except].concat [:email_otp]\n\n    super\n  end",
    "comment": "Exclude the hashed email_otp attribute",
    "label": "",
    "id": "6033"
  },
  {
    "raw_code": "def award_counts_for_user(user, limit = 100)\n      limit(limit)\n        .where(user: user)\n        .group(:name)\n        .order('count_all DESC, name ASC')\n        .count\n    end",
    "comment": "Returns the top 100 emoji awarded by the given user.  The returned value is a Hash mapping emoji names to the number of times they were awarded:  { 'thumbsup' => 2, 'thumbsdown' => 1 }  user - The User to get the awards for. limt - The maximum number of emoji to return.",
    "label": "",
    "id": "6034"
  },
  {
    "raw_code": "def impersonated?\n    false\n  end",
    "comment": "This is used for the internal logic of AuditEvents::BuildService.",
    "label": "",
    "id": "6035"
  },
  {
    "raw_code": "def export_archive_exists?\n    !!carrierwave_export_file&.exists?\n  # Handle any HTTP unexpected error\n  # https://github.com/excon/excon/blob/bbb5bd791d0bb2251593b80e3bce98dbec6e8f24/lib/excon/error.rb#L129-L169\n  rescue Excon::Error => e\n    # The HEAD request will fail with a 403 Forbidden if the file does not\n    # exist, and the user does not have permission to list the object\n    # storage bucket.\n    Gitlab::ErrorTracking.track_exception(e)\n    false\n  end",
    "comment": "This checks if the export archive is actually stored on disk. It requires a HEAD request if object storage is used.",
    "label": "",
    "id": "6036"
  },
  {
    "raw_code": "def projects\n    user.authorized_projects\n  end",
    "comment": "projects that has this key",
    "label": "",
    "id": "6037"
  },
  {
    "raw_code": "def can_delete?\n    true\n  end",
    "comment": "EE overrides this",
    "label": "",
    "id": "6038"
  },
  {
    "raw_code": "def update_last_used_at\n    Keys::LastUsedService.new(self).execute_async\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6039"
  },
  {
    "raw_code": "def add_to_authorized_keys\n    return unless Gitlab::CurrentSettings.authorized_keys_enabled?\n\n    AuthorizedKeysWorker.perform_async('add_key', shell_id, key)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6040"
  },
  {
    "raw_code": "def post_create_hook\n    SystemHooksService.new.execute_hooks_for(self, :create)\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6041"
  },
  {
    "raw_code": "def remove_from_authorized_keys\n    return unless Gitlab::CurrentSettings.authorized_keys_enabled?\n\n    AuthorizedKeysWorker.perform_async('remove_key', shell_id)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6042"
  },
  {
    "raw_code": "def refresh_user_cache\n    return unless user\n\n    Users::KeysCountService.new(user).refresh_cache\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6043"
  },
  {
    "raw_code": "def post_destroy_hook\n    SystemHooksService.new.execute_hooks_for(self, :destroy)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6044"
  },
  {
    "raw_code": "def public_key\n    @public_key ||= Gitlab::SSHPublicKey.new(key)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6045"
  },
  {
    "raw_code": "def self.populate!\n    return unless table_exists?\n\n    # The GitLab config does not change for the lifecycle of the process\n    in_config = Gitlab.config.repositories.storages.keys.map(&:to_s)\n    in_db = all.pluck(:name)\n\n    # This may race with other processes creating shards at the same time, but\n    # `by_name` will handle that correctly\n    missing = in_config - in_db\n    missing.map { |name| by_name(name) }\n  end",
    "comment": "Store shard names from the configuration file in the database. This is not a list of active shards - we just want to assign an immutable, unique ID to every shard name for easy indexing / referencing.",
    "label": "",
    "id": "6046"
  },
  {
    "raw_code": "def self.delete_bulk(merge_request, commits)\n    commit_ids = commits.map(&:sha)\n    merge_request.merge_request_context_commits.where(sha: commit_ids).delete_all\n  end",
    "comment": "delete all MergeRequestContextCommit & MergeRequestContextCommitDiffFile for given merge_request & commit SHAs",
    "label": "",
    "id": "6047"
  },
  {
    "raw_code": "def self.bulk_insert(rows, **args)\n    # Remove the new extended_trailers attribute as this shouldn't be\n    # inserted into the database. This will be removed once the old\n    # format of the trailers attribute is deprecated.\n    rows = rows.map do |row|\n      row.except(:extended_trailers).to_hash\n    end",
    "comment": "create MergeRequestContextCommit by given commit sha and it's diff file record",
    "label": "",
    "id": "6048"
  },
  {
    "raw_code": "def save_git_content\n    ensure_commit_shas\n\n    save_commits\n    save_diffs\n\n    # Another set of `after_save` hooks will be called here when we update the record\n    save\n    # We need to reset so that dirty tracking is reset when running the original set\n    # of `after_save` hooks that come after this `after_create` hook. Otherwise, the\n    # hooks that run when an attribute was changed are run twice.\n    reset\n\n    keep_around_commits unless importing?\n  end",
    "comment": "Collect information about commits and diff from repository and save it to the database as serialized data",
    "label": "",
    "id": "6049"
  },
  {
    "raw_code": "def head_commit_sha\n    if persisted? && super.nil?\n      last_commit_sha\n    else\n      super\n    end",
    "comment": "Override head_commit_sha to keep compatibility with merge request diff created before version 8.4 that does not store head_commit_sha in separate db field.",
    "label": "",
    "id": "6050"
  },
  {
    "raw_code": "def safe_start_commit_sha\n    start_commit_sha || merge_request.target_branch_sha\n  end",
    "comment": "This method will rely on repository branch sha in case start_commit_sha is nil. It's necessary for old merge request diff created before version 8.4 to work",
    "label": "",
    "id": "6051"
  },
  {
    "raw_code": "def fallback_diff_refs\n    real_refs = diff_refs\n    return real_refs if real_refs\n\n    likely_base_commit_sha = (first_commit&.parent || first_commit)&.sha\n\n    Gitlab::Diff::DiffRefs.new(\n      base_sha: likely_base_commit_sha,\n      start_sha: safe_start_commit_sha,\n      head_sha: head_commit_sha\n    )\n  end",
    "comment": "MRs created before 8.4 don't store their true diff refs (start and base), but we need to get a commit SHA for the \"View file @ ...\" link by a file, so we use an approximation of the diff refs if we can't get the actual one.  These will not be the actual diff refs if the target branch was merged into the source branch after the merge request was created, but it is good enough for the specific purpose of linking to a commit.  It is not good enough for highlighting diffs, so we can't simply pass these as `diff_refs.`",
    "label": "",
    "id": "6052"
  },
  {
    "raw_code": "def diffs_collection(diff_options = nil)\n    Gitlab::Diff::FileCollection::MergeRequestDiff.new(self, diff_options: diff_options)\n  end",
    "comment": "Should always return the DB persisted diffs collection (e.g. Gitlab::Diff::FileCollection::MergeRequestDiff. It's useful when trying to invalidate old caches through FileCollection::MergeRequestDiff#clear_cache!",
    "label": "",
    "id": "6053"
  },
  {
    "raw_code": "def opening_external_diff\n    return yield(nil) unless stored_externally?\n    return yield(@external_diff_file) if @external_diff_file\n\n    if use_external_diff? && Gitlab.config.external_diffs.object_store.enabled && external_diff_store == ObjectStorage::Store::LOCAL\n      update_column(:external_diff_store, ObjectStorage::Store::REMOTE)\n    end",
    "comment": "If enabled, yields the external file containing the diff. Otherwise, yields nil. This method is not thread-safe, but it *is* re-entrant, which allows multiple merge_request_diff_files to load their data efficiently",
    "label": "",
    "id": "6054"
  },
  {
    "raw_code": "def migrate_files_to_external_storage!\n    return if stored_externally? || !use_external_diff? || files_count == 0\n\n    rows = build_merge_request_diff_files(merge_request_diff_files)\n    rows = build_external_merge_request_diff_files(rows)\n\n    # Perform carrierwave activity before entering the database transaction.\n    # This is safe as until the `external_diff_store` column is changed, we will\n    # continue to consult the in-database content.\n    self.external_diff.store!\n\n    transaction do\n      MergeRequestDiffFile.where(merge_request_diff_id: id).delete_all\n      ApplicationRecord.legacy_bulk_insert('merge_request_diff_files', rows) # rubocop:disable Gitlab/BulkInsert\n      save!\n    end",
    "comment": "Transactionally migrate the current merge_request_diff_files entries to external storage. If external storage isn't an option for this diff, the method is a no-op.",
    "label": "",
    "id": "6055"
  },
  {
    "raw_code": "def migrate_files_to_database!\n    return unless stored_externally?\n    return if files_count == 0\n\n    rows = convert_external_diffs_to_database\n\n    transaction do\n      MergeRequestDiffFile.where(merge_request_diff_id: id).delete_all\n      ApplicationRecord.legacy_bulk_insert('merge_request_diff_files', rows) # rubocop:disable Gitlab/BulkInsert\n      update!(stored_externally: false)\n    end",
    "comment": "Transactionally migrate the current merge_request_diff_files entries from external storage, back to the database. This is the rollback operation for +migrate_files_to_external_storage!+  If this diff isn't in external storage, the method is a no-op.",
    "label": "",
    "id": "6056"
  },
  {
    "raw_code": "def cached_external_diff\n    return yield(nil) unless stored_externally?\n\n    cache_external_diff unless File.exist?(external_diff_cache_filepath)\n\n    File.open(external_diff_cache_filepath) do |file|\n      yield(file)\n    end",
    "comment": "Yields locally cached external diff if it's externally stored. Used during Project Export to speed up externally stored merge request diffs export",
    "label": "",
    "id": "6057"
  },
  {
    "raw_code": "def fetching_repository_diffs(diff_options)\n    return unless block_given?\n\n    diff_options ||= {}\n\n    # Can be read as: fetch the persisted diffs if yielded without the\n    # Compare object.\n    return yield unless without_files? || diff_options[:ignore_whitespace_change]\n    return yield unless diff_refs&.complete?\n\n    comparison = diff_refs.compare_in(repository.project)\n\n    return yield unless comparison\n\n    yield(comparison)\n  end",
    "comment": "Yields the block with the repository Compare object if it should fetch diffs from the repository instead DB.",
    "label": "",
    "id": "6058"
  },
  {
    "raw_code": "def cache_external_diff\n    return unless stored_externally?\n    return if File.exist?(external_diff_cache_filepath)\n\n    FileUtils.mkdir_p(external_diff_cache_dir)\n\n    opening_external_diff do |external_diff|\n      File.open(external_diff_cache_filepath, 'wb') do |file|\n        file.write(external_diff.read(EXTERNAL_DIFF_CACHE_CHUNK_SIZE)) until external_diff.eof?\n      end",
    "comment": "Downloads external diff to a temp storage location.",
    "label": "",
    "id": "6059"
  },
  {
    "raw_code": "def self.noteable_types\n    %w[MergeRequest Issue Commit Snippet WikiPage::Meta]\n  end",
    "comment": "Names of all implementers of `Noteable` that support discussions.",
    "label": "",
    "id": "6060"
  },
  {
    "raw_code": "def self.default\n    Gitlab::SafeRequestStore.fetch(:plan_default) do\n      # find_by allows us to find object (cheaply) against replica DB\n      # safe_find_or_create_by does stick to primary DB\n      find_by(name: DEFAULT) || safe_find_or_create_by(name: DEFAULT) { |plan| plan.title = DEFAULT.titleize }\n    end",
    "comment": "This always returns an object",
    "label": "",
    "id": "6061"
  },
  {
    "raw_code": "def self.ids_for_names(names)\n    self.where(name: names).pluck(:id)\n  end",
    "comment": "rubocop: disable Database/AvoidUsingPluckWithoutLimit -- This method is prepared for manual usage in Rails console on SaaS. Using pluck without limit in this case should be enough safe.",
    "label": "",
    "id": "6062"
  },
  {
    "raw_code": "def self.names_for_ids(plan_ids)\n    self.id_in(plan_ids).pluck(:name)\n  end",
    "comment": "rubocop: enable Database/AvoidUsingPluckWithoutLimit rubocop: disable Database/AvoidUsingPluckWithoutLimit -- This method is prepared for manual usage in Rails console on SaaS. Using pluck without limit in this case should be enough safe.",
    "label": "",
    "id": "6063"
  },
  {
    "raw_code": "def actual_limits\n    self.limits || self.build_limits\n  end",
    "comment": "rubocop: enable Database/AvoidUsingPluckWithoutLimit",
    "label": "",
    "id": "6064"
  },
  {
    "raw_code": "def matching(refs)\n    refs.select { |ref| ref.is_a?(String) ? matches?(ref) : matches?(ref.name) }\n  end",
    "comment": "Returns all branches/tags (among the given list of refs [`Gitlab::Git::Branch`] or their names [`String`]) that match the current protected ref.",
    "label": "",
    "id": "6065"
  },
  {
    "raw_code": "def matches?(ref_name)\n    return false if @ref_name_or_pattern.blank?\n\n    exact_match?(ref_name) || wildcard_match?(ref_name)\n  end",
    "comment": "Checks if the protected ref matches the given ref name.",
    "label": "",
    "id": "6066"
  },
  {
    "raw_code": "def wildcard?\n    @ref_name_or_pattern && @ref_name_or_pattern.include?('*')\n  end",
    "comment": "Checks if this protected ref contains a wildcard",
    "label": "",
    "id": "6067"
  },
  {
    "raw_code": "def self.discussion_id(note)\n    Digest::SHA1.hexdigest(build_discussion_id(note).join(\"-\"))\n  end",
    "comment": "Returns an alphanumeric discussion ID based on `build_discussion_id`",
    "label": "",
    "id": "6068"
  },
  {
    "raw_code": "def self.build_discussion_id(note)\n    [*base_discussion_id(note), SecureRandom.hex]\n  end",
    "comment": "Returns an array of discussion ID components",
    "label": "",
    "id": "6069"
  },
  {
    "raw_code": "def self.override_discussion_id(note)\n    nil\n  end",
    "comment": "When notes on a commit are displayed in context of a merge request that contains that commit, these notes are to be displayed as if they were part of one discussion, even though they were actually individual notes on the commit with different discussion IDs, so that it's clear that these are not notes on the merge request itself.  To turn a list of notes into a list of discussions, they are grouped by discussion ID, so to get these out-of-context notes to end up in the same discussion, we need to get them to return the same `discussion_id` when this grouping happens. To enable this, `Note#discussion_id` calls out to the `override_discussion_id` method on the appropriate `Discussion` subclass, as determined by the `discussion_class` method on `Note` or a subclass of `Note`.  If no override is necessary, return `nil`. For the case described above, see `OutOfContextDiscussion.override_discussion_id`.",
    "label": "",
    "id": "6070"
  },
  {
    "raw_code": "def to_global_id(options = {})\n    GlobalID.new(::Gitlab::GlobalId.build(model_name: discussion_class.to_s, id: id))\n  end",
    "comment": "Consolidate discussions GID. There is no need to have different GID for different class names as the discussion_id hash is already unique per discussion. This also fixes the issue where same discussion may return different GIDs depending on number of notes it has.",
    "label": "",
    "id": "6071"
  },
  {
    "raw_code": "def group_pages(pages, templates: false)\n      # Build a hash to map paths to created WikiDirectory objects,\n      # and recursively create them for each level of the path.\n      # For the toplevel directory we use '' as path, as that's what WikiPage#directory returns.\n      directories = Hash.new do |_, path|\n        directories[path] = new(path).tap do |directory|\n          if path.present?\n            parent = File.dirname(path)\n            parent = '' if parent == '.'\n            directories[parent].entries << directory\n            directories[parent].entries.delete_if do |item|\n              item.is_a?(WikiPage) && item.slug.casecmp?(directory.slug)\n            end",
    "comment": "Groups a list of wiki pages into a nested collection of WikiPage and WikiDirectory objects, preserving the order of the passed pages.  Returns an array with all entries for the toplevel directory.  @param [Array<WikiPage>] pages @return [Array<WikiPage, WikiDirectory>] ",
    "label": "",
    "id": "6072"
  },
  {
    "raw_code": "def to_partial_path\n    'shared/wikis/wiki_directory'\n  end",
    "comment": "Relative path to the partial to be used when rendering collections of this object.",
    "label": "",
    "id": "6073"
  },
  {
    "raw_code": "def notes_filter_for(resource)\n    self[notes_filter_field_for(resource)]\n  end",
    "comment": "Returns the current discussion filter for a given issuable or issuable type.",
    "label": "",
    "id": "6074"
  },
  {
    "raw_code": "def self.key_name_v1(user_id, session_id = '*')\n    \"#{Gitlab::Redis::Sessions::USER_SESSIONS_NAMESPACE}:#{user_id}:#{session_id}\"\n  end",
    "comment": "Deprecated",
    "label": "",
    "id": "6075"
  },
  {
    "raw_code": "def self.session_ids_for_user(user_id)\n    Gitlab::Redis::Sessions.with do |redis|\n      redis.smembers(lookup_key_name(user_id))\n    end",
    "comment": "Lists the relevant session IDs for the user.  Returns an array of strings",
    "label": "",
    "id": "6076"
  },
  {
    "raw_code": "def self.sessions_from_ids(session_ids)\n    return [] if session_ids.empty?\n\n    Gitlab::Redis::Sessions.with do |redis|\n      session_keys = rack_session_keys(session_ids)\n\n      session_keys.each_slice(SESSION_BATCH_SIZE).flat_map do |session_keys_batch|\n        Gitlab::Instrumentation::RedisClusterValidator.allow_cross_slot_commands do\n          raw_sessions = if Gitlab::Redis::ClusterUtil.cluster?(redis)\n                           Gitlab::Redis::ClusterUtil.batch_get(session_keys_batch, redis)\n                         else\n                           redis.mget(session_keys_batch)\n                         end",
    "comment": "Lists the session Hash objects for the given session IDs.  session_ids - An array of strings  Returns an array of ActiveSession objects",
    "label": "",
    "id": "6077"
  },
  {
    "raw_code": "def self.cleaned_up_lookup_entries(redis, user, removed = [])\n    lookup_key = lookup_key_name(user.id)\n    session_ids = session_ids_for_user(user.id)\n    session_ids_and_entries = raw_active_session_entries(redis, session_ids, user.id)\n\n    # remove expired keys.\n    # only the single key entries are automatically expired by redis, the\n    # lookup entries in the set need to be removed manually.\n    redis.pipelined do |pipeline|\n      session_ids_and_entries.each do |session_id, entry|\n        next if entry\n\n        pipeline.srem?(lookup_key, session_id)\n      end",
    "comment": "Cleans up the lookup set by removing any session IDs that are no longer present.  Returns an array of marshalled ActiveModel objects that are still active. Records removed keys in the optional `removed` argument array.",
    "label": "",
    "id": "6078"
  },
  {
    "raw_code": "def organization_assigned=(value)\n    organization_assigned || super(value)\n  end",
    "comment": "Do not allow to reset this",
    "label": "",
    "id": "6079"
  },
  {
    "raw_code": "def member_role_id(_group)\n      Arel::Nodes::As.new(Arel::Nodes::SqlLiteral.new('NULL'), Arel::Nodes::SqlLiteral.new('member_role_id'))\n    end",
    "comment": "overriden in EE",
    "label": "",
    "id": "6080"
  },
  {
    "raw_code": "def notifiable?(type, opts = {})\n    # always notify when there isn't a user yet\n    return true if user.blank?\n\n    NotificationRecipients::BuildService.notifiable?(user, type, notifiable_options.merge(opts))\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6081"
  },
  {
    "raw_code": "def highest_group_member\n    strong_memoize(:highest_group_member) do\n      next unless user_id && source&.ancestors&.any?\n\n      GroupMember\n        .where(source: source.ancestors, user_id: user_id)\n        .non_request\n        .order(:access_level).last\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass Find the user's group member with a highest access level",
    "label": "",
    "id": "6082"
  },
  {
    "raw_code": "def set_member_namespace_id\n    self.member_namespace_id = self.source_id\n  end",
    "comment": "TODO: https://gitlab.com/groups/gitlab-org/-/epics/7054 temporary until we can we properly remove the source columns",
    "label": "",
    "id": "6083"
  },
  {
    "raw_code": "def refresh_member_authorized_projects\n    UserProjectAccessChangedService.new(user_id).execute\n  end",
    "comment": "Refreshes authorizations of the current member.  This method schedules a job using Sidekiq and as such **must not** be called in a transaction. Doing so can lead to the job running before the transaction has been committed, resulting in the job either throwing an error or not doing any meaningful work. rubocop: disable CodeReuse/ServiceClass This method is overridden in the test environment, see stubbed_member.rb",
    "label": "",
    "id": "6084"
  },
  {
    "raw_code": "def after_accept_invite\n    run_after_commit_or_now { Members::InviteAcceptedMailer.with(member: self).email.deliver_later }\n\n    update_two_factor_requirement\n\n    post_create_member_hook\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6085"
  },
  {
    "raw_code": "def system_hook_service\n    SystemHooksService.new\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6086"
  },
  {
    "raw_code": "def notification_service\n    NotificationService.new\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6087"
  },
  {
    "raw_code": "def todo_service\n    TodoService.new\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6088"
  },
  {
    "raw_code": "def notifiable_options\n    case source\n    when Group\n      { group: source }\n    when Project\n      { project: source }\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6089"
  },
  {
    "raw_code": "def failed_pipeline\n    bool = super\n\n    bool.nil? || bool\n  end",
    "comment": "Allow people to receive both failed pipeline/fixed pipeline notifications if they already have custom notifications enabled, as these are more like mentions than the other custom settings.",
    "label": "",
    "id": "6090"
  },
  {
    "raw_code": "def self.current_without_cache\n    first\n  end",
    "comment": "Overrides CacheableAttributes.current_without_cache",
    "label": "",
    "id": "6091"
  },
  {
    "raw_code": "def find_by_id(container_id)\n      container_class.find_by_id(container_id)&.wiki\n    end",
    "comment": "This is needed to support repository lookup through Gitlab::GlRepository::Identifier",
    "label": "",
    "id": "6092"
  },
  {
    "raw_code": "def id\n    container.id\n  end",
    "comment": "This is needed in: - Storage::Hashed - Gitlab::Repositories::RepoType#identifier_for_container  We also need an `#id` to support `build_stubbed` in tests, where the value doesn't matter.  NOTE: Wikis don't have a DB record, so this ID can be the same for two wikis in different containers and should not be expected to be unique. Use `to_global_id` instead if you need a unique ID.",
    "label": "",
    "id": "6093"
  },
  {
    "raw_code": "def list_pages(\n    direction: DIRECTION_ASC,\n    load_content: false,\n    size_limit: Gitlab::Git::Blob::MAX_DATA_DISPLAY_SIZE,\n    limit: 0,\n    offset: 0\n  )\n    capture_git_error(:list, response_on_error: []) do\n      create_wiki_repository unless repository_exists?\n\n      paths = list_page_paths(limit: limit, offset: offset)\n      next [] if paths.empty?\n\n      pages = paths.map do |path|\n        page = Gitlab::Git::WikiPage.new(\n          url_path: strip_extension(path),\n          title: canonicalize_filename(path),\n          format: find_page_format(path),\n          path: path,\n          raw_data: '',\n          name: canonicalize_filename(path),\n          historical: false\n        )\n        WikiPage.new(self, page)\n      end",
    "comment": "Lists wiki pages of the repository.  limit - max number of pages returned by the method. sort - criterion by which the pages are sorted. direction - order of the sorted pages. load_content - option, which specifies whether the content inside the page will be loaded.  Returns an Array of GitLab WikiPage instances or an empty Array if this Wiki has no pages.",
    "label": "",
    "id": "6094"
  },
  {
    "raw_code": "def find_page(title, version = nil, load_content: true)\n    return unless title.present?\n\n    capture_git_error(:find, response_on_error: nil) do\n      create_wiki_repository unless repository_exists?\n\n      version = version.presence || default_branch\n      path = find_matched_file(title, version)\n      next if path.blank?\n\n      path = Gitlab::EncodingHelper.encode_utf8_no_detect(path)\n      blob_options = load_content ? {} : { limit: 0 }\n      blob = repository.blob_at(version, path, **blob_options)\n      commit = repository.commit(blob.commit_id)\n      format = find_page_format(path)\n\n      page = Gitlab::Git::WikiPage.new(\n        url_path: strip_extension(path),\n        title: canonicalize_filename(path),\n        format: format,\n        path: path,\n        raw_data: blob.data,\n        name: canonicalize_filename(path),\n        historical: version == default_branch ? false : check_page_historical(path, commit),\n        version: Gitlab::Git::WikiPageVersion.new(commit, format)\n      )\n      WikiPage.new(self, page)\n    end",
    "comment": "Finds a page within the repository based on a title or slug.  title - The human readable or parameterized title of the page.  Returns an initialized WikiPage instance or nil",
    "label": "",
    "id": "6095"
  },
  {
    "raw_code": "def after_wiki_activity; end\n\n  # Callbacks for background processing after wiki changes.\n  # These will be executed after any change to the wiki repository.\n  def after_post_receive; end\n\n  override :git_garbage_collect_worker_klass\n  def git_garbage_collect_worker_klass\n    Wikis::GitGarbageCollectWorker\n  end\n\n  def cleanup\n    @repository = nil\n  end\n\n  private\n\n  def capture_git_error(action, response_on_error: false, &block)\n    wrapped_gitaly_errors(&block)\n  rescue Gitlab::Git::Index::IndexError,\n    Gitlab::Git::CommitError,\n    Gitlab::Git::PreReceiveError,\n    Gitlab::Git::CommandError,\n    ArgumentError => e\n\n    @error_message = e.message\n\n    Gitlab::ErrorTracking.log_exception(e, action: action, wiki_id: id)\n\n    response_on_error\n  end\n\n  def update_redirection_actions(new_path, old_path = nil, **options)\n    return [] unless old_path != new_path\n\n    old_contents = repository.blob_at(default_branch, REDIRECTS_YML)\n    redirects = old_contents ? YAML.safe_load(old_contents.data).to_h : {}\n    redirects[old_path] = new_path if old_path\n    redirects.except!(new_path)\n    new_contents = YAML.dump(redirects)\n\n    if old_contents\n      repository.update_file_actions(REDIRECTS_YML, new_contents)\n    else\n      repository.create_file_actions(REDIRECTS_YML, new_contents)\n    end\n  end\n\n  def multi_commit_options(action, message = nil, title = nil)\n    commit_message = build_commit_message(action, message, title)\n    git_user = Gitlab::Git::User.from_gitlab(user)\n\n    {\n      branch_name: repository.root_ref || default_branch,\n      message: commit_message,\n      author_email: git_user.email,\n      author_name: git_user.name\n    }\n  end\n\n  def build_commit_message(action, message, title)\n    message.presence || default_message(action, title)\n  end\n\n  def default_message(action, title)\n    \"#{user.username} #{action} page: #{title}\"\n  end\n\n  def with_valid_format(format, &block)\n    default_extension = Wiki::VALID_USER_MARKUPS.dig(format.to_sym, :default_extension).to_s\n\n    if default_extension.blank?\n      @error_message = _('Invalid format selected')\n\n      return false\n    end\n\n    yield default_extension\n  end",
    "comment": "Callbacks for synchronous processing after wiki changes. These will be executed after any change made through GitLab itself (web UI and API), but not for Git pushes.",
    "label": "",
    "id": "6096"
  },
  {
    "raw_code": "def sort_pages!(pages, direction)\n    # Sort by path to ensure the files inside a sub-folder are grouped and sorted together\n    pages.sort_by!(&:path)\n    pages.reverse! if direction == DIRECTION_DESC\n  end",
    "comment": "After migrating to normal repository RPCs, it's very expensive to sort the pages by created_at. We have to either ListLastCommitsForTree RPC call or N+1 LastCommitForPath. Either are efficient for a large repository. Therefore, we decide to sort the title only.",
    "label": "",
    "id": "6097"
  },
  {
    "raw_code": "def protectable_ref_names\n    return [] if @project.empty_repo?\n\n    @protectable_ref_names ||= ref_names - non_wildcard_protected_ref_names\n  end",
    "comment": "Tags/branches which are yet to be individually protected",
    "label": "",
    "id": "6098"
  },
  {
    "raw_code": "def project\n    nil\n  end",
    "comment": "createNote mutation calls noteable.project, which in case of abuse reports is nil",
    "label": "",
    "id": "6099"
  },
  {
    "raw_code": "def self.insert_all(attributes)\n    super(attributes, unique_by: connection.schema_cache.primary_keys(table_name))\n  end",
    "comment": "This method overrides its ActiveRecord's version in order to work correctly with composite primary keys and fix the tests for Rails 6.1  Consider using BulkInsertSafe module instead since we plan to refactor it in https://gitlab.com/gitlab-org/gitlab/-/issues/331264",
    "label": "",
    "id": "6100"
  },
  {
    "raw_code": "def self.reference_pattern\n    # NOTE: The id pattern only matches when all characters on the expression\n    # are digits, so it will match ~2 but not ~2fa because that's probably a\n    # label name and we want it to be matched as such.\n    @reference_pattern ||= %r{\n      (#{Project.reference_pattern})?\n      #{Regexp.escape(reference_prefix)}\n      (?:\n          (?<label_id>\\d+(?!\\S\\w)\\b)\n        | # Integer-based label ID, or\n          (?<label_name>\n              # String-based single-word label title, or\n              #{Gitlab::Regex.sep_by_1(/:{1,2}/, /[A-Za-z0-9_\\-\\?\\.&]+/)}\n              (?<!\\.|\\?)\n            |\n              # String-based multi-word label surrounded in quotes\n              \".+?\"\n          )\n      )\n    }x\n  end",
    "comment": " Pattern used to extract label references from text  This pattern supports cross-project references. ",
    "label": "",
    "id": "6101"
  },
  {
    "raw_code": "def to_reference(from = nil, target_container: nil, format: :id, full: false)\n    format_reference = label_format_reference(format)\n    reference = \"#{self.class.reference_prefix}#{format_reference}\"\n\n    if from\n      \"#{from.to_reference_base(target_container, full: full)}#{reference}\"\n    else\n      reference\n    end",
    "comment": " Returns the String necessary to reference this Label in Markdown  format - Symbol format to use (default: :id, optional: :name)  Examples:  Label.first.to_reference                                     # => \"~1\" Label.first.to_reference(format: :name)                      # => \"~\\\"bug\\\"\" Label.first.to_reference(project, target_container: same_namespace_project)    # => \"gitlab-foss~1\" Label.first.to_reference(project, target_container: another_namespace_project) # => \"gitlab-org/gitlab-foss~1\"  Returns a String ",
    "label": "",
    "id": "6102"
  },
  {
    "raw_code": "def diff_line_count(diffs)\n      diffs.reduce(0) { |sum, d| sum + Gitlab::Git::Util.count_lines(d.diff) }\n    end",
    "comment": "Calculate number of lines to render for diffs",
    "label": "",
    "id": "6103"
  },
  {
    "raw_code": "def truncate_sha(sha)\n      sha[0..MIN_SHA_LENGTH]\n    end",
    "comment": "Truncate sha to 8 characters",
    "label": "",
    "id": "6104"
  },
  {
    "raw_code": "def self.reference_pattern\n    @reference_pattern ||= %r{\n      (?:#{Project.reference_pattern}#{reference_prefix})?\n      (?<commit>#{WHOLE_WORD_COMMIT_SHA_PATTERN})\n    }x\n  end",
    "comment": "Pattern used to extract commit references from text  This pattern supports cross-project references.",
    "label": "",
    "id": "6105"
  },
  {
    "raw_code": "def title\n    return full_title if full_title.length < 100\n\n    # Use three dots instead of the ellipsis Unicode character because\n    # some clients show the raw Unicode value in the merge commit.\n    full_title.truncate(81, separator: ' ', omission: '...')\n  end",
    "comment": "Returns the commits title.  Usually, the commit title is the first line of the commit message. In case this first line is longer than 100 characters, it is cut off after 80 characters + `...`",
    "label": "",
    "id": "6106"
  },
  {
    "raw_code": "def full_title\n    @full_title ||=\n      if safe_message.blank?\n        no_commit_message\n      else\n        safe_message.split(/[\\r\\n]/, 2).first\n      end",
    "comment": "Returns the full commits title",
    "label": "",
    "id": "6107"
  },
  {
    "raw_code": "def description\n    return safe_message if full_title.length >= 100\n    return no_commit_message if safe_message.blank?\n\n    safe_message.split(\"\\n\", 2)[1].try(:chomp)\n  end",
    "comment": "Returns full commit message if title is truncated (greater than 99 characters) otherwise returns commit message without first line",
    "label": "",
    "id": "6108"
  },
  {
    "raw_code": "def uri_type(path)\n    entry = @raw.tree_entry(path)\n    return unless entry\n\n    if entry[:type] == :blob\n      blob = ::Blob.decorate(Gitlab::Git::Blob.new(name: entry[:name]), container)\n      blob.image? || blob.video? || blob.audio? ? :raw : :blob\n    else\n      entry[:type]\n    end",
    "comment": "Get the URI type of the given path  Used to build URLs to files in the repository in GFM.  path - String path to check  Examples:  uri_type('doc/README.md') # => :blob uri_type('doc/logo.png')  # => :raw uri_type('doc/api')       # => :tree uri_type('not/found')     # => nil  Returns a symbol",
    "label": "",
    "id": "6109"
  },
  {
    "raw_code": "def self.check_schema!\n    return if connection.primary_key(table_name).present?\n\n    raise \"The `#{table_name}` table is missing a primary key constraint in the database schema\"\n  end",
    "comment": "Due to the frequency with which settings are accessed, it is likely that during a backup restore a running GitLab process will insert a new `application_settings` row before the constraints have been added to the table. This would add an extra row with ID 1 and prevent the primary key constraint from being added, which made ActiveRecord throw a IrreversibleOrderError anytime the settings were accessed (https://gitlab.com/gitlab-org/gitlab/-/issues/36405).  To prevent this from happening, we do a sanity check that the primary key constraint is present before inserting a new entry.",
    "label": "",
    "id": "6110"
  },
  {
    "raw_code": "def self.cache_backend\n    Gitlab::ProcessMemoryCache.cache_backend\n  end",
    "comment": "By default, the backend is Rails.cache, which uses ActiveSupport::Cache::RedisStore. Since loading ApplicationSetting can cause a significant amount of load on Redis, let's cache it in memory.",
    "label": "",
    "id": "6111"
  },
  {
    "raw_code": "def self.rate_limits_definition\n    {\n      autocomplete_users_limit: [:integer, { default: 300 }],\n      autocomplete_users_unauthenticated_limit: [:integer, { default: 100 }],\n      concurrent_bitbucket_import_jobs_limit: [:integer, { default: 100 }],\n      concurrent_bitbucket_server_import_jobs_limit: [:integer, { default: 100 }],\n      concurrent_github_import_jobs_limit: [:integer, { default: 1000 }],\n      concurrent_relation_batch_export_limit: [:integer, { default: 8 }],\n      downstream_pipeline_trigger_limit_per_project_user_sha: [:integer, { default: 0 }],\n      group_api_limit: [:integer, { default: 400 }],\n      group_invited_groups_api_limit: [:integer, { default: 60 }],\n      group_projects_api_limit: [:integer, { default: 600 }],\n      group_shared_groups_api_limit: [:integer, { default: 60 }],\n      groups_api_limit: [:integer, { default: 200 }],\n      members_delete_limit: [:integer, { default: 60 }],\n      create_organization_api_limit: [:integer, { default: 10 }],\n      project_api_limit: [:integer, { default: 400 }],\n      group_archive_unarchive_api_limit: [:integer, { default: 60 }],\n      project_invited_groups_api_limit: [:integer, { default: 60 }],\n      projects_api_limit: [:integer, { default: 2000 }],\n      throttle_authenticated_git_http_enabled: [:boolean, { default: false }],\n      throttle_authenticated_git_http_requests_per_period:\n        [:integer, { default: DEFAULT_AUTHENTICATED_GIT_HTTP_LIMIT }],\n      throttle_authenticated_git_http_period_in_seconds:\n        [:integer, { default: DEFAULT_AUTHENTICATED_GIT_HTTP_PERIOD }],\n      user_contributed_projects_api_limit: [:integer, { default: 100 }],\n      user_projects_api_limit: [:integer, { default: 300 }],\n      user_starred_projects_api_limit: [:integer, { default: 100 }],\n      users_api_limit_followers: [:integer, { default: 100 }],\n      users_api_limit_following: [:integer, { default: 100 }],\n      users_api_limit_status: [:integer, { default: 240 }],\n      users_api_limit_ssh_keys: [:integer, { default: 120 }],\n      users_api_limit_ssh_key: [:integer, { default: 120 }],\n      users_api_limit_gpg_keys: [:integer, { default: 120 }],\n      users_api_limit_gpg_key: [:integer, { default: 120 }]\n    }\n  end",
    "comment": "overriden in EE",
    "label": "",
    "id": "6112"
  },
  {
    "raw_code": "def grouped_diff_discussions(diff_refs = nil)\n      groups = {}\n\n      diff_notes.order_created_at_id_asc.discussions.each do |discussion|\n        group_key =\n          if discussion.on_image?\n            discussion.file_new_path\n          else\n            discussion.line_code_in_diffs(diff_refs)\n          end",
    "comment": "Group diff discussions by line code or file path. It is not needed to group by line code when comment is on an image.",
    "label": "",
    "id": "6113"
  },
  {
    "raw_code": "def system_note_with_references?\n    return unless system?\n\n    if force_cross_reference_regex_check?\n      matches_cross_reference_regex?\n    else\n      ::SystemNotes::IssuablesService.cross_reference?(note)\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6114"
  },
  {
    "raw_code": "def diff_note?\n    false\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6115"
  },
  {
    "raw_code": "def merge_requests\n    if for_commit?\n      project.merge_requests.by_commit_sha(commit_id)\n    elsif for_merge_request?\n      MergeRequest.id_in(noteable_id)\n    end",
    "comment": "Notes on merge requests and commits can be traced back to one or several MRs. This method returns a relation if the note is for one of these types, or nil if it is a note on some other object.",
    "label": "",
    "id": "6116"
  },
  {
    "raw_code": "def noteable\n    return commit if for_commit?\n\n    super\n  rescue StandardError\n    # Temp fix to prevent app crash\n    # if note commit id doesn't exist\n    nil\n  end",
    "comment": "override to return commits, which are not active record",
    "label": "",
    "id": "6117"
  },
  {
    "raw_code": "def noteable_type=(noteable_type)\n    super(noteable_type.to_s.classify.constantize.base_class.to_s)\n  end",
    "comment": "FIXME: Hack for polymorphic associations with STI For more information visit http://api.rubyonrails.org/classes/ActiveRecord/Associations/ClassMethods.html#label-Polymorphic+Associations",
    "label": "",
    "id": "6118"
  },
  {
    "raw_code": "def human_max_access\n    project&.team&.human_max_access(self.author_id)\n  end",
    "comment": "overridden in ee",
    "label": "",
    "id": "6119"
  },
  {
    "raw_code": "def last_edited_at\n    super || updated_at\n  end",
    "comment": "We used `last_edited_at` as an alias of `updated_at` before. This makes it compatible with the previous way without data migration.",
    "label": "",
    "id": "6120"
  },
  {
    "raw_code": "def edited?\n    return false if read_attribute(:last_edited_at).blank? && updated_by.blank?\n\n    super\n  end",
    "comment": "Since we used `updated_at` as `last_edited_at`, it could be touched by transforming / resolving a note. This makes sure it is only marked as edited when the note body is updated.",
    "label": "",
    "id": "6121"
  },
  {
    "raw_code": "def touch_noteable\n    # Commits are not stored in the DB so we can't touch them.\n    # Vulnerabilities should not be touched as they are tracked in the same manner as other issuable types\n    return if for_vulnerability? || for_commit?\n\n    assoc = association(:noteable)\n\n    noteable_object =\n      if assoc.loaded?\n        noteable\n      else\n        # If the object is not loaded (e.g. when notes are loaded async) we\n        # _only_ want the data we actually need.\n        assoc.scope.select(:id, :updated_at).take\n      end",
    "comment": "By default Rails will issue an \"SELECT *\" for the relation, which is overkill for just updating the timestamps. To work around this we manually touch the data so we can SELECT only the columns we need.",
    "label": "",
    "id": "6122"
  },
  {
    "raw_code": "def check_for_spam?(*)\n    return false if system? || !spammable_attribute_changed? || confidential?\n    return false if noteable.try(:confidential?) == true || noteable.try(:public?) == false\n    return false if noteable.try(:group)&.public? == false || project&.public? == false\n\n    true\n  end",
    "comment": "Override method defined in Spammable Wildcard argument because user: argument is not used",
    "label": "",
    "id": "6123"
  },
  {
    "raw_code": "def refresh_markdown_cache\n    return super if sharding_key_columns.any? { |s_column| self[s_column].present? }\n\n    # Using model callbacks to get sharding key values\n    ensure_organization_id\n    ensure_namespace_id\n\n    sharding_key_changes = changes.select { |k, _| sharding_key_columns.include?(k.to_sym) }\n    sharding_key_updates = sharding_key_changes.transform_values(&:last)\n\n    super.merge(sharding_key_updates)\n  end",
    "comment": "TODO: Remove when sharding key NOT NULL constraint is validated This should not happen often, and will only happen once for any record https://gitlab.com/gitlab-org/gitlab/-/work_items/570340",
    "label": "",
    "id": "6124"
  },
  {
    "raw_code": "def attribute_names_for_serialization\n    attributes.keys\n  end",
    "comment": "Use attributes.keys instead of attribute_names to filter out the fields that are skipped during export:  - note_html - cached_markdown_version  Note: This method is an override of an existing private ActiveRecord method",
    "label": "",
    "id": "6125"
  },
  {
    "raw_code": "def latest(order_by: 'released_at')\n      sort_by_attribute(\"#{order_by}_desc\").first\n    end",
    "comment": "In the future, we should support `order_by=semver`; see https://gitlab.com/gitlab-org/gitlab/-/issues/352945",
    "label": "",
    "id": "6126"
  },
  {
    "raw_code": "def latest_for_projects(projects, order_by: 'released_at')\n      return Release.none if projects.empty?\n\n      projects_table = Project.arel_table\n      releases_table = Release.arel_table\n\n      join_query = Release\n        .where(projects_table[:id].eq(releases_table[:project_id]))\n        .sort_by_attribute(\"#{order_by}_desc\")\n        .limit(1)\n\n      project_ids_list = projects.map { |project| \"(#{project.id})\" }.join(',')\n\n      Release\n        .from(\"(VALUES #{project_ids_list}) projects (id)\")\n        .joins(\"INNER JOIN LATERAL (#{join_query.to_sql}) #{Release.table_name} ON TRUE\")\n    end",
    "comment": "This query uses LATERAL JOIN to find the latest release for each project. To avoid joining the `projects` table, we build an in-memory table using the project ids. Example: SELECT ... FROM (VALUES (PROJECT_ID_1),(PROJECT_ID_2)) projects (id) INNER JOIN LATERAL (...)",
    "label": "",
    "id": "6127"
  },
  {
    "raw_code": "def self.created_or_pushed\n    actions = [\n      PushEventPayload.actions[:pushed],\n      PushEventPayload.actions[:created]\n    ]\n\n    joins(:push_event_payload)\n      .where(push_event_payloads: { action: actions })\n  end",
    "comment": "Returns events of pushes that either pushed to an existing ref or created a new one.",
    "label": "",
    "id": "6128"
  },
  {
    "raw_code": "def self.branch_events\n    ref_type = PushEventPayload.ref_types[:branch]\n\n    joins(:push_event_payload)\n      .where(push_event_payloads: { ref_type: ref_type })\n  end",
    "comment": "Returns events of pushes to a branch.",
    "label": "",
    "id": "6129"
  },
  {
    "raw_code": "def self.without_existing_merge_requests\n    existing_mrs = MergeRequest.except(:order, :where)\n      .select(1)\n      .where('merge_requests.source_project_id = events.project_id')\n      .where('merge_requests.source_branch = push_event_payloads.ref')\n      .with_state(:opened)\n\n    # For reasons unknown the use of #eager_load will result in the\n    # \"push_event_payload\" association not being set. Because of this we're\n    # using \"joins\" here, which does mean an additional query needs to be\n    # executed in order to retrieve the \"push_event_association\" when the\n    # returned PushEvent is used.\n    joins(:push_event_payload)\n      .where('NOT EXISTS (?)', existing_mrs)\n      .created_or_pushed\n      .branch_events\n  end",
    "comment": "Returns PushEvent instances for which no merge requests have been created.",
    "label": "",
    "id": "6130"
  },
  {
    "raw_code": "def project\n    strong_memoize(:project) do\n      projects.first\n    end",
    "comment": "This is temporal. Currently we limit DeployToken to a single project or group, later we're going to extend that to be for multiple projects and namespaces.",
    "label": "",
    "id": "6131"
  },
  {
    "raw_code": "def verification_domain\n    return unless domain.present?\n\n    \"_#{VERIFICATION_KEY}.#{domain}\"\n  end",
    "comment": "Verification codes may be TXT records for domain or verification_domain, to support the use of CNAME records on domain.",
    "label": "",
    "id": "6132"
  },
  {
    "raw_code": "def self.search_title(query)\n    fuzzy_search(query, [:title])\n  end",
    "comment": "Searches for timeboxes with a matching title.  This method uses ILIKE on PostgreSQL  query - The search query as a String  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6133"
  },
  {
    "raw_code": "def to_reference(from = nil, format: :name, full: false, absolute_path: false)\n    format_reference = timebox_format_reference(format)\n    reference = \"#{self.class.reference_prefix}#{format_reference}\"\n\n    if project\n      \"#{project.to_reference_base(from, full: full, absolute_path: absolute_path)}#{reference}\"\n    else\n      \"#{group.to_reference_base(from, full: full, absolute_path: absolute_path)}#{reference}\"\n    end",
    "comment": " Returns the String necessary to reference a milestone in Markdown. Group milestones only support name references, and do not support cross-project references.  format - Symbol format to use (default: :iid, optional: :name)  Examples:  Milestone.first.to_reference                           # => \"%1\" Milestone.first.to_reference(cross_namespace_project)  # => \"gitlab-org/gitlab-foss%1\" Milestone.first .to_reference(project, full: true, absolute_path: true) # => \"/gitlab-org/gitlab-foss%1\" ",
    "label": "",
    "id": "6134"
  },
  {
    "raw_code": "def parent_type_check\n    return unless group_id && project_id\n\n    field = project_id_changed? ? :project_id : :group_id\n    errors.add(field, _(\"milestone should belong either to a project or a group.\") % { timebox_name: timebox_name })\n  end",
    "comment": "Milestone should be either a project milestone or a group milestone",
    "label": "",
    "id": "6135"
  },
  {
    "raw_code": "def uniqueness_of_title\n    if project\n      relation = self.class.for_projects_and_groups([project_id], [project.group&.self_and_ancestors_ids])\n    elsif group\n      relation = self.class.for_projects_and_groups(group.all_project_ids, [group.self_and_hierarchy])\n    end",
    "comment": "milestone titles must be unique across project and group milestones",
    "label": "",
    "id": "6136"
  },
  {
    "raw_code": "def self.create_bulk(merge_request_diff_id, commits, project, skip_commit_data: false)\n    organization_id = project.organization_id\n    dedup_enabled = Feature.enabled?(:merge_request_diff_commits_dedup, project)\n    partition_enabled = Feature.enabled?(:merge_request_diff_commits_partition, project)\n    commit_hashes, user_triples = prepare_commits_for_bulk_insert(commits, organization_id)\n    users = MergeRequest::DiffCommitUser.bulk_find_or_create(user_triples)\n\n    rows = commit_hashes.map.with_index do |commit_hash, index|\n      raw_sha = commit_hash.delete(:id)\n      trailers = commit_hash.fetch(:trailers, {})\n\n      author = users[[commit_hash[:author_name], commit_hash[:author_email], organization_id]]\n      committer = users[[commit_hash[:committer_name], commit_hash[:committer_email], organization_id]]\n\n      # These fields are only used to determine the author/committer IDs, we\n      # don't store them in the DB.\n      #\n      # Trailers are stored in the DB here in order to allow changelog parsing.\n      # Rather than add an additional column for :extended_trailers, we're instead\n      # ignoring it for now until we deprecate the :trailers field and replace it with\n      # the new functionality.\n      commit_hash = commit_hash\n        .except(:author_name, :author_email, :committer_name, :committer_email, :extended_trailers)\n\n      commit_hash = commit_hash.merge(\n        commit_author_id: author.id,\n        committer_id: committer.id,\n        merge_request_diff_id: merge_request_diff_id,\n        relative_order: index,\n        sha: Gitlab::Database::ShaAttribute.serialize(raw_sha),\n        authored_date: Gitlab::Database.sanitize_timestamp(commit_hash[:authored_date]),\n        committed_date: Gitlab::Database.sanitize_timestamp(commit_hash[:committed_date]),\n        trailers: Gitlab::Json.dump(trailers)\n      )\n\n      # Need to add `raw_sha` to commit_hash as we will use that when\n      # inserting the `sha` in `merge_request_commits_metadata` table. We\n      # only need to do this when dedup is enabled.\n      commit_hash[:raw_sha] = raw_sha if dedup_enabled\n\n      commit_hash[:project_id] = project.id if partition_enabled\n      commit_hash = commit_hash.merge(message: '') if skip_commit_data\n\n      commit_hash\n    end",
    "comment": "Deprecated; use `bulk_insert!` from `BulkInsertSafe` mixin instead. cf. https://gitlab.com/gitlab-org/gitlab/issues/207989 for progress",
    "label": "",
    "id": "6137"
  },
  {
    "raw_code": "def project\n    @_project ||= merge_request_diff.project\n  end",
    "comment": "As of %17.10, we still don't have `project_id` on merge_request_diff_commit records. Until we do, we have to fetch it from merge_request_diff. ",
    "label": "",
    "id": "6138"
  },
  {
    "raw_code": "def belongs_to_touch_method\n    :touch\n  end",
    "comment": "Rails5 defaults to :touch_later, overwrite for normal touch",
    "label": "",
    "id": "6139"
  },
  {
    "raw_code": "def default_min_key_size(name)\n      if Gitlab::FIPS.enabled?\n        Gitlab::SSHPublicKey.supported_sizes(name).select(&:positive?).min || -1\n      else\n        0\n      end",
    "comment": "Return the default allowed minimum key size for a type. By default this is 0 (unrestricted), but in FIPS mode this will return the smallest allowed key size. If no size is available, this type is denied.  @return [Integer]",
    "label": "",
    "id": "6140"
  },
  {
    "raw_code": "def outbound_local_requests_allowlist_arrays\n    strong_memoize(:outbound_local_requests_allowlist_arrays) do\n      next [[], []] unless self.outbound_local_requests_whitelist\n\n      ip_allowlist, domain_allowlist = separate_allowlists(self.outbound_local_requests_whitelist)\n\n      [ip_allowlist, domain_allowlist]\n    end",
    "comment": "This method separates out the strings stored in the application_setting.outbound_local_requests_whitelist array into 2 arrays; an array of IPAddr objects (`[IPAddr.new('127.0.0.1')]`), and an array of domain strings (`['www.example.com']`).",
    "label": "",
    "id": "6141"
  },
  {
    "raw_code": "def performance_bar_enabled\n    performance_bar_allowed_group_id.present?\n  end",
    "comment": "Return true if the Performance Bar is enabled for a given group",
    "label": "",
    "id": "6142"
  },
  {
    "raw_code": "def pick_repository_storage\n    normalized_repository_storage_weights.max_by { |_, weight| rand**(1.0 / weight) }.first\n  end",
    "comment": "Choose one of the available repository storage options based on a normalized weighted probability.",
    "label": "",
    "id": "6143"
  },
  {
    "raw_code": "def suggested_reviewer_users\n    User.none\n  end",
    "comment": "method overridden in EE",
    "label": "",
    "id": "6144"
  },
  {
    "raw_code": "def merge_request_diff\n    fallback = latest_merge_request_diff unless association(:merge_request_diff).loaded?\n\n    fallback || super || MergeRequestDiff.new(merge_request_id: id)\n  end",
    "comment": "This is the same as latest_merge_request_diff unless: 1. There are arguments - in which case we might be trying to force-reload. 2. This association is already loaded. 3. The latest diff does not exist. 4. It doesn't have any merge_request_diffs - it returns an empty MergeRequestDiff  The second one in particular is important - MergeRequestDiff#merge_request is the inverse of MergeRequest#merge_request_diff, which means it may not be the latest diff, because we could have loaded any diff from this particular MR. If we haven't already loaded a diff, then it's fine to load the latest.",
    "label": "",
    "id": "6145"
  },
  {
    "raw_code": "def self.available_state_names\n    super + [:merged, :locked]\n  end",
    "comment": "Keep states definition to be evaluated before the state_machine block to avoid spec failures. If this gets evaluated after, the `merged` and `locked` states (which are overridden) can be nil. ",
    "label": "",
    "id": "6146"
  },
  {
    "raw_code": "def check_state?(merge_status)\n      [:unchecked, :cannot_be_merged_recheck, :checking, :cannot_be_merged_rechecking].include?(merge_status.to_sym)\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6147"
  },
  {
    "raw_code": "def public_merge_status\n    cannot_be_merged_rechecking? || preparing? ? 'checking' : merge_status\n  end",
    "comment": "Returns current merge_status except it returns `cannot_be_merged_rechecking` as `checking` to avoid exposing unnecessary internal state",
    "label": "",
    "id": "6148"
  },
  {
    "raw_code": "def self.recent_target_branches(limit: 100)\n    group(:target_branch)\n      .select(:target_branch)\n      .reorder(arel_table[:updated_at].maximum.desc)\n      .limit(limit)\n      .pluck(:target_branch)\n  end",
    "comment": "Returns the top 100 target branches  The returned value is a Array containing branch names sort by updated_at of merge request:  ['master', 'develop', 'production']  limit - The maximum number of target branch to return.",
    "label": "",
    "id": "6149"
  },
  {
    "raw_code": "def diff_head_pipeline\n    head_pipeline&.matches_sha_or_source_sha?(diff_head_sha) ? head_pipeline : nil\n  end",
    "comment": "Use this method whenever you need to make sure the head_pipeline is synced with the branch head commit, for example checking if a merge request can be merged. For more information check: https://gitlab.com/gitlab-org/gitlab-foss/issues/40004",
    "label": "",
    "id": "6150"
  },
  {
    "raw_code": "def self.reference_pattern\n    @reference_pattern ||= %r{\n      (#{Project.reference_pattern})?\n      #{Regexp.escape(reference_prefix)}#{Gitlab::Regex.merge_request}\n    }x\n  end",
    "comment": "Pattern used to extract `!123` merge request references from text  This pattern supports cross-project references.",
    "label": "",
    "id": "6151"
  },
  {
    "raw_code": "def self.in_projects(relation)\n    # unscoping unnecessary conditions that'll be applied\n    # when executing `where(\"merge_requests.id IN (#{union.to_sql})\")`\n    source = unscoped.where(source_project_id: relation)\n    target = unscoped.where(target_project_id: relation)\n\n    from_union([source, target])\n  end",
    "comment": "Returns all the merge requests from an ActiveRecord:Relation.  This method uses a UNION as it usually operates on the result of ProjectsFinder#execute. PostgreSQL in particular doesn't always like queries using multiple sub-queries especially when combined with an OR statement. UNIONs on the other hand perform much better in these cases.  relation - An ActiveRecord::Relation that returns a list of Projects.  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6152"
  },
  {
    "raw_code": "def self.set_latest_merge_request_diff_ids!\n    update = \"\n      latest_merge_request_diff_id = (\n        SELECT MAX(id)\n        FROM merge_request_diffs\n        WHERE merge_requests.id = merge_request_diffs.merge_request_id\n        AND merge_request_diffs.diff_type = #{MergeRequestDiff.diff_types[:regular]}\n      )\".squish\n\n    self.each_batch do |batch|\n      batch.update_all(update)\n    end",
    "comment": "This is used after project import, to reset the IDs to the correct values. It is not intended to be called without having already scoped the relation.  Only set `regular` merge request diffs as latest so `merge_head` diff won't be considered as `MergeRequest#merge_request_diff`.",
    "label": "",
    "id": "6153"
  },
  {
    "raw_code": "def draftless_title_changed(old_title)\n    self.class.draftless_title(old_title) != self.draftless_title\n  end",
    "comment": "Verifies if title has changed not taking into account Draft prefix for merge requests.",
    "label": "",
    "id": "6154"
  },
  {
    "raw_code": "def to_reference(from = nil, full: false)\n    reference = \"#{self.class.reference_prefix}#{iid}\"\n\n    \"#{project.to_reference_base(from, full: full)}#{reference}\"\n  end",
    "comment": "`from` argument can be a Namespace or Project.",
    "label": "",
    "id": "6155"
  },
  {
    "raw_code": "def merge_async(user_id, params)\n    jid = MergeWorker.with_status.perform_async(id, user_id, params.to_h)\n    update_column(:merge_jid, jid)\n\n    # merge_ongoing? depends on merge_jid\n    # expire etag cache since the attribute is changed without triggering callbacks\n    expire_etag_cache\n  end",
    "comment": "Calls `MergeWorker` to proceed with the merge process and updates `merge_jid` with the MergeWorker#jid. This helps tracking enqueued and ongoing merge jobs.",
    "label": "",
    "id": "6156"
  },
  {
    "raw_code": "def rebase_async(user_id, skip_ci: false)\n    with_rebase_lock do\n      raise ActiveRecord::StaleObjectError if !open? || rebase_in_progress?\n\n      # Although there is a race between setting rebase_jid here and clearing it\n      # in the RebaseWorker, it can't do any harm since we check both that the\n      # attribute is set *and* that the sidekiq job is still running. So a JID\n      # for a completed RebaseWorker is equivalent to a nil JID.\n      jid = Sidekiq::Worker.skipping_transaction_check do\n        RebaseWorker.with_status.perform_async(id, user_id, skip_ci)\n      end",
    "comment": "Set off a rebase asynchronously, atomically updating the `rebase_jid` of the MR so that the status of the operation can be tracked.",
    "label": "",
    "id": "6157"
  },
  {
    "raw_code": "def repository_diff_refs\n    Gitlab::Diff::DiffRefs.new(\n      base_sha: branch_merge_base_sha,\n      start_sha: target_branch_sha,\n      head_sha: source_branch_sha\n    )\n  end",
    "comment": "Instead trying to fetch the persisted diff_refs, this method goes straight to the repository to get the most recent data possible.",
    "label": "",
    "id": "6158"
  },
  {
    "raw_code": "def reload_diff(current_user = nil)\n    return unless open?\n\n    MergeRequests::ReloadDiffsService.new(self, current_user).execute\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6159"
  },
  {
    "raw_code": "def diffable_merge_ref?\n    open? && merge_head_diff.present? && can_be_merged?\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6160"
  },
  {
    "raw_code": "def recheck_merge_status?\n    self.class.state_machines[:merge_status].check_state?(merge_status)\n  end",
    "comment": "Returns boolean indicating the merge_status should be rechecked in order to switch to either can_be_merged or cannot_be_merged.",
    "label": "",
    "id": "6161"
  },
  {
    "raw_code": "def mergeable?(check_mergeability_retry_lease: false, skip_rebase_check: false, **mergeable_state_check_params)\n    return false unless mergeable_state?(**mergeable_state_check_params)\n\n    check_mergeability(sync_retry_lease: check_mergeability_retry_lease)\n    mergeable_git_state?(skip_rebase_check: skip_rebase_check)\n  end",
    "comment": "mergeable_state_check_params allows a hash of merge checks to skip or not skip_ci_check skip_discussions_check skip_draft_check skip_approved_check skip_blocked_check skip_external_status_check skip_requested_changes_check skip_locked_paths_check skip_jira_check skip_locked_lfs_files_check",
    "label": "",
    "id": "6162"
  },
  {
    "raw_code": "def mergeable_state?(**params)\n    execute_merge_checks(\n      self.class.mergeable_state_checks,\n      params: params,\n      execute_all: false\n    ).success?\n  end",
    "comment": "mergeable_state_check_params allows a hash of merge checks to skip or not skip_ci_check skip_discussions_check skip_draft_check skip_approved_check skip_blocked_check skip_external_status_check skip_requested_changes_check skip_jira_check",
    "label": "",
    "id": "6163"
  },
  {
    "raw_code": "def mergeable_git_state?(**params)\n    execute_merge_checks(\n      self.class.mergeable_git_state_checks,\n      params: params,\n      execute_all: false\n    ).success?\n  end",
    "comment": "This runs only git related checks",
    "label": "",
    "id": "6164"
  },
  {
    "raw_code": "def mergeability_checks_pass?(**params)\n    execute_merge_checks(\n      self.class.all_mergeability_checks,\n      params: params,\n      execute_all: false\n    ).success?\n  end",
    "comment": "This runs all the checks",
    "label": "",
    "id": "6165"
  },
  {
    "raw_code": "def cache_merge_request_closes_issues!(current_user = self.author)\n    return if closed? || merged?\n\n    issues_to_close_ids = closes_issues(current_user).reject { |issue| issue.is_a?(ExternalIssue) }.map(&:id)\n\n    transaction do\n      merge_requests_closing_issues.from_mr_description.delete_all\n\n      # These might have been created manually from the work item interface\n      issue_ids_to_update = merge_requests_closing_issues\n        .where(from_mr_description: false, issue_id: issues_to_close_ids)\n        .pluck(:issue_id)\n\n      if issue_ids_to_update.any?\n        merge_requests_closing_issues.where(issue_id: issue_ids_to_update).update_all(from_mr_description: true)\n      end",
    "comment": "If the merge request closes any issues, save this information in the `MergeRequestsClosingIssues` model. This is a performance optimization. Calculating this information for a number of merge requests requires running `ReferenceExtractor` on each of them separately. This optimization does not apply to issues from external sources.",
    "label": "",
    "id": "6166"
  },
  {
    "raw_code": "def closes_issues(current_user = self.author)\n    if target_branch == project.default_branch\n      messages = [title, description]\n      messages.concat(commits(load_from_gitaly: true).map(&:safe_message)) if merge_request_diff.persisted?\n\n      Gitlab::ClosingIssueExtractor.new(project, current_user)\n        .closed_by_message(messages.join(\"\\n\"))\n    else\n      []\n    end",
    "comment": "Return the set of issues that will be closed if this merge request is accepted.",
    "label": "",
    "id": "6167"
  },
  {
    "raw_code": "def first_multiline_commit\n    strong_memoize(:first_multiline_commit) do\n      recent_commits(load_from_gitaly: true).without_merge_commits.reverse_each.find(&:description?)\n    end",
    "comment": "Returns the oldest multi-line commit",
    "label": "",
    "id": "6168"
  },
  {
    "raw_code": "def first_multiline_commit_description\n    strong_memoize(:first_multiline_commit_description) do\n      commit = first_multiline_commit\n      commit&.description\n    end",
    "comment": "Returns the description (without the first line/title) of the first multiline commit",
    "label": "",
    "id": "6169"
  },
  {
    "raw_code": "def has_ci_enabled?\n    has_ci? || pipeline_creating?\n  end",
    "comment": "We use a heuristic of if there are pipeline created, being created, or a ci integration is setup",
    "label": "",
    "id": "6170"
  },
  {
    "raw_code": "def merge_ref_head\n    return project.repository.commit(merge_ref_sha) if merge_ref_sha\n\n    project.repository.commit(merge_ref_path)\n  end",
    "comment": "Returns the current merge-ref HEAD commit. ",
    "label": "",
    "id": "6171"
  },
  {
    "raw_code": "def predefined_variables\n    Gitlab::Ci::Variables::Collection.new.tap do |variables|\n      variables.append(key: 'CI_MERGE_REQUEST_ID', value: id.to_s)\n      variables.append(key: 'CI_MERGE_REQUEST_IID', value: iid.to_s)\n      variables.append(key: 'CI_MERGE_REQUEST_REF_PATH', value: ref_path.to_s)\n      variables.append(key: 'CI_MERGE_REQUEST_PROJECT_ID', value: project.id.to_s)\n      variables.append(key: 'CI_MERGE_REQUEST_PROJECT_PATH', value: project.full_path)\n      variables.append(key: 'CI_MERGE_REQUEST_PROJECT_URL', value: project.web_url)\n      variables.append(key: 'CI_MERGE_REQUEST_TARGET_BRANCH_NAME', value: target_branch.to_s)\n      variables.append(key: 'CI_MERGE_REQUEST_TARGET_BRANCH_PROTECTED', value: ProtectedBranch.protected?(target_project, target_branch).to_s)\n      variables.append(key: 'CI_MERGE_REQUEST_TITLE', value: title)\n      variables.append(key: 'CI_MERGE_REQUEST_DRAFT', value: work_in_progress?.to_s)\n\n      mr_description, mr_description_truncated = truncate_mr_description\n      variables.append(key: 'CI_MERGE_REQUEST_DESCRIPTION', value: mr_description)\n      variables.append(key: 'CI_MERGE_REQUEST_DESCRIPTION_IS_TRUNCATED', value: mr_description_truncated)\n      variables.append(key: 'CI_MERGE_REQUEST_ASSIGNEES', value: assignee_username_list) if assignees.present?\n      variables.append(key: 'CI_MERGE_REQUEST_MILESTONE', value: milestone.title) if milestone\n      variables.append(key: 'CI_MERGE_REQUEST_LABELS', value: label_names.join(',')) if labels.present?\n      variables.append(key: 'CI_MERGE_REQUEST_SQUASH_ON_MERGE', value: squash_on_merge?.to_s)\n      variables.concat(source_project_variables)\n    end",
    "comment": "rubocop: disable Metrics/AbcSize -- Despite being long, this method is quite straightforward. Splitting it in smaller chunks would likely reduce readability.",
    "label": "",
    "id": "6172"
  },
  {
    "raw_code": "def compare_test_reports\n    unless has_test_reports?\n      return { status: :error, status_reason: 'This merge request does not have test reports' }\n    end",
    "comment": "rubocop: enable Metrics/AbcSize",
    "label": "",
    "id": "6173"
  },
  {
    "raw_code": "def find_coverage_reports\n    unless has_coverage_reports?\n      return { status: :error, status_reason: 'This merge request does not have coverage reports' }\n    end",
    "comment": "TODO: this method and compare_test_reports use the same result type, which is handled by the controller's #reports_response. we should minimize mistakes by isolating the common parts. issue: https://gitlab.com/gitlab-org/gitlab/issues/34224",
    "label": "",
    "id": "6174"
  },
  {
    "raw_code": "def find_codequality_mr_diff_reports\n    unless has_codequality_mr_diff_report?\n      return { status: :error, status_reason: 'This merge request does not have codequality mr diff reports' }\n    end",
    "comment": "TODO: this method and compare_test_reports use the same result type, which is handled by the controller's #reports_response. we should minimize mistakes by isolating the common parts. issue: https://gitlab.com/gitlab-org/gitlab/issues/34224",
    "label": "",
    "id": "6175"
  },
  {
    "raw_code": "def find_exposed_artifacts\n    unless has_exposed_artifacts?\n      return { status: :error, status_reason: 'This merge request does not have exposed artifacts' }\n    end",
    "comment": "TODO: this method and compare_test_reports use the same result type, which is handled by the controller's #reports_response. we should minimize mistakes by isolating the common parts. issue: https://gitlab.com/gitlab-org/gitlab/issues/34224",
    "label": "",
    "id": "6176"
  },
  {
    "raw_code": "def compare_reports(service_class, current_user = nil, report_type = nil, additional_params = {})\n    with_reactive_cache(service_class.name, current_user&.id, report_type) do |data|\n      unless service_class.new(project, current_user, id: id, report_type: report_type, additional_params: additional_params)\n        .latest?(comparison_base_pipeline(service_class), diff_head_pipeline, data)\n        raise InvalidateReactiveCache\n      end",
    "comment": "TODO: consider renaming this as with exposed artifacts we generate reports, not always compare issue: https://gitlab.com/gitlab-org/gitlab/issues/34224",
    "label": "",
    "id": "6177"
  },
  {
    "raw_code": "def all_commit_shas\n    return commit_shas unless persisted?\n\n    all_commits.pluck(:sha).uniq\n  end",
    "comment": "Note that this could also return SHA from now dangling commits ",
    "label": "",
    "id": "6178"
  },
  {
    "raw_code": "def commit_to_revert\n    return unless merged?\n\n    # By default, it's equal to a merge commit\n    return merge_commit if merge_commit\n\n    # But in case of fast-forward merge merge commits are not created\n    # To solve that we can use `squash_commit` if the merge request was squashed\n    return squash_commit if squash_commit\n\n    # Edge case: one commit in the merge request without merge or squash commit\n    return project.commit(diff_head_sha) if commits_count == 1\n\n    nil\n  end",
    "comment": "Exists only for merged merge requests",
    "label": "",
    "id": "6179"
  },
  {
    "raw_code": "def update_diff_discussion_positions(old_diff_refs:, new_diff_refs:, current_user: nil)\n    return unless has_complete_diff_refs?\n    return if new_diff_refs == old_diff_refs\n\n    active_diff_discussions = self.notes.new_diff_notes.discussions.select do |discussion|\n      discussion.active?(old_diff_refs)\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6180"
  },
  {
    "raw_code": "def keep_around_commit\n    project.repository.keep_around(self.merge_commit_sha, source: self.class.name)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6181"
  },
  {
    "raw_code": "def invalidate_project_counter_caches\n    Projects::OpenMergeRequestsCountService.new(target_project).delete_cache\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6182"
  },
  {
    "raw_code": "def first_contribution?\n    return metrics&.first_contribution if merged? & metrics.present?\n\n    !project.merge_requests.merged.exists?(author_id: author_id)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6183"
  },
  {
    "raw_code": "def with_retried_nowait_lock\n    # Try at most 0.25 + (1.5 * .25) + (1.5^2 * .25) ... (1.5^5 * .25) = 5.2 s to get the lock\n    Retriable.retriable(on: ActiveRecord::LockWaitTimeout, tries: 6, base_interval: 0.25) do\n      with_lock('FOR UPDATE NOWAIT') do\n        yield\n      end",
    "comment": "If the merge request is idle in transaction or has a SELECT FOR UPDATE, we don't want to block indefinitely or this could cause a queue of SELECT FOR UPDATE calls. Instead, try to get the lock for 5 s before raising an error to the user.",
    "label": "",
    "id": "6184"
  },
  {
    "raw_code": "def wiki_size\n    super.to_i\n  end",
    "comment": "`wiki_size` and `snippets_size` have no default value in the database and the column can be nil. This means that, when the columns were added, all rows had nil values on them. Therefore, any call to any of those methods will return nil instead of 0.  These two methods provide consistency and avoid returning nil.",
    "label": "",
    "id": "6185"
  },
  {
    "raw_code": "def refresh_storage_size!\n    self.class.where(id: id).update_all(\"storage_size = #{storage_size_sum}\")\n  end",
    "comment": "Since this incremental update method does not update the storage_size directly, we have to update the storage_size separately in an after_commit action.",
    "label": "",
    "id": "6186"
  },
  {
    "raw_code": "def self.increment_statistic(project, key, increment)\n    return if project.pending_delete?\n\n    project.statistics.try do |project_statistics|\n      project_statistics.increment_statistic(key, increment)\n    end",
    "comment": "For counter attributes, storage_size will be refreshed after the counter is flushed, through counter_attribute_after_commit  For non-counter attributes, storage_size is updated depending on key => [columns] in INCREMENTABLE_COLUMNS",
    "label": "",
    "id": "6187"
  },
  {
    "raw_code": "def export_size\n    storage_size - build_artifacts_size - packages_size\n  end",
    "comment": "Build artifacts & packages are not included in the project export",
    "label": "",
    "id": "6188"
  },
  {
    "raw_code": "def self.decorate(blob, container = nil)\n    return if blob.nil?\n\n    new(blob, container)\n  end",
    "comment": "Wrap a Gitlab::Git::Blob object, or return nil when given nil  This method prevents the decorated object from evaluating to \"truthy\" when given a nil value. For example:  blob = Blob.new(nil) puts \"truthy\" if blob # => \"truthy\"  blob = Blob.decorate(nil) puts \"truthy\" if blob # No output",
    "label": "",
    "id": "6189"
  },
  {
    "raw_code": "def data\n    if binary_in_repo?\n      super\n    else\n      @data ||= super.encode(Encoding::UTF_8, invalid: :replace, undef: :replace)\n    end",
    "comment": "Returns the data of the blob.  If the blob is a text based blob the content is converted to UTF-8 and any invalid byte sequences are replaced.",
    "label": "",
    "id": "6190"
  },
  {
    "raw_code": "def raw_size\n    if stored_externally?\n      external_size\n    else\n      size\n    end",
    "comment": "Returns the size of the file that this blob represents. If this blob is an LFS pointer, this is the size of the file stored in LFS. Otherwise, this is the size of the blob itself.",
    "label": "",
    "id": "6191"
  },
  {
    "raw_code": "def binary?\n    if stored_externally?\n      if rich_viewer\n        rich_viewer.binary?\n      elsif known_extension?\n        false\n      elsif _mime_type\n        _mime_type.binary?\n      else\n        true\n      end",
    "comment": "Returns whether the file that this blob represents is binary. If this blob is an LFS pointer, we assume the file stored in LFS is binary, unless a text-based rich blob viewer matched on the file's extension. Otherwise, this depends on the type of the blob itself.",
    "label": "",
    "id": "6192"
  },
  {
    "raw_code": "def to_param\n    {\n      from: @straight ? start_commit_sha : (base_commit_sha || start_commit_sha),\n      to: head_commit_sha\n    }\n  end",
    "comment": "Return a Hash of parameters for passing to a URL helper  See `namespace_project_compare_url`",
    "label": "",
    "id": "6193"
  },
  {
    "raw_code": "def diff_export\n    content = merge_request_diff.cached_external_diff do |file|\n      file.seek(external_diff_offset)\n\n      force_encode_utf8(file.read(external_diff_size))\n    end",
    "comment": "This method is meant to be used during Project Export. It is identical to the behavior in #diff with the only difference of caching externally stored diffs on local disk in temp storage location in order to improve diff export performance.",
    "label": "",
    "id": "6194"
  },
  {
    "raw_code": "def public_or_visible_to_user(user)\n      return public_to_user unless user\n\n      public_for_user = public_to_user_arel(user)\n      visible_for_user = visible_to_user_arel(user)\n      public_or_visible = public_for_user.or(visible_for_user)\n\n      where(public_or_visible)\n    end",
    "comment": "WARNING: This method should never be used on its own please do make sure the number of rows you are filtering is small enough for this query",
    "label": "",
    "id": "6195"
  },
  {
    "raw_code": "def preset_root_ancestor_for(groups)\n      return groups if groups.size < 2\n\n      root = groups.first.root_ancestor\n      groups.drop(1).each { |group| group.root_ancestor = root }\n    end",
    "comment": "This method can be used only if all groups have the same top-level group",
    "label": "",
    "id": "6196"
  },
  {
    "raw_code": "def ids_with_disabled_email(groups)\n      inner_groups = Group.where('id = namespaces_with_emails_disabled.id')\n      inner_query = inner_groups\n        .self_and_ancestors\n        .joins(:namespace_settings)\n        .where(namespace_settings: { emails_enabled: false })\n        .select('1')\n        .limit(1)\n\n      group_ids = Namespace\n        .from('(SELECT * FROM namespaces) as namespaces_with_emails_disabled')\n        .where(namespaces_with_emails_disabled: { id: groups })\n        .where('EXISTS (?)', inner_query)\n        .pluck(:id)\n\n      Set.new(group_ids)\n    end",
    "comment": "Returns the ids of the passed group models where the `emails_enabled` column is set to false anywhere in the ancestor hierarchy.",
    "label": "",
    "id": "6197"
  },
  {
    "raw_code": "def project_creation_levels_for_user(user)\n      project_creation_allowed_on_levels = [\n        ::Gitlab::Access::DEVELOPER_PROJECT_ACCESS,\n        ::Gitlab::Access::MAINTAINER_PROJECT_ACCESS,\n        ::Gitlab::Access::OWNER_PROJECT_ACCESS,\n        nil\n      ]\n\n      if user.can_admin_all_resources?\n        project_creation_allowed_on_levels << ::Gitlab::Access::ADMINISTRATOR_PROJECT_ACCESS\n      end",
    "comment": "Handle project creation permissions based on application setting and group setting. The `default_project_creation` application setting is the default value and can be overridden by the `project_creation_level` group setting. `nil` value of namespaces.project_creation_level` means that allowed creation level has not been explicitly set by the group owner and is a placeholder value for inheriting the value from the ApplicationSetting.",
    "label": "",
    "id": "6198"
  },
  {
    "raw_code": "def notification_settings(hierarchy_order: nil)\n    source_type = self.class.base_class.name\n    settings = NotificationSetting.where(source_type: source_type, source_id: self_and_ancestors_ids)\n\n    return settings unless hierarchy_order && self_and_ancestors_ids.length > 1\n\n    settings\n      .joins(\"LEFT JOIN (#{self_and_ancestors(hierarchy_order: hierarchy_order).to_sql}) AS ordered_groups ON notification_settings.source_id = ordered_groups.id\")\n      .select('notification_settings.*, ordered_groups.depth AS depth')\n      .order(\"ordered_groups.depth #{hierarchy_order}\")\n  end",
    "comment": "Overrides notification_settings has_many association This allows to apply notification settings from parent groups to child groups and projects.",
    "label": "",
    "id": "6199"
  },
  {
    "raw_code": "def last_owner?(user)\n    return false unless user\n\n    all_owners = member_owners_excluding_project_bots_and_service_accounts\n    last_owner_in_list?(user, all_owners)\n  end",
    "comment": "Check if user is a last owner of the group. Excludes non-direct owners for top-level group Excludes project_bots",
    "label": "",
    "id": "6200"
  },
  {
    "raw_code": "def last_owner_in_list?(user, all_owners)\n    return false unless user\n\n    all_owners.size == 1 && all_owners.first.user_id == user.id\n  end",
    "comment": "This is used in BillableMember Entity to avoid multiple \"member_owners_excluding_project_bots_and_service_accounts\" calls for each billable members",
    "label": "",
    "id": "6201"
  },
  {
    "raw_code": "def member_owners_excluding_project_bots_and_service_accounts\n    members_from_hierarchy = if root?\n                               members.non_minimal_access.without_invites_and_requests\n                             else\n                               members_with_parents(only_active_users: false)\n                             end",
    "comment": "Excludes non-direct owners for top-level group Excludes project_bots Excludes service accounts",
    "label": "",
    "id": "6202"
  },
  {
    "raw_code": "def system_hook_service\n    SystemHooksService.new\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6203"
  },
  {
    "raw_code": "def refresh_members_authorized_projects(\n    priority: UserProjectAccessChangedService::HIGH_PRIORITY,\n    direct_members_only: false\n  )\n\n    user_ids = if direct_members_only\n                 users_ids_of_direct_members\n               else\n                 user_ids_for_project_authorizations\n               end",
    "comment": "rubocop: enable CodeReuse/ServiceClass rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6204"
  },
  {
    "raw_code": "def users_ids_of_direct_members\n    direct_members.pluck_user_ids\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6205"
  },
  {
    "raw_code": "def has_user?(user)\n    return false unless user\n\n    group_members.non_invite.exists?(user: user)\n  end",
    "comment": "Only for direct and not requested members with higher access level than MIMIMAL_ACCESS It returns true for non-active users",
    "label": "",
    "id": "6206"
  },
  {
    "raw_code": "def hierarchy_members\n    GroupMember\n      .active_without_invites_and_requests\n      .where(source_id: self_and_hierarchy.without_order.select(:id))\n  end",
    "comment": "Returns all members that are part of the group, it's subgroups, and ancestor groups",
    "label": "",
    "id": "6207"
  },
  {
    "raw_code": "def max_member_access_for_user(user, only_concrete_membership: false)\n    return GroupMember::NO_ACCESS unless user\n\n    unless only_concrete_membership\n      return GroupMember::OWNER if user.can_admin_all_resources?\n      return GroupMember::OWNER if user.can_admin_organization?(organization)\n    end",
    "comment": "Return the highest access level for a user  A special case is handled here when the user is a GitLab admin which implies it has \"OWNER\" access everywhere, but should not officially appear as a member of a group unless specifically added to it  @param user [User] @param only_concrete_membership [Bool] whether require admin concrete membership status",
    "label": "",
    "id": "6208"
  },
  {
    "raw_code": "def runners_token\n    return unless allow_runner_registration_token?\n\n    ensure_runners_token!\n  end",
    "comment": "each existing group needs to have a `runners_token`. we do this on read since migrating all existing groups is not a feasible solution.",
    "label": "",
    "id": "6209"
  },
  {
    "raw_code": "def open_issues_count(current_user = nil)\n    Groups::OpenIssuesCountService.new(self, current_user).count\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6210"
  },
  {
    "raw_code": "def open_merge_requests_count(current_user = nil)\n    Groups::MergeRequestsCountService.new(self, current_user).count\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6211"
  },
  {
    "raw_code": "def timelogs\n    Timelog.in_group(self)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6212"
  },
  {
    "raw_code": "def supports_group_work_items?\n    false\n  end",
    "comment": "overriden in EE",
    "label": "",
    "id": "6213"
  },
  {
    "raw_code": "def has_active_hooks?(hooks_scope = :push_hooks)\n    false\n  end",
    "comment": "overriden in EE",
    "label": "",
    "id": "6214"
  },
  {
    "raw_code": "def enterprise_user_settings_available?(user = nil)\n    false\n  end",
    "comment": "overriden in EE",
    "label": "",
    "id": "6215"
  },
  {
    "raw_code": "def self.pending_destruction\n    delete_scheduled.where('next_delete_attempt_at IS NULL OR next_delete_attempt_at < ?', Time.zone.now)\n  end",
    "comment": "needed by Packages::Destructible",
    "label": "",
    "id": "6216"
  },
  {
    "raw_code": "def registry\n    strong_memoize_with_expiration(:registry, self.class.registry_client_expiration_time) do\n      token = Auth::ContainerRegistryAuthenticationService.full_access_token(path)\n\n      url = Gitlab.config.registry.api_url\n      host_port = Gitlab.config.registry.host_port\n\n      ContainerRegistry::Registry.new(url, token: token, path: host_port)\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6217"
  },
  {
    "raw_code": "def path\n    @path ||= [project.full_path, name]\n      .select(&:present?).join('/').downcase\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6218"
  },
  {
    "raw_code": "def tag(tag)\n    if gitlab_api_client.supports_gitlab_api?\n      page = tags_page(name: tag)\n      return if page[:tags].blank?\n\n      page[:tags].find { |result_tag| result_tag.name == tag }\n    else\n      ContainerRegistry::Tag.new(self, tag)\n    end",
    "comment": "If the container registry GitLab API is available, the API does a search of tags containing the name and we filter them to find the exact match. Otherwise, we instantiate a tag.",
    "label": "",
    "id": "6219"
  },
  {
    "raw_code": "def stop_with_actions!\n    return unless available?\n\n    if stop_actions.any? || auto_stop_setting_always?\n      stop!\n    end",
    "comment": "TODO: move this method and dependencies into Environments::StopService",
    "label": "",
    "id": "6220"
  },
  {
    "raw_code": "def safe_external_url\n    return unless self.external_url.present?\n\n    new_external_url = Addressable::URI.parse(self.external_url)\n\n    if Gitlab::Utils::SanitizeNodeLink::UNSAFE_PROTOCOLS.include?(new_external_url.normalized_scheme)\n      errors.add(:external_url, \"#{new_external_url.normalized_scheme} scheme is not allowed\")\n    end",
    "comment": "We deliberately avoid using AddressableUrlValidator to allow users to update their environments even if they have misconfigured `environment:url` keyword. The external URL is presented as a clickable link on UI and not consumed in GitLab internally, thus we sanitize the URL before the persistence to make sure the rendered link is XSS safe. See https://gitlab.com/gitlab-org/gitlab/-/issues/337417",
    "label": "",
    "id": "6221"
  },
  {
    "raw_code": "def guess_tier\n    case name\n    when /(dev|review|trunk)/i\n      self.class.tiers[:development]\n    when /(test|tst|int|ac(ce|)pt|qa|qc|control|quality)/i\n      self.class.tiers[:testing]\n    when /(st(a|)g|mod(e|)l|pre|demo|non)/i\n      self.class.tiers[:staging]\n    when /(pr(o|)d|live)/i\n      self.class.tiers[:production]\n    else\n      self.class.tiers[:other]\n    end",
    "comment": "Guessing the tier of the environment if it's not explicitly specified by users. See https://en.wikipedia.org/wiki/Deployment_environment for industry standard deployment environments",
    "label": "",
    "id": "6222"
  },
  {
    "raw_code": "def flush_records!(filter)\n      raise ArgumentError, \"filter cannot be empty\" if filter.blank?\n\n      where(filter).delete_all\n    end",
    "comment": "Flushing records is generally safe in a sense that those records are going to be re-created when needed.  A filter condition has to be provided to not accidentally flush records for all projects.",
    "label": "",
    "id": "6223"
  },
  {
    "raw_code": "def generate\n      InternalId.internal_id_transactions_increment(operation: :generate, usage: usage)\n\n      next_iid = update_record!(subject, scope, usage, arel_table[:last_value] + 1)\n\n      return next_iid if next_iid\n\n      create_record!(subject, scope, usage, initial_value(subject, scope) + 1)\n    rescue RecordAlreadyExists\n      retry\n    end",
    "comment": "Generates next internal id and returns it init: Block that gets called to initialize InternalId record if not present Make sure to not throw exceptions in the absence of records (if this is expected).",
    "label": "",
    "id": "6224"
  },
  {
    "raw_code": "def reset(value)\n      return false unless value\n\n      InternalId.internal_id_transactions_increment(operation: :reset, usage: usage)\n\n      iid = update_record!(subject, scope.merge(last_value: value), usage, arel_table[:last_value] - 1)\n      iid == value - 1\n    end",
    "comment": "Reset tries to rewind to `value-1`. This will only succeed, if `value` stored in database is equal to `last_value`. value: The expected last_value to decrement",
    "label": "",
    "id": "6225"
  },
  {
    "raw_code": "def track_greatest(new_value)\n      InternalId.internal_id_transactions_increment(operation: :track_greatest, usage: usage)\n\n      function = Arel::Nodes::NamedFunction.new('GREATEST', [arel_table[:last_value], new_value.to_i])\n\n      next_iid = update_record!(subject, scope, usage, function)\n      return next_iid if next_iid\n\n      create_record!(subject, scope, usage, [initial_value(subject, scope), new_value].max)\n    rescue RecordAlreadyExists\n      retry\n    end",
    "comment": "Create a record in internal_ids if one does not yet exist and set its new_value if it is higher than the current last_value",
    "label": "",
    "id": "6226"
  },
  {
    "raw_code": "def show_diff_preview_in_email?\n    return show_diff_preview_in_email unless namespace.has_parent?\n\n    all_ancestors_allow_diff_preview_in_email?\n  end",
    "comment": "Where this function is used, a returned \"nil\" is considered a truthy value",
    "label": "",
    "id": "6227"
  },
  {
    "raw_code": "def update_tracked_fields!(request)\n    return if Gitlab::Database.read_only?\n\n    update_tracked_fields(request)\n\n    Gitlab::ExclusiveLease.throttle(id) do\n      ::Ability.forgetting(/admin/) do\n        Users::UpdateService.new(self, user: self).execute(validate: false)\n      end",
    "comment": "Override Devise::Models::Trackable#update_tracked_fields! to limit database writes to at most once every hour rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6228"
  },
  {
    "raw_code": "def dashboard=(value)\n    if should_use_flipped_dashboard_mapping_for_rollout?\n      numeric_value = dashboard_enum_mapping[value.to_s]\n      super(numeric_value)\n    else\n      super(value)\n    end",
    "comment": "Override enum setter for `dashboard` to support flipped mapping for rollout",
    "label": "",
    "id": "6229"
  },
  {
    "raw_code": "def effective_dashboard_for_routing\n    return dashboard unless should_use_flipped_dashboard_mapping_for_rollout?\n\n    case dashboard\n    when 'projects'\n      'homepage'\n    when 'homepage'\n      'projects'\n    else\n      dashboard\n    end",
    "comment": "Returns the effective dashboard value for routing purposes For GitLab team members with feature flag enabled, flips homepage/projects values",
    "label": "",
    "id": "6230"
  },
  {
    "raw_code": "def should_use_flipped_dashboard_mapping_for_rollout?\n    Feature.enabled?(:personal_homepage, self)\n  end",
    "comment": "Determines if this user should use flipped dashboard enum mapping",
    "label": "",
    "id": "6231"
  },
  {
    "raw_code": "def owner_class_attribute_default; end\n\n    event :block do\n      transition active: :blocked\n      transition deactivated: :blocked\n      transition ldap_blocked: :blocked\n      transition blocked_pending_approval: :blocked\n    end",
    "comment": "state_machine uses this method at class loading time to fetch the default value for the `state` column but in doing so it also evaluates all other columns default values which could trigger the recursive generation of ApplicationSetting records. We're setting it to `nil` here because we don't have a database default for the `state` column. ",
    "label": "",
    "id": "6232"
  },
  {
    "raw_code": "def inactive_message\n    if blocked_pending_approval?\n      :blocked_pending_approval\n    elsif blocked?\n      :blocked\n    elsif internal?\n      :forbidden\n    else\n      super\n    end",
    "comment": "The messages for these keys are defined in `devise.en.yml`",
    "label": "",
    "id": "6233"
  },
  {
    "raw_code": "def self.limit_to_todo_authors(user: nil, with_todos: false, todo_state: nil)\n    if user && with_todos\n      where(id: Todo.where(user: user, state: todo_state).select(:author_id))\n    else\n      all\n    end",
    "comment": "Limits the users to those that have TODOs, optionally in the given state.  user - The user to get the todos for.  with_todos - If we should limit the result set to users that are the authors of todos.  todo_state - An optional state to require the todos to be in.",
    "label": "",
    "id": "6234"
  },
  {
    "raw_code": "def self.union_with_user(user_id = nil)\n    if user_id.present?\n      # We use \"unscoped\" here so that any inner conditions are not repeated for\n      # the outer query, which would be redundant.\n      User.unscoped.from_union([all, User.unscoped.where(id: user_id)])\n    else\n      all\n    end",
    "comment": "Returns a relation that optionally includes the given user.  user_id - The ID of the user to include.",
    "label": "",
    "id": "6235"
  },
  {
    "raw_code": "def password_length\n      Gitlab::CurrentSettings.minimum_password_length..Devise.password_length.max\n    end",
    "comment": "Devise method overridden to allow support for dynamic password lengths",
    "label": "",
    "id": "6236"
  },
  {
    "raw_code": "def random_password\n      Devise.friendly_token(password_length.max)\n    end",
    "comment": "Generate a random password that conforms to the current password length settings",
    "label": "",
    "id": "6237"
  },
  {
    "raw_code": "def find_for_database_authentication(warden_conditions)\n      conditions = warden_conditions.dup\n      if login = conditions.delete(:login)\n        where(conditions).find_by(\"lower(username) = :value OR lower(email) = :value\", value: login.downcase.strip)\n      else\n        find_by(conditions)\n      end",
    "comment": "Devise method overridden to allow sign in with email or username",
    "label": "",
    "id": "6238"
  },
  {
    "raw_code": "def find_by_any_email(email, confirmed: false)\n      return unless email\n\n      by_any_email(email, confirmed: confirmed).take\n    end",
    "comment": "Find a User by their primary email or any associated confirmed secondary email",
    "label": "",
    "id": "6239"
  },
  {
    "raw_code": "def by_any_email(emails, confirmed: false)\n      return none if Array(emails).all?(&:nil?)\n\n      from_users = by_user_email(emails)\n      from_users = from_users.confirmed if confirmed\n\n      from_emails = by_emails(emails).merge(Email.confirmed)\n      from_emails = from_emails.confirmed if confirmed\n\n      items = [from_users, from_emails]\n\n      # TODO: https://gitlab.com/gitlab-org/gitlab/-/issues/461885\n      # What about private commit emails with capitalized username, we'd never find them and\n      # since the private_commit_email derives from the username, it can\n      # be uppercase in parts. So we'll never find an existing user during the invite\n      # process by email if that is true as we are case sensitive in this case.\n      user_ids = Gitlab::PrivateCommitEmail.user_ids_for_emails(Array(emails).map(&:downcase))\n      items << where(id: user_ids) if user_ids.present?\n\n      from_union(items)\n    end",
    "comment": "Returns a relation containing all found users by their primary email or any associated confirmed secondary email  @param emails [String, Array<String>] email addresses to check @param confirmed [Boolean] Only return users where the primary email is confirmed",
    "label": "",
    "id": "6240"
  },
  {
    "raw_code": "def search(query, **options)\n      return none unless query.is_a?(String)\n\n      query = query&.delete_prefix('@')\n      return none if query.blank?\n\n      query = query.downcase\n\n      order = <<~SQL\n        CASE\n          WHEN LOWER(users.public_email) = :query THEN 0\n          WHEN LOWER(users.username) = :query THEN 1\n          WHEN LOWER(users.name) = :query THEN 2\n          ELSE 3\n        END\n      SQL\n\n      sanitized_order_sql = Arel.sql(sanitize_sql_array([order, { query: query }]))\n\n      use_minimum_char_limit = options[:use_minimum_char_limit]\n\n      scope =\n        if options[:with_private_emails]\n          with_primary_or_secondary_email(\n            query, use_minimum_char_limit: use_minimum_char_limit, partial_email_search: options[:partial_email_search]\n          )\n        else\n          with_public_email(query)\n        end",
    "comment": "Searches users matching the given query.  This method uses ILIKE on PostgreSQL.  query - The search query as a String with_private_emails - include private emails in search partial_email_search - only for admins to preserve email privacy. Only for self-managed instances.  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6241"
  },
  {
    "raw_code": "def gfm_autocomplete_search(query)\n      where(\n        \"REPLACE(users.name, ' ', '') ILIKE :pattern OR users.username ILIKE :pattern\",\n        pattern: \"%#{sanitize_sql_like(query)}%\"\n      ).order(\n        Arel.sql(sanitize_sql(\n          [\n            \"CASE WHEN REPLACE(users.name, ' ', '') ILIKE :prefix_pattern OR users.username ILIKE :prefix_pattern THEN 1 ELSE 2 END\",\n            { prefix_pattern: \"#{sanitize_sql_like(query)}%\" }\n          ]\n        )),\n        :username,\n        :id\n      )\n    end",
    "comment": "This should be kept in sync with the frontend filtering in https://gitlab.com/gitlab-org/gitlab/-/blob/5d34e3488faa3982d30d7207773991c1e0b6368a/app/assets/javascripts/gfm_auto_complete.js#L68 and https://gitlab.com/gitlab-org/gitlab/-/blob/5d34e3488faa3982d30d7207773991c1e0b6368a/app/assets/javascripts/gfm_auto_complete.js#L1053",
    "label": "",
    "id": "6242"
  },
  {
    "raw_code": "def where_not_in(users = nil)\n      users ? where.not(id: users) : all\n    end",
    "comment": "Limits the result set to users _not_ in the given query/list of IDs.  users - The list of users to ignore. This can be an `ActiveRecord::Relation`, or an Array.",
    "label": "",
    "id": "6243"
  },
  {
    "raw_code": "def search_by_name_or_username(query, use_minimum_char_limit: nil)\n      use_minimum_char_limit = user_search_minimum_char_limit if use_minimum_char_limit.nil?\n\n      where(\n        fuzzy_arel_match(:name, query, use_minimum_char_limit: use_minimum_char_limit)\n          .or(fuzzy_arel_match(:username, query, use_minimum_char_limit: use_minimum_char_limit))\n      )\n    end",
    "comment": "searches user by given pattern it compares name and username fields with given pattern This method uses ILIKE on PostgreSQL.",
    "label": "",
    "id": "6244"
  },
  {
    "raw_code": "def user_search_minimum_char_limit\n      true\n    end",
    "comment": "This method is overridden in JiHu. https://gitlab.com/gitlab-org/gitlab/-/issues/348509",
    "label": "",
    "id": "6245"
  },
  {
    "raw_code": "def find_by_ssh_key_id(key_id)\n      find_by('EXISTS (?)', Key.select(1).where('keys.user_id = users.id').auth.regular_keys.where(id: key_id))\n    end",
    "comment": "Returns a user for the given SSH key. Deploy keys are excluded.",
    "label": "",
    "id": "6246"
  },
  {
    "raw_code": "def reference_pattern\n      @reference_pattern ||=\n        %r{\n          (?<!\\w)\n          #{Regexp.escape(reference_prefix)}\n          (?<user>#{Gitlab::PathRegex::FULL_NAMESPACE_FORMAT_REGEX})\n        }x\n    end",
    "comment": "Pattern used to extract `@user` user references from text",
    "label": "",
    "id": "6247"
  },
  {
    "raw_code": "def single_user?\n      User.non_internal.limit(2).count == 1\n    end",
    "comment": "Return true if there is only single non-internal user in the deployment, ghost user is ignored.",
    "label": "",
    "id": "6248"
  },
  {
    "raw_code": "def full_path\n    username\n  end",
    "comment": " Instance methods ",
    "label": "",
    "id": "6249"
  },
  {
    "raw_code": "def valid_password?(password)\n    return false unless password_allowed?(password)\n    return false if password_automatically_set?\n    return false unless allow_password_authentication?\n\n    super\n  end",
    "comment": "Overwrites valid_password? from Devise::Models::DatabaseAuthenticatable In constant-time, check both that the password isn't on a denylist AND that the password is the user's password",
    "label": "",
    "id": "6250"
  },
  {
    "raw_code": "def remember_me!\n    super if ::Gitlab::Database.read_write? && ::Gitlab::CurrentSettings.allow_user_remember_me?\n  end",
    "comment": "Override Devise Rememberable#remember_me!  In Devise this method sets `remember_created_at` and writes the session token to the session cookie. When remember me is disabled this method ensures these values aren't set.",
    "label": "",
    "id": "6251"
  },
  {
    "raw_code": "def invalidate_all_remember_tokens!\n    return unless persisted?\n\n    self.remember_token = nil if respond_to?(:remember_token)\n    self.remember_created_at = nil\n    save(validate: false)\n  end",
    "comment": "This is a copy of #forget_me! without the check for `expire_all_remember_me_on_sign_out` https://github.com/heartcombo/devise/blob/v4.9.4/lib/devise/models/rememberable.rb#L58-L63  We need a separate method because we disabled that setting but we also need to be able to manually expire these tokens when a session is manually destroyed",
    "label": "",
    "id": "6252"
  },
  {
    "raw_code": "def remember_me?(token, generated_at)\n    return false unless ::Gitlab::CurrentSettings.allow_user_remember_me?\n\n    super\n  end",
    "comment": "Override Devise Rememberable#remember_me?  In Devise this method compares the remember me token received from the user session and compares to the stored value. When remember me is disabled this method ensures the upstream comparison does not happen.",
    "label": "",
    "id": "6253"
  },
  {
    "raw_code": "def will_save_change_to_login?\n    will_save_change_to_username? || will_save_change_to_email?\n  end",
    "comment": "will_save_change_to_attribute? is used by Devise to check if it is necessary to clear any existing reset_password_tokens before updating an authentication_key and login in our case is a virtual attribute to allow login by username or email.",
    "label": "",
    "id": "6254"
  },
  {
    "raw_code": "def check_for_verified_email\n    skip_reconfirmation! if emails.confirmed.where(email: self.email).any?\n  end",
    "comment": "see if the new email is already a verified secondary email",
    "label": "",
    "id": "6255"
  },
  {
    "raw_code": "def authorized_groups\n    Group.unscoped do\n      direct_groups_cte = Gitlab::SQL::CTE.new(:direct_groups, groups)\n      direct_groups_cte_alias = direct_groups_cte.table.alias(Group.table_name)\n\n      groups_from_authorized_projects = Group.id_in(authorized_projects.select(:namespace_id)).self_and_ancestors\n      groups_from_shares = Group.joins(:shared_with_group_links)\n                             .where(group_group_links: { shared_with_group_id: Group.from(direct_groups_cte_alias) })\n                             .self_and_descendants\n\n      Group\n        .with(direct_groups_cte.to_arel)\n        .from_union([\n          Group.from(direct_groups_cte_alias).self_and_descendants,\n          groups_from_authorized_projects,\n          groups_from_shares\n        ])\n    end",
    "comment": "Returns the groups a user has access to, either through direct or inherited membership or a project authorization",
    "label": "",
    "id": "6256"
  },
  {
    "raw_code": "def search_on_authorized_groups(query, use_minimum_char_limit: true)\n    authorized_groups_cte = Gitlab::SQL::CTE.new(:authorized_groups, authorized_groups)\n    authorized_groups_cte_alias = authorized_groups_cte.table.alias(Group.table_name)\n    Group\n      .with(authorized_groups_cte.to_arel)\n      .from(authorized_groups_cte_alias)\n      .search(query, use_minimum_char_limit: use_minimum_char_limit)\n  end",
    "comment": "Used to search on the user's authorized_groups effeciently by using a CTE",
    "label": "",
    "id": "6257"
  },
  {
    "raw_code": "def membership_groups\n    groups.self_and_descendants\n  end",
    "comment": "Returns the groups a user is a member of, either directly or through a parent group",
    "label": "",
    "id": "6258"
  },
  {
    "raw_code": "def all_expanded_groups\n    return groups if groups.empty?\n\n    Gitlab::ObjectHierarchy.new(groups).all_objects\n  end",
    "comment": "Returns a relation of groups the user has access to, including their parent and child groups (recursively).",
    "label": "",
    "id": "6259"
  },
  {
    "raw_code": "def refresh_authorized_projects(source: nil)\n    Users::RefreshAuthorizedProjectsService.new(self, source: source).execute\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6260"
  },
  {
    "raw_code": "def authorized_projects(min_access_level = nil)\n    # We're overriding an association, so explicitly call super with no\n    # arguments or it would be passed as `force_reload` to the association\n    projects = super()\n\n    if min_access_level\n      projects = projects\n        .where('project_authorizations.access_level >= ?', min_access_level)\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6261"
  },
  {
    "raw_code": "def authorizations_for_projects(min_access_level: nil, related_project_column: 'projects.id')\n    authorizations = project_authorizations\n                      .select(1)\n                      .where(\"project_authorizations.project_id = #{related_project_column}\")\n\n    return authorizations unless min_access_level.present?\n\n    authorizations.where('project_authorizations.access_level >= ?', min_access_level)\n  end",
    "comment": "Typically used in conjunction with projects table to get projects a user has been given access to. The param `related_project_column` is the column to compare to the project_authorizations. By default is projects.id  Example use: `Project.where('EXISTS(?)', user.authorizations_for_projects)`",
    "label": "",
    "id": "6262"
  },
  {
    "raw_code": "def projects_where_can_admin_issues\n    authorized_projects(Gitlab::Access::PLANNER).non_archived.with_issues_enabled\n  end",
    "comment": "Returns projects which user can admin issues on (for example to move an issue to that project).  This logic is duplicated from `Ability#project_abilities` into a SQL form.",
    "label": "",
    "id": "6263"
  },
  {
    "raw_code": "def require_ssh_key?\n    count = Users::KeysCountService.new(self).count\n\n    count == 0 && Gitlab::ProtocolAccess.allowed?('ssh')\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6264"
  },
  {
    "raw_code": "def require_password_creation_for_web?\n    allow_password_authentication_for_web? && password_automatically_set?\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6265"
  },
  {
    "raw_code": "def recent_push(project = nil)\n    service = Users::LastPushEventService.new(self)\n\n    if project\n      service.last_event_for_project(project)\n    else\n      service.last_event_for_user\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6266"
  },
  {
    "raw_code": "def namespace_id\n    namespace.try :id\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6267"
  },
  {
    "raw_code": "def solo_owned_organizations\n    ownerships_cte = Gitlab::SQL::CTE.new(:ownerships, organization_users.owners, materialized: false)\n\n    owned_orgs_from_cte = Organizations::Organization\n      .joins('INNER JOIN ownerships ON ownerships.organization_id = organizations.id')\n\n    counts = Organizations::OrganizationUser\n      .owners\n      .where('organization_users.organization_id = organizations.id')\n      .group(:organization_id)\n      .having('count(organization_users.user_id) = 1')\n\n    Organizations::Organization\n      .with(ownerships_cte.to_arel)\n      .from(owned_orgs_from_cte, :organizations)\n      .where_exists(counts)\n  end",
    "comment": "All organizations that are owned by this user, and only this user.",
    "label": "",
    "id": "6268"
  },
  {
    "raw_code": "def avatar_url(size: nil, scale: 2, **args)\n    GravatarService.new.execute(email, size, scale, username: username)\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6269"
  },
  {
    "raw_code": "def primary_email_verified?\n    return false unless confirmed? && !temp_oauth_email?\n\n    !email_changed? || new_record?\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6270"
  },
  {
    "raw_code": "def remove_key_cache\n    Users::KeysCountService.new(self).delete_cache\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6271"
  },
  {
    "raw_code": "def notification_service\n    NotificationService.new\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6272"
  },
  {
    "raw_code": "def log_info(message)\n    Gitlab::AppLogger.info message\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6273"
  },
  {
    "raw_code": "def system_hook_service\n    SystemHooksService.new\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6274"
  },
  {
    "raw_code": "def starred?(project)\n    starred_projects.exists?(project.id)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6275"
  },
  {
    "raw_code": "def contributed_projects\n    events = Event.select(:project_id)\n      .contributions.where(author_id: self)\n      .created_after(Time.current - 1.year)\n      .distinct\n      .reorder(nil)\n\n    Project.where(id: events).not_aimed_for_deletion\n  end",
    "comment": "Returns the projects a user contributed to in the last year.  This method relies on a subquery as this performs significantly better compared to a JOIN when coupled with, for example, `Project.visible_to_user`. That is, consider the following code:  some_user.contributed_projects.visible_to_user(other_user)  If this method were to use a JOIN the resulting query would take roughly 200 ms on a database with a similar size to GitLab.com's database. On the other hand, using a subquery means we can get the exact same data in about 40 ms.",
    "label": "",
    "id": "6276"
  },
  {
    "raw_code": "def can_be_removed?\n    return solo_owned_groups.none? && solo_owned_organizations.none? if Feature.enabled?(:ui_for_organizations)\n\n    solo_owned_groups.none?\n  end",
    "comment": "Returns true if the user can be removed, false otherwise. A user can be removed if they do not own any groups or organizations where they are the sole owner Method `none?` is used to ensure faster retrieval, See https://gitlab.com/gitlab-org/gitlab/-/issues/417105",
    "label": "",
    "id": "6277"
  },
  {
    "raw_code": "def ci_available_runners\n    Ci::Runner.from_union([ci_available_project_runners, ci_available_group_runners])\n  end",
    "comment": "Lists runners that are available to the user (group runners assigned to groups where the user has owner access to and project runners assigned to projects the user has maintainer access to)",
    "label": "",
    "id": "6278"
  },
  {
    "raw_code": "def global_notification_setting\n    return @global_notification_setting if defined?(@global_notification_setting)\n\n    # lookup in preloaded notification settings first, before making another query\n    if notification_settings.loaded?\n      @global_notification_setting = notification_settings.find do |notification|\n        notification.source_id.nil? && notification.source_type.nil?\n      end",
    "comment": "Lazy load global notification setting Initializes User setting with Participating level if setting not persisted",
    "label": "",
    "id": "6279"
  },
  {
    "raw_code": "def closest_non_global_group_notification_setting(group)\n    return unless group\n\n    notification_level = NotificationSetting.levels[:global]\n\n    if notification_settings.loaded?\n      group.self_and_ancestors_asc.find do |group|\n        notification_setting = notification_setting_find_by_source(group)\n\n        next unless notification_setting\n        next if NotificationSetting.levels[notification_setting&.level] == notification_level\n        break notification_setting if notification_setting.present?\n      end",
    "comment": "Returns the notification_setting of the lowest group in hierarchy with non global level",
    "label": "",
    "id": "6280"
  },
  {
    "raw_code": "def increment_failed_attempts!\n    return if ::Gitlab::Database.read_only?\n\n    increment_failed_attempts\n\n    if attempts_exceeded?\n      lock_access! unless access_locked?\n    else\n      Users::UpdateService.new(self, user: self).execute(validate: false)\n    end",
    "comment": "This is copied from Devise::Models::Lockable#valid_for_authentication?, as our auth flow means we don't call that automatically (and can't conveniently do so).  See: <https://github.com/plataformatec/devise/blob/v4.7.1/lib/devise/models/lockable.rb#L104>  rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6281"
  },
  {
    "raw_code": "def access_level\n    if admin?\n      :admin\n    else\n      :regular\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6282"
  },
  {
    "raw_code": "def feed_token\n    ensure_feed_token! unless Gitlab::CurrentSettings.disable_feed_token\n  end",
    "comment": "each existing user needs to have a `feed_token`. we do this on read since migrating all existing users is not a feasible solution.",
    "label": "",
    "id": "6283"
  },
  {
    "raw_code": "def static_object_token\n    ensure_static_object_token!\n  end",
    "comment": "Each existing user needs to have a `static_object_token`. We do this on read since migrating all existing users is not a feasible solution.",
    "label": "",
    "id": "6284"
  },
  {
    "raw_code": "def lock_access!(opts = {})\n    Gitlab::AppLogger.info(\"Account Locked: username=#{username}\")\n    audit_lock_access(reason: opts.delete(:reason))\n    super\n  end",
    "comment": "override, from Devise",
    "label": "",
    "id": "6285"
  },
  {
    "raw_code": "def unlock_access!(unlocked_by: self)\n    audit_unlock_access(author: unlocked_by)\n\n    super()\n  end",
    "comment": "override, from Devise",
    "label": "",
    "id": "6286"
  },
  {
    "raw_code": "def max_member_access_for_project_ids(project_ids)\n    Gitlab::SafeRequestLoader.execute(\n      resource_key: max_member_access_for_resource_key(Project),\n      resource_ids: project_ids,\n      default_value: Gitlab::Access::NO_ACCESS\n    ) do |project_ids|\n      project_authorizations.where(project: project_ids)\n                            .group(:project_id)\n                            .maximum(:access_level)\n    end",
    "comment": "Determine the maximum access level for a group of projects in bulk.  Returns a Hash mapping project ID -> maximum access level.",
    "label": "",
    "id": "6287"
  },
  {
    "raw_code": "def max_member_access_for_group_ids(group_ids)\n    Gitlab::SafeRequestLoader.execute(\n      resource_key: max_member_access_for_resource_key(Group),\n      resource_ids: group_ids,\n      default_value: Gitlab::Access::NO_ACCESS\n    ) do |group_ids|\n      group_members.where(source: group_ids).group(:source_id).maximum(:access_level)\n    end",
    "comment": "Determine the maximum access level for a group of groups in bulk.  Returns a Hash mapping project ID -> maximum access level.",
    "label": "",
    "id": "6288"
  },
  {
    "raw_code": "def user_preference\n    super.presence || build_user_preference\n  end",
    "comment": "Avoid migrations only building user preference object when needed.",
    "label": "",
    "id": "6289"
  },
  {
    "raw_code": "def current_highest_access_level\n    members.non_request.maximum(:access_level)\n  end",
    "comment": "Load the current highest access by looking directly at the user's memberships",
    "label": "",
    "id": "6290"
  },
  {
    "raw_code": "def ci_job_token_scope\n    Gitlab::SafeRequestStore[ci_job_token_scope_cache_key]\n  end",
    "comment": "This attribute hosts a Ci::JobToken::Scope object which is set when the user is authenticated successfully via CI_JOB_TOKEN.",
    "label": "",
    "id": "6291"
  },
  {
    "raw_code": "def support_pin_data\n    strong_memoize(:support_pin_data) do\n      Users::SupportPin::RetrieveService.new(self).execute\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6292"
  },
  {
    "raw_code": "def support_pin\n    support_pin_data&.fetch(:pin, nil)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6293"
  },
  {
    "raw_code": "def password_required?\n    return false if internal? || project_bot? || security_policy_bot? || placeholder?\n\n    super\n  end",
    "comment": "override, from Devise::Validatable",
    "label": "",
    "id": "6294"
  },
  {
    "raw_code": "def confirmation_period_valid?\n    return super if ::Gitlab::CurrentSettings.email_confirmation_setting_soft?\n\n    # Following devise logic for method, we want to return `true`\n    # See: https://github.com/heartcombo/devise/blob/ec0674523e7909579a5a008f16fb9fe0c3a71712/lib/devise/models/confirmable.rb#L191-L218\n    true\n  end",
    "comment": "override from Devise::Confirmable",
    "label": "",
    "id": "6295"
  },
  {
    "raw_code": "def consume_otp!\n    if self.consumed_timestep != current_otp_timestep\n      self.consumed_timestep = current_otp_timestep\n      return Gitlab::Database.read_only? ? true : save(validate: false)\n    end",
    "comment": "This is copied from Devise::Models::TwoFactorAuthenticatable#consume_otp!  An OTP cannot be used more than once in a given timestep Storing timestep of last valid OTP is sufficient to satisfy this requirement  See: <https://github.com/tinfoil/devise-two-factor/blob/master/lib/devise_two_factor/models/two_factor_authenticatable.rb#L66> ",
    "label": "",
    "id": "6296"
  },
  {
    "raw_code": "def add_primary_email_to_emails!\n    Emails::CreateService.new(self, user: self, email: self.email).execute(confirmed_at: self.confirmed_at)\n  end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6297"
  },
  {
    "raw_code": "def ci_project_ids_for_project_members(level)\n    project_members.where('access_level >= ?', level).pluck(:source_id)\n  end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6298"
  },
  {
    "raw_code": "def check_ldap_if_ldap_blocked!\n    return unless ::Gitlab::Auth::Ldap::Config.enabled? && ldap_blocked?\n\n    ::Gitlab::Auth::Ldap::Access.allowed?(self)\n  end",
    "comment": "An `ldap_blocked` user will be unblocked if LDAP indicates they are allowed.",
    "label": "",
    "id": "6299"
  },
  {
    "raw_code": "def audit_lock_access(reason: nil); end\n\n  # method overridden in EE\n  def audit_unlock_access(author: self); end\n\n  def username_not_assigned_to_pages_unique_domain\n    if ProjectSetting.unique_domain_exists?(username)\n      # We cannot disclose the Pages unique domain, hence returning generic error message\n      errors.add(:username, _('has already been taken'))\n    end\n  end\n\n  def groups_allowing_project_creation\n    Groups::AcceptingProjectCreationsFinder.new(self).execute\n  end\nend",
    "comment": "method overridden in EE",
    "label": "",
    "id": "6300"
  },
  {
    "raw_code": "def mark_upload_as_finished!\n    return unless file && file.exists?\n\n    update_column(:upload_ready, true)\n  end",
    "comment": "We have to mark the PagesDeployment upload as ready to ensure we only serve PagesDeployment which files are already uploaded.  This is required because we're uploading the file outside of the db transaction (https://gitlab.com/gitlab-org/gitlab/-/merge_requests/114774)",
    "label": "",
    "id": "6301"
  },
  {
    "raw_code": "def deactivate_deployments_with_same_path_prefix\n    project.pages_deployments.active.id_not_in(id).with_path_prefix(path_prefix).each(&:deactivate)\n  end",
    "comment": "Stop existing active deployment with same path when a deleted one is restored",
    "label": "",
    "id": "6302"
  },
  {
    "raw_code": "def reuse_previous_path?(file_entry)\n    file_entry[:file_path].blank? &&\n      EMPTY_FILE_PATTERN.match?(file_entry[:previous_path])\n  end",
    "comment": "If the user removed the file_path and the previous_path matches the EMPTY_FILE_PATTERN, we don't need to rename the file and build a new empty file name, we can just reuse the existing file name",
    "label": "",
    "id": "6303"
  },
  {
    "raw_code": "def action_name\n    if push_action?\n      push_action_name\n    elsif design?\n      design_action_names[action.to_sym]\n    elsif closed_action?\n      \"closed\"\n    elsif merged_action?\n      \"accepted\"\n    elsif joined_action?\n      'joined'\n    elsif left_action?\n      'left'\n    elsif expired_action?\n      'removed due to membership expiration from'\n    elsif destroyed_action?\n      'destroyed'\n    elsif commented_action?\n      \"commented on\"\n    elsif created_wiki_page?\n      'created'\n    elsif updated_wiki_page?\n      'updated'\n    elsif created_project_action?\n      created_project_action_name\n    elsif approved_action?\n      'approved'\n    else\n      \"opened\"\n    end",
    "comment": "rubocop: disable Metrics/CyclomaticComplexity rubocop: disable Metrics/PerceivedComplexity",
    "label": "",
    "id": "6304"
  },
  {
    "raw_code": "def target_iid\n    target.respond_to?(:iid) ? target.iid : target_id\n  end",
    "comment": "rubocop: enable Metrics/CyclomaticComplexity rubocop: enable Metrics/PerceivedComplexity",
    "label": "",
    "id": "6305"
  },
  {
    "raw_code": "def required_minimum_access_level_for_private_project(feature)\n      feature = ensure_feature!(feature)\n\n      PRIVATE_FEATURES_MIN_ACCESS_LEVEL_FOR_PRIVATE_PROJECT.fetch(feature) do\n        required_minimum_access_level(feature)\n      end",
    "comment": "Guest users can perform certain features on public and internal projects, but not private projects.",
    "label": "",
    "id": "6306"
  },
  {
    "raw_code": "def self.with_feature_available_for_user(feature, user)\n    visible = [ENABLED, PUBLIC]\n\n    if user&.can_read_all_resources?\n      with_feature_enabled(feature)\n    elsif user\n      min_access_level = required_minimum_access_level(feature)\n      column = quoted_access_level_column(feature)\n\n      where(\n        \"#{column} IS NULL OR #{column} IN (:public_visible) OR (#{column} = :private_visible AND EXISTS (:authorizations))\",\n        {\n          public_visible: visible,\n          private_visible: PRIVATE,\n          authorizations: user.authorizations_for_projects(min_access_level: min_access_level, related_project_column: 'project_features.project_id')\n        }\n      )\n    else\n      # This has to be added to include features whose value is nil in the db\n      visible << nil\n      with_feature_access_level(feature, visible)\n    end",
    "comment": "project features may be \"disabled\", \"internal\", \"enabled\" or \"public\". If \"internal\", they are only available to team members. This scope returns features where the feature is either public, enabled, or internal with permission for the user. Note: this scope doesn't enforce that the user has access to the projects, it just checks that the user has access to the feature. It's important to use this scope with others that checks project authorizations first (e.g. `filter_by_feature_visibility`).  This method uses an optimized version of `with_feature_access_level` for logged in users to more efficiently get private projects with the given feature.",
    "label": "",
    "id": "6307"
  },
  {
    "raw_code": "def public_pages?\n    return true unless Gitlab.config.pages.access_control\n\n    return false if ::Gitlab::Pages.access_control_is_forced?\n\n    pages_access_level == PUBLIC || (pages_access_level == ENABLED && project.public?)\n  end",
    "comment": "This method checks whether access control is enabled on project level, to include the access setting from ancestors, use project.public_pages?",
    "label": "",
    "id": "6308"
  },
  {
    "raw_code": "def repository_children_level\n    validator = ->(field) do\n      level = public_send(field) || ENABLED # rubocop:disable GitlabSecurity/PublicSend\n      not_allowed = level > repository_access_level\n      self.errors.add(field, \"cannot have higher visibility level than repository access level\") if not_allowed\n    end",
    "comment": "Validates builds and merge requests access level which cannot be higher than repository access level",
    "label": "",
    "id": "6309"
  },
  {
    "raw_code": "def search(query)\n      fuzzy_search(query, [:title, :description, :file_name])\n    end",
    "comment": "Searches for snippets with a matching title, description or file name.  This method uses ILIKE on PostgreSQL.  query - The search query as a String.  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6310"
  },
  {
    "raw_code": "def reference_pattern\n      @reference_pattern ||= %r{\n      (#{Project.reference_pattern})?\n      #{Regexp.escape(reference_prefix)}(?<snippet>\\d+)\n    }x\n    end",
    "comment": "Pattern used to extract `$123` snippet references from text  This pattern supports cross-project references.",
    "label": "",
    "id": "6311"
  },
  {
    "raw_code": "def path_to_repo\n    @path_to_repo ||=\n      begin\n        storage = Gitlab.config.repositories.storages[shard]\n\n        File.expand_path(\n          File.join(storage.legacy_disk_path, disk_path + '.git')\n        )\n      end",
    "comment": "Don't use this! It's going away. Use Gitaly to read or write from repos.",
    "label": "",
    "id": "6312"
  },
  {
    "raw_code": "def commit_by(oid:)\n    return @commit_cache[oid] if @commit_cache.key?(oid)\n\n    @commit_cache[oid] = find_commit(oid)\n  end",
    "comment": "Finding a commit by the passed SHA Also takes care of caching, based on the SHA",
    "label": "",
    "id": "6313"
  },
  {
    "raw_code": "def new_commits(newrev)\n    commits = raw.new_commits(newrev)\n\n    ::Commit.decorate(commits, container)\n  end",
    "comment": "Returns a list of commits that are not present in any reference",
    "label": "",
    "id": "6314"
  },
  {
    "raw_code": "def find_commits_by_message(query, ref = nil, path = nil, limit = 1000, offset = 0)\n    return [] unless exists? && has_visible_content? && query.present?\n\n    raw_commits = raw_repository.find_commits_by_message(query.strip, ref, path, limit, offset)\n    commits = raw_commits.map { |c| commit(c) }\n    CommitCollection.new(container, commits, ref)\n  end",
    "comment": "Gitaly migration: https://gitlab.com/gitlab-org/gitaly/issues/384",
    "label": "",
    "id": "6315"
  },
  {
    "raw_code": "def has_ambiguous_refs?\n    return false unless branch_names.present? && tag_names.present?\n\n    with_slash = []\n    no_slash = []\n    (branch_names + tag_names).each do |ref|\n      slash_index = ref.index('/')\n      if slash_index.present?\n        with_slash << ref.first(slash_index)\n      else\n        no_slash << ref\n      end",
    "comment": "It's possible for a tag name to be a prefix (including slash) of a branch name, or vice versa. For instance, a tag named `foo` means we can't create a tag `foo/bar`, but we _can_ create a branch `foo/bar`.  If we know a repository has no refs of this type (which is the common case) then separating refs from paths - as in ExtractsRef - can be faster.  This method only checks one level deep, so only prefixes that contain no slashes are considered. If a repository has a tag `foo/bar` and a branch `foo/bar/baz`, it will return false.",
    "label": "",
    "id": "6316"
  },
  {
    "raw_code": "def refresh_method_caches(types)\n    return if types.empty?\n\n    to_refresh = []\n\n    types.each do |type|\n      methods = METHOD_CACHES_FOR_FILE_TYPES[type.to_sym]\n\n      to_refresh.concat(Array(methods)) if methods\n    end",
    "comment": "Refreshes the method caches of this repository.  types - An Array of file types (e.g. `:readme`) used to refresh extra caches.",
    "label": "",
    "id": "6317"
  },
  {
    "raw_code": "def expire_emptiness_caches\n    return unless empty?\n\n    expire_method_caches(%i[has_visible_content?])\n    raw_repository.expire_has_local_branches_cache\n  end",
    "comment": "Expires the cache(s) used to determine if a repository is empty or not.",
    "label": "",
    "id": "6318"
  },
  {
    "raw_code": "def expire_content_cache\n    expire_tags_cache\n    expire_branches_cache\n    expire_root_ref_cache\n    expire_emptiness_caches\n    expire_exists_cache\n    expire_statistics_caches\n  end",
    "comment": "expire cache that doesn't depend on repository data (when expiring)",
    "label": "",
    "id": "6319"
  },
  {
    "raw_code": "def after_create\n    expire_status_cache\n\n    repository_event(:create_repository)\n  end",
    "comment": "Runs code after a repository has been created.",
    "label": "",
    "id": "6320"
  },
  {
    "raw_code": "def before_delete\n    expire_exists_cache\n    expire_all_method_caches\n    expire_branch_cache if exists?\n    expire_content_cache\n\n    repository_event(:remove_repository)\n  end",
    "comment": "Runs code just before a repository is deleted.",
    "label": "",
    "id": "6321"
  },
  {
    "raw_code": "def before_change_head\n    # Cached divergent commit counts are based on repository head\n    expire_branch_cache\n    expire_root_ref_cache\n\n    repository_event(:change_default_branch)\n  end",
    "comment": "Runs code just before the HEAD of a repository is changed.",
    "label": "",
    "id": "6322"
  },
  {
    "raw_code": "def before_push_tag\n    repository_event(:push_tag)\n  end",
    "comment": "Runs code before pushing (= creating or removing) a tag.  Note that this doesn't expire the tags. You may need to call expire_caches_for_tags or expire_tags_cache.",
    "label": "",
    "id": "6323"
  },
  {
    "raw_code": "def before_remove_tag\n    expire_caches_for_tags\n\n    repository_event(:remove_tag)\n  end",
    "comment": "Runs code before removing a tag.",
    "label": "",
    "id": "6324"
  },
  {
    "raw_code": "def after_remove_tag\n    expire_caches_for_tags\n  end",
    "comment": "Runs code after removing a tag.",
    "label": "",
    "id": "6325"
  },
  {
    "raw_code": "def after_change_head\n    expire_all_method_caches\n    container.after_repository_change_head\n  end",
    "comment": "Runs code after the HEAD of a repository is changed.",
    "label": "",
    "id": "6326"
  },
  {
    "raw_code": "def after_push_commit(branch_name)\n    expire_statistics_caches\n    expire_branch_cache(branch_name)\n\n    repository_event(:push_commit, branch: branch_name)\n  end",
    "comment": "Runs code after a new commit has been pushed.",
    "label": "",
    "id": "6327"
  },
  {
    "raw_code": "def after_create_branch(expire_cache: true)\n    expire_branches_cache if expire_cache\n\n    repository_event(:push_branch)\n  end",
    "comment": "Runs code after a new branch has been created.",
    "label": "",
    "id": "6328"
  },
  {
    "raw_code": "def before_remove_branch\n    expire_branches_cache\n\n    repository_event(:remove_branch)\n  end",
    "comment": "Runs code before removing an existing branch.",
    "label": "",
    "id": "6329"
  },
  {
    "raw_code": "def after_remove_branch(expire_cache: true)\n    if expire_cache\n      expire_branches_cache\n      expire_root_ref_cache\n    end",
    "comment": "Runs code after an existing branch has been removed.",
    "label": "",
    "id": "6330"
  },
  {
    "raw_code": "def blobs_at(items, blob_size_limit: Gitlab::Git::Blob::MAX_DATA_DISPLAY_SIZE)\n    return [] unless exists?\n\n    raw_repository.batch_blobs(items, blob_size_limit: blob_size_limit).map do |blob|\n      Blob.decorate(blob, container)\n    end",
    "comment": "items is an Array like: [[oid, path], [oid1, path1]]",
    "label": "",
    "id": "6331"
  },
  {
    "raw_code": "def exists?\n    return false unless full_path\n\n    raw_repository.exists?\n  end",
    "comment": "Gitaly migration: https://gitlab.com/gitlab-org/gitaly/issues/314",
    "label": "",
    "id": "6332"
  },
  {
    "raw_code": "def empty?\n    return true unless exists?\n\n    !has_visible_content?\n  end",
    "comment": "We don't need to cache the output of this method because both exists? and has_visible_content? are already memoized and cached. There's no guarantee that the values are expired and loaded atomically.",
    "label": "",
    "id": "6333"
  },
  {
    "raw_code": "def size\n    exists? ? raw_repository.size : 0.0\n  end",
    "comment": "The size of this repository in megabytes.",
    "label": "",
    "id": "6334"
  },
  {
    "raw_code": "def recent_objects_size\n    exists? ? raw_repository.recent_objects_size : 0.0\n  end",
    "comment": "The recent objects size of this repository in mebibytes.",
    "label": "",
    "id": "6335"
  },
  {
    "raw_code": "def issue_template_names_hash\n    Gitlab::Template::IssueTemplate.repository_template_names(project)\n  end",
    "comment": "store issue_template_names as hash",
    "label": "",
    "id": "6336"
  },
  {
    "raw_code": "def contributors(ref: nil, order_by: nil, sort: 'asc')\n    commits = self.commits(ref, limit: 2000, offset: 0, skip_merges: true)\n\n    commits = commits.group_by(&:author_email).map do |email, commits|\n      contributor = Gitlab::Contributor.new\n      contributor.email = email\n\n      commits.each do |commit|\n        if contributor.name.blank?\n          contributor.name = commit.author_name\n        end",
    "comment": "Params:  order_by: name|email|commits sort: asc|desc default: 'asc'",
    "label": "",
    "id": "6337"
  },
  {
    "raw_code": "def merged_branch_names(branch_names = [], include_identical: false)\n    # Currently we should skip caching if requesting all branch names\n    # This is only used in a few places, notably app/services/branches/delete_merged_service.rb,\n    # and it could potentially result in a very large cache.\n    return raw_repository.merged_branch_names(branch_names, include_identical: include_identical) if branch_names.empty? || include_identical\n\n    cache = redis_hash_cache\n\n    merged_branch_names_hash = cache.fetch_and_add_missing(:merged_branch_names, branch_names) do |missing_branch_names, hash|\n      merged = raw_repository.merged_branch_names(missing_branch_names, include_identical: include_identical)\n\n      missing_branch_names.each do |bn|\n        # Redis only stores strings in hset keys, use a fancy encoder\n        hash[bn] = Gitlab::Redis::Boolean.new(merged.include?(bn))\n      end",
    "comment": "If this method is not provided a set of branch names to check merge status, it fetches all branches.",
    "label": "",
    "id": "6338"
  },
  {
    "raw_code": "def self.pick_storage_shard(expire: true)\n    Gitlab::CurrentSettings.expire_current_application_settings if expire\n    Gitlab::CurrentSettings.pick_repository_storage\n  end",
    "comment": "Choose one of the available repository storage options based on a normalized weighted probability. We should always use the latest settings, to avoid picking a deleted shard.",
    "label": "",
    "id": "6339"
  },
  {
    "raw_code": "def adjust_containing_limit(limit:, exclude_refs:)\n    return limit if limit == 0\n\n    limit + exclude_refs.size\n  end",
    "comment": "Increase the limit by number of excluded refs to prevent a situation when we return less refs than requested",
    "label": "",
    "id": "6340"
  },
  {
    "raw_code": "def adjust_containing_refs(limit:, refs:)\n    return refs if limit == 0\n\n    refs.take(limit)\n  end",
    "comment": "Limit number of returned refs in case the result has more refs than requested",
    "label": "",
    "id": "6341"
  },
  {
    "raw_code": "def find_commit(oid_or_ref)\n    commit = if oid_or_ref.is_a?(Gitlab::Git::Commit)\n               oid_or_ref\n             else\n               Gitlab::Git::Commit.find(raw_repository, oid_or_ref)\n             end",
    "comment": "TODO Genericize finder, later split this on finders by Ref or Oid https://gitlab.com/gitlab-org/gitlab/issues/19877",
    "label": "",
    "id": "6342"
  },
  {
    "raw_code": "def initialize(wiki, page = nil)\n    @wiki       = wiki\n    @page       = page\n    @attributes = {}.with_indifferent_access\n\n    set_attributes if persisted?\n  end",
    "comment": "Construct a new WikiPage  @param [Wiki] wiki @param [Gitlab::Git::WikiPage] page",
    "label": "",
    "id": "6343"
  },
  {
    "raw_code": "def slug\n    attributes[:slug].presence || ::Wiki.preview_slug(title, format)\n  end",
    "comment": "The escaped URL path of this page.",
    "label": "",
    "id": "6344"
  },
  {
    "raw_code": "def title\n    attributes[:title] || ''\n  end",
    "comment": "The formatted title of this page.",
    "label": "",
    "id": "6345"
  },
  {
    "raw_code": "def title=(new_title)\n    attributes[:title] = new_title\n  end",
    "comment": "Sets the title of this page.",
    "label": "",
    "id": "6346"
  },
  {
    "raw_code": "def directory\n    wiki.page_title_and_dir(slug)&.last.to_s\n  end",
    "comment": "The hierarchy of the directory this page is contained in.",
    "label": "",
    "id": "6347"
  },
  {
    "raw_code": "def format\n    attributes[:format] || :markdown\n  end",
    "comment": "The markup format for the page.",
    "label": "",
    "id": "6348"
  },
  {
    "raw_code": "def message\n    version.try(:message)\n  end",
    "comment": "The commit message for this page version.",
    "label": "",
    "id": "6349"
  },
  {
    "raw_code": "def version\n    return unless persisted?\n\n    @version ||= @page.version || last_version\n  end",
    "comment": "The GitLab Commit instance for this page.",
    "label": "",
    "id": "6350"
  },
  {
    "raw_code": "def versions(options = {})\n    return [] unless persisted?\n\n    default_per_page = Kaminari.config.default_per_page\n    offset = [options[:page].to_i - 1, 0].max * options.fetch(:per_page, default_per_page)\n\n    wiki.repository.commits(\n      wiki.default_branch,\n      path: page.path,\n      limit: options.fetch(:limit, default_per_page),\n      offset: offset\n    )\n  end",
    "comment": "Returns a CommitCollection  Queries the commits for current page's path, equivalent to `git log path/to/page`. Filters and options supported: https://gitlab.com/gitlab-org/gitaly/-/blob/master/proto/commit.proto#L322-344",
    "label": "",
    "id": "6351"
  },
  {
    "raw_code": "def historical?\n    return false unless last_commit_sha && version\n\n    page.historical? && last_commit_sha != version.sha\n  end",
    "comment": "Returns boolean True or False if this instance is an old version of the page.",
    "label": "",
    "id": "6352"
  },
  {
    "raw_code": "def latest?\n    !historical?\n  end",
    "comment": "Returns boolean True or False if this instance is the latest commit version of the page.",
    "label": "",
    "id": "6353"
  },
  {
    "raw_code": "def persisted?\n    page.present?\n  end",
    "comment": "Returns boolean True or False if this instance has been fully created on disk or not.",
    "label": "",
    "id": "6354"
  },
  {
    "raw_code": "def create(attrs = {})\n    update_attributes(attrs)\n\n    save do\n      wiki.create_page(title, raw_content, format, attrs[:message])\n    end",
    "comment": "Creates a new Wiki Page.  attr - Hash of attributes to set on the new page. :title   - The title (optionally including dir) for the new page. :content - The raw markup content. :format  - Optional symbol representing the content format. Can be any type listed in the Wiki::VALID_USER_MARKUPS Hash. :message - Optional commit message to set on the new page.  Returns the String SHA1 of the newly created page or False if the save was unsuccessful.",
    "label": "",
    "id": "6355"
  },
  {
    "raw_code": "def update(attrs = {})\n    last_commit_sha = attrs.delete(:last_commit_sha)\n\n    if last_commit_sha && last_commit_sha != self.last_commit_sha\n      raise PageChangedError, s_(\n        'WikiPageConflictMessage|Someone edited the page the same time you did. Please check out %{wikiLinkStart}the page%{wikiLinkEnd} and make sure your changes will not unintentionally remove theirs.')\n    end",
    "comment": "Updates an existing Wiki Page, creating a new version.  attrs - Hash of attributes to be updated on the page. :content         - The raw markup content to replace the existing. :format          - Optional symbol representing the content format. See Wiki::VALID_USER_MARKUPS Hash for available formats. :message         - Optional commit message to set on the new version. :last_commit_sha - Optional last commit sha to validate the page unchanged. :title           - The Title (optionally including dir) to replace existing title  Returns the String SHA1 of the newly created page or False if the save was unsuccessful.",
    "label": "",
    "id": "6356"
  },
  {
    "raw_code": "def delete\n    if wiki.delete_page(page)\n      true\n    else\n      false\n    end",
    "comment": "Destroys the Wiki Page.  Returns boolean True or False.",
    "label": "",
    "id": "6357"
  },
  {
    "raw_code": "def to_partial_path\n    'shared/wikis/wiki_page'\n  end",
    "comment": "Relative path to the partial to be used when rendering collections of this object.",
    "label": "",
    "id": "6358"
  },
  {
    "raw_code": "def update_attributes(attrs)\n    attrs[:title] = process_title(attrs[:title]) if attrs[:title].present?\n    update_front_matter(attrs)\n\n    attrs.slice!(:content, :format, :message, :title)\n    clear_memoization(:parsed_content) if attrs.has_key?(:content)\n\n    attributes.merge!(attrs)\n  end",
    "comment": "Updates the current @attributes hash by merging a hash of params",
    "label": "",
    "id": "6359"
  },
  {
    "raw_code": "def process_title(title)\n    return if title.blank?\n\n    title = deep_title_squish(title)\n    current_dirname = File.dirname(title)\n\n    if persisted?\n      return title[1..] if current_dirname == '/'\n      return File.join([directory.presence, title].compact) if current_dirname == '.'\n    end",
    "comment": "Process and format the title based on the user input.",
    "label": "",
    "id": "6360"
  },
  {
    "raw_code": "def deep_title_squish(title)\n    components = title.split(File::SEPARATOR).map(&:squish)\n\n    File.join(components)\n  end",
    "comment": "This method squishes all the filename i.e: '   foo   /  bar  / page_name' => 'foo/bar/page_name'",
    "label": "",
    "id": "6361"
  },
  {
    "raw_code": "def latest_diff_file\n    strong_memoize(:latest_diff_file) do\n      next if for_design?\n\n      position.diff_file(repository)\n    end",
    "comment": "Returns the diff file from `position`",
    "label": "",
    "id": "6362"
  },
  {
    "raw_code": "def diff_file(create_missing_diff_file: true)\n    strong_memoize(:diff_file) do\n      next if for_design?\n\n      enqueue_diff_file_creation_job if create_missing_diff_file && should_create_diff_file?\n\n      fetch_diff_file\n    end",
    "comment": "Returns the diff file from `original_position`",
    "label": "",
    "id": "6363"
  },
  {
    "raw_code": "def supports_suggestion?\n    return false unless noteable&.supports_suggestion? && on_text?\n    # We don't want to trigger side-effects of `diff_file` call.\n    return false unless file = latest_diff_file\n    return false unless line = file.line_for_position(self.position)\n\n    line&.suggestible?\n  end",
    "comment": "Checks if the current `position` line in the diff exists and is suggestible (not a deletion).  Avoid using in iterations as it requests Gitaly.",
    "label": "",
    "id": "6364"
  },
  {
    "raw_code": "def owner_class_attribute_default\n        nil\n      end",
    "comment": "Supress warning: both enum and its state_machine have defined a different default for \"state\". State machine uses `nil` and the enum should use the same.",
    "label": "",
    "id": "6365"
  },
  {
    "raw_code": "def all_projects\n      Project.where(id: project.id)\n    end",
    "comment": "It's always 1 project but it has to be an AR relation",
    "label": "",
    "id": "6366"
  },
  {
    "raw_code": "def max_member_access_for_user(user, only_concrete_membership: false)\n      return Gitlab::Access::NO_ACCESS unless user\n\n      if !only_concrete_membership && (user.can_admin_all_resources? || user.can_admin_organization?(organization))\n        return Gitlab::Access::OWNER\n      end",
    "comment": "Return the highest access level for a user  A special case is handled here when the user is a GitLab admin which implies it has \"OWNER\" access everywhere, but should not officially appear as a member unless specifically added to it  @param user [User] @param only_concrete_membership [Bool] whether require admin concrete membership status",
    "label": "",
    "id": "6367"
  },
  {
    "raw_code": "def shortest_traversal_ids_prefixes\n          prefixes = []\n\n          # The array needs to be sorted (O(nlogn)) to ensure shortest elements are always first\n          # This allows to do O(n) search of shortest prefixes\n          all_traversal_ids = all.order('namespaces.traversal_ids').pluck('namespaces.traversal_ids')\n          last_prefix = [nil]\n\n          all_traversal_ids.each do |traversal_ids|\n            next if last_prefix == traversal_ids[0..(last_prefix.count - 1)]\n\n            last_prefix = traversal_ids\n            prefixes << traversal_ids\n          end",
    "comment": "This method looks into a list of namespaces trying to optimize a returned traversal_ids into a list of shortest prefixes, due to fact that the shortest prefixes include all children. Example: INPUT: [[4909902], [4909902,51065789], [4909902,51065793], [7135830], [15599674, 1], [15599674, 1, 3], [15599674, 2]] RESULT: [[4909902], [7135830], [15599674, 1], [15599674, 2]]",
    "label": "",
    "id": "6368"
  },
  {
    "raw_code": "def root_ancestor\n        strong_memoize(:root_ancestor) do\n          if parent_loaded_and_present?\n            parent.root_ancestor\n          elsif parent_id_present_and_traversal_ids_empty?\n            # Parent is in the database, so find our root ancestor using our parent's traversal_ids.\n            parent = Namespace.where(id: parent_id).select(:traversal_ids)\n            Namespace.from(\"(#{parent.to_sql}) AS parent_namespace, namespaces\")\n                     .find_by('namespaces.id = parent_namespace.traversal_ids[1]')\n          elsif parent_id.nil?\n            # There is no parent, so we are the root ancestor.\n            self\n          else\n            Namespace.find_by(id: traversal_ids.first)\n          end",
    "comment": "Return the top most ancestor of this namespace. This method aims to minimize the number of queries by trying to re-use data that has already been loaded.",
    "label": "",
    "id": "6369"
  },
  {
    "raw_code": "def ancestors_upto(top = nil, hierarchy_order: nil)\n        return super unless use_traversal_ids?\n\n        # We can't use a default value in the method definition above because\n        # we need to preserve those specific parameters for super.\n        hierarchy_order ||= :desc\n\n        top_index = ancestors_upto_top_index(top)\n        ids = traversal_ids[top_index...-1].reverse\n\n        # WITH ORDINALITY lets us order the result to match traversal_ids order.\n        ids_string = ids.map { |id| Integer(id) }.join(',')\n        from_sql = <<~SQL\n          unnest(ARRAY[#{ids_string}]::bigint[]) WITH ORDINALITY AS ancestors(id, ord)\n          INNER JOIN namespaces ON namespaces.id = ancestors.id\n        SQL\n\n        self.class\n          .from(Arel.sql(from_sql))\n          .order('ancestors.ord': hierarchy_order)\n      end",
    "comment": "Returns all ancestors up to but excluding the top. When no top is given, all ancestors are returned. When top is not found, returns all ancestors.  This copies the behavior of the recursive method. We will deprecate this behavior soon.",
    "label": "",
    "id": "6370"
  },
  {
    "raw_code": "def sync_traversal_ids\n        run_callbacks :sync_traversal_ids do\n          # Clear any previously memoized root_ancestor as our ancestors have changed.\n          clear_memoization(:root_ancestor)\n\n          Namespace::TraversalHierarchy.for_namespace(self).sync_traversal_ids!\n        end",
    "comment": "Update the traversal_ids for the full hierarchy.  NOTE: self.traversal_ids will be stale. Reload for a fresh record.",
    "label": "",
    "id": "6371"
  },
  {
    "raw_code": "def lock_both_roots\n        parent_ids = [\n          parent_id_was || self.id,\n          parent_id || self.id\n        ].compact\n\n        roots = Gitlab::ObjectHierarchy\n          .new(Namespace.id_in(parent_ids))\n          .base_and_ancestors\n          .without_order\n          .top_level\n\n        Namespace.lock('FOR NO KEY UPDATE').select(:id).id_in(roots).order(id: :asc).load\n      end",
    "comment": "Lock the root of the hierarchy we just left, and lock the root of the hierarchy we just joined. In most cases the two hierarchies will be the same.",
    "label": "",
    "id": "6372"
  },
  {
    "raw_code": "def lineage(top: nil, bottom: nil, hierarchy_order: nil, skope: self.class)\n        raise UnboundedSearch, 'Must bound search by either top or bottom' unless top || bottom\n\n        if top\n          skope = skope.where(\"traversal_ids @> ('{?}')\", top.id)\n        end",
    "comment": "Search this namespace's lineage. Bound inclusively by top node.",
    "label": "",
    "id": "6373"
  },
  {
    "raw_code": "def parent_loaded_and_present?\n        association(:parent).loaded? && parent.present?\n      end",
    "comment": "This case is possible when parent has not been persisted or we're inside a transaction.",
    "label": "",
    "id": "6374"
  },
  {
    "raw_code": "def parent_id_present_and_traversal_ids_empty?\n        parent_id.present? && traversal_ids.empty?\n      end",
    "comment": "This case occurs when parent is persisted but we are not.",
    "label": "",
    "id": "6375"
  },
  {
    "raw_code": "def scope_with_cached_ids(consistent_ids_scope, model, cached_ids_column)\n        # Look up the cached ids and unnest them into rows if the cache is up to date.\n        cache_lookup_query = Namespaces::Descendants\n          .where(outdated_at: nil, namespace_id: id)\n          .select(cached_ids_column.as('ids'))\n\n        # Invoke the consistent lookup query and collect the ids as a single array value\n        consistent_descendant_ids_scope = model\n          .from(consistent_ids_scope.arel.as(model.table_name))\n          .reselect(Arel::Nodes::NamedFunction.new('ARRAY_AGG', [model.arel_table[:id]]).as('ids'))\n          .unscope(where: :type)\n\n        from = <<~SQL\n        UNNEST(\n          COALESCE(\n            (SELECT ids FROM (#{cache_lookup_query.to_sql}) cached_query),\n            (SELECT ids FROM (#{consistent_descendant_ids_scope.to_sql}) consistent_query))\n        ) AS #{model.table_name}(id)\n        SQL\n\n        model\n          .from(from)\n          .unscope(where: :type)\n          .select(:id)\n      end",
    "comment": "This method implements an OR based cache lookup using COALESCE, similar what you would do in Ruby: return cheap_cached_data || expensive_uncached_data",
    "label": "",
    "id": "6376"
  },
  {
    "raw_code": "def self_and_hierarchy\n        object_hierarchy(self.class.where(id: id))\n          .all_objects\n      end",
    "comment": "Returns all ancestors, self, and descendants of the current namespace.",
    "label": "",
    "id": "6377"
  },
  {
    "raw_code": "def ancestors(hierarchy_order: nil, skope: self.class)\n        return skope.none unless parent_id\n\n        object_hierarchy(skope.where(id: parent_id))\n          .base_and_ancestors(hierarchy_order: hierarchy_order)\n      end",
    "comment": "Returns all the ancestors of the current namespaces.",
    "label": "",
    "id": "6378"
  },
  {
    "raw_code": "def ancestors_upto(top = nil, hierarchy_order: nil)\n        object_hierarchy(self.class.where(id: id))\n          .ancestors(upto: top, hierarchy_order: hierarchy_order)\n      end",
    "comment": "returns all ancestors upto but excluding the given namespace when no namespace is given, all ancestors upto the top are returned",
    "label": "",
    "id": "6379"
  },
  {
    "raw_code": "def descendants\n        object_hierarchy(self.class.where(parent_id: id)).base_and_descendants\n      end",
    "comment": "Returns all the descendants of the current namespace.",
    "label": "",
    "id": "6380"
  },
  {
    "raw_code": "def as_ids\n          select(Arel.sql('namespaces.traversal_ids[array_length(namespaces.traversal_ids, 1)]').as('id'))\n        end",
    "comment": "When filtering namespaces by the traversal_ids column to compile a list of namespace IDs, it can be faster to reference the ID in traversal_ids than the primary key ID column.",
    "label": "",
    "id": "6381"
  },
  {
    "raw_code": "def normal_select\n          unscoped.from(all, :namespaces)\n        end",
    "comment": "Produce a query of the form: SELECT * FROM namespaces;  When we have queries that break this SELECT * format we can run in to errors. For example `SELECT DISTINCT on(...)` will fail when we chain a `.count` c",
    "label": "",
    "id": "6382"
  },
  {
    "raw_code": "def self.merge_request_for_sha(project_id, sha)\n      MergeRequest\n        .joins(:generated_ref_commits)\n        .where(\n          merge_requests: { target_project_id: project_id },\n          generated_ref_commits: {\n            project_id: project_id,\n            commit_sha: sha\n          }\n        )\n        .limit(1)\n    end",
    "comment": "Since the MergeRequestsFinder expects us to provide a relation instead of a single object, we are returning here a relation, but limited to 1 since we only care about the merge request with the given commit sha and project id, which will be unique.",
    "label": "",
    "id": "6383"
  },
  {
    "raw_code": "def visibility_level_field\n      :visibility_level\n    end",
    "comment": "Required for Gitlab::VisibilityLevel module",
    "label": "",
    "id": "6384"
  },
  {
    "raw_code": "def scoped_paths?\n      Feature.enabled?(:organization_scoped_paths, self) && !default?\n    end",
    "comment": "For example: scoped path   - /o/my-org/my-group/my-project unscoped path - /my-group/my-project",
    "label": "",
    "id": "6385"
  },
  {
    "raw_code": "def check_visibility_level\n      max_group_level = root_groups.maximum(:visibility_level)\n      return unless max_group_level\n\n      return if visibility_level >= max_group_level\n\n      errors.add(:visibility_level, _(\"can not be more restrictive than group visibility levels\"))\n    end",
    "comment": "The visibility must be broader than the visibility of any contained root groups.",
    "label": "",
    "id": "6386"
  },
  {
    "raw_code": "def check_organization_reserved_name\n      return if default?\n      return unless Namespace.filter_by_path('o').top_level.exists?\n\n      errors.add(\n        :base,\n        _('Cannot create organization. The `o` namespace is a reserved path. ' \\\n          'Please rename the group or user before creating an organization.')\n      )\n    end",
    "comment": "The 'o' path is reserved as it's used for routing organization resources Example: /o/:organization_path",
    "label": "",
    "id": "6387"
  },
  {
    "raw_code": "def should_prevent_visibility_restriction?\n      default_group_visibility_changed? || restricted_visibility_levels_changed?\n    end",
    "comment": "This method is based off the ApplicationSetting method with the same name. This method is an optimization to perform visibility validation only when some fields have changed.",
    "label": "",
    "id": "6388"
  },
  {
    "raw_code": "def reference_prefix\n      '@'\n    end",
    "comment": "Referable methods should be the same as User",
    "label": "",
    "id": "6389"
  },
  {
    "raw_code": "def migrate_legacy_version!(data:, version:, build:)\n      current_file = latest_version.file.read\n      current_version = parse_serial(current_file) || (version - 1)\n\n      update!(versioning_enabled: true)\n\n      reload_latest_version.update!(version: current_version, file: CarrierWaveStringFile.new(current_file))\n      create_new_version!(data: data, version: version, build: build)\n    end",
    "comment": " If a Terraform state was created before versioning support was introduced, it will have a single version record whose file uses a legacy naming scheme in object storage. To update these states and versions to use the new behaviour, we must do the following when creating the next version:  * Read the current, non-versioned file from the old location. * Update the :versioning_enabled flag, which determines the naming scheme * Resave the existing file with the updated name and location, using a version number one prior to the new version * Create the new version as normal  This migration only needs to happen once for each state, from then on the state will behave as if it was always versioned.  The code can be removed in the next major version (14.0), after which any states that haven't been migrated will need to be recreated: https://gitlab.com/gitlab-org/gitlab/-/issues/258960",
    "label": "",
    "id": "6390"
  },
  {
    "raw_code": "def usage_data_event_for(tracking_action)\n      return unless originator\n      return unless TRACKABLE_ACTOR_EVENTS.include?(tracking_action)\n\n      \"#{EVENT_PREFIX}_#{tracking_action}_#{originator_suffix}\"\n    end",
    "comment": "counter name for unique user tracking (for MAU)",
    "label": "",
    "id": "6391"
  },
  {
    "raw_code": "def self.for_push_exists_for_projects_and_repository_paths(projects_repository_paths)\n        return none if projects_repository_paths.blank?\n\n        project_ids, repository_paths = projects_repository_paths.transpose\n\n        cte_query =\n          select('*').from(\n            sanitize_sql_array([\n              'unnest(ARRAY[:project_ids]::bigint[], ARRAY[:repository_paths]::text[]) ' \\\n                'AS projects_repository_paths(project_id, repository_path)',\n              { project_ids: project_ids, repository_paths: repository_paths }\n            ])\n          )\n\n        cte_name = :projects_repository_paths_cte\n        cte = Gitlab::SQL::CTE.new(cte_name, cte_query)\n\n        rules_cte_project_id = \"#{cte_name}.#{connection.quote_column_name('project_id')}\"\n        rules_cte_repository_path = \"#{cte_name}.#{connection.quote_column_name('repository_path')}\"\n\n        protection_rule_exsits_subquery =\n          select(1)\n            .where(\"#{rules_cte_project_id} = project_id\")\n            .where(\"#{rules_cte_repository_path} ILIKE #{::Gitlab::SQL::Glob.to_like('repository_path_pattern')}\")\n\n        query = select(\n          rules_cte_project_id,\n          rules_cte_repository_path,\n          sanitize_sql_array(['EXISTS(?) AS protected', protection_rule_exsits_subquery])\n        ).from(Arel.sql(cte_name.to_s))\n\n        connection.exec_query(query.with(cte.to_arel).to_sql)\n      end",
    "comment": " Accepts a list of projects and repository paths and returns a result set indicating whether the repository path is protected.  @param [Array<Array>] projects_repository_paths an array of arrays where each sub-array contains a project id and a repository path. @return [ActiveRecord::Result] a result set indicating whether each project and repository path is protected.  Example: ContainerRegistry::Protection::Rule.for_push_exists_for_projects_and_repository_paths([ [1, '/my_group/my_project_1/image_1'], [1, '/my_group/my_project_1/image_2'], [2, '/my_group/my_project_2/image_1'], ... ])  [ {'project_id' => 1, 'repository_path_pattern' => '/my_group/my_project_1/image_1', 'protected' => true}, {'project_id' => 1, 'repository_path_pattern' => '/my_group/my_project_1/image_2', 'protected' => false}, {'project_id' => 2, 'repository_path_pattern' => '/my_group/my_project_2/image_1', 'protected' => true}, ... ] ",
    "label": "",
    "id": "6392"
  },
  {
    "raw_code": "def render_error\n      if too_large?\n        :too_large\n      end",
    "comment": "This method is used on the server side to check whether we can attempt to render the diff_file at all. The human-readable error message can be retrieved by #render_error_message.",
    "label": "",
    "id": "6393"
  },
  {
    "raw_code": "def render_error\n      nil\n    end",
    "comment": "We can always render a static viewer, even if the diff is too large.",
    "label": "",
    "id": "6394"
  },
  {
    "raw_code": "def filepath_format_valid?\n      return if filepath.nil? # valid use case\n      return errors.add(:filepath, \"is too long (maximum is #{FILEPATH_MAX_LENGTH} characters)\") if filepath.length > FILEPATH_MAX_LENGTH\n\n      errors.add(:filepath, 'is in an invalid format') unless FILEPATH_REGEX.match? filepath\n    end",
    "comment": "we use a custom validator here to prevent running the regex if the string is too long see https://gitlab.com/gitlab-org/gitlab/-/issues/273771",
    "label": "",
    "id": "6395"
  },
  {
    "raw_code": "def summary\n      safe_summary = read_attribute(:summary)\n\n      safe_summary.dig('release', 'milestones')&.each do |milestone|\n        milestone.delete('issues')\n      end",
    "comment": " Return `summary` without sensitive information.  Removing issues from summary in order to prevent leaking confidential ones. See more https://gitlab.com/gitlab-org/gitlab/issues/121930",
    "label": "",
    "id": "6396"
  },
  {
    "raw_code": "def base_dir\n      namespace.full_path\n    end",
    "comment": "Base directory  @return [String] directory where repository is stored",
    "label": "",
    "id": "6397"
  },
  {
    "raw_code": "def disk_path\n      project.full_path\n    end",
    "comment": "Disk path is used to build repository and project's wiki path on disk  @return [String] combination of base_dir and the repository own name without `.git` or `.wiki.git` extensions",
    "label": "",
    "id": "6398"
  },
  {
    "raw_code": "def base_dir\n      \"#{@prefix}/#{disk_hash[0..1]}/#{disk_hash[2..3]}\" if disk_hash\n    end",
    "comment": "Base directory  @return [String] directory where repository is stored",
    "label": "",
    "id": "6399"
  },
  {
    "raw_code": "def disk_path\n      \"#{base_dir}/#{disk_hash}\" if disk_hash\n    end",
    "comment": "Disk path is used to build repository path on disk  @return [String] combination of base_dir and the repository own name without `.git`, `.wiki.git`, or any other extension",
    "label": "",
    "id": "6400"
  },
  {
    "raw_code": "def disk_hash\n      @disk_hash ||= Digest::SHA2.hexdigest(container.id.to_s) if container.id\n    end",
    "comment": "Generates the hash for the repository path and name on disk If you need to refer to the repository on disk, use the `#disk_path`",
    "label": "",
    "id": "6401"
  },
  {
    "raw_code": "def collect_commits\n      # https://gitlab.com/gitlab-org/gitlab-foss/issues/58013\n      Gitlab::GitalyClient.allow_n_plus_1_calls do\n        # Decorate with app/model/network/commit.rb\n        list_commits(count_to_display_commit_in_center).map do |commit|\n          Network::Commit.new(commit)\n        end",
    "comment": "Get commits from repository ",
    "label": "",
    "id": "6402"
  },
  {
    "raw_code": "def index_commits\n      days = []\n      @map = {}\n      @reserved = {}\n\n      @commits.each_with_index do |c, i|\n        c.time = i\n        days[i] = c.committed_date\n        @map[c.id] = c\n        @reserved[i] = []\n      end",
    "comment": "Method is adding time and space on the list of commits. As well as returns date list correlated with time set on commits.  @return [Array<TimeDate>] list of commit dates correlated with time on commits",
    "label": "",
    "id": "6403"
  },
  {
    "raw_code": "def count_to_display_commit_in_center\n      offset = -1\n      skip = 0\n      while offset == -1\n        tmp_commits = list_commits(skip)\n\n        if tmp_commits.present?\n          index = tmp_commits.index do |c|\n            c.id == @commit.id\n          end",
    "comment": "TODO: `skip` is not a valid parameter for `ListCommitsRequest` RPC Issue: https://gitlab.com/gitlab-org/gitlab/-/issues/481720 Skip count that the target commit is displayed in center.",
    "label": "",
    "id": "6404"
  },
  {
    "raw_code": "def place_chain(commit, parent_time = nil)\n      leaves = take_left_leaves(commit)\n      if leaves.empty?\n        return\n      end",
    "comment": "Add space mark on commit and its parents  @param [::Commit] the commit object.",
    "label": "",
    "id": "6405"
  },
  {
    "raw_code": "def take_left_leaves(raw_commit)\n      commit = @map[raw_commit.id]\n      leaves = []\n      leaves.push(commit) if commit.space == 0\n\n      loop do\n        return leaves if commit.parents(@map).count == 0\n\n        commit = commit.parents(@map).first\n\n        return leaves unless commit.space == 0\n\n        leaves.push(commit)\n      end",
    "comment": "Takes most left subtree branch of commits which don't have space mark yet.  @param [::Commit] the commit object.  @return [Array<Network::Commit>] list of branch commits",
    "label": "",
    "id": "6406"
  },
  {
    "raw_code": "def parsed_payload\n      strong_memoize(:parsed_payload) do\n        Gitlab::AlertManagement::Payload.parse(project, payload, monitoring_tool: monitoring_tool)\n      end",
    "comment": "Representation of the alert's payload. Avoid accessing #payload attribute directly.",
    "label": "",
    "id": "6407"
  },
  {
    "raw_code": "def prevent_token_assignment\n      if token.present? && token_changed?\n        self.token = nil\n        self.encrypted_token = encrypted_token_was\n        self.encrypted_token_iv = encrypted_token_iv_was\n      end",
    "comment": "Blank token assignment triggers token reset",
    "label": "",
    "id": "6408"
  },
  {
    "raw_code": "def self.update_by_partition(records)\n      records.group_by(&:partition).each do |partition, records_within_partition|\n        partitioned_scope = status_pending\n          .for_partition(partition)\n          .where(id: records_within_partition.map(&:id))\n\n        yield(partitioned_scope)\n      end",
    "comment": "Your scope must select_ref_and_identity before calling this method as it relies on partition being explicitly selected",
    "label": "",
    "id": "6409"
  },
  {
    "raw_code": "def self.search(query)\n    fuzzy_search(query, [:name, :description], use_minimum_char_limit: false)\n  end",
    "comment": "Searches for organizations with a matching name or description.  This method uses ILIKE on PostgreSQL  query - The search query as a String  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6410"
  },
  {
    "raw_code": "def self.search(query)\n    fuzzy_search(query, [:first_name, :last_name, :email, :description], use_minimum_char_limit: false)\n  end",
    "comment": "Searches for contacts with a matching first name, last name, email or description.  This method uses ILIKE on PostgreSQL  query - The search query as a String  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6411"
  },
  {
    "raw_code": "def authenticatable_salt\n    return super unless pbkdf2_password?\n\n    Devise::Pbkdf2Encryptable::Encryptors::Pbkdf2Sha512.split_digest(encrypted_password)[:salt]\n  end",
    "comment": "Use Devise DatabaseAuthenticatable#authenticatable_salt unless encrypted password is PBKDF2+SHA512.",
    "label": "",
    "id": "6412"
  },
  {
    "raw_code": "def valid_password?(password)\n    return false unless password_matches?(password)\n\n    migrate_password!(password)\n  end",
    "comment": "Called by Devise during database authentication. Also migrates the user password to the configured encryption type (BCrypt or PBKDF2+SHA512), if needed.",
    "label": "",
    "id": "6413"
  },
  {
    "raw_code": "def hash_this_password(password)\n    if Gitlab::FIPS.enabled?\n      Devise::Pbkdf2Encryptable::Encryptors::Pbkdf2Sha512.digest(\n        password,\n        Devise::Pbkdf2Encryptable::Encryptors::Pbkdf2Sha512::STRETCHES,\n        Devise.friendly_token(PBKDF2_SALT_LENGTH))\n    else\n      Devise::Encryptor.digest(self.class, password)\n    end",
    "comment": "Generates a hashed password for the configured encryption method (BCrypt or PBKDF2+SHA512). DOES NOT SAVE IT IN ANY WAY.",
    "label": "",
    "id": "6414"
  },
  {
    "raw_code": "def attr_mentionable(attr, options = {})\n      attr = attr.to_s\n      mentionable_attrs << [attr, options]\n    end",
    "comment": "Indicate which attributes of the Mentionable to search for GFM references.",
    "label": "",
    "id": "6415"
  },
  {
    "raw_code": "def gfm_reference(from = nil)\n    # \"MergeRequest\" > \"merge_request\" > \"Merge request\" > \"merge request\"\n    friendly_name = self.class.to_s.underscore.humanize.downcase\n\n    \"#{friendly_name} #{to_reference(from)}\"\n  end",
    "comment": "Returns the text used as the body of a Note when this object is referenced  By default this will be the class name and the result of calling `to_reference` on the object.",
    "label": "",
    "id": "6416"
  },
  {
    "raw_code": "def local_reference\n    self\n  end",
    "comment": "The GFM reference to this Mentionable, which shouldn't be included in its #references.",
    "label": "",
    "id": "6417"
  },
  {
    "raw_code": "def referenced_mentionables(current_user = self.author)\n    return [] unless matches_cross_reference_regex?\n\n    refs = all_references(current_user)\n\n    # We're using this method instead of Array diffing because that requires\n    # both of the object's `hash` values to be the same, which may not be the\n    # case for otherwise identical Commit objects.\n    extracted_mentionables(refs).reject { |ref| ref == local_reference }\n  end",
    "comment": "Extract GFM references to other Mentionables from this Mentionable. Always excludes its #local_reference.",
    "label": "",
    "id": "6418"
  },
  {
    "raw_code": "def matches_cross_reference_regex?\n    reference_pattern = if !project || project.default_issues_tracker?\n                          ReferenceRegexes.default_pattern\n                        else\n                          ReferenceRegexes.external_pattern\n                        end",
    "comment": "Uses regex to quickly determine if mentionables might be referenced Allows heavy processing to be skipped",
    "label": "",
    "id": "6419"
  },
  {
    "raw_code": "def create_cross_references!(author = self.author, without = [])\n    refs = referenced_mentionables(author)\n\n    # We're using this method instead of Array diffing because that requires\n    # both of the object's `hash` values to be the same, which may not be the\n    # case for otherwise identical Commit objects.\n    refs.reject! { |ref| without.include?(ref) || cross_reference_exists?(ref) }\n\n    refs.each do |ref|\n      SystemNoteService.cross_reference(ref, local_reference, author)\n    end",
    "comment": "Create a cross-reference Note for each GFM reference to another Mentionable found in the +mentionable_attrs+.",
    "label": "",
    "id": "6420"
  },
  {
    "raw_code": "def create_new_cross_references!(author = self.author)\n    changes = detect_mentionable_changes\n\n    return if changes.empty?\n\n    create_cross_references!(author)\n  end",
    "comment": "When a mentionable field is changed, creates cross-reference notes that don't already exist",
    "label": "",
    "id": "6421"
  },
  {
    "raw_code": "def user_mention_identifier\n    {\n      user_mention_association.foreign_key => id,\n      note_id: nil\n    }\n  end",
    "comment": "Identifier for the user mention that is parsed from model description rather then its related notes. Models that have a description attribute like Issue, MergeRequest, Epic, Snippet may have such a user mention. Other mentionable models like DesignManagement::Design, will never have such record as those do not have a description attribute.",
    "label": "",
    "id": "6422"
  },
  {
    "raw_code": "def detect_mentionable_changes\n    source = (changes.presence || previous_changes).dup\n\n    mentionable = self.class.mentionable_attrs.map { |attr, options| attr }\n\n    # Only include changed fields that are mentionable\n    source.select { |key, val| mentionable.include?(key) }\n  end",
    "comment": "Returns a Hash of changed mentionable fields  Preference is given to the `changes` Hash, but falls back to `previous_changes` if it's empty (i.e., the changes have already been persisted).  See ActiveModel::Dirty.  Returns a Hash.",
    "label": "",
    "id": "6423"
  },
  {
    "raw_code": "def cross_reference_exists?(target)\n    SystemNoteService.cross_reference_exists?(target, local_reference)\n  end",
    "comment": "Determine whether or not a cross-reference Note has already been created between this Mentionable and the specified target.",
    "label": "",
    "id": "6424"
  },
  {
    "raw_code": "def issues_finder_params\n    {}\n  end",
    "comment": "override in a class that includes this module to get a faster query from IssuesFinder",
    "label": "",
    "id": "6425"
  },
  {
    "raw_code": "def legacy_bulk_insert(table, rows, return_ids: false, disable_quote: [], on_conflict: nil)\n      return if rows.empty?\n\n      keys = rows.first.keys\n      columns = keys.map { |key| connection.quote_column_name(key) }\n\n      disable_quote = Array(disable_quote).to_set\n      tuples = rows.map do |row|\n        keys.map do |k|\n          disable_quote.include?(k) ? row[k] : connection.quote(row[k])\n        end",
    "comment": "Bulk inserts a number of rows into a table, optionally returning their IDs.  This method is deprecated, and you should use the BulkInsertSafe module instead.  table - The name of the table to insert the rows into. rows - An Array of Hash instances, each mapping the columns to their values. return_ids - When set to true the return value will be an Array of IDs of the inserted rows disable_quote - A key or an Array of keys to exclude from quoting (You become responsible for protection from SQL injection for these keys!) on_conflict - Defines an upsert. Values can be: :disabled (default) or :do_nothing",
    "label": "",
    "id": "6426"
  },
  {
    "raw_code": "def counter_attribute_after_commit(&callback)\n      after_commit_callbacks << callback\n    end",
    "comment": "perform registered callbacks after increments have been committed to the database",
    "label": "",
    "id": "6427"
  },
  {
    "raw_code": "def detect_race_on_record(log_fields: {})\n    # Ensure attributes is always an array before we log\n    log_fields[:attributes] = Array(log_fields[:attributes])\n\n    Gitlab::AppLogger.info(\n      message: 'Acquiring lease for project statistics update',\n      model: self.class.name,\n      model_id: id,\n      **parent_log_fields,\n      **log_fields,\n      **Gitlab::ApplicationContext.current\n    )\n\n    in_lock(database_lock_key, retries: 0) do\n      yield\n    end",
    "comment": "This method uses a lease to monitor access to the model row. This is needed to detect concurrent attempts to increment columns, which could result in a race condition.  As the purpose is to detect and warn concurrent attempts, it falls back to direct update on the row if it fails to obtain the lease.  It does not guarantee that there will not be any concurrent updates.",
    "label": "",
    "id": "6428"
  },
  {
    "raw_code": "def deployment_platform(environment: nil)\n    return unless self.namespace.certificate_based_clusters_enabled?\n\n    @deployment_platform ||= {}\n\n    @deployment_platform[environment] ||= find_deployment_platform(environment)\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "6429"
  },
  {
    "raw_code": "def confirmation_period_expired?\n    return false if skip_confirmation_period_expiry_check\n\n    super\n  end",
    "comment": "Override, from Devise::Models::Confirmable Link: https://github.com/heartcombo/devise/blob/main/lib/devise/models/confirmable.rb",
    "label": "",
    "id": "6430"
  },
  {
    "raw_code": "def supports_recaptcha?\n    false\n  end",
    "comment": "Override in Spammable if recaptcha is supported",
    "label": "",
    "id": "6431"
  },
  {
    "raw_code": "def render_recaptcha?\n    return false unless Gitlab::Recaptcha.enabled? && supports_recaptcha?\n\n    return false if self.errors.count > 1 # captcha should not be rendered if are still other errors\n\n    self.needs_recaptcha?\n  end",
    "comment": " Indicates if a recaptcha should be rendered before allowing this model to be saved. ",
    "label": "",
    "id": "6432"
  },
  {
    "raw_code": "def check_for_spam?(*)\n    spammable_attribute_changed?\n  end",
    "comment": "Override in included class if further checks are necessary",
    "label": "",
    "id": "6433"
  },
  {
    "raw_code": "def allow_possible_spam?(*)\n    Gitlab::CurrentSettings.allow_possible_spam\n  end",
    "comment": "Override in included class if you want to allow possible spam under specific circumstances",
    "label": "",
    "id": "6434"
  },
  {
    "raw_code": "def bulk_insert!(\n      items,\n      validate: true,\n      skip_duplicates: false,\n      returns: nil,\n      unique_by: nil,\n      batch_size: DEFAULT_BATCH_SIZE,\n      &handle_attributes\n    )\n      _bulk_insert_all!(\n        items,\n        validate: validate,\n        on_duplicate: skip_duplicates ? :skip : :raise,\n        returns: returns,\n        unique_by: unique_by,\n        batch_size: batch_size,\n        &handle_attributes\n      )\n    end",
    "comment": "Inserts the given ActiveRecord [items] to the table mapped to this class. Items will be inserted in batches of a given size, where insertion semantics are \"atomic across all batches\".  @param [Boolean] validate          Whether validations should run on [items] @param [Integer] batch_size        How many items should at most be inserted at once @param [Boolean] skip_duplicates   Marks duplicates as allowed, and skips inserting them @param [Symbol]  returns           Pass :ids to return an array with the primary key values for all inserted records or nil to omit the underlying RETURNING SQL clause entirely. @param [Symbol/Array] unique_by    Defines index or columns to use to consider item duplicate @param [Proc]    handle_attributes Block that will receive each item attribute hash prior to insertion for further processing  Unique indexes can be identified by columns or name: - unique_by: :isbn - unique_by: %i[ author_id name ] - unique_by: :index_books_on_isbn  Note that this method will throw on the following occasions: - [PrimaryKeySetError]            when primary keys are set on entities prior to insertion - [ActiveRecord::RecordInvalid]   on entity validation failures - [ActiveRecord::RecordNotUnique] on duplicate key errors  @return true if operation succeeded, throws otherwise. ",
    "label": "",
    "id": "6435"
  },
  {
    "raw_code": "def bulk_upsert!(\n      items,\n      unique_by:,\n      returns: nil,\n      validate: true,\n      batch_size: DEFAULT_BATCH_SIZE,\n      &handle_attributes\n    )\n      _bulk_insert_all!(\n        items,\n        validate: validate,\n        on_duplicate: :update,\n        returns: returns,\n        unique_by: unique_by,\n        batch_size: batch_size,\n        &handle_attributes\n      )\n    end",
    "comment": "Upserts the given ActiveRecord [items] to the table mapped to this class. Items will be inserted or updated in batches of a given size, where insertion semantics are \"atomic across all batches\".  @param [Boolean] validate          Whether validations should run on [items] @param [Integer] batch_size        How many items should at most be inserted at once @param [Symbol/Array] unique_by    Defines index or columns to use to consider item duplicate @param [Symbol]  returns           Pass :ids to return an array with the primary key values for all inserted or updated records or nil to omit the underlying RETURNING SQL clause entirely. @param [Proc]    handle_attributes Block that will receive each item attribute hash prior to insertion for further processing  Unique indexes can be identified by columns or name: - unique_by: :isbn - unique_by: %i[ author_id name ] - unique_by: :index_books_on_isbn  Note that this method will throw on the following occasions: - [PrimaryKeySetError]            when primary keys are set on entities prior to insertion - [ActiveRecord::RecordInvalid]   on entity validation failures - [ActiveRecord::RecordNotUnique] on duplicate key errors  @return true if operation succeeded, throws otherwise. ",
    "label": "",
    "id": "6436"
  },
  {
    "raw_code": "def _bulk_insert_saved_from_belongs_to?(name, args)\n      args.first == :before && args.second.to_s.start_with?('autosave_associated_records_for_')\n    end",
    "comment": "belongs_to associations will install a before_save hook during class loading",
    "label": "",
    "id": "6437"
  },
  {
    "raw_code": "def protected_branch_group\n      protected_branch.group\n    end",
    "comment": "We cannot delegate to :protected_branch here (even with allow_nil: true) like above because it results in 'undefined method `project_group_links' for nil:NilClass' errors.",
    "label": "",
    "id": "6438"
  },
  {
    "raw_code": "def iwhere(params)\n      criteria = self\n\n      params.each do |column, value|\n        column = arel_table[column] unless column.is_a?(Arel::Attribute)\n\n        criteria = case value\n                   when Array\n                     criteria.where(value_in(column, value))\n                   else\n                     criteria.where(value_equal(column, value))\n                   end",
    "comment": "Queries the given columns regardless of the casing used.  Unlike other ActiveRecord methods this method only operates on a Hash.",
    "label": "",
    "id": "6439"
  },
  {
    "raw_code": "def to_reference(_from = nil, full:)\n    ''\n  end",
    "comment": "Returns the String necessary to reference this object in Markdown  from - Referring parent object  This should be overridden by the including class.  Examples:  Issue.first.to_reference               # => \"#1\" Issue.last.to_reference(other_project) # => \"cross-project#1\"  Returns a String",
    "label": "",
    "id": "6440"
  },
  {
    "raw_code": "def to_reference_base(from = nil, full:, absolute_path: false)\n    to_reference(from, full: full)\n  end",
    "comment": "If this referable object can serve as the base for the reference of child objects (e.g. projects are the base of issues), but it is formatted differently, then you may wish to override this method.",
    "label": "",
    "id": "6441"
  },
  {
    "raw_code": "def reference_prefix\n      ''\n    end",
    "comment": "The character that prefixes the actual reference identifier  This should be overridden by the including class.  Examples:  Issue.reference_prefix        # => '#' MergeRequest.reference_prefix # => '!'  Returns a String",
    "label": "",
    "id": "6442"
  },
  {
    "raw_code": "def reference_pattern\n      raise NotImplementedError, \"#{self} does not implement #{__method__}\"\n    end",
    "comment": "Regexp pattern used to match references to this object  This must be overridden by the including class.  Returns a Regexp",
    "label": "",
    "id": "6443"
  },
  {
    "raw_code": "def order_votes(emoji_name, direction, base_class_name = base_class.name, awardable_id_column = :id)\n      awardable_table = self.arel_table\n      awards_table = AwardEmoji.arel_table\n\n      join_clause = awardable_table\n        .join(awards_table, Arel::Nodes::OuterJoin)\n        .on(awards_table[:awardable_id].eq(awardable_table[awardable_id_column])\n              .and(awards_table[:awardable_type].eq(base_class_name).and(awards_table[:name].eq(emoji_name))))\n        .join_sources\n\n      joins(join_clause).group(awardable_table[:id]).reorder(\n        Arel.sql(\"COUNT(award_emoji.id) #{direction}\")\n      )\n    end",
    "comment": "Order votes by emoji, optional sort order param `descending` defaults to true",
    "label": "",
    "id": "6444"
  },
  {
    "raw_code": "def inner_filter_query(user, opts = {})\n      award_emoji_table = Arel::Table.new('award_emoji')\n\n      emoji_name = opts[:name]\n      base_class_name = opts[:base_class_name] || base_class.name\n      awardable_id_column = opts[:awardable_id_column] || self.arel_table[:id]\n\n      inner_query =\n        award_emoji_table\n          .project('true')\n          .where(award_emoji_table[:user_id].eq(user.id))\n          .where(award_emoji_table[:awardable_type].eq(base_class_name))\n          .where(award_emoji_table[:awardable_id].eq(awardable_id_column))\n\n      inner_query.where(award_emoji_table[:name].eq(emoji_name)) if emoji_name.present?\n\n      inner_query\n    end",
    "comment": "Fragment used to build queries when filtering objects by award emoji",
    "label": "",
    "id": "6445"
  },
  {
    "raw_code": "def self.find_by_full_path(path, follow_redirects: false, route_scope: nil)\n    return unless path.present?\n\n    # Convert path to string to prevent DB error: function lower(integer) does not exist\n    path = path.to_s\n\n    # Case sensitive match first (it's cheaper and the usual case)\n    # If we didn't have an exact match, we perform a case insensitive search\n    #\n    # We need to qualify the columns with the table name, to support both direct lookups on\n    # Route/RedirectRoute, and scoped lookups through the Routable classes.\n    path_condition = { path: path }\n\n    source_type_condition = route_scope ? { source_type: route_scope.klass.base_class } : {}\n\n    route =\n      Route.where(source_type_condition).find_by(path_condition) ||\n      Route.where(source_type_condition).iwhere(path_condition).take\n\n    if follow_redirects\n      route ||= RedirectRoute.where(source_type_condition).iwhere(path_condition).take\n    end",
    "comment": "Finds a Routable object by its full path, without knowing the class.  Usage:  Routable.find_by_full_path('groupname')             # -> Group Routable.find_by_full_path('groupname/projectname') # -> Project  Returns a single object, or nil.",
    "label": "",
    "id": "6446"
  },
  {
    "raw_code": "def find_by_full_path(path, follow_redirects: false)\n      route_scope = all\n\n      Routable.find_by_full_path(\n        path,\n        follow_redirects: follow_redirects,\n        route_scope: route_scope\n      )\n    end",
    "comment": "Finds a single object by full path match in routes table.  Usage:  Klass.find_by_full_path('gitlab-org/gitlab-foss')  Returns a single object, or nil.",
    "label": "",
    "id": "6447"
  },
  {
    "raw_code": "def where_full_path_in(paths, preload_routes: true)\n      return none if paths.empty?\n\n      path_condition = paths.map do |path|\n        \"(LOWER(routes.path) = LOWER(#{connection.quote(path)}))\"\n      end.join(' OR ')\n\n      route_scope = all\n      source_type_condition = { source_type: route_scope.klass.base_class }\n\n      routes_matching_condition = Route\n        .where(source_type_condition)\n        .where(path_condition)\n\n      source_ids = routes_matching_condition.select(:source_id)\n      result = route_scope.where(id: source_ids)\n\n      if preload_routes\n        result.preload(:route)\n      else\n        result\n      end",
    "comment": "Builds a relation to find multiple objects by their full paths.  Usage:  Klass.where_full_path_in(%w{gitlab-org/gitlab-foss gitlab-org/gitlab})  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6448"
  },
  {
    "raw_code": "def parent_loaded?\n    association(:parent).loaded?\n  end",
    "comment": "Overriden in the Project model parent_id condition prevents issues with parent reassignment",
    "label": "",
    "id": "6449"
  },
  {
    "raw_code": "def owned_by?(user)\n    owner == user\n  end",
    "comment": "Group would override this to check from association",
    "label": "",
    "id": "6450"
  },
  {
    "raw_code": "def full_attribute(attribute)\n    attribute_from_route_or_self = ->(attribute) do\n      route&.public_send(attribute) || send(\"build_full_#{attribute}\")\n    end",
    "comment": "rubocop: disable GitlabSecurity/PublicSend",
    "label": "",
    "id": "6451"
  },
  {
    "raw_code": "def set_path_errors\n    route_path_errors = self.errors.delete(:\"route.path\")\n    route_path_errors&.each do |msg|\n      self.errors.add(:path, msg)\n    end",
    "comment": "rubocop: enable GitlabSecurity/PublicSend",
    "label": "",
    "id": "6452"
  },
  {
    "raw_code": "def select_project_owner_for_project_authorization\n      select([\"projects.id AS project_id\", \"#{Gitlab::Access::OWNER} AS access_level\"])\n    end",
    "comment": "workaround until we migrate Project#owners to have membership with OWNER access level",
    "label": "",
    "id": "6453"
  },
  {
    "raw_code": "def valid_attribute?(attribute)\n    self.errors.empty? || self.errors.messages[attribute].nil?\n  end",
    "comment": "Checks whether an attribute has failed validation or not  +attribute+ The symbolised name of the attribute i.e :name",
    "label": "",
    "id": "6454"
  },
  {
    "raw_code": "def feature_validation_exclusion\n    []\n  end",
    "comment": "Features that we should exclude from the validation",
    "label": "",
    "id": "6455"
  },
  {
    "raw_code": "def cascading_attr(*attributes)\n      attributes.map(&:to_sym).each do |attribute|\n        # public methods\n        define_attr_reader(attribute)\n        define_attr_writer(attribute)\n        define_lock_methods(attribute)\n\n        # private methods\n        define_validator_methods(attribute)\n        define_attr_before_save(attribute)\n\n        validate :\"#{attribute}_changeable?\"\n\n        before_save :\"before_save_#{attribute}\", if: -> { will_save_change_to_attribute?(attribute) }\n      end",
    "comment": "logic based on the cascading setting logic CascadingNamespaceSettingAttribute  Code remove for this module: - logic related to 'lock_#{attribute}', because projects don't need to lock attributes. - logic related to descendants, because projects don't have descendants. - logic related to a `nil` value for the setting, because the first/only cascading project setting (`duo_features_enabled`) has a db-level not nil constraint.",
    "label": "",
    "id": "6456"
  },
  {
    "raw_code": "def optionally_search(query = nil, **options)\n      query.present? ? search(query, **options) : all\n    end",
    "comment": "Optionally limits a result set to those matching the given search query.",
    "label": "",
    "id": "6457"
  },
  {
    "raw_code": "def define_set_operator(operator)\n    method_name = \"from_#{operator.name.demodulize.downcase}\"\n    method_name = method_name.to_sym\n\n    raise \"Trying to redefine method '#{method(method_name)}'\" if methods.include?(method_name)\n\n    define_method(method_name) do |*members, remove_duplicates: true, remove_order: true, alias_as: table_name|\n      members = flatten_ar_array(members)\n\n      operator_sql =\n        if members.any?\n          operator.new(members, remove_duplicates: remove_duplicates, remove_order: remove_order).to_sql\n        else\n          where(\"1=0\").to_sql\n        end",
    "comment": "Define a high level method to more easily work with the SQL set operations of UNION, INTERSECT, and EXCEPT as defined by Gitlab::SQL::Union, Gitlab::SQL::Intersect, and Gitlab::SQL::Except respectively.",
    "label": "",
    "id": "6458"
  },
  {
    "raw_code": "def spend_time(options)\n    @time_spent = options[:duration]\n    @time_spent_note_id = options[:note_id]\n    @time_spent_user = User.find(options[:user_id])\n    @spent_at = options[:spent_at]\n    @summary = options[:summary]\n    @original_total_time_spent = nil\n    @category_id = category_id(options[:category])\n\n    return if @time_spent == 0\n\n    @timelog = if @time_spent == :reset\n                 reset_spent_time\n               else\n                 add_or_subtract_spent_time\n               end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "6459"
  },
  {
    "raw_code": "def total_time_spent\n    sum = timelogs.sum(:time_spent)\n\n    # A new restriction has been introduced to limit total time spent to -\n    # Timelog::MAX_TOTAL_TIME_SPENT or 3.154e+7 seconds (approximately a year, a generous limit)\n    # Since there could be existing records that breach the limit, check and return the maximum/minimum allowed value.\n    # (some issuable might have total time spent that's negative because a validation was missing.)\n    sum.clamp(-Timelog::MAX_TOTAL_TIME_SPENT, Timelog::MAX_TOTAL_TIME_SPENT)\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "6460"
  },
  {
    "raw_code": "def add_or_subtract_spent_time\n    timelogs.new(\n      time_spent: time_spent,\n      note_id: @time_spent_note_id,\n      user: @time_spent_user,\n      spent_at: @spent_at,\n      summary: @summary,\n      timelog_category_id: @category_id\n    )\n  end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "6461"
  },
  {
    "raw_code": "def check_negative_time_spent\n    return if time_spent.nil? || time_spent == :reset\n\n    if time_spent < 0 && (time_spent.abs > original_total_time_spent)\n      errors.add(:base, _('Time to subtract exceeds the total time spent'))\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "6462"
  },
  {
    "raw_code": "def original_total_time_spent\n    @original_total_time_spent ||= total_time_spent\n  end",
    "comment": "we need to cache the total time spent so multiple calls to #valid? doesn't give a false error",
    "label": "",
    "id": "6463"
  },
  {
    "raw_code": "def set_repository_read_only!(skip_git_transfer_check: false)\n    with_lock do\n      raise RepositoryReadOnlyError, _('Git transfer in progress') if\n        !skip_git_transfer_check && git_transfer_in_progress?\n\n      raise RepositoryReadOnlyError, _('Repository already read-only') if\n        safe_read_repository_read_only_column\n\n      raise ActiveRecord::RecordNotSaved, _('Database update failed') unless\n        update_repository_read_only_column(true)\n\n      nil\n    end",
    "comment": "Tries to set repository as read_only, checking for existing Git transfers in progress beforehand. Setting a repository read-only will fail if it is already in that state.  It is assumed that `with_lock` is used here to ensure that no race condition appears between reading and writing the read-only column.  @return nil. Failures will raise an exception",
    "label": "",
    "id": "6464"
  },
  {
    "raw_code": "def set_repository_writable!\n    raise ActiveRecord::RecordNotSaved, _('Database update failed') unless\n      update_repository_read_only_column(false)\n  end",
    "comment": "Set repository as writable again. Unlike setting it read-only, this will succeed if the repository is already writable.",
    "label": "",
    "id": "6465"
  },
  {
    "raw_code": "def safe_read_repository_read_only_column\n    # This was added originally this way because of\n    # https://gitlab.com/gitlab-org/gitlab/-/commit/43f9b98302d3985312c9f8b66018e2835d8293d2\n    self.class.where(id: id).pick(:repository_read_only)\n  end",
    "comment": "Not all resources that can move repositories have the `repository_read_only` in their table, for example groups. We need these methods to override the behavior in those classes in order to access the column.",
    "label": "",
    "id": "6466"
  },
  {
    "raw_code": "def load_schema!\n      super\n\n      return if Rails.env.production?\n\n      sha_attribute_fields.each do |field|\n        column = columns_hash[field.to_s]\n\n        if column && column.type != :binary\n          raise ShaAttributeTypeMismatchError.new(column.name, column.type)\n        end",
    "comment": "This only gets executed in non-production environments as an additional check to ensure the column is the correct type.  In production it should behave like any other attribute. See https://gitlab.com/gitlab-org/gitlab/merge_requests/5502 for more discussion",
    "label": "",
    "id": "6467"
  },
  {
    "raw_code": "def try_replace_with_function_call(arel)\n      # The beginning of this method speculatively assumes that the arel passed in represents a query of the form\n      # SELECT <all columns> FROM <table> WHERE id = <number> generated by a rails find-by-primary-key style query\n      # As we rely on details of the arel tree, we return nil (meaning that we failed to replace with a function call)\n      # if the structure is not what we expected\n      return unless arel.is_a?(Arel::SelectManager)\n\n      ast = arel.ast\n      where_arel = ast.cores.first&.wheres&.first\n      return unless where_arel.is_a?(Arel::Nodes::Equality)\n\n      pk_value_attribute = where_arel.right # If this exists, it's the literal id side of WHERE <pk> = <literal id>\n      pk_value = pk_value_attribute&.value # This is the actual numeric value of the literal id\n      return unless pk_value\n\n      verification_arel = where(primary_key => pk_value).limit(1).arel\n      # Double check that the entire sql statement is what we expect it to be\n      # by reconstructing it from the extracted parts and verifying the same arel ast.\n      # If the arel of the original query wasn't SELECT <all columns> FROM <table> WHERE id = <number>\n      # we return here indicating that the arel could not be replaced with the function call\n\n      return unless verification_arel.ast == arel.ast\n\n      function_call = Arel::Nodes::NamedFunction.new(\"find_#{table_name}_by_id\", [pk_value_attribute]).as(table_name)\n      filter_empty_row = \"#{quoted_table_name}.#{connection.quote_column_name(primary_key)} IS NOT NULL\"\n\n      from(function_call).where(filter_empty_row).limit(1)\n    end",
    "comment": "Tries to replace an arel representation of a primary key lookup with an optimized function call.  Returns nil if the optimization was not possible Returns a relation (not arel!) if the optimization was successful This needs to take arel and return a relation because in one code path to transform rails is passing arel, and in another it's passing a relation. After the patch we need to return the same type, so depending on the patch location we call .arel on the return if we need arel back.",
    "label": "",
    "id": "6468"
  },
  {
    "raw_code": "def mount_file_store_uploader(uploader, skip_store_file: false, file_field: :file)\n      raise ArgumentError, \"file_field not allowed: #{file_field}\" unless ALLOWED_FILE_FIELDS.include?(file_field)\n\n      mount_uploader(file_field, uploader)\n\n      define_method(\"update_#{file_field}_store\") do\n        # The file.object_store is set during `uploader.store!` and `uploader.migrate!`\n        file_field_object_store = public_send(file_field).object_store # rubocop:disable GitlabSecurity/PublicSend\n        return if self[\"#{file_field}_store\"] == file_field_object_store # update only if necessary\n\n        update_column(\"#{file_field}_store\", file_field_object_store)\n      end",
    "comment": "When `skip_store_file: true` is used, the model MUST explicitly call `store_#{file_field}_now!`",
    "label": "",
    "id": "6469"
  },
  {
    "raw_code": "def available_link_types\n      [TYPE_RELATES_TO]\n    end",
    "comment": "Used to get the available types for the API overriden in EE",
    "label": "",
    "id": "6470"
  },
  {
    "raw_code": "def gap_size(context, gaps:, at_end:, starting_from:)\n      total_width = IDEAL_DISTANCE * gaps\n      size = if at_end && starting_from + total_width >= MAX_POSITION\n               (MAX_POSITION - starting_from) / gaps\n             elsif !at_end && starting_from - total_width <= MIN_POSITION\n               (starting_from - MIN_POSITION) / gaps\n             else\n               IDEAL_DISTANCE\n             end",
    "comment": "@api private",
    "label": "",
    "id": "6471"
  },
  {
    "raw_code": "def move_nulls(objects, at_end:)\n      objects = objects.reject(&:relative_position)\n      return 0 if objects.empty?\n\n      objects.first.check_repositioning_allowed!\n\n      number_of_gaps = objects.size # 1 to the nearest neighbour, and one between each\n      representative = RelativePositioning.mover.context(objects.first)\n\n      position = if at_end\n                   representative.max_relative_position\n                 else\n                   representative.min_relative_position\n                 end",
    "comment": "@api private @param [Array<RelativePositioning>] objects The objects to give positions to. The relative order will be preserved (i.e. when this method returns, objects.first.relative_position < objects.last.relative_position) @param [Boolean] at_end: The placement. If `true`, then all objects with `null` positions are placed _after_ all siblings with positions. If `false`, all objects with `null` positions are placed _before_ all siblings with positions. @returns [Number] The number of moved records.",
    "label": "",
    "id": "6472"
  },
  {
    "raw_code": "def check_repositioning_allowed!\n    nil\n  end",
    "comment": "To be overriden on child classes whenever blocking position updates is necessary.",
    "label": "",
    "id": "6473"
  },
  {
    "raw_code": "def update_relative_siblings(relation, range, delta)\n    relation\n      .where(relative_position: range)\n      .update_all(\"relative_position = relative_position + #{delta}\")\n  end",
    "comment": "This method is used during rebalancing - override it to customise the update logic:",
    "label": "",
    "id": "6474"
  },
  {
    "raw_code": "def exclude_self(relation, excluded: self)\n    relation.id_not_in(excluded.id)\n  end",
    "comment": "This method is used to exclude the current self (or another object) from a relation. Customize this if `id <> :id` is not sufficient",
    "label": "",
    "id": "6475"
  },
  {
    "raw_code": "def could_not_move(exception); end\n\n  # Override if the implementing class is not a simple application record, for\n  # example if the record is loaded from a union.\n  def reset_relative_position\n    reset.relative_position\n  end\n\n  # Override if the model class needs a more complicated computation (e.g. the\n  # object is a member of a union).\n  def model_class\n    self.class\n  end\nend",
    "comment": "Override if you want to be notified of failures to move",
    "label": "",
    "id": "6476"
  },
  {
    "raw_code": "def hierarchy(hierarchy_top = nil, preloaded = nil, opts = {})\n    preloaded ||= ancestors_upto(hierarchy_top)\n    expand_hierarchy_for_child(self, self, hierarchy_top, preloaded, opts)\n  end",
    "comment": "Returns the hierarchy of a project or group in the from of a hash upto a given top.  Options: upto_preloaded_ancestors_only: boolean - When `true`, the hierarchy expansions stops at the highest level preloaded ancestor. The hierarchy isn't guaranteed to reach the `hierarchy_top`.  > project.hierarchy => { parent_group => { child_group => project } }",
    "label": "",
    "id": "6477"
  },
  {
    "raw_code": "def self.build_hierarchy(descendants, hierarchy_top = nil, opts = {})\n    descendants = Array.wrap(descendants).uniq\n    return [] if descendants.empty?\n\n    unless descendants.all?(GroupDescendant)\n      raise ArgumentError, _('element is not a hierarchy')\n    end",
    "comment": "Merges all hierarchies of the given groups or projects into an array of hashes. All ancestors need to be loaded into the given `descendants` to avoid queries down the line.  Options: upto_preloaded_ancestors_only: boolean - When `true`, the hierarchy expansions stops at the highest level preloaded ancestor. The hierarchy isn't guaranteed to reach the `hierarchy_top`.  > GroupDescendant.merge_hierarchy([project, child_group, child_group2, parent]) => { parent => [{ child_group => project}, child_group2] }",
    "label": "",
    "id": "6478"
  },
  {
    "raw_code": "def safe_url(allowed_usernames: [])\n    return if url.nil?\n\n    escaped = Addressable::URI.escape(url)\n    uri = URI.parse(escaped)\n    uri.password = '*****' if uri.password\n    uri.user = '*****' if uri.user && allowed_usernames.exclude?(uri.user)\n    Addressable::URI.unescape(uri.to_s)\n  rescue URI::Error, TypeError, Addressable::URI::InvalidURIError\n  end",
    "comment": "Return the URL with obfuscated userinfo and keeping it intact",
    "label": "",
    "id": "6479"
  },
  {
    "raw_code": "def allow_next_run_at_update?\n    true\n  end",
    "comment": "override this method to change the behavior",
    "label": "",
    "id": "6480"
  },
  {
    "raw_code": "def matching(ref_name, protected_refs: nil)\n      (protected_refs || all).select { |protected_ref| protected_ref.matches?(ref_name) }\n    end",
    "comment": "Returns all protected refs that match the given ref name. This checks all records from the scope built up so far, and does _not_ return a relation.  This method optionally takes in a list of `protected_refs` to search through, to avoid calling out to the database.",
    "label": "",
    "id": "6481"
  },
  {
    "raw_code": "def define_instance_internal_id_methods(scope, column, init)\n      define_method(\"ensure_#{scope}_#{column}!\") do\n        scope_value = internal_id_read_scope(scope)\n        value = read_attribute(column)\n        return value unless scope_value\n\n        if value.nil?\n          # We don't have a value yet and use a InternalId record to generate\n          # the next value.\n          value = InternalId.generate_next(\n            self,\n            internal_id_scope_attrs(scope),\n            internal_id_scope_usage,\n            init)\n          write_attribute(column, value)\n\n          @internal_id_set_manually = false\n        end",
    "comment": "Defines instance methods: - ensure_{scope}_{column}! - track_{scope}_{column}! - reset_{scope}_{column} - {column}=",
    "label": "",
    "id": "6482"
  },
  {
    "raw_code": "def define_singleton_internal_id_methods(scope, column, init)\n      define_singleton_method(\"with_#{scope}_#{column}_supply\") do |scope_value, &block|\n        subject = find_by(scope => scope_value) || self\n        scope_attrs = ::AtomicInternalId.scope_attrs(scope_value)\n        usage = ::AtomicInternalId.scope_usage(self)\n\n        supply = Supply.new(-> { InternalId.generate_next(subject, scope_attrs, usage, init) })\n        block.call(supply)\n      end",
    "comment": "Defines class methods:  - with_{scope}_{column}_supply This method can be used to allocate a stream of IID values during bulk operations (importing/copying, etc).  Pass in a block that receives a `Supply` instance. To allocate a new IID value, call `Supply#next_value`.  Example:  MyClass.with_project_iid_supply(project) do |supply| attributes = MyClass.where(project: project).find_each do |record| record.attributes.merge(iid: supply.next_value) end  bulk_insert(attributes) end  - track_#{scope}_#{column}! This method can be used to set a new greatest IID value during import operations.  Example:  MyClass.track_project_iid!(project, value)",
    "label": "",
    "id": "6483"
  },
  {
    "raw_code": "def subscribed_without_subscriptions?(user, project)\n    false\n  end",
    "comment": "Override this method to define custom logic to consider a subscribable as subscribed without an explicit subscription record.",
    "label": "",
    "id": "6484"
  },
  {
    "raw_code": "def fast_destroy_all\n      params = begin_fast_destroy\n\n      delete_all\n\n      finalize_fast_destroy(params)\n    end",
    "comment": " This method delete rows and associated external data efficiently  This method can replace `destroy` and `destroy_all` without having `after_destroy` hook",
    "label": "",
    "id": "6485"
  },
  {
    "raw_code": "def begin_fast_destroy\n      raise NotImplementedError\n    end",
    "comment": " This method returns identifiers to delete associated external data (e.g. file paths, redis keys)  This method must be defined in fast destroyable model",
    "label": "",
    "id": "6486"
  },
  {
    "raw_code": "def finalize_fast_destroy(params)\n      raise NotImplementedError\n    end",
    "comment": " This method deletes associated external data with the identifiers returned by `begin_fast_destroy`  This method must be defined in fast destroyable model",
    "label": "",
    "id": "6487"
  },
  {
    "raw_code": "def use_fast_destroy(relation, opts = {})\n        set_callback :destroy, :before, opts.merge(prepend: true) do\n          perform_fast_destroy(public_send(relation)) # rubocop:disable GitlabSecurity/PublicSend\n        end",
    "comment": " This method is to be defined on models which have fast destroyable models as children, and let us avoid to use `dependent: :destroy` hook",
    "label": "",
    "id": "6488"
  },
  {
    "raw_code": "def lfs_http_url_to_repo(_operation = nil)\n    http_url_to_repo\n  end",
    "comment": "Is overridden in EE::Project for Geo support",
    "label": "",
    "id": "6489"
  },
  {
    "raw_code": "def wiki_enabled=(value)\n    write_feature_attribute_boolean(:wiki_access_level, value)\n  end",
    "comment": "TODO: remove in API v5, replaced by *_access_level",
    "label": "",
    "id": "6490"
  },
  {
    "raw_code": "def builds_enabled=(value)\n    write_feature_attribute_boolean(:builds_access_level, value)\n  end",
    "comment": "TODO: remove in API v5, replaced by *_access_level",
    "label": "",
    "id": "6491"
  },
  {
    "raw_code": "def merge_requests_enabled=(value)\n    write_feature_attribute_boolean(:merge_requests_access_level, value)\n  end",
    "comment": "TODO: remove in API v5, replaced by *_access_level",
    "label": "",
    "id": "6492"
  },
  {
    "raw_code": "def issues_enabled=(value)\n    write_feature_attribute_boolean(:issues_access_level, value)\n  end",
    "comment": "TODO: remove in API v5, replaced by *_access_level",
    "label": "",
    "id": "6493"
  },
  {
    "raw_code": "def snippets_enabled=(value)\n    write_feature_attribute_boolean(:snippets_access_level, value)\n  end",
    "comment": "TODO: remove in API v5, replaced by *_access_level",
    "label": "",
    "id": "6494"
  },
  {
    "raw_code": "def container_registry_enabled=(value)\n    write_feature_attribute_boolean(:container_registry_access_level, value)\n  end",
    "comment": "TODO: Remove this method after we drop support for project create/edit APIs to set the container_registry_enabled attribute. They can instead set the container_registry_access_level attribute.",
    "label": "",
    "id": "6495"
  },
  {
    "raw_code": "def update_namespace_statistics(namespace_statistics_name:, statistic_attribute: :size)\n      @namespace_statistics_name = namespace_statistics_name\n      @statistic_attribute = statistic_attribute\n\n      after_save(:schedule_namespace_statistics_refresh, if: :update_namespace_statistics?)\n      after_destroy(:schedule_namespace_statistics_refresh)\n    end",
    "comment": "Configure the model to update `namespace_statistics_name` on NamespaceStatistics, when `statistic_attribute` changes  - namespace_statistics_name: A column of `NamespaceStatistics` to update - statistic_attribute: An attribute of the current model, default to `size`",
    "label": "",
    "id": "6496"
  },
  {
    "raw_code": "def with_bulk_insert(enabled: true)\n      previous = bulk_inserts_enabled?\n      Thread.current['bulk_inserts_enabled'] = enabled\n      yield\n    ensure\n      Thread.current['bulk_inserts_enabled'] = previous\n    end",
    "comment": "All associations that are [BulkInsertSafe] and that as a result of calls to [save] or [save!] would be written to the database, will be inserted using [bulk_insert!] instead.  Note that this will only work for entities that have not been persisted yet.  @param [Boolean] enabled When [true], bulk-inserts will be attempted within the given block. If [false], bulk-inserts will be disabled. This behavior can be nested.",
    "label": "",
    "id": "6497"
  },
  {
    "raw_code": "def serializable_hash(_opts = {})\n      { title: title, name: name, id: id }\n    end",
    "comment": "Ensure these models match the interface required for exporting",
    "label": "",
    "id": "6498"
  },
  {
    "raw_code": "def search(query)\n      fuzzy_search(query, [:title, :description])\n    end",
    "comment": "Searches for timeboxes with a matching title or description.  This method uses ILIKE on PostgreSQL  query - The search query as a String  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6499"
  },
  {
    "raw_code": "def managed_prometheus?\n      false\n    end",
    "comment": "Overridden in app/models/clusters/applications/prometheus.rb",
    "label": "",
    "id": "6500"
  },
  {
    "raw_code": "def configured?\n      raise NotImplementedError\n    end",
    "comment": "This is a light-weight check if a prometheus client is properly configured.",
    "label": "",
    "id": "6501"
  },
  {
    "raw_code": "def can_query?\n      prometheus_client.present?\n    end",
    "comment": "This is a heavy-weight check if a prometheus is properly configured and accessible from GitLab. This actually sends a request to an external service and often it could take a long time, Please consider using `configured?` instead if the process is running on Puma threads.",
    "label": "",
    "id": "6502"
  },
  {
    "raw_code": "def calculate_reactive_cache(query_class_name, *args)\n      return unless prometheus_client\n\n      data = Object.const_get(query_class_name, false).new(prometheus_client).query(*args)\n      {\n        success: true,\n        data: data,\n        last_update: Time.current.utc\n      }\n    rescue Gitlab::PrometheusClient::Error => e\n      { success: false, result: e.message }\n    end",
    "comment": "Cache metrics for specific environment",
    "label": "",
    "id": "6503"
  },
  {
    "raw_code": "def parent_ids\n    []\n  end",
    "comment": "We don't save these, because they would need a table or a serialized field. They aren't used anywhere, so just pretend the commit has no parents.",
    "label": "",
    "id": "6504"
  },
  {
    "raw_code": "def referenced_by\n    []\n  end",
    "comment": "These are not saved",
    "label": "",
    "id": "6505"
  },
  {
    "raw_code": "def loose_index_scan(column:, order: :asc)\n      arel_table = self.arel_table\n\n      # Handle different column types\n      arel_column, column_alias, column_for_select = extract_column_and_alias_and_select(column, arel_table)\n\n      cte = Gitlab::SQL::RecursiveCTE.new(:loose_index_scan_cte, union_args: { remove_order: false })\n\n      cte_query = except(:select)\n        .select(column_for_select)\n        .order(column_alias => order)\n        .limit(1)\n\n      inner_query = except(:select)\n\n      cte_query, inner_query = yield([cte_query, inner_query]) if block_given?\n      cte << cte_query\n\n      inner_query = if order == :asc\n                      inner_query.where(arel_column.gt(cte.table[column_alias]))\n                    else\n                      inner_query.where(arel_column.lt(cte.table[column_alias]))\n                    end",
    "comment": "Builds a recursive query to read distinct values from a column.  Example 1: collect all distinct author ids for the `issues` table  Bad: The DB reads all issues, sorts and dedups them in memory  > Issue.select(:author_id).distinct.map(&:author_id)  Good: Use loose index scan (skip index scan)  > Issue.loose_index_scan(column: :author_id).map(&:author_id)  Example 2: List of users for the DONE todos selector. Select all users who created a todo.  Bad: Loads all DONE todos for the given user and extracts the author_ids  > User.where(id: Todo.where(user_id: 4156052).done.select(:author_id))  Good: Loads distinct author_ids from todos and then loads users  > distinct_authors = Todo.where(user_id: 4156052).done.loose_index_scan(column: :author_id).select(:author_id) > User.where(id: distinct_authors)",
    "label": "",
    "id": "6506"
  },
  {
    "raw_code": "def update_project_statistics(project_statistics_name:, statistic_attribute: :size)\n      @project_statistics_name = project_statistics_name\n      @statistic_attribute = statistic_attribute\n\n      after_save(:update_project_statistics_after_save, if: :update_project_statistics_after_save?)\n      after_destroy(:update_project_statistics_after_destroy, if: :update_project_statistics_after_destroy?)\n    end",
    "comment": "Configure the model to update `project_statistics_name` on ProjectStatistics, when `statistic_attribute` changes  - project_statistics_name: A column of `ProjectStatistics` to update - statistic_attribute: An attribute of the current model, default to `size`",
    "label": "",
    "id": "6507"
  },
  {
    "raw_code": "def available_features_for_issue_types\n      {\n        assignee: %w[issue incident],\n        confidentiality: %w[issue incident objective key_result task],\n        time_tracking: %w[issue incident],\n        move_and_clone: %w[issue incident]\n      }.with_indifferent_access\n    end",
    "comment": "EE only features are listed on EE::IssueAvailableFeatures",
    "label": "",
    "id": "6508"
  },
  {
    "raw_code": "def update_highest_role\n    return unless update_highest_role?\n\n    run_after_commit_or_now do\n      lease_key = \"update_highest_role:#{update_highest_role_attribute}\"\n      lease = Gitlab::ExclusiveLease.new(lease_key, timeout: HIGHEST_ROLE_LEASE_TIMEOUT)\n\n      if lease.try_obtain\n        UpdateHighestRoleWorker.perform_in(HIGHEST_ROLE_JOB_DELAY, update_highest_role_attribute)\n      else\n        # use same logging as ExclusiveLeaseGuard\n        Gitlab::AppLogger.error('Cannot obtain an exclusive lease. There must be another instance already in execution.')\n      end",
    "comment": "Schedule a Sidekiq job to update the highest role for a User  The job will be called outside of a transaction in order to ensure the changes to be committed before attempting to update the highest role. The exlusive lease will not be released after completion to prevent multiple jobs being executed during the defined timeout.",
    "label": "",
    "id": "6509"
  },
  {
    "raw_code": "def build_vulnerability_finding(security_finding)\n    report_finding = report_finding_for(security_finding)\n    return Vulnerabilities::Finding.new unless report_finding\n\n    finding_data = report_finding.to_hash.except(\n      :compare_key, :identifiers, :location, :scanner, :links, :signatures, :flags, :evidence, :confidence\n    )\n\n    identifiers = report_finding.identifiers.uniq(&:fingerprint).map do |identifier|\n      Vulnerabilities::Identifier.new(identifier.to_hash.merge({ project: project }))\n    end",
    "comment": "rubocop: disable Metrics/AbcSize -- existing violations to be refactored in followup work",
    "label": "",
    "id": "6510"
  },
  {
    "raw_code": "def calculate_false_positive?\n    project.licensed_feature_available?(:sast_fp_reduction)\n  end",
    "comment": "rubocop: enable Metrics/AbcSize",
    "label": "",
    "id": "6511"
  },
  {
    "raw_code": "def where_composite(permitted_keys, hashes)\n      raise ArgumentError, 'no permitted_keys' unless permitted_keys.present?\n\n      # accept any hash-like thing, such as Structs\n      hashes = TooManyIds.guard(Array.wrap(hashes)).map(&:to_h)\n\n      return none if hashes.empty?\n\n      case permitted_keys.size\n      when 1\n        key = permitted_keys.first\n        where(key => hashes.map { |hash| hash.fetch(key) })\n      else\n        clauses = hashes.map do |hash|\n          permitted_keys.map do |key|\n            arel_table[key].eq(hash.fetch(key))\n          end.reduce(:and)\n        end",
    "comment": "Apply a set of constraints that function as composite IDs.  This is the plural form of the standard ActiveRecord idiom: `where(foo: x, bar: y)`, except it allows multiple pairs of `x` and `y` to be specified, with the semantics that translate to:  ```sql WHERE (foo = x_0 AND bar = y_0) OR (foo = x_1 AND bar = y_1) OR ... ```  or the equivalent:  ```sql WHERE (foo, bar) IN ((x_0, y_0), (x_1, y_1), ...) ```  @param permitted_keys [Array<Symbol>] The keys each hash must have. There must be at least one key (but really, it ought to be at least two) @param hashes [Array<#to_h>|#to_h] The constraints. Each parameter must have a value for the keys named in `permitted_keys`  e.g.: ``` where_composite(%i[foo bar], [{foo: 1, bar: 2}, {foo: 1, bar: 3}]) ``` ",
    "label": "",
    "id": "6512"
  },
  {
    "raw_code": "def current_without_cache\n      last\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "6513"
  },
  {
    "raw_code": "def defaults\n      {}\n    end",
    "comment": "Can be overridden",
    "label": "",
    "id": "6514"
  },
  {
    "raw_code": "def replyable_types\n      %w[Issue MergeRequest AbuseReport WikiPage::Meta]\n    end",
    "comment": "`Noteable` class names that support replying to individual notes.",
    "label": "",
    "id": "6515"
  },
  {
    "raw_code": "def resolvable_types\n      %w[Issue MergeRequest DesignManagement::Design AbuseReport WikiPage::Meta]\n    end",
    "comment": "`Noteable` class names that support resolvable notes.",
    "label": "",
    "id": "6516"
  },
  {
    "raw_code": "def email_creatable_types\n      %w[Issue]\n    end",
    "comment": "`Noteable` class names that support creating/forwarding individual notes.",
    "label": "",
    "id": "6517"
  },
  {
    "raw_code": "def system_note_timestamp\n    @system_note_timestamp || Time.current # rubocop:disable Gitlab/ModuleWithInstanceVariables\n  end",
    "comment": "The timestamp of the note (e.g. the :created_at or :updated_at attribute if provided via API call)",
    "label": "",
    "id": "6518"
  },
  {
    "raw_code": "def human_class_name\n    @human_class_name ||= base_class_name.titleize.downcase\n  end",
    "comment": "Convert this Noteable class name to a format usable by notifications.  Examples:  noteable.class           # => MergeRequest noteable.human_class_name # => \"merge request\"",
    "label": "",
    "id": "6519"
  },
  {
    "raw_code": "def discussion_root_note_ids(notes_filter:, sort: :created_asc)\n    sort = :created_asc if sort.nil?\n    relations = []\n\n    relations << discussion_notes.select(\n      \"'notes' AS table_name\",\n      'MIN(id) AS id',\n      'MIN(created_at) AS created_at',\n      'ARRAY_AGG(id) AS ids'\n    ).with_notes_filter(notes_filter)\n     .group(:discussion_id)\n\n    if notes_filter != UserPreference::NOTES_FILTERS[:only_comments]\n      relations += synthetic_note_ids_relations\n    end",
    "comment": "This does not consider OutOfContextDiscussions in MRs where notes from commits are overriden so that they have the same discussion_id",
    "label": "",
    "id": "6520"
  },
  {
    "raw_code": "def resolvable_discussions\n    @resolvable_discussions ||=\n      if defined?(@discussions)\n        @discussions.select(&:resolvable?)\n      else\n        discussion_notes.resolvable.discussions(self)\n      end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "6521"
  },
  {
    "raw_code": "def discussions_resolvable?\n    resolvable_discussions.any?(&:resolvable?)\n  end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "6522"
  },
  {
    "raw_code": "def creatable_note_email_address(author)\n    return unless supports_creating_notes_by_email?\n\n    project_email = project&.new_issuable_address(author, base_class_name.underscore)\n    return unless project_email\n\n    project_email.sub('@', \"-#{iid}@\")\n  end",
    "comment": "Email address that an authorized user can send/forward an email to be added directly to an issue or merge request. example: incoming+h5bp-html5-boilerplate-8-1234567890abcdef123456789-issue-34@localhost.com",
    "label": "",
    "id": "6523"
  },
  {
    "raw_code": "def synthetic_note_ids_relations\n    relations = []\n\n    # currently multiple models include Noteable concern, but not all of them support\n    # all resource events, so we check if given model supports given resource event.\n    if respond_to?(:resource_label_events)\n      relations << resource_label_events.select(\"'resource_label_events'\", 'MIN(id)', :created_at, 'ARRAY_AGG(id)')\n                     .group(:created_at, :user_id)\n    end",
    "comment": "Synthetic system notes don't have discussion IDs because these are generated dynamically in Ruby. These are always root notes anyway so we don't need to group by discussion ID.",
    "label": "",
    "id": "6524"
  },
  {
    "raw_code": "def to_param\n    iid.to_s\n  end",
    "comment": " This automagically enforces all related routes to use `iid` instead of `id` If you want to use `iid` for some routes and `id` for other routes, this module should not to be included, instead you should define `iid` or `id` explicitly at each route generators. e.g. pipeline_path(project.id, pipeline.iid)",
    "label": "",
    "id": "6525"
  },
  {
    "raw_code": "def unsigned_commit_shas(commit_shas)\n      return [] if commit_shas.empty?\n\n      signed = by_commit_sha(commit_shas).pluck(:commit_sha)\n      commit_shas - signed\n    end",
    "comment": "Find commits that are lacking a signature in the database at present",
    "label": "",
    "id": "6526"
  },
  {
    "raw_code": "def verification_status\n    persisted_status = read_attribute(:verification_status)\n    return persisted_status unless Feature.enabled?(:check_for_mailmapped_commit_emails, project)\n    return persisted_status unless verified? || verified_system?\n    return persisted_status unless commit\n\n    return 'unverified_author_email' if emails_for_verification&.exclude?(commit.author_email)\n\n    persisted_status\n  end",
    "comment": "If commit is persisted as verified, check that commit email is still correct.",
    "label": "",
    "id": "6527"
  },
  {
    "raw_code": "def columns_changing_default(*columns)\n      self.columns_with_changing_default = columns.map(&:to_s)\n    end",
    "comment": "Indicate that one or more columns will have their database default change.  By indicating those columns here, this helper prevents a case where explicitly writing the old database default will be mutated to the new database default.",
    "label": "",
    "id": "6528"
  },
  {
    "raw_code": "def changes_applied\n    super.tap do\n      previous_changes.each do |attr, (previous, current)|\n        if reportable_changes_store.include?(attr)\n          reportable_changes_store[attr][1] = current\n        else\n          reportable_changes_store[attr] = [previous, current]\n        end",
    "comment": "Maintains a hash of cumulative changes to attributes between when the object was loaded from persistence and its most recent save.  This is called by ActiveRecord (and other implementations of ActiveModel::Dirty) once attribute changes are persisted.",
    "label": "",
    "id": "6529"
  },
  {
    "raw_code": "def reportable_changes\n    reportable_changes_store.clone\n  end",
    "comment": "Returns a hash of attributes that were changed between when the object was initially loaded from persistence (or newly created) and its most recent save. This is in constrast to ActiveModel::Dirty#previous_changes which resets the change state after every save.  person = Person.find_by_name(\"bob\") person.name # => \"bob\"  person.name = \"robert\" person.save person.previous_changes   # => {\"name\" => [\"bob\", \"robert\"]} person.reportable_changes # => {\"name\" => [\"bob\", \"robert\"]}  person.title = \"mr\" person.save person.previous_changes   # => {\"title\" => [nil, \"mr\"]} person.reportable_changes # => {\"name\" => [\"bob\", \"robert\"], \"title\" => [nil, \"mr\"]}  person.name = \"rob\" person.save person.previous_changes   # => {\"name\" => [\"robert\", \"rob\"]} person.reportable_changes # => {\"name\" => [\"bob\", \"rob\"], \"title\" => [nil, \"mr\"]}",
    "label": "",
    "id": "6530"
  },
  {
    "raw_code": "def reload(*)\n    super.tap do\n      reportable_changes_store.clear\n    end",
    "comment": "Reset the reportable changes only when the record is reloaded from persistence. See ActiveRecord::AttributeMethods::Dirty#reload",
    "label": "",
    "id": "6531"
  },
  {
    "raw_code": "def clear_changes_information(*)\n    super.tap do\n      reportable_changes_store.clear\n    end",
    "comment": "Reset the reportable changes when explicitly requested. See ActiveModel::Dirty#clear_changes_information",
    "label": "",
    "id": "6532"
  },
  {
    "raw_code": "def self.search(query, **options)\n      # make sure we prevent passing in disallowed columns\n      search_in = case options[:search_in]\n                  when [:title]\n                    [:title]\n                  when [:description]\n                    [:description]\n                  else\n                    [:title, :description]\n                  end",
    "comment": "Searches for labels with a matching title or description.  This method uses ILIKE on PostgreSQL.  query - The search query as a String.  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6533"
  },
  {
    "raw_code": "def self.min_chars_for_partial_matching\n      1\n    end",
    "comment": "Override Gitlab::SQL::Pattern.min_chars_for_partial_matching as label queries are never global, and so will not use a trigram index. That means we can have just one character in the LIKE.",
    "label": "",
    "id": "6534"
  },
  {
    "raw_code": "def supports_milestone?\n    respond_to?(:milestone_id)\n  end",
    "comment": " Overridden on EE module ",
    "label": "",
    "id": "6535"
  },
  {
    "raw_code": "def each_batch(of: 1000, column: primary_key, order: :asc, order_hint: nil, reset_order: true)\n      unless column\n        raise ArgumentError,\n          'the column: argument must be set to a column name to use for ordering rows'\n      end",
    "comment": "Iterates over the rows in a relation in batches, similar to Rails' `in_batches` but in a more efficient way.  Unlike `in_batches` provided by Rails this method does not support a custom start/end range, nor does it provide support for the `load:` keyword argument.  This method will yield an ActiveRecord::Relation to the supplied block, or return an Enumerator if no block is given.  Example:  User.each_batch do |relation| relation.update_all(updated_at: Time.current) end  The supplied block is also passed an optional batch index:  User.each_batch do |relation, index| puts index # => 1, 2, 3, ... end  You can also specify an alternative column to use for ordering the rows:  User.each_batch(column: :created_at) do |relation| ... end  This will produce SQL queries along the lines of:  User Load (0.7ms)  SELECT  \"users\".\"id\" FROM \"users\" WHERE (\"users\".\"id\" >= 41654) ORDER BY \"users\".\"id\" ASC LIMIT 1 OFFSET 1000 (0.7ms)  SELECT COUNT(*) FROM \"users\" WHERE (\"users\".\"id\" >= 41654) AND (\"users\".\"id\" < 42687)  of - The number of rows to retrieve per batch. column - The column to use for ordering the batches. order_hint - An optional column to append to the `ORDER BY id` clause to help the query planner. PostgreSQL might perform badly with a LIMIT 1 because the planner is guessing that scanning the index in ID order will come across the desired row in less time it will take the planner than using another index. The order_hint does not affect the search results. For example, `ORDER BY id ASC, updated_at ASC` means the same thing as `ORDER BY id ASC`.",
    "label": "",
    "id": "6536"
  },
  {
    "raw_code": "def distinct_each_batch(column:, order: :asc, of: 1000)\n      start = except(:select)\n        .select(column)\n        .reorder(column => order)\n\n      start = start.take\n\n      return unless start\n\n      start_id = start[column]\n      arel_table = self.arel_table\n      arel_column = arel_table[column.to_s]\n\n      1.step do |index|\n        stop = loose_index_scan(column: column, order: order) do |cte_query, inner_query|\n          if order == :asc\n            [cte_query.where(arel_column.gteq(start_id)), inner_query]\n          else\n            [cte_query.where(arel_column.lteq(start_id)), inner_query]\n          end",
    "comment": "Iterates over the rows in a relation in batches by skipping duplicated values in the column. Example: counting the number of distinct authors in `issues`  - Table size: 100_000 - Column: author_id - Distinct author_ids in the table: 1000  The query will read maximum 1000 rows if we have index coverage on user_id.  > count = 0 > Issue.distinct_each_batch(column: 'author_id', of: 1000) { |r| count += r.count(:author_id) }",
    "label": "",
    "id": "6537"
  },
  {
    "raw_code": "def each_batch_count(of: 1000, column: :id, last_count: 0, last_value: nil)\n      arel_table = self.arel_table\n      window = Arel::Nodes::Window.new.order(arel_table[column])\n      last_value_column = Arel::Nodes::NamedFunction\n        .new('LAST_VALUE', [arel_table[column]])\n        .over(window)\n        .as(column.to_s)\n\n      loop do\n        count_column = Arel::Nodes::Addition\n          .new(Arel::Nodes::NamedFunction.new('ROW_NUMBER', []).over(window), last_count)\n          .as('count')\n\n        projections = [count_column, last_value_column]\n        scope = limit(1).offset(of - 1)\n        scope = scope.where(arel_table[column].gt(last_value)) if last_value\n        new_count, last_value = scope.pick(*projections)\n\n        # When reaching the last batch the offset query might return no data, to address this\n        # problem, we invoke a specialized query that takes the last row out of the resultset.\n        # We could do this for each batch, however it would add unnecessary overhead to all\n        # queries.\n        if new_count.nil?\n          inner_query = scope\n            .select(*projections)\n            .limit(nil)\n            .offset(nil)\n            .arel\n            .as(quoted_table_name)\n\n          new_count, last_value =\n            unscoped\n            .from(inner_query)\n            .unscope(where: :type)\n            .order(count: :desc)\n            .limit(1)\n            .pick(:count, column)\n\n          last_count = new_count if new_count\n          last_value = nil\n          break\n        end",
    "comment": "Iterates over the relation and counts the rows. The counting logic is combined with the iteration query which saves one query compared to a standard each_batch approach.  Basic usage: count, _last_value = Project.each_batch_count  The counting can be stopped by passing a block and making the last statement true. Example:  query_count = 0 count, last_value = Project.each_batch_count do query_count += 1 query_count == 5 # stop counting after 5 loops end  Resume where the previous counting has stopped:  count, last_value = Project.each_batch_count(last_count: count, last_value: last_value)  Another example, counting issues in project:  project = Project.find(1) count, _ = project.issues.each_batch_count(column: :iid)",
    "label": "",
    "id": "6538"
  },
  {
    "raw_code": "def diffs_in_batch(_batch_page, _batch_size, diff_options:)\n    diffs(diff_options)\n  end",
    "comment": "Comparisons going back to the repository will need proper batch loading (https://gitlab.com/gitlab-org/gitlab/issues/32859). For now, we're returning all the diffs available with no pagination data.",
    "label": "",
    "id": "6539"
  },
  {
    "raw_code": "def task_list_items\n    return [] if description.blank?\n\n    @task_list_items ||= Taskable.get_tasks(description) # rubocop:disable Gitlab/ModuleWithInstanceVariables\n  end",
    "comment": "Called by `TaskList::Summary`",
    "label": "",
    "id": "6540"
  },
  {
    "raw_code": "def tasks?\n    tasks.summary.items?\n  end",
    "comment": "Return true if this object's description has any task list items.",
    "label": "",
    "id": "6541"
  },
  {
    "raw_code": "def task_status(short: false)\n    return '' if description.blank?\n\n    sum = tasks.summary\n    checklist_item_noun = n_('checklist item', 'checklist items', sum.item_count)\n    if short\n      format(s_('Tasks|%{complete_count}/%{total_count} %{checklist_item_noun}'),\n        checklist_item_noun: checklist_item_noun, complete_count: sum.complete_count, total_count: sum.item_count)\n    else\n      format(s_('Tasks|%{complete_count} of %{total_count} %{checklist_item_noun} completed'),\n        checklist_item_noun: checklist_item_noun, complete_count: sum.complete_count, total_count: sum.item_count)\n    end",
    "comment": "Return a string that describes the current state of this Taskable's task list items, e.g. \"12 of 20 checklist items completed\"",
    "label": "",
    "id": "6542"
  },
  {
    "raw_code": "def task_status_short\n    task_status(short: true)\n  end",
    "comment": "Return a short string that describes the current state of this Taskable's task list items -- for small screens",
    "label": "",
    "id": "6543"
  },
  {
    "raw_code": "def lock_access!(opts = {})\n    return super unless override_devise_lockable?\n\n    super({ send_instructions: false })\n  end",
    "comment": "When overridden, do not send Devise unlock instructions when locking access.",
    "label": "",
    "id": "6544"
  },
  {
    "raw_code": "def attempts_exceeded?\n    return super unless override_devise_lockable?\n\n    failed_attempts >= MAXIMUM_ATTEMPTS\n  end",
    "comment": "We cannot override the class methods `maximum_attempts` and `unlock_in`, because we want to check for 2FA being enabled on the instance. So instead override the Devise Lockable methods where those values are used.",
    "label": "",
    "id": "6545"
  },
  {
    "raw_code": "def truncated_diff_lines(highlight: true, diff_limit: nil)\n    return [] unless on_text?\n    return [] if diff_line.nil?\n\n    diff_limit = [diff_limit, NUMBER_OF_TRUNCATED_DIFF_LINES].compact.min\n    lines = highlight ? highlighted_diff_lines : diff_lines\n\n    initial_line_index = [diff_line.index - diff_limit + 1, 0].max\n\n    prev_lines = []\n\n    lines = lines[initial_line_index..diff_line.index]\n\n    return [] if lines.nil?\n\n    lines.each do |line|\n      if line.meta?\n        prev_lines.clear\n      else\n        prev_lines << line\n      end",
    "comment": "Returns an array of at most 16 highlighted lines above a diff note",
    "label": "",
    "id": "6546"
  },
  {
    "raw_code": "def cascading_attr(*attributes)\n      attributes.map(&:to_sym).each do |attribute|\n        # public methods\n        define_attr_reader(attribute)\n        define_attr_writer(attribute)\n        define_lock_attr_writer(attribute)\n        define_lock_methods(attribute)\n        alias_boolean(attribute)\n\n        # private methods\n        define_validator_methods(attribute)\n        define_attr_before_save(attribute)\n        define_after_update(attribute)\n\n        validate :\"#{attribute}_changeable?\"\n        validate :\"lock_#{attribute}_changeable?\"\n\n        before_save :\"before_save_#{attribute}\", if: -> { will_save_change_to_attribute?(attribute) }\n        after_update :\"clear_descendant_#{attribute}_locks\", if: -> {\n          saved_change_to_attribute?(\"lock_#{attribute}\", to: true)\n        }\n      end",
    "comment": "Facilitates the cascading lookup of values and, similar to Rails' `attr_accessor`, defines convenience methods such as a reader, writer, and validators.  Example: `cascading_attr :toggle_security_policy_custom_ci`  Public methods defined: - `toggle_security_policy_custom_ci` - `toggle_security_policy_custom_ci=` - `toggle_security_policy_custom_ci_locked?` - `toggle_security_policy_custom_ci_locked_by_ancestor?` - `toggle_security_policy_custom_ci_locked_by_application_setting?` - `toggle_security_policy_custom_ci?` (only defined for boolean attributes) - `toggle_security_policy_custom_ci_locked_ancestor` - Returns locked namespace settings object (only namespace_id)  Defined validators ensure attribute value cannot be updated if locked by an ancestor or application settings.  Requires database columns be present in both `namespace_settings` and `application_settings`.",
    "label": "",
    "id": "6547"
  },
  {
    "raw_code": "def define_attr_reader(attribute)\n      define_method(attribute) do\n        strong_memoize(attribute) do\n          next self[attribute] if will_save_change_to_attribute?(attribute)\n          next locked_value(attribute) if cascading_attribute_locked?(attribute, include_self: false)\n          next self[attribute] unless self[attribute].nil?\n\n          cascaded_value = cascaded_ancestor_value(attribute)\n          next cascaded_value unless cascaded_value.nil?\n\n          application_setting_value(attribute)\n        end",
    "comment": "The cascading attribute reader method handles lookups with the following criteria:  1. Returns the dirty value, if the attribute has changed. 2. Return locked ancestor value. 3. Return locked instance-level application settings value. 4. Return this namespace's attribute, if not nil. 5. Return value from nearest ancestor where value is not nil. 6. Return instance-level application setting.",
    "label": "",
    "id": "6548"
  },
  {
    "raw_code": "def define_validator_methods(attribute)\n      define_method(\"#{attribute}_changeable?\") do\n        return unless cascading_attribute_changed?(attribute)\n        return unless cascading_attribute_locked?(attribute, include_self: false)\n\n        errors.add(attribute, s_('CascadingSettings|cannot be changed because it is locked by an ancestor'))\n      end",
    "comment": "Defines two validations - one for the cascadable attribute itself and one for the lock attribute. Only allows the respective value to change if an ancestor has not already locked the value.",
    "label": "",
    "id": "6549"
  },
  {
    "raw_code": "def define_after_update(attribute)\n      define_method(\"clear_descendant_#{attribute}_locks\") do\n        self.class.where(namespace_id: descendants).update_all(\"lock_#{attribute}\" => false)\n      end",
    "comment": "When a particular group locks the attribute, clear all sub-group locks since the higher lock takes priority.",
    "label": "",
    "id": "6550"
  },
  {
    "raw_code": "def populate_sharding_key(sharding_attribute, source: nil, field: sharding_attribute, &block)\n      value_proc = block || proc { send(source)&.public_send(field) } # rubocop:disable GitlabSecurity/PublicSend -- send is intended\n\n      before_validation -> { assign_attributes(sharding_attribute => instance_exec(self, &value_proc)) },\n        unless: :\"#{sharding_attribute}?\"\n    end",
    "comment": "Simple DSL to isolate sharding key population in models Examples: populate_sharding_key :project_id, source: :issue populate_sharding_key :project_id, source: :merge_request, field: :target_project_id populate_sharding_key :project_id do issue.project_id end populate_sharding_key :project_id, &:get_sharding_key Also see `populate_sharding_key` spec matcher",
    "label": "",
    "id": "6551"
  },
  {
    "raw_code": "def calculate_next_run_at(start_time = Time.zone.now)\n    ideal_next_run = ideal_next_run_from(start_time)\n\n    if ideal_next_run == cron_worker_next_run_from(start_time)\n      ideal_next_run\n    else\n      cron_worker_next_run_from(ideal_next_run)\n    end",
    "comment": " The `next_run_at` column is set to the actual execution date of worker that triggers the schedule. This way, a schedule like `*/1 * * * *` won't be triggered in a short interval when the worker runs irregularly by Sidekiq Memory Killer.",
    "label": "",
    "id": "6552"
  },
  {
    "raw_code": "def resolve!(current_user)\n      now = Time.current\n      unresolved.update_all(updated_at: now, resolved_at: now, resolved_by_id: current_user.id)\n    end",
    "comment": "This method must be kept in sync with `#resolve!`",
    "label": "",
    "id": "6553"
  },
  {
    "raw_code": "def unresolve!\n      resolved.update_all(updated_at: Time.current, resolved_at: nil, resolved_by_id: nil)\n    end",
    "comment": "This method must be kept in sync with `#unresolve!`",
    "label": "",
    "id": "6554"
  },
  {
    "raw_code": "def resolvable_types\n      RESOLVABLE_TYPES\n    end",
    "comment": "overridden on EE",
    "label": "",
    "id": "6555"
  },
  {
    "raw_code": "def potentially_resolvable?\n    self.class.resolvable_types.include?(self.class.name) && noteable&.supports_resolvable_notes?\n  end",
    "comment": "Keep this method in sync with the `potentially_resolvable` scope",
    "label": "",
    "id": "6556"
  },
  {
    "raw_code": "def resolvable?\n    potentially_resolvable? && !system?\n  end",
    "comment": "Keep this method in sync with the `resolvable` scope",
    "label": "",
    "id": "6557"
  },
  {
    "raw_code": "def resolve_without_save(current_user, resolved_by_push: false)\n    return false unless resolvable?\n    return false if resolved?\n\n    now = Time.current\n    self.updated_at = now\n    self.resolved_at = now\n    self.resolved_by = current_user\n    self.resolved_by_push = resolved_by_push\n\n    true\n  end",
    "comment": "If you update this method remember to also update `.resolve!`",
    "label": "",
    "id": "6558"
  },
  {
    "raw_code": "def unresolve_without_save\n    return false unless resolvable?\n    return false unless resolved?\n\n    self.updated_at = Time.current\n    self.resolved_at = nil\n    self.resolved_by = nil\n\n    true\n  end",
    "comment": "If you update this method remember to also update `.unresolve!`",
    "label": "",
    "id": "6559"
  },
  {
    "raw_code": "def validate_binary_column_exists!(name)\n      return unless database_exists?\n\n      unless table_exists?\n        warn \"WARNING: x509_serial_number_attribute #{name.inspect} is invalid since the table doesn't exist - you may need to run database migrations\"\n        return\n      end",
    "comment": "This only gets executed in non-production environments as an additional check to ensure the column is the correct type.  In production it should behave like any other attribute. See https://gitlab.com/gitlab-org/gitlab/merge_requests/5502 for more discussion",
    "label": "",
    "id": "6560"
  },
  {
    "raw_code": "def _returning_columns_for_insert(...)\n      (super + Array(primary_key)).uniq\n    end",
    "comment": "This PR assigns auto populated columns: https://github.com/rails/rails/pull/48241 However, a column is identified as auto-populated if it contains default value with nextval function. The id column of partitioned tables is auto-populated via a trigger and it's not identified as an auto-populated column by Rails. Let's explicitly include [primary_key] in a function that expects a list of auto-populated columns",
    "label": "",
    "id": "6561"
  },
  {
    "raw_code": "def run_after_commit_or_now(&block)\n    if self.class.inside_transaction?\n      if connection.current_transaction.records&.include?(self)\n        run_after_commit(&block)\n      else\n        # If the current transaction does not include this record, we can run\n        # the block now, even if it queues a Sidekiq job.\n        Sidekiq::Worker.skipping_transaction_check do\n          instance_eval(&block)\n        end",
    "comment": "When within a database transaction, execute the given block after the transaction is committed. Otherwise execute the given block ATTENTION: because this uses `instance_eval` to evaluate the block, instance variables within the block will be evaluated based on the object on which `run_after_commit_or_now` gets executed. https://gitlab.com/gitlab-org/gitlab/-/merge_requests/146208#note_1800349073",
    "label": "",
    "id": "6562"
  },
  {
    "raw_code": "def token_authenticatable_fields\n      @token_authenticatable_fields ||= []\n    end",
    "comment": "Stores fields that already have been configured via add_authentication_token_field",
    "label": "",
    "id": "6563"
  },
  {
    "raw_code": "def token_authenticatable_sensitive_fields\n      @token_authenticatable_sensitive_fields ||= []\n    end",
    "comment": "Returns all sensitive fields related to the add_authentication_token_field e.g. token, token_encrypted, token_digest",
    "label": "",
    "id": "6564"
  },
  {
    "raw_code": "def banzai_render_context(field)\n    raise ArgumentError, \"Unknown field: #{field.inspect}\" unless\n      cached_markdown_fields.key?(field)\n\n    # Always include a project key, or Banzai complains\n    project = self.project if self.respond_to?(:project)\n    group   = self.group if self.respond_to?(:group)\n    context = cached_markdown_fields[field].merge(project: project, group: group)\n\n    # Banzai is less strict about authors, so don't always have an author key\n    context[:author] = self.author if self.respond_to?(:author)\n\n    context[:user] = self.parent_user if Feature.enabled?(:personal_snippet_reference_filters, context[:author])\n\n    context\n  end",
    "comment": "Returns the default Banzai render context for the cached markdown field.",
    "label": "",
    "id": "6565"
  },
  {
    "raw_code": "def refresh_markdown_cache\n    updates = cached_markdown_fields.markdown_fields.to_h do |markdown_field|\n      [\n        cached_markdown_fields.html_field(markdown_field),\n        rendered_field_content(markdown_field)\n      ]\n    end",
    "comment": "Update every applicable column in a row if any one is invalidated, as we only store one version per row",
    "label": "",
    "id": "6566"
  },
  {
    "raw_code": "def updated_cached_html_for(markdown_field)\n    return unless cached_markdown_fields.key?(markdown_field)\n\n    if attribute_invalidated?(cached_markdown_fields.html_field(markdown_field))\n      # Invalidated due to Markdown content change\n      # We should not persist the updated HTML here since this will depend on whether the\n      # Markdown content change will be persisted. Both will be persisted together when the model is saved.\n      if changed_attributes.key?(markdown_field)\n        refresh_markdown_cache\n      else\n        # Invalidated due to stale HTML cache\n        # This could happen when the Markdown cache version is bumped\n        # or when a model is imported and the HTML is empty.\n        # We persist the updated HTML here so that subsequent calls\n        # to this method do not have to regenerate the HTML again.\n        refresh_markdown_cache!\n      end",
    "comment": "Updates the markdown cache if necessary, then returns the field Unlike `cached_html_for` it returns `nil` if the field does not exist",
    "label": "",
    "id": "6567"
  },
  {
    "raw_code": "def mentioned_filtered_user_ids_for(refs)\n    refs.mentioned_user_ids.presence\n  end",
    "comment": "Overriden on objects that needs to filter mentioned users that cannot read them, for example, guest users that are referenced on a confidential note.",
    "label": "",
    "id": "6568"
  },
  {
    "raw_code": "def cache_markdown_field(markdown_field, context = {})\n      cached_markdown_fields[markdown_field] = context\n\n      html_field = cached_markdown_fields.html_field(markdown_field)\n      invalidation_method = :\"#{html_field}_invalidated?\"\n\n      # The HTML becomes invalid if any dependent fields change. For now, assume\n      # author and project invalidate the cache in all circumstances.\n      define_method(invalidation_method) do\n        return false if skip_markdown_cache_validation?\n\n        changed_fields = changed_attributes.keys\n        invalidations  = changed_fields & [markdown_field.to_s, *INVALIDATED_BY]\n        !invalidations.empty? || !cached_html_up_to_date?(markdown_field)\n      end",
    "comment": "Specify that a field is markdown. Its rendered output will be cached in a corresponding _html field. Any custom rendering options may be provided as a context.",
    "label": "",
    "id": "6569"
  },
  {
    "raw_code": "def upload_paths(identifier)\n    avatar_mounter.blank_uploader.store_dirs.map { |store, path| File.join(path, identifier) }\n  end",
    "comment": "Path that is persisted in the tracking Upload model. Used to fetch the upload from the model.",
    "label": "",
    "id": "6570"
  },
  {
    "raw_code": "def inside_transaction?\n      return false unless connected?\n\n      base = Rails.env.test? ? open_transactions_baseline.to_i : 0\n\n      connection.open_transactions > base\n    end",
    "comment": "inside_transaction? will return true if the caller is running within a transaction. Handles special cases when running inside a test environment, where tests may be wrapped in transactions",
    "label": "",
    "id": "6571"
  },
  {
    "raw_code": "def set_open_transactions_baseline\n      @open_transactions_baseline = connection.open_transactions\n    end",
    "comment": "These methods that access @open_transactions_baseline are not thread-safe. These are fine though because we only call these in RSpec's main thread. If we decide to run specs multi-threaded, we would need to use something like ThreadGroup to keep track of this value",
    "label": "",
    "id": "6572"
  },
  {
    "raw_code": "def ignore_columns(*columns, remove_after: nil, remove_with: nil, remove_never: false)\n      unless remove_never\n        raise ArgumentError, 'Please indicate when we can stop ignoring columns with remove_after (date string YYYY-MM-DD), example: ignore_columns(:name, remove_after: \\'2019-12-01\\', remove_with: \\'12.6\\')' unless remove_after && Gitlab::Regex.utc_date_regex.match?(remove_after)\n        raise ArgumentError, 'Please indicate in which release we can stop ignoring columns with remove_with, example: ignore_columns(:name, remove_after: \\'2019-12-01\\', remove_with: \\'12.6\\')' unless remove_with\n      end",
    "comment": "Ignore database columns in a model  Indicate the earliest date and release we can stop ignoring the column with +remove_after+ (a date string) and +remove_with+ (a release)",
    "label": "",
    "id": "6573"
  },
  {
    "raw_code": "def deploy_key_access_allowed?(current_user, _current_project)\n    deploy_key_owned_by?(current_user) && valid_deploy_key_status?\n  end",
    "comment": "current_project is only available when evaluating a group-level protected branch. We only allow role based access levels at the group-level so we can ignore it here.",
    "label": "",
    "id": "6574"
  },
  {
    "raw_code": "def send_devise_notification(notification, *args)\n    return true unless can_receive_notification(notification)\n\n    run_after_commit_or_now do\n      devise_mailer.__send__(notification, self, *args).deliver_later # rubocop:disable GitlabSecurity/PublicSend\n    end",
    "comment": "Added according to https://github.com/plataformatec/devise/blob/7df57d5081f9884849ca15e4fde179ef164a575f/README.md#activejob-integration",
    "label": "",
    "id": "6575"
  },
  {
    "raw_code": "def trigger_note_subscription_create(events: self)\n    GraphqlTriggers.work_item_note_created(work_item.to_gid, events)\n  end",
    "comment": "System notes are not updated or deleted, so firing just the noteCreated event.",
    "label": "",
    "id": "6576"
  },
  {
    "raw_code": "def synthetic_note_class\n    raise NoMethodError, <<~MESSAGE.squish\n      `#{self.class.name}#synthetic_note_class` method must be implemented\n      (return nil if event does not require a note)\n    MESSAGE\n  end",
    "comment": "Class used to create the even synthetic note If the event does not require a synthetic note the method must return false",
    "label": "",
    "id": "6577"
  },
  {
    "raw_code": "def nullify_deprecated_assignee\n    return unless persisted? && Gitlab::Database.read_only?\n\n    update_column(:assignee_id, nil)\n  end",
    "comment": "This will make the background migration process quicker (#26496) as it'll have less assignee_id rows to look through.",
    "label": "",
    "id": "6578"
  },
  {
    "raw_code": "def pending_assignees_population?\n    persisted? && deprecated_assignee_id && merge_request_assignees.empty?\n  end",
    "comment": "This code should be removed in the clean-up phase of the background migration (#59457).",
    "label": "",
    "id": "6579"
  },
  {
    "raw_code": "def update_assignees_relation\n    if pending_assignees_population?\n      transaction do\n        merge_request_assignees.create!(user_id: deprecated_assignee_id, merge_request_id: id)\n        update_column(:assignee_id, nil)\n      end",
    "comment": "If there's an assignee_id and no relation, it means the background migration at #26496 didn't reach this merge request yet. This code should be removed in the clean-up phase of the background migration (#59457).",
    "label": "",
    "id": "6580"
  },
  {
    "raw_code": "def locking_enabled?\n      will_save_change_to_title? || will_save_change_to_description?\n    end",
    "comment": "We want to use optimistic lock for cases when only title or description are involved http://api.rubyonrails.org/classes/ActiveRecord/Locking/Optimistic.html",
    "label": "",
    "id": "6581"
  },
  {
    "raw_code": "def search(query)\n      fuzzy_search(query, [:title])\n    end",
    "comment": "Searches for records with a matching title.  This method uses ILIKE on PostgreSQL.  query - The search query as a String  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6582"
  },
  {
    "raw_code": "def available_state_names\n      [:opened, :closed]\n    end",
    "comment": "Available state names used to persist state_id column using state machine  Override this on subclasses if different states are needed  Check MergeRequest.available_states_names for example",
    "label": "",
    "id": "6583"
  },
  {
    "raw_code": "def full_search(query, matched_columns: nil, use_minimum_char_limit: true)\n      if matched_columns\n        matched_columns = matched_columns.to_s.split(',')\n        matched_columns &= SEARCHABLE_FIELDS\n        matched_columns.map!(&:to_sym)\n      end",
    "comment": "Searches for records with a matching title or description.  This method uses ILIKE on PostgreSQL.  query - The search query as a String matched_columns - Modify the scope of the query. 'title', 'description' or joining them with a comma.  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6584"
  },
  {
    "raw_code": "def grouping_columns(sort)\n      sort = sort.to_s\n      grouping_columns = [arel_table[:id]]\n\n      if %w[milestone_due_desc milestone_due_asc milestone].include?(sort)\n        milestone_table = Milestone.arel_table\n        grouping_columns << milestone_table[:id]\n        grouping_columns << milestone_table[:due_date]\n      elsif %w[merged_at_desc merged_at_asc merged_at].include?(sort)\n        grouping_columns << MergeRequest::Metrics.arel_table[:id]\n        grouping_columns << MergeRequest::Metrics.arel_table[:merged_at]\n      elsif %w[closed_at_desc closed_at_asc closed_at].include?(sort)\n        grouping_columns << MergeRequest::Metrics.arel_table[:id]\n        grouping_columns << MergeRequest::Metrics.arel_table[:latest_closed_at]\n      end",
    "comment": "Includes table keys in group by clause when sorting preventing errors in Postgres  Returns an array of Arel columns ",
    "label": "",
    "id": "6585"
  },
  {
    "raw_code": "def issue_grouping_columns(use_cte: false)\n      if use_cte\n        attribute_names.map { |attr| arel_table[attr.to_sym] }\n      else\n        [arel_table[:id]]\n      end",
    "comment": "Includes all table keys in group by clause when sorting preventing errors in Postgres when using CTE search optimization  Returns an array of Arel columns ",
    "label": "",
    "id": "6586"
  },
  {
    "raw_code": "def hook_association_changes(old_associations)\n    changes = {}\n\n    if old_assignees(old_associations) != assignees\n      changes[:assignees] = [old_assignees(old_associations).map(&:hook_attrs), assignees.map(&:hook_attrs)]\n    end",
    "comment": "rubocop:disable Metrics/PerceivedComplexity -- Related issue: https://gitlab.com/gitlab-org/gitlab/-/issues/437679",
    "label": "",
    "id": "6587"
  },
  {
    "raw_code": "def hook_reviewer_changes(old_associations)\n    changes = {}\n    old_reviewers = old_associations.fetch(:reviewers, reviewers)\n\n    changes[:reviewers] = [old_reviewers.map(&:hook_attrs), reviewers.map(&:hook_attrs)] if old_reviewers != reviewers\n\n    changes\n  end",
    "comment": "rubocop:enable Metrics/PerceivedComplexity",
    "label": "",
    "id": "6588"
  },
  {
    "raw_code": "def to_ability_name\n    self.class.to_ability_name\n  end",
    "comment": "Convert this Issuable class name to a format usable by Ability definitions  Examples:  issuable.class           # => MergeRequest issuable.to_ability_name # => \"merge_request\"",
    "label": "",
    "id": "6589"
  },
  {
    "raw_code": "def card_attributes\n    {\n      'Author' => author.try(:name),\n      'Assignee' => assignee_list\n    }\n  end",
    "comment": "Returns a Hash of attributes to be used for Twitter card metadata",
    "label": "",
    "id": "6590"
  },
  {
    "raw_code": "def can_move?(*)\n    false\n  end",
    "comment": " Method that checks if issuable can be moved to another project.  Should be overridden if issuable can be moved. ",
    "label": "",
    "id": "6591"
  },
  {
    "raw_code": "def first_contribution?\n    false\n  end",
    "comment": " Override in issuable specialization ",
    "label": "",
    "id": "6592"
  },
  {
    "raw_code": "def draftless_title_changed(old_title)\n    old_title != title\n  end",
    "comment": " Overridden in MergeRequest ",
    "label": "",
    "id": "6593"
  },
  {
    "raw_code": "def without_reactive_cache(*args, &blk)\n      return with_reactive_cache(*args, &blk) unless Rails.env.development?\n\n      data = self.class.reactive_cache_worker_finder.call(id, *args).calculate_reactive_cache(*args)\n      yield data\n    end",
    "comment": "This method is used for debugging purposes and should not be used otherwise.",
    "label": "",
    "id": "6594"
  },
  {
    "raw_code": "def cache_key\n    \"#{self.class.table_name}/#{id}-#{read_attribute_before_type_cast(:updated_at)}\"\n  end",
    "comment": "A faster version of Rails' \"cache_key\" method.  Rails' default \"cache_key\" method uses all kind of complex logic to figure out the cache key. In many cases this complexity and overhead may not be needed.  This method does not do any timestamp parsing as this process is quite expensive and not needed when generating cache keys. This method also relies on the table name instead of the cache namespace name as the latter uses complex logic to generate the exact same value (as when using the table name) in 99% of the cases.",
    "label": "",
    "id": "6595"
  },
  {
    "raw_code": "def participant(attr)\n      participant_attrs << attr\n    end",
    "comment": "Adds a list of participant attributes. Attributes can either be symbols or Procs.  When using a Proc instead of a Symbol the Proc will be given two arguments:  1. The current user (as an instance of User) 2. An instance of `Gitlab::ReferenceExtractor`  It is expected that a Proc populates the given reference extractor instance with data. The return value of the Proc is ignored.  attr - The name of the attribute or a Proc",
    "label": "",
    "id": "6596"
  },
  {
    "raw_code": "def participants(user = nil)\n    filtered_participants_hash[user]\n  end",
    "comment": "Returns the users participating in a discussion.  This method processes attributes of objects in breadth-first order.  Returns an Array of User instances.",
    "label": "",
    "id": "6597"
  },
  {
    "raw_code": "def visible_participants(user)\n    filter_by_ability(raw_participants(user, verify_access: true))\n  end",
    "comment": "Returns only participants visible for the user  Returns an Array of User instances.",
    "label": "",
    "id": "6598"
  },
  {
    "raw_code": "def participant?(user)\n    can_read_participable?(user) &&\n      all_participants_hash[user].include?(user)\n  end",
    "comment": "Checks if the user is a participant in a discussion.  This method processes attributes of objects in breadth-first order.  Returns a Boolean.",
    "label": "",
    "id": "6599"
  },
  {
    "raw_code": "def read_ability_for(participable_source)\n    name =  participable_source.try(:to_ability_name) || participable_source.model_name.element\n\n    { name: :\"read_#{name}\", subject: participable_source }\n  end",
    "comment": "Returns Hash containing ability name and subject needed to read a specific participable. Should be overridden if a different ability is required.",
    "label": "",
    "id": "6600"
  },
  {
    "raw_code": "def serializable_hash(options = nil)\n      options = options.try(:dup) || {}\n      options[:except] = Array(options[:except]).dup\n\n      options[:except].concat self.class.attributes_exempt_from_serializable_hash\n\n      options[:except].concat self.class.sensitive_attributes\n\n      super(options)\n    end",
    "comment": "Override serializable_hash to exclude sensitive attributes by default  In general, prefer NOT to use serializable_hash / to_json / as_json in favor of serializers / entities instead which has an allowlist of attributes",
    "label": "",
    "id": "6601"
  },
  {
    "raw_code": "def self_deletion_in_progress?\n      raise NotImplementedError\n    end",
    "comment": "Models need to define this method, usually based on the value of a database attribute",
    "label": "",
    "id": "6602"
  },
  {
    "raw_code": "def ancestors_scheduled_for_deletion\n      []\n    end",
    "comment": "Returns an array of the record's ancestors that are scheduled for deletion. This method can be overriden.",
    "label": "",
    "id": "6603"
  },
  {
    "raw_code": "def self_deletion_scheduled_deletion_created_on\n      marked_for_deletion_on if respond_to?(:marked_for_deletion_on)\n    end",
    "comment": "Returns the date when the scheduled deletion was created.",
    "label": "",
    "id": "6604"
  },
  {
    "raw_code": "def self_deletion_scheduled?\n      self_deletion_scheduled_deletion_created_on.present?\n    end",
    "comment": "Returns true if the record is scheduled for deletion.",
    "label": "",
    "id": "6605"
  },
  {
    "raw_code": "def first_scheduled_for_deletion_in_hierarchy_chain\n      return self if self_deletion_scheduled?\n\n      ancestors_scheduled_for_deletion.first\n    end",
    "comment": "Returns the first record that's scheduled for deletion in self's ancestors chain (including itself).",
    "label": "",
    "id": "6606"
  },
  {
    "raw_code": "def scheduled_for_deletion_in_hierarchy_chain?\n      first_scheduled_for_deletion_in_hierarchy_chain.present?\n    end",
    "comment": "Returns true if the record or any of its ancestors is scheduled for deletion.",
    "label": "",
    "id": "6607"
  },
  {
    "raw_code": "def deletion_in_progress_or_scheduled_in_hierarchy_chain?\n      self_deletion_in_progress? || scheduled_for_deletion_in_hierarchy_chain?\n    end",
    "comment": "Returns true if the record or any of its ancestors is being deleted or scheduled for deletion.",
    "label": "",
    "id": "6608"
  },
  {
    "raw_code": "def normalize_for_sast_reports(reports, builds)\n      return reports unless reports.delete(:sast)\n\n      reports.tap do |r|\n        build_names = builds.map(&:name)\n\n        r.push(:sast_iac) if build_names.delete('kics-iac-sast')\n\n        # When using adavanced sast, sast should also show in the report names\n        r.push(:sast, :sast_advanced) if build_names.delete('gitlab-advanced-sast')\n\n        r.push(:sast) if build_names.any? { |name| name.include? '-sast' }\n      end.uniq\n    end",
    "comment": "Because :sast_iac and :sast_advanced reports belong to a report with a name of 'sast', we have to do extra checking to determine which reports have been included",
    "label": "",
    "id": "6609"
  },
  {
    "raw_code": "def self.use_namespaced_name?(purl_type)\n      case purl_type\n      when 'apk', 'deb', 'rpm'\n        false\n      else\n        true\n      end",
    "comment": "We do not use the namespaced names for OS component types even if the PURL specification declares otherwise, since this will preserve the name format established by the container-scanning analyzers. For example, a namespaced name for an Alpine cURL component might look like `apk/curl` when found by an SBOM generator. If found by a container-scanning analyzer, this same component would be reported as `curl`. The differences in naming would impact dependency and vulnerability deduplication, and if left as is would create dependency lists and vulnerability reports that are inaccurate.  For full details, see https://gitlab.com/gitlab-org/gitlab/-/issues/442847",
    "label": "",
    "id": "6610"
  },
  {
    "raw_code": "def self.failure_reasons\n        {\n          unknown_failure: nil,\n          script_failure: 1,\n          api_failure: 2,\n          stuck_or_timeout_failure: 3,\n          runner_system_failure: 4,\n          missing_dependency_failure: 5,\n          runner_unsupported: 6,\n          stale_schedule: 7,\n          job_execution_timeout: 8,\n          archived_failure: 9,\n          unmet_prerequisites: 10,\n          scheduler_failure: 11,\n          data_integrity_failure: 12,\n          forward_deployment_failure: 13, # Deprecated in favor of failed_outdated_deployment_job.\n          user_blocked: 14,\n          project_deleted: 15,\n          ci_quota_exceeded: 16,\n          pipeline_loop_detected: 17,\n          no_matching_runner: 18,\n          trace_size_exceeded: 19,\n          builds_disabled: 20,\n          environment_creation_failure: 21,\n          deployment_rejected: 22,\n          failed_outdated_deployment_job: 23,\n          runner_provisioning_timeout: 24,\n          protected_environment_failure: 1_000,\n          insufficient_bridge_permissions: 1_001,\n          downstream_bridge_project_not_found: 1_002,\n          invalid_bridge_trigger: 1_003,\n          upstream_bridge_project_not_found: 1_004,\n          insufficient_upstream_permissions: 1_005,\n          bridge_pipeline_is_child_pipeline: 1_006, # not used anymore, but cannot be deleted because of old data\n          downstream_pipeline_creation_failed: 1_007,\n          secrets_provider_not_found: 1_008,\n          reached_max_descendant_pipelines_depth: 1_009,\n          ip_restriction_failure: 1_010,\n          reached_max_pipeline_hierarchy_size: 1_011,\n          reached_downstream_pipeline_trigger_rate_limit: 1_012\n        }\n      end",
    "comment": "Returns the Hash to use for creating the `failure_reason` enum for `CommitStatus`.",
    "label": "",
    "id": "6611"
  },
  {
    "raw_code": "def self.file_type\n        {\n          archive: 1,\n          metadata: 2,\n          trace: 3,\n          junit: 4,\n          sast: 5, ## EE-specific\n          dependency_scanning: 6, ## EE-specific\n          container_scanning: 7, ## EE-specific\n          dast: 8, ## EE-specific\n          codequality: 9, ## EE-specific\n          license_scanning: 101, ## EE-specific\n          performance: 11, ## EE-specific till 13.2\n          metrics: 12, ## EE-specific\n          metrics_referee: 13, ## runner referees\n          network_referee: 14, ## runner referees\n          lsif: 15, # LSIF data for code navigation\n          dotenv: 16,\n          cobertura: 17,\n          terraform: 18, # Transformed json\n          accessibility: 19,\n          cluster_applications: 20,\n          secret_detection: 21, ## EE-specific\n          requirements: 22, ## EE-specific\n          coverage_fuzzing: 23, ## EE-specific\n          browser_performance: 24, ## EE-specific\n          load_performance: 25, ## EE-specific\n          api_fuzzing: 26, ## EE-specific\n          cluster_image_scanning: 27, ## EE-specific\n          cyclonedx: 28, ## EE-specific\n          requirements_v2: 29, ## EE-specific\n          annotations: 30,\n          repository_xray: 31, ## EE-specific\n          jacoco: 32\n        }\n      end",
    "comment": "Returns the Hash to use for creating the `file_type` enum for `JobArtifact`.",
    "label": "",
    "id": "6612"
  },
  {
    "raw_code": "def self.failure_reasons\n        {\n          unknown_failure: 0,\n          config_error: 1,\n          external_validation_failure: 2,\n          user_not_verified: 3,\n          size_limit_exceeded: 21,\n          job_activity_limit_exceeded: 22,\n          deployments_limit_exceeded: 23,\n          # 24 was previously used by the deprecated `user_blocked`\n          project_deleted: 25,\n          filtered_by_rules: 26,\n          filtered_by_workflow_rules: 27,\n          composite_identity_forbidden: 28,\n          pipeline_ref_creation_failure: 29\n        }\n      end",
    "comment": "Returns the `Hash` to use for creating the `failure_reason` enum for `Ci::Pipeline`.",
    "label": "",
    "id": "6613"
  },
  {
    "raw_code": "def self.sources\n        {\n          unknown: nil,\n          push: 1,\n          web: 2,\n          trigger: 3,\n          schedule: 4,\n          api: 5,\n          external: 6,\n          pipeline: 7,\n          chat: 8,\n          webide: 9,\n          merge_request_event: 10,\n          external_pull_request_event: 11,\n          parent_pipeline: 12,\n          ondemand_dast_scan: 13,\n          ondemand_dast_validation: 14,\n          security_orchestration_policy: 15,\n          container_registry_push: 16,\n          duo_workflow: 17,\n          pipeline_execution_policy_schedule: 18\n        }\n      end",
    "comment": "Returns the `Hash` to use for creating the `sources` enum for `Ci::Pipeline`.",
    "label": "",
    "id": "6614"
  },
  {
    "raw_code": "def self.dangling_sources\n        sources.slice(\n          :webide,\n          :parent_pipeline,\n          :ondemand_dast_scan,\n          :ondemand_dast_validation,\n          :security_orchestration_policy,\n          :container_registry_push,\n          :duo_workflow,\n          :pipeline_execution_policy_schedule\n        )\n      end",
    "comment": "Dangling sources are those events that generate pipelines for which we don't want to directly affect the ref CI status. - when a webide pipeline fails it does not change the ref CI status to failed - when a child pipeline (from parent_pipeline source) fails it affects its parent pipeline. It's up to the parent to affect the ref CI status - when an ondemand_dast_scan pipeline runs it is for testing purpose and should not affect the ref CI status. - when an ondemand_dast_validation pipeline runs it is for validating a DAST site profile and should not affect the ref CI status. - when a container_registry_push pipeline runs it is for security testing purpose and should not affect the ref CI status.",
    "label": "",
    "id": "6615"
  },
  {
    "raw_code": "def self.workload_sources\n        dangling_sources.slice(\n          :duo_workflow\n        )\n      end",
    "comment": "Workloads are always dangling but they also have almost all sources of CI variables disabled by default as they do not need access most of the kinds of CI variables.",
    "label": "",
    "id": "6616"
  },
  {
    "raw_code": "def self.ci_sources\n        sources.except(*dangling_sources.keys)\n      end",
    "comment": "CI sources are those pipeline events that affect the CI status of the ref they run for. By definition it excludes dangling pipelines.",
    "label": "",
    "id": "6617"
  },
  {
    "raw_code": "def self.config_sources\n        {\n          unknown_source: nil,\n          repository_source: 1,\n          auto_devops_source: 2,\n          webide_source: 3,\n          remote_source: 4,\n          external_project_source: 5,\n          bridge_source: 6,\n          parameter_source: 7,\n          compliance_source: 8,\n          security_policies_default_source: 9,\n          pipeline_execution_policy_forced: 10\n        }\n      end",
    "comment": "Returns the `Hash` to use for creating the `config_sources` enum for `Ci::Pipeline`.",
    "label": "",
    "id": "6618"
  },
  {
    "raw_code": "def scoped_variables(environment: expanded_environment_name, dependencies: true)\n      track_duration do\n        pipeline\n          .variables_builder\n          .scoped_variables(self, environment: environment, dependencies: dependencies)\n      end",
    "comment": " Variables in the environment name scope. ",
    "label": "",
    "id": "6619"
  },
  {
    "raw_code": "def simple_variables\n      strong_memoize(:simple_variables) do\n        scoped_variables(environment: nil)\n      end",
    "comment": " Variables that do not depend on the environment name. ",
    "label": "",
    "id": "6620"
  },
  {
    "raw_code": "def save_tags\n      super unless Thread.current[BULK_INSERT_TAG_THREAD_KEY]\n    end",
    "comment": "overrides save_tags from acts-as-taggable",
    "label": "",
    "id": "6621"
  },
  {
    "raw_code": "def composite_status\n        Gitlab::Ci::Status::Composite\n          .new(all, with_allow_failure: columns_hash.key?('allow_failure'))\n          .status\n      end",
    "comment": "This will be removed with ci_remove_ensure_stage_service",
    "label": "",
    "id": "6622"
  },
  {
    "raw_code": "def deployment_status\n      return unless deployment_job?\n\n      if success?\n        return successful_deployment_status\n      elsif failed?\n        return :failed\n      end",
    "comment": "Virtual deployment status depending on the environment status.",
    "label": "",
    "id": "6623"
  },
  {
    "raw_code": "def actual_persisted_environment\n      persisted_environment.respond_to?(:__sync) ? persisted_environment.__sync : persisted_environment\n    end",
    "comment": "If build.persisted_environment is a BatchLoader, we need to remove the method proxy in order to clone into new item here https://github.com/exAspArk/batch-loader/issues/31",
    "label": "",
    "id": "6624"
  },
  {
    "raw_code": "def environment_processing_metadata\n      options&.fetch(:environment, {}) || {}\n    end",
    "comment": "Processing options are only available until a pipeline is archived. To retrieve options for historical jobs, use `#environment_permanent_metadata`.",
    "label": "",
    "id": "6625"
  },
  {
    "raw_code": "def ref_slug\n      Gitlab::Utils.slugify(ref.to_s)\n    end",
    "comment": "A slugified version of the build ref, suitable for inclusion in URLs and domain names. Rules:  * Lowercased * Anything not matching [a-z0-9-] is replaced with a - * Maximum length is 63 bytes * First/Last Character is not a hyphen",
    "label": "",
    "id": "6626"
  },
  {
    "raw_code": "def ensure_metadata\n      metadata || build_metadata(project: project)\n    end",
    "comment": "Remove this method with FF `stop_writing_builds_metadata`",
    "label": "",
    "id": "6627"
  },
  {
    "raw_code": "def update_timeout_state\n      timeout = ::Ci::Builds::TimeoutCalculator.new(self).applicable_timeout\n      return unless timeout\n\n      if can_write_metadata?\n        success = ensure_metadata.update(timeout: timeout.value, timeout_source: timeout.source)\n        return false unless success\n      end",
    "comment": "This method is called from within a Ci::Build state transition; it returns nil/true (success) or false (failure)",
    "label": "",
    "id": "6628"
  },
  {
    "raw_code": "def refresh_jid_expiration\n        return unless jid\n\n        Gitlab::SidekiqStatus.set(jid, Gitlab::Import::StuckImportJob::IMPORT_JOBS_EXPIRATION)\n      end",
    "comment": "Refreshes the expiration time of the associated import job ID.  This method can be used by asynchronous importers to refresh the status, preventing the Gitlab::Import::StuckProjectImportJobsWorker from marking the import as failed.",
    "label": "",
    "id": "6629"
  },
  {
    "raw_code": "def data_field(*args)\n        args.each do |arg|\n          self.class_eval <<-RUBY, __FILE__, __LINE__ + 1\n            unless method_defined?(arg)\n              def #{arg}\n                value = data_fields.send('#{arg}')\n                value.nil? ? properties&.dig('#{arg}') : value\n              end\n            end",
    "comment": "Provide convenient accessor methods for data fields. TODO: Simplify as part of https://gitlab.com/gitlab-org/gitlab/issues/29404",
    "label": "",
    "id": "6630"
  },
  {
    "raw_code": "def hook_url\n      raise NotImplementedError\n    end",
    "comment": "Return the URL to be used for the webhook.",
    "label": "",
    "id": "6631"
  },
  {
    "raw_code": "def url_variables\n      raise NotImplementedError\n    end",
    "comment": "Return the url variables to be used for the webhook.",
    "label": "",
    "id": "6632"
  },
  {
    "raw_code": "def hook_ssl_verification\n      if respond_to?(:enable_ssl_verification)\n        enable_ssl_verification\n      else\n        true\n      end",
    "comment": "Return whether the webhook should use SSL verification.",
    "label": "",
    "id": "6633"
  },
  {
    "raw_code": "def update_web_hook!\n      hook = service_hook || build_service_hook\n\n      # Avoid reencryption\n      hook.url = hook_url if hook.url != hook_url\n      hook.url_variables = url_variables if hook.url_variables != url_variables\n\n      hook.enable_ssl_verification = hook_ssl_verification\n      hook.save! if hook.changed?\n      hook\n    end",
    "comment": "Create or update the webhook, raising an exception if it cannot be saved.",
    "label": "",
    "id": "6634"
  },
  {
    "raw_code": "def execute_web_hook!(...)\n      update_web_hook!\n      service_hook.execute(...)\n    end",
    "comment": "Execute the webhook, creating it if necessary.",
    "label": "",
    "id": "6635"
  },
  {
    "raw_code": "def base_reference_pattern(only_long: false)\n          return REFERENCE_PATTERN_LONG_REGEXP if only_long\n\n          REFERENCE_PATTERN_REGEXP\n        end",
    "comment": "Pattern used to extract links from comments Override this method on services that uses different patterns This pattern does not support cross-project references The other code assumes that this pattern is a superset of all overridden patterns. See ReferenceRegexes.external_pattern",
    "label": "",
    "id": "6636"
  },
  {
    "raw_code": "def set_default_data\n        return unless issues_tracker.present?\n\n        # we don't want to override if we have set something\n        return if project_url || issues_url || new_issue_url\n\n        data_fields.project_url = issues_tracker['project_url']\n        data_fields.issues_url = issues_tracker['issues_url']\n        data_fields.new_issue_url = issues_tracker['new_issue_url']\n      end",
    "comment": "Initialize with default properties values",
    "label": "",
    "id": "6637"
  },
  {
    "raw_code": "def reference_pattern(only_long: false)\n          return @reference_pattern if defined?(@reference_pattern)\n\n          regex_suffix = \"|(#{Issue.reference_prefix}#{Gitlab::Regex.issue})\"\n          @reference_pattern = /(?<issue>\\b[A-Za-z][A-Za-z0-9_]*-\\d+\\b)#{regex_suffix if only_long}/\n        end",
    "comment": "{PROJECT-KEY}-{NUMBER} Examples: YT-1, PRJ-1, gl-030",
    "label": "",
    "id": "6638"
  },
  {
    "raw_code": "def validate_discord_url_field?(url_field = 'webhook')\n        return false unless activated?\n        return false unless properties[url_field.to_s].present?\n        return true if active_changed?(to: true)\n\n        # rubocop:disable GitlabSecurity/PublicSend -- url_field must be a field defined on the class, verified by accessing properties\n        url_was = public_send(:\"#{url_field}_was\")\n        url_changed = public_send(:\"#{url_field}_changed?\")\n        # rubocop:enable GitlabSecurity/PublicSend\n\n        return false unless url_changed\n        return true if url_was.blank?\n\n        url_was.match?(DISCORD_URI_REGEX)\n      end",
    "comment": "Prevents breaking changes for users with existing, active, and invalid discord webhooks or event webhook override URLs that were otherwise working for the user. This validation does not run on blank URL fields because presence validation will be done via the `required: true` on each field.",
    "label": "",
    "id": "6639"
  },
  {
    "raw_code": "def normalize_color(color)\n        return (color[0, 1] * 2) + (color[1, 1] * 2) + (color[2, 1] * 2) if color.length == 3\n\n        color\n      end",
    "comment": "Expands the short notation to the full colorcode notation 123456 -> 123456 123    -> 112233",
    "label": "",
    "id": "6640"
  },
  {
    "raw_code": "def field(name, storage: field_storage, **attrs)\n          fields << ::Integrations::Field.new(name: name, integration_class: self, **attrs)\n\n          case storage\n          when :attribute\n            # noop\n          when :properties\n            prop_accessor(name)\n          when :data_fields\n            data_field(name)\n          else\n            raise ArgumentError, \"Unknown field storage: #{storage}\"\n          end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- existing code moved as is :nocov: Tested on subclasses.",
    "label": "",
    "id": "6641"
  },
  {
    "raw_code": "def fields\n          @fields ||= []\n        end",
    "comment": ":nocov: rubocop:enable Gitlab/NoCodeCoverageComment",
    "label": "",
    "id": "6642"
  },
  {
    "raw_code": "def prop_accessor(*args)\n          args.each do |arg|\n            class_eval <<~RUBY, __FILE__, __LINE__ + 1\n              unless method_defined?(arg)\n                def #{arg}\n                  properties['#{arg}'] if properties.present?\n                end\n              end",
    "comment": "Provide convenient accessor methods for each serialized property. Also keep track of updated properties in a similar way as ActiveModel::Dirty",
    "label": "",
    "id": "6643"
  },
  {
    "raw_code": "def boolean_accessor(*args)\n          args.each do |arg|\n            class_eval <<~RUBY, __FILE__, __LINE__ + 1\n              # Make the original getter available as a private method.\n              alias_method :#{arg}_before_type_cast, :#{arg}\n              private(:#{arg}_before_type_cast)\n\n              def #{arg}\n                Gitlab::Utils.to_boolean(#{arg}_before_type_cast)\n              end\n\n              def #{arg}?\n                # '!!' is used because nil or empty string is converted to nil\n                !!#{arg}\n              end\n            RUBY\n          end",
    "comment": "Provide convenient boolean accessor methods for each serialized property. Also keep track of updated properties in a similar way as ActiveModel::Dirty",
    "label": "",
    "id": "6644"
  },
  {
    "raw_code": "def nonexistent_integration_types_for(\n          scope,\n          include_group_specific: false,\n          include_instance_specific: false)\n          # Using #map instead of #pluck to save one query count. This is because\n          # ActiveRecord loaded the object here, so we don't need to query again later.\n          available_integration_types(\n            include_project_specific: false,\n            include_group_specific: include_group_specific,\n            include_instance_specific: include_instance_specific\n          ) - scope.map(&:type)\n        end",
    "comment": "Returns a list of integration types that do not exist in the given scope. Example: [\"AsanaService\", ...]",
    "label": "",
    "id": "6645"
  },
  {
    "raw_code": "def available_integration_names(\n          include_project_specific: true,\n          include_group_specific: true,\n          include_instance_specific: true,\n          include_dev: true,\n          include_disabled: false)\n          names = integration_names.dup\n          names.concat(project_specific_integration_names) if include_project_specific\n          names.concat(dev_integration_names) if include_dev\n          names.concat(instance_specific_integration_names) if include_instance_specific\n\n          if include_project_specific || include_group_specific\n            names.concat(project_and_group_specific_integration_names)\n          end",
    "comment": "Returns a list of available integration names. Example: [\"asana\", ...]",
    "label": "",
    "id": "6646"
  },
  {
    "raw_code": "def available_integration_types(...)\n          available_integration_names(...).map do # rubocop:disable Style/NumberedParameters -- existing code moved as is\n            integration_name_to_type(_1)\n          end",
    "comment": "Returns a list of available integration types. Example: [\"Integrations::Asana\", ...]",
    "label": "",
    "id": "6647"
  },
  {
    "raw_code": "def disabled_integration_names\n          # The GitLab for Slack app integration is only available when enabled through settings.\n          # The Slack Slash Commands integration is only available for customers\n          # who cannot use the GitLab for Slack app.\n          disabled = Gitlab::CurrentSettings.slack_app_enabled ? ['slack_slash_commands'] : ['gitlab_slack_application']\n          disabled += ['jira_cloud_app'] unless Gitlab::CurrentSettings.jira_connect_application_key.present?\n          disabled\n        end",
    "comment": "Returns a list of disabled integration names. Example: [\"gitlab_slack_application\", ...]",
    "label": "",
    "id": "6648"
  },
  {
    "raw_code": "def integration_name_to_model(name)\n          type = integration_name_to_type(name)\n          integration_type_to_model(type)\n        end",
    "comment": "Returns the model for the given integration name. Example: :asana => Integrations::Asana",
    "label": "",
    "id": "6649"
  },
  {
    "raw_code": "def integration_name_to_type(name)\n          name = name.to_s\n          if all_integration_names.include?(name)\n            \"Integrations::#{name.camelize}\"\n          else\n            Gitlab::ErrorTracking.track_and_raise_for_dev_exception(UnknownType.new(name.inspect))\n          end",
    "comment": "Returns the STI type for the given integration name. Example: \"asana\" => \"Integrations::Asana\"",
    "label": "",
    "id": "6650"
  },
  {
    "raw_code": "def integration_type_to_model(type)\n          type.constantize\n        end",
    "comment": "Returns the model for the given STI type. Example: \"Integrations::Asana\" => Integrations::Asana",
    "label": "",
    "id": "6651"
  },
  {
    "raw_code": "def all_integration_names\n          available_integration_names(include_disabled: true)\n        end",
    "comment": "Returns the names of all integrations, including:  - All project, group and instance-level only integrations - Integrations that are not available on the instance - Development-only integrations",
    "label": "",
    "id": "6652"
  },
  {
    "raw_code": "def create_from_active_default_integrations(owner, association)\n          default_integrations(\n            owner,\n            active.where.not(type: instance_specific_integration_types)\n          ).count { |_type, integration| build_from_integration(integration, association => owner.id).save }\n        end",
    "comment": "Returns the number of successfully saved integrations Duplicate integrations are excluded from this count by their validations.",
    "label": "",
    "id": "6653"
  },
  {
    "raw_code": "def sorted_ancestors(scope)\n          if scope.root_ancestor.use_traversal_ids?\n            Namespace.from(scope.ancestors(hierarchy_order: :asc))\n          else\n            scope.ancestors\n          end",
    "comment": "Ancestors sorted by hierarchy depth in bottom-top order.",
    "label": "",
    "id": "6654"
  },
  {
    "raw_code": "def after_build_from_integration(new_integration)\n        # no-op\n      end",
    "comment": "Hook for integrations to configure associations after duplication. Override in subclasses when associations need the parent context.",
    "label": "",
    "id": "6655"
  },
  {
    "raw_code": "def json_fields\n        %w[active]\n      end",
    "comment": "Expose a list of fields in the JSON endpoint.  This list is used in `Integration#as_json(only: json_fields)`.",
    "label": "",
    "id": "6656"
  },
  {
    "raw_code": "def to_database_hash\n        column = self.class.attribute_aliases.fetch('type', 'type')\n\n        attributes_for_database\n          .except(*BASE_ATTRIBUTES)\n          .merge(column => type)\n          .merge(reencrypt_properties)\n      end",
    "comment": "Returns a hash of attributes (columns => values) used for inserting into the database.",
    "label": "",
    "id": "6657"
  },
  {
    "raw_code": "def testable?\n        project_level?\n      end",
    "comment": "Disable test for instance-level and group-level integrations. https://gitlab.com/gitlab-org/gitlab/-/issues/213138",
    "label": "",
    "id": "6658"
  },
  {
    "raw_code": "def updated_properties\n        @updated_properties ||= ActiveSupport::HashWithIndifferentAccess.new\n      end",
    "comment": "Returns a hash of the properties that have been assigned a new value since last save, indicating their original values (attr => original value). ActiveRecord does not provide a mechanism to track changes in serialized keys, so we need a specific implementation for integration properties. This allows to track changes to properties set with the accessor methods, but not direct manipulation of properties hash.",
    "label": "",
    "id": "6659"
  },
  {
    "raw_code": "def supports_data_fields?\n        false\n      end",
    "comment": "override if needed",
    "label": "",
    "id": "6660"
  },
  {
    "raw_code": "def supports_data_fields?\n          false\n        end",
    "comment": "Normally external issue trackers in GitLab save `project_url`, `issue_url` and `new_issue_url` in a separate DB table, but for the Linear integration we just use the `encrypted_properties` column  That's why we overwrite `supports_data_fields?` and `handle_properties`  For Linear only the workspace url is needed and the rest of the URLs can be derived ",
    "label": "",
    "id": "6661"
  },
  {
    "raw_code": "def build_page(sha, ref)\n        # override this method in the including class\n      end",
    "comment": "Return complete url to build page  Ex. http://jenkins.example.com:8888/job/test1/scm/bySHA1/12d65c ",
    "label": "",
    "id": "6662"
  },
  {
    "raw_code": "def commit_status(sha, ref)\n        # override this method in the including class\n      end",
    "comment": "Return string with build status or :error symbol  Allowed states: 'success', 'failed', 'running', 'pending', 'skipped'   Ex. @integration.commit_status('13be4ac', 'master') # => 'success'  @integration.commit_status('2abe4ac', 'dev') # => 'running'  ",
    "label": "",
    "id": "6663"
  },
  {
    "raw_code": "def build_page(sha, _ref)\n          Gitlab::Utils.append_path(\n            mock_service_url,\n            \"#{project.namespace.path}/#{project.path}/status/#{sha}\")\n        end",
    "comment": "Return complete url to build page  Ex. http://jenkins.example.com:8888/job/test1/scm/bySHA1/12d65c ",
    "label": "",
    "id": "6664"
  },
  {
    "raw_code": "def commit_status(sha, _ref)\n          response = Clients::HTTP.get(commit_status_path(sha), verify: enable_ssl_verification)\n          read_commit_status(response)\n        rescue Errno::ECONNREFUSED\n          :error\n        end",
    "comment": "Return string with build status or :error symbol  Allowed states: 'success', 'failed', 'running', 'pending', 'skipped'  Ex. @service.commit_status('13be4ac', 'master') # => 'success'  @service.commit_status('2abe4ac', 'dev') # => 'running' ",
    "label": "",
    "id": "6665"
  },
  {
    "raw_code": "def has_public_url_validation_options?\n          false\n        end",
    "comment": "Allows chat integrations to indicate there are specific addressable_url validation options to use instead of using default public_url options",
    "label": "",
    "id": "6666"
  },
  {
    "raw_code": "def configurable_channels?\n        false\n      end",
    "comment": "With some integrations the webhook is already tied to a specific channel, for others the channels are configurable for each event.",
    "label": "",
    "id": "6667"
  },
  {
    "raw_code": "def notify(message, opts)\n        raise NotImplementedError\n      end",
    "comment": "every notifier must implement this independently",
    "label": "",
    "id": "6668"
  },
  {
    "raw_code": "def get_message(object_kind, data)\n        case object_kind\n        when \"push\", \"tag_push\"\n          Integrations::ChatMessage::PushMessage.new(data) if notify_for_ref?(data)\n        when \"issue\", \"incident\"\n          Integrations::ChatMessage::IssueMessage.new(data) unless update?(data)\n        when \"merge_request\"\n          Integrations::ChatMessage::MergeMessage.new(data) unless update?(data)\n        when \"note\"\n          Integrations::ChatMessage::NoteMessage.new(data)\n        when \"pipeline\"\n          Integrations::ChatMessage::PipelineMessage.new(data) if should_pipeline_be_notified?(data)\n        when \"wiki_page\"\n          Integrations::ChatMessage::WikiPageMessage.new(data)\n        when \"deployment\"\n          Integrations::ChatMessage::DeploymentMessage.new(data) if notify_for_ref?(data)\n        when \"group_mention\"\n          Integrations::ChatMessage::GroupMentionMessage.new(data)\n        end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity -- Legacy use",
    "label": "",
    "id": "6669"
  },
  {
    "raw_code": "def build_event_channels\n        event_channel_names.map do |channel_field|\n          Field.new(\n            name: channel_field,\n            type: :text,\n            placeholder: default_channel_placeholder,\n            integration_class: self\n          )\n        end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "6670"
  },
  {
    "raw_code": "def reference_pattern(*)\n          @reference_pattern ||= /\\b(?<![@-])(?<issue>T\\d+)\\b/\n        end",
    "comment": "See https://we.phorge.it/source/phorge/browse/master/src/infrastructure/markup/rule/PhabricatorObjectRemarkupRule.php for a canonical source of the regular expression used to parse Phorge object references.  > The \"(?<![#@-])\" prevents us from linking \"#abcdef\" or similar, and > \"ABC-T1\" (see T5714), and from matching \"@T1\" as a task (it is a user) > (see T9479).  Note that object references in Phorge are prefixed with letters unique to their underlying application, so T123 (a Maniphest task) is distinct from D123 (a Differential patch). Keeping the T as part of the task ID is appropriate here as it leaves room for expanding reference parsing/linking to other types of Phorge entities.  Also note, a prefix of # is being allowed here due to: 1) an assumed likelihood of use; and b) lack of collision with native GitLab issues since all Phorge identifiers have the application specific alpha prefix.",
    "label": "",
    "id": "6671"
  },
  {
    "raw_code": "def help\n          '<p>Use this service to send notifications about events in ' \\\n            'GitLab projects to your Microsoft Teams channels. ' \\\n            '<a href=\"https://docs.gitlab.com/ee/user/project/integrations/microsoft_teams.html\" target=\"_blank\" ' \\\n            'rel=\"noopener noreferrer\">How do I configure this integration?</a></p>'\n        end",
    "comment": "rubocop:disable Gitlab/DocumentationLinks/HardcodedUrl -- legacy use",
    "label": "",
    "id": "6672"
  },
  {
    "raw_code": "def supported_events\n          %w[push issue confidential_issue merge_request note confidential_note tag_push\n            pipeline wiki_page]\n        end",
    "comment": "rubocop:enable Gitlab/DocumentationLinks/HardcodedUrl",
    "label": "",
    "id": "6673"
  },
  {
    "raw_code": "def last_edited_by\n      updated_by\n    end",
    "comment": "Alias to make application_helper#edited_time_ago_with_tooltip helper work properly with notes. See https://gitlab.com/gitlab-org/gitlab-foss/merge_requests/10392/diffs#note_28719102",
    "label": "",
    "id": "6674"
  },
  {
    "raw_code": "def discussion_id(noteable = nil)\n      discussion_class(noteable).override_discussion_id(self) || super() || ensure_discussion_id\n    end",
    "comment": "See `Discussion.override_discussion_id` for details.",
    "label": "",
    "id": "6675"
  },
  {
    "raw_code": "def to_discussion(noteable = nil)\n      ::Discussion.build([self], noteable, inverse_relations: false)\n    end",
    "comment": "Returns a discussion containing just this note. This method exists as an alternative to `#discussion` to use when the methods we intend to call on the Discussion object don't require it to have all of its notes, and just depend on the first note or the type of discussion. This saves us a DB query.",
    "label": "",
    "id": "6676"
  },
  {
    "raw_code": "def discussion=(discussion)\n      @discussion = discussion # rubocop:disable Gitlab/ModuleWithInstanceVariables -- Overriding a memoized value below\n    end",
    "comment": "Set the discussion manually. This is used to setup the inverse relationship between the note and its discussion",
    "label": "",
    "id": "6677"
  },
  {
    "raw_code": "def discussion\n      full_discussion = noteable.notes.find_discussion(discussion_id) if noteable && part_of_discussion?\n\n      full_discussion || to_discussion\n    end",
    "comment": "Returns the entire discussion this note is part of. Consider using `#to_discussion` if we do not need to render the discussion and all its notes and if we don't care about the discussion's resolvability status.",
    "label": "",
    "id": "6678"
  },
  {
    "raw_code": "def allow_local_requests?\n        Gitlab::CurrentSettings.allow_local_requests_from_web_hooks_and_services?\n      end",
    "comment": "Allow urls pointing localhost and the local network",
    "label": "",
    "id": "6679"
  },
  {
    "raw_code": "def rate_limited?\n        rate_limiter.rate_limited?\n      end",
    "comment": "@return [Boolean] Whether or not the WebHook is currently throttled.",
    "label": "",
    "id": "6680"
  },
  {
    "raw_code": "def rate_limit\n        rate_limiter.limit\n      end",
    "comment": "@return [Integer] The rate limit for the WebHook. `0` for no limit.",
    "label": "",
    "id": "6681"
  },
  {
    "raw_code": "def parent; end\n\n      # Custom attributes to be included in the worker context.\n      def application_context\n        { related_class: self.class.to_s }\n      end\n\n      # Exclude binary columns by default - they have no sensible JSON encoding\n      def serializable_hash(options = nil)\n        options = options.try(:dup) || {}\n        options[:except] = Array(options[:except]).dup\n        options[:except].concat [:encrypted_url_variables, :encrypted_url_variables_iv]\n\n        super\n      end\n\n      def interpolated_url(url = self.url, url_variables = self.url_variables)\n        return url unless url.include?('{')\n\n        vars = url_variables\n        url.gsub(VARIABLE_REFERENCE_RE) do |match|\n          vars.fetch(match.delete_prefix('{').delete_suffix('}'))\n        end\n      rescue KeyError => e\n        raise InterpolationError, \"Invalid URL template. Missing key #{e.key}\"\n      end",
    "comment": "Returns the associated Project or Group for the WebHook if one exists. Overridden by inheriting classes.",
    "label": "",
    "id": "6682"
  },
  {
    "raw_code": "def backoff!\n      return unless auto_disabling_enabled? && executable?\n\n      new_recent_failures = next_failure_count\n\n      attrs = { recent_failures: new_recent_failures }\n      attrs[:disabled_until] = next_backoff.from_now if new_recent_failures > TEMPORARILY_DISABLED_FAILURE_THRESHOLD\n\n      assign_attributes(attrs)\n\n      return unless changed?\n\n      logger.info(hook_id: id, action: 'backoff', **attrs)\n      save(validate: false)\n    end",
    "comment": "Don't actually back-off until a grace level of TEMPORARILY_DISABLED_FAILURE_THRESHOLD failures have been seen tracked in the recent_failures counter",
    "label": "",
    "id": "6683"
  },
  {
    "raw_code": "def subject_class\n        start_event.object_type\n      end",
    "comment": "The model class that is going to be queried, Issue or MergeRequest",
    "label": "",
    "id": "6684"
  },
  {
    "raw_code": "def self.most_recent_for(packages, extra_join: nil, extra_where: nil)\n      cte_name = :packages_cte\n      cte = Gitlab::SQL::CTE.new(cte_name, packages.select(:id))\n\n      package_files = ::Packages::PackageFile.installable\n                                             .limit_recent(1)\n                                             .where(arel_table[:package_id].eq(Arel.sql(\"#{cte_name}.id\")))\n\n      package_files = package_files.joins(extra_join) if extra_join\n      package_files = package_files.where(extra_where) if extra_where\n\n      query = select('finder.*')\n                .from([Arel.sql(cte_name.to_s), package_files.arel.lateral.as('finder')])\n\n      query.with(cte.to_arel)\n    end",
    "comment": "Returns the most recent installable package file for *each* of the given packages. The order is not guaranteed.",
    "label": "",
    "id": "6685"
  },
  {
    "raw_code": "def last_build_info\n      build_infos.last\n    end",
    "comment": "Technical debt: to be removed in https://gitlab.com/gitlab-org/gitlab/-/issues/281937",
    "label": "",
    "id": "6686"
  },
  {
    "raw_code": "def pipeline\n      last_build_info&.pipeline\n    end",
    "comment": "Technical debt: to be removed in https://gitlab.com/gitlab-org/gitlab/-/issues/281937",
    "label": "",
    "id": "6687"
  },
  {
    "raw_code": "def self.scope_of(package_name)\n      return unless package_name\n      return unless package_name.starts_with?('@')\n      return unless package_name.include?('/')\n\n      package_name.match(Gitlab::Regex.npm_package_name_regex)&.captures&.first\n    end",
    "comment": "from \"@scope/package-name\" return \"scope\" or nil",
    "label": "",
    "id": "6688"
  },
  {
    "raw_code": "def initialize(major = 0, minor = 0, patch = 0, prerelease = nil, build = nil, prefixed: false)\n    @major = major\n    @minor = minor\n    @patch = patch\n    @prerelease = prerelease\n    @build = build\n    @prefixed = prefixed\n  end",
    "comment": "TODO: Move logic into the SemanticVersionable concern https://gitlab.com/gitlab-org/gitlab/-/issues/455670",
    "label": "",
    "id": "6689"
  },
  {
    "raw_code": "def self.event_allowed?(event_type)\n      return true if UNIQUE_EVENTS_ALLOWED.include?(event_type.to_sym)\n\n      false\n    end",
    "comment": "Remove some of the events, for now, so we don't hammer Redis too hard. See: https://gitlab.com/gitlab-org/gitlab/-/issues/280770",
    "label": "",
    "id": "6690"
  },
  {
    "raw_code": "def self.unique_counters_for(event_scope, event_type, originator_type)\n      return [] unless event_allowed?(event_type)\n      return [] if originator_type.to_s == 'guest'\n\n      [\"#{EVENT_PREFIX}_#{event_scope}_#{originator_type}\"]\n    end",
    "comment": "counter names for unique user tracking (for MAU)",
    "label": "",
    "id": "6691"
  },
  {
    "raw_code": "def self.for_push_exists_for_projects_and_packages(projects_and_packages)\n        return none if projects_and_packages.blank?\n\n        project_ids, package_names, package_types = projects_and_packages.transpose\n\n        cte_query_sql = <<~SQL\n          unnest(\n            ARRAY[:project_ids]::bigint[],\n            ARRAY[:package_names]::text[],\n            ARRAY[:package_types]::smallint[]\n          ) AS projects_and_packages(project_id, package_name, package_type)\n        SQL\n\n        cte_query =\n          select('*').from(sanitize_sql_array(\n            [cte_query_sql, { project_ids: project_ids, package_names: package_names, package_types: package_types }]\n          ))\n\n        cte_name = :projects_and_packages_cte\n        cte = Gitlab::SQL::CTE.new(cte_name, cte_query)\n\n        rules_cte_project_id = \"#{cte_name}.#{connection.quote_column_name('project_id')}\"\n        rules_cte_package_name = \"#{cte_name}.#{connection.quote_column_name('package_name')}\"\n        rules_cte_package_type = \"#{cte_name}.#{connection.quote_column_name('package_type')}\"\n\n        protection_rule_exsits_subquery = select(1)\n          .where(\"#{rules_cte_project_id} = project_id\")\n          .where(arel_table[:package_type].eq(Arel.sql(rules_cte_package_type)))\n          .where(\"#{rules_cte_package_name} ILIKE #{::Gitlab::SQL::Glob.to_like('package_name_pattern')}\")\n\n        query = select(\n          rules_cte_project_id,\n          rules_cte_package_type,\n          rules_cte_package_name,\n          sanitize_sql_array(['EXISTS(?) AS protected', protection_rule_exsits_subquery])\n        ).from(Arel.sql(cte_name.to_s))\n\n        connection.exec_query(query.with(cte.to_arel).to_sql)\n      end",
    "comment": " Accepts a list of projects and packages and returns a result set indicating whether the package name is protected.  @param [Array<Array>] projects_and_packages an array of arrays where each sub-array contains the project id (bigint), the package name (string) and the package type (smallint). @return [ActiveRecord::Result] a result set indicating whether each project, package name and package type is protected.  Example: Packages::Protection::Rule.for_push_exists_for_projects_and_packages([ [1, '@my_group/my_project_1/package_1', 2], [1, '@my_group/my_project_1/package_2', 2], [2, '@my_group/my_project_2/package_1', 3], ... ])  [ {'project_id' => 1, 'package_name' => '@my_group/my_project_1/package_1', 'package_type' => 2, 'protected' => true}, {'project_id' => 1, 'package_name' => '@my_group/my_project_1/package_2', 'package_type' => 2, 'protected' => false}, {'project_id' => 2, 'package_name' => '@my_group/my_project_2/package_1', 'package_type' => 3, 'protected' => true}, ... ] ",
    "label": "",
    "id": "6692"
  },
  {
    "raw_code": "def normalized_pypi_name\n        return name unless pypi?\n\n        name.gsub(/#{Gitlab::Regex::Packages::PYPI_NORMALIZED_NAME_REGEX_STRING}/o, '-').downcase\n      end",
    "comment": "As defined in PEP 503 https://peps.python.org/pep-0503/#normalized-names",
    "label": "",
    "id": "6693"
  },
  {
    "raw_code": "def self.active\n        where.not(keep_n_duplicated_package_files: 'all')\n      end",
    "comment": "used by Schedulable",
    "label": "",
    "id": "6694"
  },
  {
    "raw_code": "def follows_npm_naming_convention?\n        return false unless project&.root_namespace&.path\n\n        project.root_namespace.path == ::Packages::Npm.scope_of(name)\n      end",
    "comment": "https://docs.gitlab.com/ee/user/packages/npm_registry/#package-naming-convention",
    "label": "",
    "id": "6695"
  },
  {
    "raw_code": "def begin_fast_destroy\n        all_stores.each_with_object({}) do |store, result|\n          relation = public_send(store) # rubocop:disable GitlabSecurity/PublicSend\n          keys = get_store_class(store).keys(relation)\n\n          result[store] = keys if keys.present?\n        end",
    "comment": " FastDestroyAll concerns",
    "label": "",
    "id": "6696"
  },
  {
    "raw_code": "def finalize_fast_destroy(keys)\n        keys.each do |store, value|\n          get_store_class(store).delete_keys(value)\n        end",
    "comment": " FastDestroyAll concerns",
    "label": "",
    "id": "6697"
  },
  {
    "raw_code": "def with_read_consistency(build, &block)\n        ::Gitlab::Database::Consistency\n          .with_read_consistency(&block)\n      end",
    "comment": " Sometime we need to ensure that the first read goes to a primary database, what is especially important in EE. This method does not change the behavior in CE. ",
    "label": "",
    "id": "6698"
  },
  {
    "raw_code": "def metadata_attributes\n        attribute_names - %w[raw_data]\n      end",
    "comment": " Sometimes we do not want to read raw data. This method makes it easier to find attributes that are just metadata excluding raw data. ",
    "label": "",
    "id": "6699"
  },
  {
    "raw_code": "def persist_data!\n      in_lock(lock_key, **lock_params) do # exclusive Redis lock is acquired first\n        raise FailedToPersistDataError, 'Modifed build trace chunk detected' if has_changes_to_save?\n\n        self.class.with_read_consistency(build) do\n          reset.unsafe_persist_data!\n        end",
    "comment": " It is possible that we run into two concurrent migrations. It might happen that a chunk gets migrated after being loaded by another worker but before the worker acquires a lock to perform the migration.  We are using Redis locking to ensure that we perform this operation inside an exclusive lock, but this does not prevent us from running into race conditions related to updating a model representation in the database. Optimistic locking is another mechanism that help here.  We are using optimistic locking combined with Redis locking to ensure that a chunk gets migrated properly.  We are using until_executed deduplication strategy for workers, which should prevent duplicated workers running in parallel for the same build trace, and causing an exception related to an exclusive lock not being acquired ",
    "label": "",
    "id": "6700"
  },
  {
    "raw_code": "def final?\n      build.pending_state.present? && chunks_max_index == chunk_index\n    end",
    "comment": " Build trace chunk is final (the last one that we do not expect to ever become full) when a runner submitted a build pending state and there is no chunk with higher index in the database. ",
    "label": "",
    "id": "6701"
  },
  {
    "raw_code": "def allow_stale_runner_pruning?\n      false\n    end",
    "comment": "Overridden in EE::Namespace",
    "label": "",
    "id": "6702"
  },
  {
    "raw_code": "def allow_stale_runner_pruning=(_value)\n      raise NotImplementedError\n    end",
    "comment": "Overridden in EE::Namespace",
    "label": "",
    "id": "6703"
  },
  {
    "raw_code": "def set_status(new_status)\n      retry_optimistic_lock(self, name: 'ci_stage_set_status') do\n        case new_status\n        when 'created' then nil\n        when 'waiting_for_resource' then request_resource\n        when 'preparing' then prepare\n        when 'waiting_for_callback' then wait_for_callback\n        when 'pending' then enqueue\n        when 'running' then run\n        when 'success' then succeed\n        when 'failed' then drop\n        when 'canceling' then start_cancel\n        when 'canceled' then cancel\n        when 'manual' then block\n        when 'scheduled' then delay\n        when 'skipped', nil then skip\n        else\n          raise Ci::HasStatus::UnknownStatusError, \"Unknown status `#{new_status}`\"\n        end",
    "comment": "rubocop: disable Metrics/CyclomaticComplexity -- breaking apart hurts readability, consider refactoring issue #439268",
    "label": "",
    "id": "6704"
  },
  {
    "raw_code": "def update_legacy_status\n      set_status(latest_stage_status.to_s)\n    end",
    "comment": "rubocop: enable Metrics/CyclomaticComplexity This will be removed with ci_remove_ensure_stage_service",
    "label": "",
    "id": "6705"
  },
  {
    "raw_code": "def confirm_manual_job?\n      manual_jobs = processables.manual\n      return false unless manual_jobs.exists?\n\n      manual_jobs.includes(:pipeline, :metadata, [deployment: [environment: :project]]).any? do |job|\n        job.playable? && job.manual_confirmation_message\n      end",
    "comment": "We only check jobs that are played by `Ci::PlayManualStageService`.",
    "label": "",
    "id": "6706"
  },
  {
    "raw_code": "def latest_stage_status\n      statuses.latest.composite_status || 'skipped'\n    end",
    "comment": "This will be removed with ci_remove_ensure_stage_service",
    "label": "",
    "id": "6707"
  },
  {
    "raw_code": "def can_attempt_archival_now?\n      return false unless archival_attempts_available?\n      return true unless last_archival_attempt_at\n\n      (last_archival_attempt_at + backoff).past?\n    end",
    "comment": "The job is retried around 5 times during the 7 days retention period for trace chunks as defined in `Ci::BuildTraceChunks::RedisBase::CHUNK_REDIS_TTL`",
    "label": "",
    "id": "6708"
  },
  {
    "raw_code": "def self.search(query)\n      with_encrypted_tokens(encode(query)).or(fuzzy_search(query, [:description]))\n    end",
    "comment": "Searches for runners matching the given query.  This method uses ILIKE on PostgreSQL for the description field and performs a full match on tokens.  query - The search query as a String.  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "6709"
  },
  {
    "raw_code": "def deprecated_rest_status\n      return :stale if stale?\n\n      if contacted_at.nil?\n        :never_contacted\n      elsif active?\n        online? ? :online : :offline\n      else\n        :paused\n      end",
    "comment": "DEPRECATED TODO Remove in v5 in favor of `status` for REST calls, see https://gitlab.com/gitlab-org/gitlab/-/issues/344648",
    "label": "",
    "id": "6710"
  },
  {
    "raw_code": "def dot_com_gitlab_hosted?\n      Gitlab.com? && instance_type?\n    end",
    "comment": "CI_JOB_JWT_V2 that uses this method is deprecated  On .com all instance runners are hosted so instance_type is used to distingish hosted from non-hosted",
    "label": "",
    "id": "6711"
  },
  {
    "raw_code": "def dedicated_gitlab_hosted?\n      false\n    end",
    "comment": "false in FOSS",
    "label": "",
    "id": "6712"
  },
  {
    "raw_code": "def model_class\n      ::Ci::Build\n    end",
    "comment": "Dependencies can only be of Ci::Build type because only builds can create artifacts",
    "label": "",
    "id": "6713"
  },
  {
    "raw_code": "def local\n      strong_memoize(:local) do\n        next [] if no_local_dependencies_specified?\n        next [] unless processable.pipeline_id # we don't have any dependency when creating the pipeline\n\n        deps = model_class.where(pipeline_id: processable.pipeline_id, partition_id: processable.partition_id).latest\n        deps = find_dependencies(processable, deps)\n\n        from_dependencies(deps).to_a\n      end",
    "comment": "Dependencies local to the given pipeline",
    "label": "",
    "id": "6714"
  },
  {
    "raw_code": "def cross_pipeline\n      strong_memoize(:cross_pipeline) do\n        fetch_dependencies_in_hierarchy\n      end",
    "comment": "Dependencies from the same parent-pipeline hierarchy excluding the current job's pipeline",
    "label": "",
    "id": "6715"
  },
  {
    "raw_code": "def cross_project\n      []\n    end",
    "comment": "Dependencies that are defined by project and ref",
    "label": "",
    "id": "6716"
  },
  {
    "raw_code": "def model_name\n        ActiveModel::Name.new(self, nil, 'job')\n      end",
    "comment": "This is needed for url_for to work, as the controller is JobsController",
    "label": "",
    "id": "6717"
  },
  {
    "raw_code": "def self.has_any_job_definition?\n      left_joins(:job_definition_instance).limit(1).pick(:job_id).present?\n    end",
    "comment": "TODO: remove this method with FF `read_from_new_ci_destinations`",
    "label": "",
    "id": "6718"
  },
  {
    "raw_code": "def supports_canceling?\n      cancel_gracefully?\n    end",
    "comment": "A Ci::Bridge may transition to `canceling` as a result of strategy: :depend but only a Ci::Build will transition to `canceling`` via `.cancel`",
    "label": "",
    "id": "6719"
  },
  {
    "raw_code": "def play(current_user, job_variables_attributes = nil)\n      Ci::PlayBuildService.new(current_user: current_user, build: self, variables: job_variables_attributes).execute\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6720"
  },
  {
    "raw_code": "def cancelable?\n      (active? || created?) && !canceling?\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6721"
  },
  {
    "raw_code": "def variables\n      strong_memoize(:variables) do\n        Gitlab::Ci::Variables::Collection.new\n          .concat(base_variables)\n          .concat(pages_variables)\n      end",
    "comment": " All variables, including persisted environment variables. ",
    "label": "",
    "id": "6722"
  },
  {
    "raw_code": "def available_artifacts?\n      (!artifacts_expired? || pipeline.artifacts_locked?) && job_artifacts_archive&.exists?\n    end",
    "comment": "This method is similar to #artifacts? but it includes the artifacts locking mechanics. A new method was created to prevent breaking existing behavior and avoid introducing N+1s.",
    "label": "",
    "id": "6723"
  },
  {
    "raw_code": "def doom!\n      transaction do\n        now = Time.current\n        attributes = {\n          status: :failed,\n          failure_reason: :data_integrity_failure,\n          updated_at: now\n        }\n        attributes[:finished_at] = now unless finished_at.present?\n\n        update_columns(attributes)\n        all_queuing_entries.delete_all\n        all_runtime_metadata.delete_all\n      end",
    "comment": "Consider this object to have an unknown job problem",
    "label": "",
    "id": "6724"
  },
  {
    "raw_code": "def all_queuing_entries\n      ::Ci::PendingBuild.where(build_id: id)\n    end",
    "comment": " We can have only one queuing entry or running build tracking entry, because there is a unique index on `build_id` in each table, but we need a relation to remove these entries more efficiently in a single statement without actually loading data. ",
    "label": "",
    "id": "6725"
  },
  {
    "raw_code": "def waiting_for_runner_ack?\n      pending? && runner_id.present? && runner_manager_id_waiting_for_ack.present?\n    end",
    "comment": " Support for two-phase runner job acceptance acknowledgement ",
    "label": "",
    "id": "6726"
  },
  {
    "raw_code": "def set_waiting_for_runner_ack(runner_manager_id)\n      return unless runner_manager_id.present?\n\n      with_redis do |redis|\n        # Store runner manager ID for this job, only if key does not yet exist\n        redis.set(runner_build_ack_queue_key, runner_manager_id, ex: RUNNER_ACK_QUEUE_EXPIRY_TIME, nx: true)\n      end",
    "comment": "Create a Redis cache entry containing the runner manager id on which we're waiting on for acknowledgement (job accepted or job declined)",
    "label": "",
    "id": "6727"
  },
  {
    "raw_code": "def heartbeat_runner_ack_wait(runner_manager_id)\n      return unless runner_manager_id.present? && runner_manager_id == runner_manager_id_waiting_for_ack\n\n      with_redis do |redis|\n        # Update TTL, only if key already exists\n        redis.set(runner_build_ack_queue_key, runner_manager_id, ex: RUNNER_ACK_QUEUE_EXPIRY_TIME, xx: true)\n      end",
    "comment": "Update the ttl for the Redis cache entry containing the runner manager id on which we're waiting on for acknowledgement (job accepted or job declined)",
    "label": "",
    "id": "6728"
  },
  {
    "raw_code": "def cancel_wait_for_runner_ack\n      with_redis do |redis|\n        redis.del(runner_build_ack_queue_key)\n      end",
    "comment": "Remove the Redis cache entry containing the runner manager id on which we're waiting on for acknowledgement (job accepted or job declined)",
    "label": "",
    "id": "6729"
  },
  {
    "raw_code": "def should_delete?\n      pipeline.status.to_sym.in?(::Ci::Pipeline.stopped_statuses)\n    end",
    "comment": "This needs to be kept in sync with `Ci::Pipeline#after_transition` calling `pipeline.persistent_ref.delete`",
    "label": "",
    "id": "6730"
  },
  {
    "raw_code": "def self.populate_scheduling_type!\n      needs = Ci::BuildNeed.scoped_build.select(1)\n      where(scheduling_type: nil).update_all(\n        \"scheduling_type = CASE WHEN (EXISTS (#{needs.to_sql}))\n         THEN #{scheduling_types[:dag]}\n         ELSE #{scheduling_types[:stage]}\n         END\"\n      )\n    end",
    "comment": "Old processables may have scheduling_type as nil, so we need to ensure the data exists before using it.",
    "label": "",
    "id": "6731"
  },
  {
    "raw_code": "def scoped_user\n      # If jobs are retried by human users (not composite identity) we want to\n      # ignore the persisted `scoped_user_id`, because that is propagated\n      # together with `options` to cloned jobs.\n      # We also handle the case where `user` is `nil` (legacy behavior in specs).\n      return unless user&.composite_identity_enforced?\n\n      User.find_by_id(scoped_user_id)\n    end",
    "comment": "Scoped user is present when the user creating the pipeline supports composite identity. For example: a service account like GitLab Duo. The scoped user is used to further restrict the permissions of the CI job token associated to the `job.user`.",
    "label": "",
    "id": "6732"
  },
  {
    "raw_code": "def scheduling_type_dag?\n      scheduling_type.nil? ? find_legacy_scheduling_type == :dag : super\n    end",
    "comment": "Overriding scheduling_type enum's method for nil `scheduling_type`s",
    "label": "",
    "id": "6733"
  },
  {
    "raw_code": "def find_legacy_scheduling_type\n      strong_memoize(:find_legacy_scheduling_type) do\n        needs.exists? ? :dag : :stage\n      end",
    "comment": "scheduling_type column of previous builds/bridges have not been populated, so we calculate this value on runtime when we need it.",
    "label": "",
    "id": "6734"
  },
  {
    "raw_code": "def destroy\n      nullify_dependent_associations_in_batches\n\n      super\n    end",
    "comment": "Using destroy instead of before_destroy as we want nullify_dependent_associations_in_batches to run first and not in a transaction block. This prevents timeouts for schedules with numerous pipelines",
    "label": "",
    "id": "6735"
  },
  {
    "raw_code": "def self.begin_fast_destroy\n      service = ::Ci::JobArtifacts::DestroyAssociationsService.new(self)\n      service.destroy_records\n      service\n    end",
    "comment": " FastDestroyAll concerns rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6736"
  },
  {
    "raw_code": "def self.finalize_fast_destroy(service)\n      service.update_statistics\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass  FastDestroyAll concerns",
    "label": "",
    "id": "6737"
  },
  {
    "raw_code": "def file_stored_after_transaction_hooks; end\n\n    # method overridden in EE\n    def file_stored_in_transaction_hooks; end\n\n    def set_size\n      self.size = file.size\n    end\n\n    def project_destroyed?\n      # Use job.project to avoid extra DB query for project\n      job.project.pending_delete?\n    end\n\n    def log_create\n      Gitlab::Ci::Artifacts::Logger.log_created(self)\n    end\n\n    def log_destroy\n      Gitlab::Ci::Artifacts::Logger.log_deleted(self, __method__)\n    end\n\n    def validate_exposed_paths\n      return if exposed_paths.is_a?(Array) && exposed_paths.all? { |path| path.match?(EXPOSED_PATH_REGEX) }\n\n      errors.add(:exposed_paths, 'must be an array of strings without `*`')\n    end\n  end\nend",
    "comment": "method overridden in EE",
    "label": "",
    "id": "6738"
  },
  {
    "raw_code": "def save\n      raise UnpersistedJobError unless job.persisted?\n\n      with_redis do |redis|\n        redis.multi do |transaction|\n          transaction.hset(redis_key, attributes_for_redis)\n          transaction.expire(redis_key, REDIS_TTL)\n        end",
    "comment": "We need a job_id to save the record in Redis",
    "label": "",
    "id": "6739"
  },
  {
    "raw_code": "def self.newest_first(ref: nil, sha: nil, limit: nil, source: nil)\n      relation = order(id: :desc)\n      relation = relation.where(ref: ref) if ref\n      relation = relation.where(sha: sha) if sha\n      relation = relation.where(source: source) if source\n\n      if limit\n        ids = relation.limit(limit).select(:id)\n        relation = relation.where(id: ids)\n      end",
    "comment": "Returns the pipelines in descending order (= newest first), optionally limited to a number of references.  ref - The name (or names) of the branch(es)/tag(s) to limit the list of pipelines to. sha - The commit SHA (or multiple SHAs) to limit the list of pipelines to. limit - Number of pipelines to return. Chaining with sampling methods (#pick, #take) will cause unnecessary subqueries.",
    "label": "",
    "id": "6740"
  },
  {
    "raw_code": "def self.latest_pipeline_per_commit(commits, ref = nil)\n      sql = select('DISTINCT ON (sha) *')\n              .where(sha: commits)\n              .order(:sha, id: :desc)\n\n      sql = sql.where(ref: ref) if ref\n\n      sql.index_by(&:sha)\n    end",
    "comment": "Returns a Hash containing the latest pipeline for every given commit.  The keys of this Hash are the commit SHAs, the values the pipelines.  commits - The list of commit SHAs to get the pipelines for. ref - The ref to scope the data to (e.g. \"master\"). If the ref is not given we simply get the latest pipelines for the commits, regardless of what refs the pipelines belong to.",
    "label": "",
    "id": "6741"
  },
  {
    "raw_code": "def commit\n      @commit ||= Commit.lazy(project, sha)\n    end",
    "comment": "NOTE: This is loaded lazily and will never be nil, even if the commit cannot be found.  Use constructs like: `pipeline.commit.present?`",
    "label": "",
    "id": "6742"
  },
  {
    "raw_code": "def retry_failed(current_user)\n      Ci::RetryPipelineService.new(project, current_user)\n        .execute(self)\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass",
    "label": "",
    "id": "6743"
  },
  {
    "raw_code": "def lazy_ref_commit\n      BatchLoader.for(ref).batch(key: project.id) do |refs, loader|\n        next unless project.repository_exists?\n\n        project.repository.list_commits_by_ref_name(refs).then do |commits|\n          commits.each { |key, commit| loader.call(key, commits[key]) }\n        end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6744"
  },
  {
    "raw_code": "def latest_report_artifacts\n      ::Gitlab::SafeRequestStore.fetch(\"pipeline:#{self.id}:latest_report_artifacts\") do\n        ::Ci::JobArtifact.where(\n          id: job_artifacts.all_reports\n            .select(\"max(#{Ci::JobArtifact.quoted_table_name}.id) as id\")\n            .group(:file_type)\n        )\n          .preload(:job)\n          .group_by(&:file_type)\n      end",
    "comment": "This batch loads the latest reports for each CI job artifact type (e.g. sast, dast, etc.) in a single SQL query to eliminate the need to do N different `job_artifacts.where(file_type: X).last` calls.  Return a hash of file type => array of 1 job artifact",
    "label": "",
    "id": "6745"
  },
  {
    "raw_code": "def set_failed(failure_reason)\n      self.failure_reason = failure_reason.to_s\n      self.status = 'failed'\n    end",
    "comment": "Like #drop!, but does not persist the pipeline nor trigger any state machine callbacks.",
    "label": "",
    "id": "6746"
  },
  {
    "raw_code": "def error_messages\n      messages.select(&:error?)\n    end",
    "comment": "We can't use `messages.error` scope here because messages should also be read when the pipeline is not persisted. Using the scope will return no results as it would query persisted data.",
    "label": "",
    "id": "6747"
  },
  {
    "raw_code": "def notes=(notes_to_save)\n      notes_to_save.reject! do |note_to_save|\n        notes.any? do |note|\n          [note_to_save.note, note_to_save.created_at.to_i] == [note.note, note.created_at.to_i]\n        end",
    "comment": "Manually set the notes for a Ci::Pipeline There is no ActiveRecord relation between Ci::Pipeline and notes as they are related to a commit sha. This method helps importing them using the +Gitlab::ImportExport::Project::RelationFactory+ class.",
    "label": "",
    "id": "6748"
  },
  {
    "raw_code": "def set_status(new_status)\n      retry_optimistic_lock(self, name: 'ci_pipeline_set_status') do\n        case new_status\n        when 'created' then nil\n        when 'waiting_for_resource' then request_resource\n        when 'preparing' then prepare\n        when 'waiting_for_callback' then wait_for_callback\n        when 'pending' then enqueue\n        when 'running' then run\n        when 'success' then succeed\n        when 'failed' then drop\n        when 'canceling' then start_cancel\n        when 'canceled' then cancel\n        when 'skipped' then skip\n        when 'manual' then block\n        when 'scheduled' then delay\n        else\n          raise Ci::HasStatus::UnknownStatusError, \"Unknown status `#{new_status}`\"\n        end",
    "comment": "rubocop: disable Metrics/CyclomaticComplexity -- breaking apart hurts readability",
    "label": "",
    "id": "6749"
  },
  {
    "raw_code": "def protected_ref?\n      strong_memoize(:protected_ref) do\n        merge_request? ? protected_for_merge_request? : project.protected_for?(git_ref)\n      end",
    "comment": "rubocop: enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "6750"
  },
  {
    "raw_code": "def all_merge_requests\n      @all_merge_requests ||=\n        if merge_request?\n          MergeRequest.where(id: merge_request_id)\n        else\n          MergeRequest.where(source_project_id: project_id, source_branch: ref)\n            .by_commit_sha(sha)\n        end",
    "comment": "All the merge requests for which the current pipeline runs/ran against",
    "label": "",
    "id": "6751"
  },
  {
    "raw_code": "def related_merge_requests\n      if merge_request?\n        # We look for all other MRs that this branch might be pointing to\n        MergeRequest.where(\n          source_project_id: merge_request.source_project_id,\n          source_branch: merge_request.source_branch)\n      else\n        MergeRequest.where(\n          source_project_id: project_id,\n          source_branch: ref)\n      end",
    "comment": "This returns a list of MRs that point to the same source project/branch",
    "label": "",
    "id": "6752"
  },
  {
    "raw_code": "def open_merge_requests_refs\n      strong_memoize(:open_merge_requests_refs) do\n        # We ensure that triggering user can actually read the pipeline\n        related_merge_requests\n          .opened\n          .limit(MAX_OPEN_MERGE_REQUESTS_REFS)\n          .order(id: :desc)\n          .preload(:target_project)\n          .select { |mr| can?(user, :read_merge_request, mr) }\n          .map { |mr| mr.to_reference(project, full: true) }\n      end",
    "comment": "We cannot use `all_merge_requests`, due to race condition This returns a list of at most 4 open MRs",
    "label": "",
    "id": "6753"
  },
  {
    "raw_code": "def self_and_upstreams\n      object_hierarchy.base_and_ancestors\n    end",
    "comment": "With multi-project and parent-child pipelines",
    "label": "",
    "id": "6754"
  },
  {
    "raw_code": "def self_and_downstreams\n      object_hierarchy.base_and_descendants\n    end",
    "comment": "With multi-project and parent-child pipelines",
    "label": "",
    "id": "6755"
  },
  {
    "raw_code": "def upstream_and_all_downstreams\n      object_hierarchy.all_objects\n    end",
    "comment": "With multi-project and parent-child pipelines",
    "label": "",
    "id": "6756"
  },
  {
    "raw_code": "def self_and_project_ancestors\n      object_hierarchy(project_condition: :same).base_and_ancestors\n    end",
    "comment": "With only parent-child pipelines",
    "label": "",
    "id": "6757"
  },
  {
    "raw_code": "def self_and_project_descendants\n      object_hierarchy(project_condition: :same).base_and_descendants\n    end",
    "comment": "With only parent-child pipelines",
    "label": "",
    "id": "6758"
  },
  {
    "raw_code": "def all_child_pipelines\n      object_hierarchy(project_condition: :same).descendants\n    end",
    "comment": "With only parent-child pipelines",
    "label": "",
    "id": "6759"
  },
  {
    "raw_code": "def root_ancestor\n      return self unless child?\n\n      object_hierarchy(project_condition: :same)\n        .base_and_ancestors(hierarchy_order: :desc)\n        .first\n    end",
    "comment": "Follow the parent-child relationships and return the top-level parent",
    "label": "",
    "id": "6760"
  },
  {
    "raw_code": "def upstream_root\n      @upstream_root ||= object_hierarchy.base_and_ancestors(hierarchy_order: :desc).first\n    end",
    "comment": "Follow the upstream pipeline relationships, regardless of multi-project or parent-child, and return the top-level ancestor.",
    "label": "",
    "id": "6761"
  },
  {
    "raw_code": "def complete_hierarchy_count\n      upstream_root.self_and_downstreams.count\n    end",
    "comment": "Applies to all parent-child and multi-project pipelines",
    "label": "",
    "id": "6762"
  },
  {
    "raw_code": "def downloadable_artifacts_in_self_and_project_descendants\n      hierarchy_builds = builds_in_self_and_project_descendants\n\n      artifacts = Ci::JobArtifact\n        .with_job\n        .where(job_id: hierarchy_builds.select(:id))\n        .downloadable\n        .in_partition(self)\n\n      non_expired = artifacts.not_expired\n\n      locked_pipeline_ids = self_and_project_descendants.artifacts_locked.select(:id)\n      from_locked_pipelines = artifacts.joins(:job).where(p_ci_builds: { commit_id: locked_pipeline_ids })\n\n      non_expired.or(from_locked_pipelines).distinct\n    end",
    "comment": "Expired artifacts are still valid if \"Keep artifacts from most recent successful jobs\" is enabled",
    "label": "",
    "id": "6763"
  },
  {
    "raw_code": "def modified_paths\n      strong_memoize(:modified_paths) do\n        if merge_request?\n          merge_request.modified_paths\n        elsif branch_updated?\n          push_details.modified_paths\n        elsif external_pull_request?\n          external_pull_request.modified_paths\n        end",
    "comment": "Returns the modified paths.  The returned value is * Array: List of modified paths that should be evaluated * nil: Modified path can not be evaluated",
    "label": "",
    "id": "6764"
  },
  {
    "raw_code": "def ensure_scheduling_type!\n      processables.populate_scheduling_type!\n    end",
    "comment": "Set scheduling type of processables if they were created before scheduling_type data was deployed (https://gitlab.com/gitlab-org/gitlab/-/merge_requests/22246).",
    "label": "",
    "id": "6765"
  },
  {
    "raw_code": "def reset_source_bridge!(current_user)\n      # break recursion when no source_pipeline bridge (first upstream pipeline)\n      return unless bridge_waiting?\n      return unless current_user.can?(:update_pipeline, source_bridge.pipeline)\n\n      # Before enqueuing the trigger job again, its status must be one of :created, :skipped, :manual, and :scheduled.\n      # Also, we use `skip_pipeline_processing` to prevent processing the pipeline to avoid redundant process.\n      source_bridge.created!(current_user, skip_pipeline_processing: true)\n      Ci::EnqueueJobService.new(source_bridge, current_user: current_user).execute # rubocop:disable CodeReuse/ServiceClass\n    end",
    "comment": "For dependent bridge jobs we reset the upstream bridge recursively to reflect that a downstream pipeline is running again",
    "label": "",
    "id": "6766"
  },
  {
    "raw_code": "def merge_train_pipeline?\n      false\n    end",
    "comment": "EE-only",
    "label": "",
    "id": "6767"
  },
  {
    "raw_code": "def object_hierarchy(options = {})\n      ::Gitlab::Ci::PipelineObjectHierarchy\n        .new(self.class.unscoped.where(id: id), options: options)\n    end",
    "comment": "Without using `unscoped`, caller scope is also included into the query. Using `unscoped` here will be redundant after Rails 6.1",
    "label": "",
    "id": "6768"
  },
  {
    "raw_code": "def self.fabricate(project, stage, statuses = nil)\n      statuses ||= stage.latest_statuses\n\n      statuses\n        .sort_by(&:sortable_name).group_by(&:group_name)\n        .map do |group_name, grouped_statuses|\n          self.new(project, stage, name: group_name, jobs: grouped_statuses)\n        end",
    "comment": "Construct a grouping of statuses for this stage. We allow the caller to pass in statuses for efficiency (avoiding N+1 queries).",
    "label": "",
    "id": "6769"
  },
  {
    "raw_code": "def set_project_id\n      self.project_id ||= build&.project_id\n    end",
    "comment": "TODO: This is temporary code to assist the backfilling of records for this epic: https://gitlab.com/groups/gitlab-org/-/epics/12323 To be removed in 17.7: https://gitlab.com/gitlab-org/gitlab/-/issues/488163 ",
    "label": "",
    "id": "6770"
  },
  {
    "raw_code": "def stale_processables\n        Ci::Processable.where(id: retained.select(:build_id))\n                       .complete\n                       .updated_at_before(5.minutes.ago)\n      end",
    "comment": "In some cases, state machine hooks in `Ci::Build` are skipped even if the job status transitions to a complete state. For example, `Ci::Build#doom!` (a.k.a `data_integrity_failure`) doesn't execute state machine hooks. To handle these edge cases, we check the staleness of the jobs that currently assigned to the resources, and release if it's stale. See https://gitlab.com/gitlab-org/gitlab/-/issues/335537#note_632925914 for more information.",
    "label": "",
    "id": "6771"
  },
  {
    "raw_code": "def play(current_user, job_variables_attributes = nil)\n      Ci::PlayBridgeService\n        .new(project, current_user)\n        .execute(self)\n    end",
    "comment": "rubocop: disable CodeReuse/ServiceClass We don't need it but we are taking `job_variables_attributes` parameter to make it consistent with `Ci::Build#play` method.",
    "label": "",
    "id": "6772"
  },
  {
    "raw_code": "def job_artifacts\n      Ci::JobArtifact.none\n    end",
    "comment": "rubocop: enable CodeReuse/ServiceClass",
    "label": "",
    "id": "6773"
  },
  {
    "raw_code": "def assign_resource_to(processable)\n      attrs = {\n        build_id: processable.id,\n        partition_id: processable.partition_id\n      }\n\n      success = resources.free.limit(1).update_all(attrs) > 0\n      log_event(success: success, processable: processable, action: \"assign resource to processable\")\n\n      success\n    end",
    "comment": " NOTE: This is concurrency-safe method that the subquery in the `UPDATE` works as explicit locking.",
    "label": "",
    "id": "6774"
  },
  {
    "raw_code": "def sort_by_job_status\n      <<~SQL\n        CASE status\n          WHEN 'waiting_for_resource' THEN 0\n          ELSE 1\n        END ASC\n      SQL\n    end",
    "comment": "In order to avoid deadlock, we do NOT specify the job execution order in the same pipeline. The system processes wherever ready to transition to `pending` status from `waiting_for_resource`. See https://gitlab.com/gitlab-org/gitlab/-/issues/202186 for more information.",
    "label": "",
    "id": "6775"
  },
  {
    "raw_code": "def append_data(model, new_data, offset)\n        if offset > 0\n          truncated_data = data(model).to_s.byteslice(0, offset)\n          new_data = append_strings(truncated_data, new_data)\n        end",
    "comment": "This is the sequence that causes append_data to be called:  1. Runner sends a PUT /api/v4/jobs/:id to indicate the job is canceled or finished. 2. UpdateBuildStateService#accept_build_state! persists all live job logs to object storage (or filesystem). 3. UpdateBuildStateService#accept_build_state! returns a 202 to the runner. 4. The runner continues to send PATCH requests with job logs until all logs have been sent and received. 5. If the last PATCH request arrives after the job log has been persisted, we retrieve the data from object storage to append the remaining lines.",
    "label": "",
    "id": "6776"
  },
  {
    "raw_code": "def find_catalog_component(component_name)\n        # Multiple versions of a project can have the same sha, so we return the latest one.\n        version = project.catalog_resource_versions.by_sha(sha).latest\n        return unless version\n\n        version.components.template.find_by_name(component_name)\n      end",
    "comment": "TODO: This may retrieve the wrong component object if a simple and a complex component have the same name for the given catalog resource version. We should complete https://gitlab.com/gitlab-org/gitlab/-/issues/450737 to ensure unique component names.",
    "label": "",
    "id": "6777"
  },
  {
    "raw_code": "def simple_template_path(component_name)\n        File.join(TEMPLATES_DIR, \"#{component_name}.yml\")\n      end",
    "comment": "A simple template consists of a single file",
    "label": "",
    "id": "6778"
  },
  {
    "raw_code": "def complex_template_path(component_name)\n        File.join(TEMPLATES_DIR, component_name, TEMPLATE_FILE)\n      end",
    "comment": "A complex template is directory-based and may consist of multiple files. Given a path like \"my-org/sub-group/the-project/templates/component\" returns the entry point path: \"templates/component/template.yml\".",
    "label": "",
    "id": "6779"
  },
  {
    "raw_code": "def sync!(event)\n          # There may be orphaned records since this table does not enforce FKs\n          event.catalog_resource&.sync_with_project!\n        end",
    "comment": "Used by Ci::ProcessSyncEventsService",
    "label": "",
    "id": "6780"
  },
  {
    "raw_code": "def update_latest_released_at!\n        update!(latest_released_at: versions.latest&.released_at)\n      end",
    "comment": "Triggered in Ci::Catalog::Resources::Version and Release model callbacks",
    "label": "",
    "id": "6781"
  },
  {
    "raw_code": "def sync_with_project\n        self.name = project.name\n        self.description = project.description\n        self.visibility_level = project.visibility_level\n      end",
    "comment": "These denormalized columns are first synced when a new catalog resource is created. A PG trigger adds a SyncEvent when the associated project updates any of these columns. A worker processes the SyncEvents with Ci::ProcessSyncEventsService.",
    "label": "",
    "id": "6782"
  },
  {
    "raw_code": "def validate_published_by_is_release_author\n          return if published_by == release.author\n\n          errors.add(:published_by, 'must be the same as the release author')\n        end",
    "comment": "We require the published_by to be the same as the release author because creating a release and publishing a version must be done in a single session via CLI tools.",
    "label": "",
    "id": "6783"
  },
  {
    "raw_code": "def update_by_partition(records)\n            records.group_by(&:partition).each do |partition, records_within_partition|\n              partitioned_scope = status_pending\n                .for_partition(partition)\n                .where(id: records_within_partition.map(&:id))\n\n              yield(partitioned_scope)\n            end",
    "comment": "You must use .select_with_partition before calling this method as it requires the partition to be explicitly selected.",
    "label": "",
    "id": "6784"
  },
  {
    "raw_code": "def self.capture(origin_project:, accessed_project:)\n        label = origin_project == accessed_project ? 'same-project' : 'cross-project'\n        track_internal_event(\n          'authorize_job_token_with_disabled_scope',\n          project: accessed_project,\n          additional_properties: {\n            label: label\n          }\n        )\n\n        # We are tracking ci job token access to project resources, but we\n        # are not yet persisting this log until a request successfully\n        # completes. We will do that in a middleware. This is because the policy\n        # rule about job token scope may be satisfied but a subsequent rule in\n        # the Declarative Policies may block the authorization.\n        add_to_request_store_hash(accessed_project_id: accessed_project.id, origin_project_id: origin_project.id)\n      end",
    "comment": "Record in SafeRequestStore a cross-project access attempt",
    "label": "",
    "id": "6785"
  },
  {
    "raw_code": "def self.log_captures_async\n        authorizations = captured_authorizations\n        return unless authorizations\n\n        accessed_project_id = authorizations[:accessed_project_id]\n        origin_project_id = authorizations[:origin_project_id]\n        return unless accessed_project_id && origin_project_id\n\n        policies = authorizations.fetch(:policies, []).map(&:to_s)\n        Ci::JobToken::LogAuthorizationWorker # rubocop:disable CodeReuse/Worker -- This method is called from a middleware and it's better tested\n          .perform_in(CAPTURE_DELAY, accessed_project_id, origin_project_id, policies)\n      end",
    "comment": "Schedule logging of captured authorizations in a background worker. We add a 5 minutes delay with deduplication logic so that we log the same authorization at most every 5 minutes. Otherwise, in high traffic projects we could be logging authorizations very frequently.",
    "label": "",
    "id": "6786"
  },
  {
    "raw_code": "def inbound_linked_as_accessible?(accessed_project)\n        inbound_accessible_projects(accessed_project).includes_project?(current_project)\n      end",
    "comment": "We don't check the inbound allowlist here. That is because the access check starts from the current project but the inbound allowlist contains projects that can access the current project.",
    "label": "",
    "id": "6787"
  },
  {
    "raw_code": "def inbound_allowlist\n        Ci::JobToken::Allowlist.new(current_project, direction: :inbound)\n      end",
    "comment": "User created list of projects allowed to access the current project",
    "label": "",
    "id": "6788"
  },
  {
    "raw_code": "def outbound_allowlist\n        Ci::JobToken::Allowlist.new(current_project, direction: :outbound)\n      end",
    "comment": "User created list of projects that can be accessed from the current project",
    "label": "",
    "id": "6789"
  },
  {
    "raw_code": "def as_json(serializer: AnalyticsStageSerializer)\n      presenter = Analytics::CycleAnalytics::StagePresenter.new(stage)\n\n      serializer.new.represent(ProjectLevelStage.new(\n        title: presenter.title,\n        description: presenter.description,\n        legend: presenter.legend,\n        name: stage.name,\n        project_median: median\n      ))\n    end",
    "comment": "rubocop: disable CodeReuse/Presenter",
    "label": "",
    "id": "6790"
  },
  {
    "raw_code": "def events\n      data_collector.records_fetcher.serialized_records\n    end",
    "comment": "rubocop: enable CodeReuse/Presenter",
    "label": "",
    "id": "6791"
  },
  {
    "raw_code": "def allow_local_requests?\n    Gitlab::CurrentSettings.allow_local_requests_from_system_hooks?\n  end",
    "comment": "Allow urls pointing localhost and the local network",
    "label": "",
    "id": "6792"
  },
  {
    "raw_code": "def self.delete_batch_for(web_hook, batch_size:)\n    raise ArgumentError, 'batch_size is too small' if batch_size < 1\n\n    where(web_hook: web_hook).limit(batch_size).delete_all == batch_size\n  end",
    "comment": "Delete a batch of log records. Returns true if there may be more remaining.",
    "label": "",
    "id": "6793"
  },
  {
    "raw_code": "def lease_timeout\n    default_lease_timeout\n  end",
    "comment": "Used by ExclusiveLeaseGuard",
    "label": "",
    "id": "6794"
  },
  {
    "raw_code": "def lease_key\n    \"namespace:namespaces_root_statistics:#{namespace_id}\"\n  end",
    "comment": "Used by ExclusiveLeaseGuard",
    "label": "",
    "id": "6795"
  },
  {
    "raw_code": "def lease_release?\n    false\n  end",
    "comment": "Used by ExclusiveLeaseGuard Overriding value as we never release the lease before the timeout in order to prevent multiple RootStatisticsWorker to start in a short span of time",
    "label": "",
    "id": "6796"
  },
  {
    "raw_code": "def add_creator(user)\n    update_attribute(:creator, user)\n  end",
    "comment": "This method should not be called directly. Instead, it is available on the namespace via delegation and should be called after the namespace is saved. Failure to do so will result in errors due to a database trigger that automatically creates the namespace_details after a namespace is created. If we attempt to build the namespace details before the namespace is saved, the trigger will fire and rails will subsequently try to create the namespace_details which will result in an error due to a primary key conflict. Any other modifications to the namespace details should be performed after the associated namespace is saved for the same reason.  See https://gitlab.com/gitlab-org/gitlab/-/merge_requests/82958/diffs#diff-content-c02244956d423e6837379548e5f9b1fa093bb289",
    "label": "",
    "id": "6797"
  },
  {
    "raw_code": "def sync_traversal_ids!\n      TraversalHierarchy.sync_traversal_ids!(root)\n    end",
    "comment": "Update all traversal_ids in the current namespace hierarchy.",
    "label": "",
    "id": "6798"
  },
  {
    "raw_code": "def incorrect_traversal_ids\n      Namespace\n        .joins(\"INNER JOIN (#{TraversalHierarchy.recursive_traversal_ids(root)}) as cte ON namespaces.id = cte.id\")\n        .where('namespaces.traversal_ids::bigint[] <> cte.traversal_ids')\n    end",
    "comment": "Identify all incorrect traversal_ids in the current namespace hierarchy.",
    "label": "",
    "id": "6799"
  },
  {
    "raw_code": "def sync_traversal_ids!(node)\n        Namespace.transaction do\n          acquire_locks(node)\n          sync_traversal_ids_tree!(node)\n        end",
    "comment": "Update all traversal_ids for the given namespace and it's descendants.",
    "label": "",
    "id": "6800"
  },
  {
    "raw_code": "def recursive_traversal_ids(node)\n        node_id = Integer(node.id)\n        ancestor_ids = if node.parent_id\n                         node\n                           .becomes(Namespace)\n                           .recursive_self_and_ancestor_ids\n                           .reverse\n                           .join(',')\n                       else\n                         node_id\n                       end",
    "comment": "Determine traversal_ids for the node and it's descendants using recursive methods. Generate a collection of [id, traversal_ids] rows.  Note that the traversal_ids represent a calculated traversal path for the namespace and not the value stored within the traversal_ids attribute. rubocop:disable Cop/AvoidBecomes -- Normalize STI queries",
    "label": "",
    "id": "6801"
  },
  {
    "raw_code": "def sync_traversal_ids_tree!(node)\n        # An issue in Rails since 2013 prevents this kind of join based update in\n        # ActiveRecord. https://github.com/rails/rails/issues/13496\n        # Ideally it would be:\n        #   `incorrect_traversal_ids.update_all('traversal_ids = cte.traversal_ids')`\n        sql = <<-SQL\n          UPDATE namespaces\n          SET traversal_ids = cte.traversal_ids\n          FROM (#{recursive_traversal_ids(node)}) as cte\n          WHERE namespaces.id = cte.id\n            AND namespaces.traversal_ids::bigint[] <> cte.traversal_ids\n        SQL\n\n        # Hint: when a user is created, it also creates a Namespaces::UserNamespace in\n        # `ensure_namespace_correct`. This method is then called within the same\n        # transaction of the user INSERT.\n        Gitlab::Database::QueryAnalyzers::PreventCrossDatabaseModification.temporary_ignore_tables_in_transaction(\n          %w[namespaces], url: 'https://gitlab.com/gitlab-org/gitlab/-/issues/424279'\n        ) do\n          Namespace.connection.exec_query(sql)\n        end",
    "comment": "Update all traversal_ids in the current namespace hierarchy. Not thread safe, ensure the appropriate nodes are locked before calling.",
    "label": "",
    "id": "6802"
  },
  {
    "raw_code": "def recursive_root_ancestor(namespace)\n        Gitlab::ObjectHierarchy\n          .new(Namespace.where(id: namespace))\n          .base_and_ancestors\n          .without_order\n          .find_top_level\n      end",
    "comment": "This is essentially Namespace#root_ancestor which will soon be rewritten to use traversal_ids. We replicate here as a reliable way to find the root using recursive methods.",
    "label": "",
    "id": "6803"
  },
  {
    "raw_code": "def acquire_locks(node)\n        Gitlab::Database::Transaction::Settings.with('LOCK_TIMEOUT', LOCK_TIMEOUT) do\n          # lock ancestors with shared lock\n          node.recursive_ancestors.lock('FOR SHARE').load if node.parent_id\n\n          # Lock self and descendants with update lock.\n          # Locking self is sufficient provided descendants also acquire ancestoral locks.\n          node.becomes(Namespace).lock!('FOR NO KEY UPDATE') # rubocop:disable Cop/AvoidBecomes -- Normalize STI queries\n        end",
    "comment": "rubocop:disable Database/RescueQueryCanceled -- Measuring specific query timeouts",
    "label": "",
    "id": "6804"
  },
  {
    "raw_code": "def self.pluck_batch_numbers\n      pluck(:batch_number)\n    end",
    "comment": "rubocop: disable Database/AvoidUsingPluckWithoutLimit -- We should use this method only when scoped to a tracker. Batches are self-limiting per tracker based on the amount of data being imported.",
    "label": "",
    "id": "6805"
  },
  {
    "raw_code": "def export_service_for(relation)\n        if tree_relation?(relation)\n          ::BulkImports::TreeExportService\n        elsif file_relation?(relation)\n          ::BulkImports::FileExportService\n        else\n          raise ::BulkImports::Error, 'Unsupported export relation'\n        end",
    "comment": "Returns an export service class for the given relation. @return TreeExportService if a relation is serializable and is listed in import_export.yml @return FileExportService if a relation is a file (uploads, lfs objects, git repository, etc.)",
    "label": "",
    "id": "6806"
  },
  {
    "raw_code": "def self.extract_sentry_external_url(url)\n      url&.sub('api/0/projects/', '')\n    end",
    "comment": "http://HOST/api/0/projects/ORG/PROJECT -> http://HOST/ORG/PROJECT",
    "label": "",
    "id": "6807"
  },
  {
    "raw_code": "def to_sentry_error\n    Gitlab::ErrorTracking::Error.new(\n      id: id,\n      title: title_truncated,\n      message: description,\n      culprit: actor,\n      first_seen: first_seen_at,\n      last_seen: last_seen_at,\n      status: status,\n      count: events_count\n    )\n  end",
    "comment": "For compatibility with sentry integration",
    "label": "",
    "id": "6808"
  },
  {
    "raw_code": "def to_sentry_detailed_error\n    Gitlab::ErrorTracking::DetailedError.new(\n      id: id,\n      title: title_truncated,\n      message: description,\n      culprit: actor,\n      first_seen: first_seen_at.to_s,\n      last_seen: last_seen_at.to_s,\n      count: events_count,\n      user_count: 0, # we don't support user count yet.\n      project_id: project.id,\n      status: status,\n      tags: { level: nil, logger: nil },\n      external_url: external_url,\n      external_base_url: external_base_url,\n      integrated: true,\n      first_release_version: first_event&.release,\n      last_release_version: last_event&.release\n    )\n  end",
    "comment": "For compatibility with sentry integration",
    "label": "",
    "id": "6809"
  },
  {
    "raw_code": "def external_url\n    Gitlab::Routing.url_helpers.details_namespace_project_error_tracking_index_url(\n      namespace_id: project.namespace,\n      project_id: project,\n      issue_id: id)\n  end",
    "comment": "For compatibility with sentry integration",
    "label": "",
    "id": "6810"
  },
  {
    "raw_code": "def external_base_url\n    Gitlab::Routing.url_helpers.project_url(project)\n  end",
    "comment": "For compatibility with sentry integration",
    "label": "",
    "id": "6811"
  },
  {
    "raw_code": "def to_sentry_error_event\n    Gitlab::ErrorTracking::ErrorEvent.new(\n      issue_id: error_id,\n      date_received: occurred_at,\n      stack_trace_entries: stacktrace\n    )\n  end",
    "comment": "For compatibility with sentry integration",
    "label": "",
    "id": "6812"
  },
  {
    "raw_code": "def self.find_or_create(metadata = {})\n    find_or_create_by!(project_id: metadata['project_id'], sha: metadata['sha']) do |record|\n      record.committer = metadata['committer']\n      record.commit_author = metadata['commit_author']\n      record.message = metadata['message']\n      record.authored_date = metadata['authored_date']\n      record.committed_date = metadata['committed_date']\n    end",
    "comment": "Creates a new row, or returns an existing one if a row already exists.",
    "label": "",
    "id": "6813"
  },
  {
    "raw_code": "def self.bulk_find(project_id, shas)\n    rows = []\n\n    shas.each_slice(1_000) do |slice|\n      # rubocop:disable Database/AvoidUsingPluckWithoutLimit -- Already limited to 1K SHAs\n      rows.concat(where(project_id: project_id, sha: slice).pluck(:id, :sha))\n      # rubocop:enable Database/AvoidUsingPluckWithoutLimit\n    end",
    "comment": "Finds many commits by project_id and array of SHAs in bulk. The return value is an array of ID and SHA pairs.",
    "label": "",
    "id": "6814"
  },
  {
    "raw_code": "def self.bulk_find_or_create(project_id, commit_rows)\n    mapping = {}\n    create = []\n\n    # rubocop:disable Database/AvoidUsingPluckWithoutLimit -- This is an array of hashes\n    commits_shas = commit_rows.pluck(:raw_sha)\n    # rubocop:enable Database/AvoidUsingPluckWithoutLimit\n\n    # Find commits that are already existing in `merge_request_commits_metadata`\n    # table. Store them in `mapping` hash so we can map this with rows in\n    # `merge_request_diff_commits` table by SHA.\n    #\n    # `row.last` is the SHA and `row.first` is the ID.\n    bulk_find(project_id, commits_shas).each do |row|\n      mapping[row.last] = row.first\n    end",
    "comment": "Finds or creates rows for the given project ID and commit rows.  The `commit_rows` argument must be an array of hashes. Each hash should have the following keys:  - :commit_author_id - :committer_id - :raw_sha - :authored_date - :committed_date - :message  The return value is a hash that maps ID of each found or created commits metadata row to a SHA.",
    "label": "",
    "id": "6815"
  },
  {
    "raw_code": "def self.prepare(value)\n    value.present? ? value[0..511] : nil\n  end",
    "comment": "Prepares a value to be inserted into a column in the table `merge_request_diff_commit_users`. Values in this table are limited to 512 characters.  We treat empty strings as NULL values, as there's no point in (for example) storing a row where both the name and Email are an empty string. In addition, if we treated them differently we could end up with two rows: one where field X is NULL, and one where field X is an empty string. This is redundant, so we avoid storing such data.",
    "label": "",
    "id": "6816"
  },
  {
    "raw_code": "def self.find_or_create(name, email, organization_id)\n    find_or_create_by!(name: name, email: email, organization_id: organization_id)\n  rescue ActiveRecord::RecordNotUnique\n    retry\n  end",
    "comment": "Creates a new row, or returns an existing one if a row already exists.",
    "label": "",
    "id": "6817"
  },
  {
    "raw_code": "def self.bulk_find(input)\n    queries = {}\n    rows = []\n\n    input.each do |item|\n      name, email, organization_id = item\n      conditions = { name: name, email: email, organization_id: organization_id }\n      queries[conditions.values] = where(conditions).to_sql\n    end",
    "comment": "Finds many (name, email, organization_id) triples in bulk.",
    "label": "",
    "id": "6818"
  },
  {
    "raw_code": "def self.bulk_find_or_create(input)\n    mapping = {}\n\n    # Find existing records with exact matches\n    existing_records = bulk_find(input)\n\n    existing_records.each do |row|\n      # Map all found records with triples\n      mapping[[row.name, row.email, row.organization_id]] = row\n    end",
    "comment": "Finds or creates rows for the given input.  The input argument must be: - Array/Set of triples like [[name, email, organization_id], ...]  This method expects that the names and emails have already been trimmed to at most 512 characters.  The return value is a Hash that maps input to instances of this model.",
    "label": "",
    "id": "6819"
  },
  {
    "raw_code": "def render_error\n      if too_large?\n        :too_large\n      elsif collapsed?\n        :collapsed\n      end",
    "comment": "This method is used on the server side to check whether we can attempt to render the blob at all. Human-readable error messages are found in the `BlobHelper#blob_render_error_reason` helper.  This method does not and should not load the entire blob contents into memory, and should not be overridden to do so in order to validate the format of the blob.  Prefer to implement a client-side viewer, where the JS component loads the binary from `blob_raw_path` and does its own format validation and error rendering, especially for potentially large binary formats.",
    "label": "",
    "id": "6820"
  },
  {
    "raw_code": "def render_error\n      nil\n    end",
    "comment": "We can always render a static viewer, even if the blob is too large.",
    "label": "",
    "id": "6821"
  },
  {
    "raw_code": "def self.can_render?(blob, verify_binary: true)\n      blob.path == blob.project.ci_config_path_or_default\n    end",
    "comment": "rubocop:disable Lint/UnusedMethodArgument -- The keyword argument is required by the parent class but not here.",
    "label": "",
    "id": "6822"
  },
  {
    "raw_code": "def validation_message(opts)\n      return @validation_message if defined?(@validation_message)\n\n      prepare!\n\n      @validation_message = Gitlab::Ci::Lint\n        .new(project: opts[:project], current_user: opts[:user], sha: opts[:sha], verify_project_sha: false)\n        .legacy_validate(blob.data).errors.first\n    end",
    "comment": "rubocop:enable Lint/UnusedMethodArgument",
    "label": "",
    "id": "6823"
  },
  {
    "raw_code": "def self.supported_events\n      %w[commit merge_request]\n    end",
    "comment": "When these are false GitLab does not create cross reference comments on Jira except when an issue gets transitioned.",
    "label": "",
    "id": "6824"
  },
  {
    "raw_code": "def reference_pattern(*)\n      @reference_pattern ||= jira_issue_match_regex\n    end",
    "comment": "{PROJECT-KEY}-{NUMBER} Examples: JIRA-1, PROJECT-1",
    "label": "",
    "id": "6825"
  },
  {
    "raw_code": "def transition_issue(issue)\n      return transition_issue_to_done(issue) if jira_issue_transition_automatic\n\n      jira_issue_transition_id.scan(Gitlab::Regex.jira_transition_id_regex).all? do |transition_id|\n        transition_issue_to_id(issue, transition_id)\n      end",
    "comment": "jira_issue_transition_id can have multiple values split by , or ; the issue is transitioned at the order given by the user if any transition fails it will log the error message and stop the transition sequence",
    "label": "",
    "id": "6826"
  },
  {
    "raw_code": "def jira_request(path)\n      yield\n    rescue StandardError => e\n      @error = e\n      log_exception(e, message: 'Error sending message', client_url: client_url, client_path: path,\n        client_status: e.try(:code))\n      nil\n    end",
    "comment": "Handle errors when doing Jira API calls",
    "label": "",
    "id": "6827"
  },
  {
    "raw_code": "def sections\n      [\n        {\n          type: SECTION_TYPE_CONNECTION,\n          title: s_('DatadogIntegration|Datadog account'),\n          description: help\n        },\n        {\n          type: SECTION_TYPE_CONFIGURATION,\n          title: s_('DatadogIntegration|CI Visibility'),\n          description: s_('DatadogIntegration|Additionally, enable CI Visibility to send pipeline information to Datadog to monitor for job failures and troubleshoot performance issues.')\n        }\n      ]\n    end",
    "comment": "The config is divided in two sections: - General account configuration, which allows setting up a Datadog site and API key - CI Visibility configuration, which is specific to job & pipeline events",
    "label": "",
    "id": "6828"
  },
  {
    "raw_code": "def enable_ssl_verification\n      true\n    end",
    "comment": "This is a stub method to work with deprecated API response TODO: remove enable_ssl_verification after 14.0 https://gitlab.com/gitlab-org/gitlab/-/issues/222808",
    "label": "",
    "id": "6829"
  },
  {
    "raw_code": "def enable_ssl_verification=(_value)\n      self.properties = properties.except('enable_ssl_verification') # Remove unused key\n    end",
    "comment": "Since SSL verification will always be enabled for Buildkite, we no longer need to store the boolean. This is a stub method to work with deprecated API param. TODO: remove enable_ssl_verification after 14.0 https://gitlab.com/gitlab-org/gitlab/-/issues/222808",
    "label": "",
    "id": "6830"
  },
  {
    "raw_code": "def test(*args)\n      return { success: false, result: 'Prometheus configuration error' } unless prometheus_client\n\n      prometheus_client.ping\n      { success: true, result: 'Checked API endpoint' }\n    rescue Gitlab::PrometheusClient::Error => e\n      { success: false, result: e }\n    end",
    "comment": "Check we can connect to the Prometheus API",
    "label": "",
    "id": "6831"
  },
  {
    "raw_code": "def sync_http_integration!\n      return unless manual_configuration_changed? && !manual_configuration_was.nil?\n\n      project.alert_management_http_integrations\n        .for_endpoint_identifier('legacy-prometheus')\n        .take\n        &.update_columns(active: manual_configuration)\n    end",
    "comment": "Remove in next required stop after %16.4 https://gitlab.com/gitlab-org/gitlab/-/issues/338838",
    "label": "",
    "id": "6832"
  },
  {
    "raw_code": "def speak(room_name, message, auth)\n      room = rooms(auth).find { |r| r[\"name\"] == room_name }\n      return unless room\n\n      path = \"/room/#{room['id']}/speak.json\"\n      body = {\n        body: {\n          message: {\n            type: 'TextMessage',\n            body: message\n          }\n        }\n      }\n      res = Clients::HTTP.post(path, base_uri: base_uri, **auth.merge(body))\n      res.code == 201 ? res : nil\n    end",
    "comment": "Post a message into a room, returns the message Hash in case of success. Returns nil otherwise. https://github.com/basecamp/campfire-api/blob/master/sections/messages.md#create-message",
    "label": "",
    "id": "6833"
  },
  {
    "raw_code": "def rooms(auth)\n      res = Clients::HTTP.get(\"/rooms.json\", base_uri: base_uri, **auth)\n      res.code == 200 ? res[\"rooms\"] : []\n    end",
    "comment": "Returns a list of rooms, or []. https://github.com/basecamp/campfire-api/blob/master/sections/rooms.md#get-rooms",
    "label": "",
    "id": "6834"
  },
  {
    "raw_code": "def format_time(time)\n        time = Time.zone.parse(time.to_s)\n        time.strftime(\"%B #{time.day.ordinalize}, %Y %l:%M%p %Z\")\n      end",
    "comment": "This formats time into the following format April 23rd, 2020 1:06AM UTC",
    "label": "",
    "id": "6835"
  },
  {
    "raw_code": "def attachments\n        raise NotImplementedError\n      end",
    "comment": "NOTE: Make sure to call `#strip_markup` on any untrusted user input that's added to the `title`, `subtitle`, `text`, `fallback`, or `author_name` fields.",
    "label": "",
    "id": "6836"
  },
  {
    "raw_code": "def activity\n        raise NotImplementedError\n      end",
    "comment": "NOTE: Make sure to call `#strip_markup` on any untrusted user input that's added to the `title`, `subtitle`, `text`, `fallback`, or `author_name` fields.",
    "label": "",
    "id": "6837"
  },
  {
    "raw_code": "def message\n        raise NotImplementedError\n      end",
    "comment": "NOTE: Make sure to call `#strip_markup` on any untrusted user input that's added to the string.",
    "label": "",
    "id": "6838"
  },
  {
    "raw_code": "def strip_markup(string)\n        SlackMarkdownSanitizer.sanitize(string)\n      end",
    "comment": "Remove unsafe markup from user input, which can be used to hijack links in our own markup, or insert new ones.  This currently removes Markdown and Slack \"mrkdwn\" links (keeping the link label), and all HTML markup (keeping the text nodes). We can't just escape the markup characters, because each chat app handles this differently.  See: - https://api.slack.com/reference/surfaces/formatting#escaping - https://gitlab.com/gitlab-org/slack-notifier#escaping",
    "label": "",
    "id": "6839"
  },
  {
    "raw_code": "def self.update_scopes(integration_ids, scopes)\n        return if integration_ids.empty?\n\n        scope_ids = scopes.pluck(:id)\n\n        attrs = scope_ids.flat_map do |scope_id|\n          integration_ids.map { |si_id| { slack_integration_id: si_id, slack_api_scope_id: scope_id } }\n        end",
    "comment": "Efficient scope propagation",
    "label": "",
    "id": "6840"
  },
  {
    "raw_code": "def find_or_create(last_known_slug, wiki_page)\n        raise WikiPageInvalid unless wiki_page.valid?\n\n        container = wiki_page.wiki.container\n        known_slugs = [last_known_slug, wiki_page.slug].compact.uniq\n        raise 'No slugs found! This should not be possible.' if known_slugs.empty?\n\n        transaction do\n          updates = wiki_page_updates(wiki_page)\n          found = find_by_canonical_slug(known_slugs, container)\n          meta = found || create!(updates.merge(container_attrs(container)))\n\n          meta.update_state(found.nil?, known_slugs, wiki_page, updates)\n\n          # We don't need to run validations here, since find_by_canonical_slug\n          # guarantees that there is no conflict in canonical_slug, and DB\n          # constraints on title and project_id/group_id enforce our other invariants\n          # This saves us a query.\n          meta\n        end",
    "comment": "Return the (updated) WikiPage::Meta record for a given wiki page  If none is found, then a new record is created, and its fields are set to reflect the wiki_page passed.  @param [String] last_known_slug @param [WikiPage] wiki_page  This method raises errors on validation issues.",
    "label": "",
    "id": "6841"
  },
  {
    "raw_code": "def readable_by?(user)\n      Ability.allowed?(user, :read_wiki, self)\n    end",
    "comment": "Used by app/policies/todo_policy.rb",
    "label": "",
    "id": "6842"
  },
  {
    "raw_code": "def validate_dates?\n      new_record? || any_dates_changed?\n    end",
    "comment": "Validate for new records or when any date field has changed",
    "label": "",
    "id": "6843"
  },
  {
    "raw_code": "def validate_description_length?\n      return false unless description_changed?\n\n      previous_description = changes_to_save['description'].first\n      # previous_description will be nil for new records\n      return true if previous_description.blank?\n\n      previous_description.bytesize <= DESCRIPTION_LENGTH_MAX\n    end",
    "comment": "we will need to switch this validation off for record backfilling process to avoid breaking validations for some of existing records which were created before we introduced a length restriction",
    "label": "",
    "id": "6844"
  },
  {
    "raw_code": "def self.allowed_group_level_types(resource_parent)\n      if Feature.enabled?(:create_group_level_work_items, resource_parent, type: :wip)\n        base_types.keys.excluding('epic')\n      else\n        []\n      end",
    "comment": "method overridden in EE to perform the corresponding checks for the Epic type",
    "label": "",
    "id": "6845"
  },
  {
    "raw_code": "def widgets(_resource_parent)\n      enabled_widget_definitions.filter(&:widget_class)\n    end",
    "comment": "resource_parent is used in EE",
    "label": "",
    "id": "6846"
  },
  {
    "raw_code": "def supported_conversion_base_types(_resource_parent, _user)\n      WorkItems::Type.base_types.keys.excluding(*EE_BASE_TYPES)\n    end",
    "comment": "resource_parent is used in EE",
    "label": "",
    "id": "6847"
  },
  {
    "raw_code": "def authorized_types(types, _resource_parent, _relation)\n      types\n    end",
    "comment": "overridden in EE to check for EE-specific restrictions",
    "label": "",
    "id": "6848"
  },
  {
    "raw_code": "def fixed?\n        rollupable_dates.fixed?\n      end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- overridden and tested in EE :nocov:",
    "label": "",
    "id": "6849"
  },
  {
    "raw_code": "def start_date\n        return work_item&.start_date unless dates_source_present?\n\n        rollupable_dates.start_date\n      end",
    "comment": ":nocov: rubocop:enable Gitlab/NoCodeCoverageComment",
    "label": "",
    "id": "6850"
  },
  {
    "raw_code": "def self.in_use?\n      transaction { exists? }\n    rescue ActiveRecord::StatementInvalid\n      false\n    end",
    "comment": "Returns true if there are any replication slots in use. PostgreSQL-compatible databases such as Aurora don't support replication slots, so this will return false as well.",
    "label": "",
    "id": "6851"
  },
  {
    "raw_code": "def self.lag_too_great?(max = 100.megabytes)\n      return false unless in_use?\n\n      lag_function = \"pg_wal_lsn_diff\" \\\n        \"(pg_current_wal_insert_lsn(), restart_lsn)::bigint\"\n\n      # We force the use of a transaction here so the query always goes to the\n      # primary, even when using the DB load balancer.\n      sizes = transaction { pluck(Arel.sql(lag_function)) }\n      too_great = sizes.compact.count { |size| size >= max }\n\n      # If too many replicas are falling behind too much, the availability of a\n      # GitLab instance might suffer. To prevent this from happening we require\n      # at least 1 replica to have data recent enough.\n      if sizes.any? && too_great > 0\n        (sizes.length - too_great) <= 1\n      else\n        false\n      end",
    "comment": "Returns true if the lag observed across all replication slots exceeds a given threshold.  max - The maximum replication lag size, in bytes. Based on GitLab.com statistics it takes between 1 and 5 seconds to replicate around 100 MB of data.",
    "label": "",
    "id": "6852"
  },
  {
    "raw_code": "def self.slots_retained_bytes\n      connection.execute(<<-SQL.squish).to_a\n        SELECT slot_name, database,\n              active, pg_wal_lsn_diff(pg_current_wal_insert_lsn(), restart_lsn)\n          AS retained_bytes\n          FROM pg_replication_slots;\n      SQL\n    end",
    "comment": "array of slots and the retained_bytes https://www.skillslogic.com/blog/databases/checking-postgres-replication-lag http://bdr-project.org/docs/stable/monitoring-peers.html",
    "label": "",
    "id": "6853"
  },
  {
    "raw_code": "def self.max_retained_wal\n      connection.execute(<<-SQL.squish).first.fetch('coalesce').to_i\n        SELECT COALESCE(MAX(pg_wal_lsn_diff(pg_current_wal_insert_lsn(), restart_lsn)), 0)\n          FROM pg_replication_slots;\n      SQL\n    end",
    "comment": "returns the max number WAL space (in bytes) being used across the replication slots",
    "label": "",
    "id": "6854"
  },
  {
    "raw_code": "def self.identifier_min_max_queries\n        {\n          # Increase query performance by not using filters when fetching min/max ids\n          identifiers[:issues] => {\n            minimum_query: -> { ::Issue.minimum(:id) },\n            maximum_query: -> { ::Issue.maximum(:id) }\n          }\n        }\n      end",
    "comment": "Customized min and max calculation, in some cases using the original scope is too slow.",
    "label": "",
    "id": "6855"
  },
  {
    "raw_code": "def self.record_id_by_hash_sha256(organization_id, hash)\n        hash_record = find_by(organization_id: organization_id, hash_sha256: hash)\n        return hash_record.id if hash_record\n\n        casted_organization_id = Arel::Nodes\n          .build_quoted(organization_id, arel_table[:organization_id])\n          .to_sql\n\n        casted_hash_code = Arel::Nodes\n          .build_quoted(hash, arel_table[:hash_sha256])\n          .to_sql\n\n        # Atomic, safe insert without retrying\n        query = <<~SQL\n        WITH insert_cte AS MATERIALIZED (\n          INSERT INTO #{quoted_table_name} (organization_id, hash_sha256) VALUES (#{casted_organization_id}, #{casted_hash_code}) ON CONFLICT DO NOTHING RETURNING ID\n        )\n        SELECT ids.id FROM (\n          (SELECT id FROM #{quoted_table_name} WHERE organization_id=#{casted_organization_id} AND hash_sha256=#{casted_hash_code} LIMIT 1)\n            UNION ALL\n          (SELECT id FROM insert_cte LIMIT 1)\n        ) AS ids LIMIT 1\n        SQL\n\n        connection.execute(query).first['id']\n      end",
    "comment": "Creates or queries the id of the corresponding stage event hash code",
    "label": "",
    "id": "6856"
  },
  {
    "raw_code": "def duration_until_the_next_aggregation_job\n    (10 - (DateTime.current.minute % 10)).minutes.seconds\n  end",
    "comment": "The aggregation job is scheduled every 10 minutes: */10 * * * *",
    "label": "",
    "id": "6857"
  },
  {
    "raw_code": "def user_access_config\n      user_access_authorizations&.config\n    end",
    "comment": "As of today, all config values of associated authorization rows have the same value. See `UserAccess::RefreshService` for more information.",
    "label": "",
    "id": "6858"
  },
  {
    "raw_code": "def glagent_prefix\n      self.class.glagent_prefix\n    end",
    "comment": "Instance method required by TokenAuthenticatable for token generation",
    "label": "",
    "id": "6859"
  },
  {
    "raw_code": "def self.glagent_prefix\n      ::Authn::TokenField::PrefixHelper.prepend_instance_prefix(TOKEN_PREFIX)\n    end",
    "comment": "Class method used for token validation and as single source of truth for prefix",
    "label": "",
    "id": "6860"
  },
  {
    "raw_code": "def base_and_ancestors\n      cte = recursive_cte\n      cte_alias = cte.table.alias(model.table_name)\n\n      model\n        .unscoped\n        .where.not('clusters.id' => nil)\n        .with\n        .recursive(cte.to_arel)\n        .from(cte_alias)\n        .order(depth_order_clause)\n    end",
    "comment": "Returns clusters in order from deepest to highest group",
    "label": "",
    "id": "6861"
  },
  {
    "raw_code": "def same_namespace_management_clusters_query\n      clusterable.management_clusters\n        .project_type\n        .select([clusters_star, 'NULL AS group_parent_id', \"0 AS #{DEPTH_COLUMN}\"])\n        .for_project_namespace(clusterable.namespace_id)\n    end",
    "comment": "Returns project-level clusters where the project is the management project for the cluster. The management project has to be in the same namespace / group as the cluster's project.  Support for management project in sub-groups is planned in https://gitlab.com/gitlab-org/gitlab/issues/34650  NB: group_parent_id is un-used but we still need to match the same number of columns as other queries in the CTE.",
    "label": "",
    "id": "6862"
  },
  {
    "raw_code": "def depth_order_clause\n      return { DEPTH_COLUMN => :asc } unless clusterable.is_a?(::Project)\n\n      order = <<~SQL\n        (CASE clusters.management_project_id\n          WHEN :project_id THEN 0\n          ELSE #{DEPTH_COLUMN}\n        END) ASC\n      SQL\n\n      values = {\n        project_id: clusterable.id\n      }\n\n      Arel.sql(model.sanitize_sql_array([Arel.sql(order), values]))\n    end",
    "comment": "Management clusters should be first in the hierarchy so we use 0 for the depth column.  Only applicable if the clusterable is a project (most especially when requesting project.deployment_platform).",
    "label": "",
    "id": "6863"
  },
  {
    "raw_code": "def legacy_auto_devops_domain\n      if project_type?\n        project&.auto_devops&.domain.presence ||\n          project.variables.find_by(key: 'AUTO_DEVOPS_DOMAIN')&.value.presence ||\n          project.group&.variables&.find_by(key: 'AUTO_DEVOPS_DOMAIN')&.value.presence\n      elsif group_type?\n        group.variables.find_by(key: 'AUTO_DEVOPS_DOMAIN')&.value.presence\n      end",
    "comment": "To keep backward compatibility with AUTO_DEVOPS_DOMAIN environment variable, we need to ensure KUBE_INGRESS_BASE_DOMAIN is set if AUTO_DEVOPS_DOMAIN is set on any of the following options: ProjectAutoDevops#Domain, project variables or group variables, as the AUTO_DEVOPS_DOMAIN is needed for CI_ENVIRONMENT_URL  This method should is scheduled to be removed on https://gitlab.com/gitlab-org/gitlab-foss/issues/56959",
    "label": "",
    "id": "6864"
  },
  {
    "raw_code": "def allowed_to_uninstall?\n          true\n        end",
    "comment": "All new applications should uninstall by default Override if there's dependencies that needs to be uninstalled first",
    "label": "",
    "id": "6865"
  },
  {
    "raw_code": "def id\n      \"#{design.id}.#{version.id}\"\n    end",
    "comment": "The ID, needed by GraphQL types and as part of the Lazy-fetch protocol, includes information about both the design and the version.  The particular format is not interesting, and should be treated as opaque by all callers.",
    "label": "",
    "id": "6866"
  },
  {
    "raw_code": "def info_attributes\n      @info_attributes ||= Gitlab::Git::AttributesParser.new(MANAGED_GIT_ATTRIBUTES)\n    end",
    "comment": "Override of a method called on Repository instances but sent via method_missing to Gitlab::Git::Repository where it is defined",
    "label": "",
    "id": "6867"
  },
  {
    "raw_code": "def attributes(path)\n      info_attributes.attributes(path)\n    end",
    "comment": "Override of a method called on Repository instances but sent via method_missing to Gitlab::Git::Repository where it is defined",
    "label": "",
    "id": "6868"
  },
  {
    "raw_code": "def gitattribute(path, name)\n      attributes(path)[name]\n    end",
    "comment": "Override of a method called on Repository instances but sent via method_missing to Gitlab::Git::Repository where it is defined",
    "label": "",
    "id": "6869"
  },
  {
    "raw_code": "def attributes_at(_ref = nil)\n      info_attributes\n    end",
    "comment": "Override of a method called on Repository instances but sent via method_missing to Gitlab::Git::Repository where it is defined",
    "label": "",
    "id": "6870"
  },
  {
    "raw_code": "def initialize(design, action, content = nil)\n      @design = design\n      @action = action\n      @content = content\n      validate!\n    end",
    "comment": "Parameters: - design [DesignManagement::Design]: the design that was changed - action [Symbol]: the action that gitaly performed",
    "label": "",
    "id": "6871"
  },
  {
    "raw_code": "def performed\n      design.clear_version_cache\n    end",
    "comment": "This action has been performed - do any post-creation actions such as clearing method caches.",
    "label": "",
    "id": "6872"
  },
  {
    "raw_code": "def self.create_for_designs(design_actions, sha, author)\n      issue_id, not_uniq = design_actions.map(&:issue_id).compact.uniq\n      raise NotSameIssue, 'All designs must belong to the same issue!' if not_uniq\n\n      transaction do\n        version = new(sha: sha, issue_id: issue_id, author: author, namespace: Issue.find_by_id(issue_id)&.namespace)\n        version.save(validate: false) # We need it to have an ID. Validate later when designs are present\n\n        rows = design_actions.map { |action| action.row_attrs(version) }\n\n        ApplicationRecord.legacy_bulk_insert(DesignManagement::Action.table_name, rows) # rubocop:disable Gitlab/BulkInsert\n        version.designs.reset\n        version.validate!\n        design_actions.each(&:performed)\n\n        version\n      end",
    "comment": "This is the one true way to create a Version.  This method means you can avoid the paradox of versions being invalid without designs, and not being able to add designs without a saved version. Also this method inserts designs in bulk, rather than one by one.  Before calling this method, callers must guard against concurrent modification by obtaining the lock on the design repository. See: `DesignManagement::Version.with_lock`.  Parameters: - design_actions [DesignManagement::DesignAction]: the actions that have been performed in the repository. - sha [String]: the SHA of the commit that performed them - author [User]: the user who performed the commit returns [DesignManagement::Version]",
    "label": "",
    "id": "6873"
  },
  {
    "raw_code": "def visible_in?(version)\n      map = strong_memoize(:visible_in) do\n        Hash.new do |h, k|\n          h[k] = self.class.visible_at_version(k).where(id: id).exists?\n        end",
    "comment": "A design is visible_in? a version if: * it was created before that version * the most recent action before the version was not a deletion",
    "label": "",
    "id": "6874"
  },
  {
    "raw_code": "def to_reference(from = nil, full: false)\n      infix = full ? '/designs' : ''\n      safe_name = Sanitize.fragment(filename)\n\n      \"#{issue.to_reference(from, full: full)}#{infix}[#{safe_name}]\"\n    end",
    "comment": "A reference for a design is the issue reference, indexed by the filename with an optional infix when full.  e.g. #123[homescreen.png] other-project#72[sidebar.jpg] #38/designs[transition.gif] #12[\"filename with [] in it.jpg\"]",
    "label": "",
    "id": "6875"
  },
  {
    "raw_code": "def resource_parent\n      project\n    end",
    "comment": "Part of the interface of objects we can create events about",
    "label": "",
    "id": "6876"
  },
  {
    "raw_code": "def enterprise_bypass_placeholder_confirmation_allowed?\n      false\n    end",
    "comment": "rubocop:disable Gitlab/NoCodeCoverageComment -- method is tested in EE :nocov: Overridden in EE",
    "label": "",
    "id": "6877"
  },
  {
    "raw_code": "def model_relations_for_source_user_reference(model:, source_user:, user_reference_column:, alias_version:)\n        aliased_model = PlaceholderReferences::AliasResolver.aliased_model(model, version: alias_version)\n        aliased_user_reference_column = PlaceholderReferences::AliasResolver.aliased_column(\n          model, user_reference_column, version: alias_version\n        )\n        primary_key = aliased_model.primary_key\n\n        where(model:, source_user:, user_reference_column:, alias_version:).each_batch(of: MODEL_BATCH_LIMIT) do\n          |placeholder_reference_batch|\n          model_relation = nil\n\n          # This is the simplest way to check for composite pkey for now. In Rails 7.1, composite primary keys will be\n          # fully supported: https://guides.rubyonrails.org/7_1_release_notes.html#composite-primary-keys.\n          # The `elseif primary_key.is_a?(Array)` block exists for Rails 7.1 support, so will not execute in Rails 7.0,\n          # thus the code is not covered by specs and we can ignore underecoverage reports about it until we upgrade.\n          # .pluck is used instead of .select to avoid CrossSchemaAccessErrors on CI tables\n          # rubocop: disable Database/AvoidUsingPluckWithoutLimit -- plucking limited by placeholder batch\n          if primary_key.nil?\n            composite_keys = placeholder_reference_batch.pluck(:composite_key)\n\n            model_relation = aliased_model.where(\n              \"#{composite_key_columns(composite_keys)} IN #{composite_key_values(composite_keys)}\"\n            )\n          elsif primary_key.is_a?(Array)\n            composite_keys = placeholder_reference_batch.pluck(:composite_key)\n            key = composite_keys.first.keys\n            values = composite_keys.map(&:values)\n            model_relation = aliased_model.where({ key => values })\n          else\n            model_relation = aliased_model.primary_key_in(placeholder_reference_batch.pluck(:numeric_key))\n          end",
    "comment": "Model relations are yielded in a block to ensure all relations will be batched, regardless of the model",
    "label": "",
    "id": "6878"
  },
  {
    "raw_code": "def validate_model_is_not_member\n      model_class = model&.safe_constantize\n      return unless model_class.present? && model_class.new.is_a?(Member)\n\n      errors.add(:model, :invalid, message: 'cannot be a Member')\n    end",
    "comment": "Membership data is handled in `Import::Placeholders::Membership` records instead. Use `Import::PlaceholderMemberships::CreateService` to save the membership data.",
    "label": "",
    "id": "6879"
  },
  {
    "raw_code": "def self.permissible_access_level_roles(_, _)\n    # This method is a stopgap in preparation for https://gitlab.com/gitlab-org/gitlab/-/issues/364087\n    access_level_roles\n  end",
    "comment": "For those who get to see a modal with a role dropdown, here are the options presented",
    "label": "",
    "id": "6880"
  },
  {
    "raw_code": "def real_source_type\n    Group.sti_name\n  end",
    "comment": "Because source_type is `Namespace`...",
    "label": "",
    "id": "6881"
  },
  {
    "raw_code": "def permissible_access_level_roles(current_user, project)\n      return {} if current_user.nil?\n\n      if Ability.allowed?(current_user, :manage_owners, project)\n        Gitlab::Access.options_with_owner\n      else\n        max_access_level = project.team.max_member_access(current_user.id)\n        return {} unless max_access_level.present?\n\n        Authz::Role.roles_user_can_assign(max_access_level)\n      end",
    "comment": "For those who get to see a modal with a role dropdown, here are the options presented",
    "label": "",
    "id": "6882"
  },
  {
    "raw_code": "def permissible_access_level_roles_for_project_access_token(current_user, project)\n      permissible_access_level_roles(current_user, project)\n    end",
    "comment": "TODO: Remove this method and call permissible_access_level_roles directly See: https://gitlab.com/gitlab-org/gitlab/-/issues/550264",
    "label": "",
    "id": "6883"
  },
  {
    "raw_code": "def all_members\n      GroupMember.from_union([\n        members_from_self_and_ancestors,\n        members_from_self_and_ancestor_group_shares\n      ])\n    end",
    "comment": "Returns all members for group and parents, with no filters",
    "label": "",
    "id": "6884"
  },
  {
    "raw_code": "def members(active_users: false, minimal_access: false)\n      raise ArgumentError, 'active_users: is deprecated' if active_users && minimal_access\n\n      group_hierarchy_members = members_from_self_and_ancestors\n\n      group_hierarchy_members =\n        if active_users\n          group_hierarchy_members.active_without_invites_and_requests\n        else\n          filter_invites_and_requests(group_hierarchy_members, minimal_access)\n        end",
    "comment": "Returns members based on filter options:  - `active_users`. DEPRECATED. If true, returns only members for active users - `minimal_access`. Used only in EE (GitLab Premium). If true, returns members which has minimal access. If false (default), does not return members with minimal access  NOTE : this method does not return pending invites, nor requests.",
    "label": "",
    "id": "6885"
  },
  {
    "raw_code": "def filter_invites_and_requests(members, _minimal_access)\n      members.without_invites_and_requests(minimal_access: false)\n    end",
    "comment": "NOTE: minimal access is Premium, so in FOSS we will not include minimal access member",
    "label": "",
    "id": "6886"
  },
  {
    "raw_code": "def find_by_fallback_token(attr, plain_secret)\n          return if plain_secret.start_with?('$pbkdf2-') || # PBKDF2 format\n            (plain_secret.length == 128 && plain_secret.match?(/\\A[a-f0-9]{128}\\z/i)) # SHA512 format\n\n          # Try each fallback strategy until we find a match\n          fallback_strategies.each do |fallback_strategy|\n            stored_token = fallback_strategy.transform_secret(plain_secret)\n\n            resource = find_by(attr => stored_token)\n            if resource\n              upgrade_fallback_value(resource, attr, plain_secret)\n              return resource\n            end",
    "comment": "Allow looking up previously plain tokens as a fallback IFF a fallback strategy has been defined  This method overrides the upstream Doorkeeper implementation to support multiple fallback strategies instead of a single fallback_secret_strategy.  @param attr [Symbol] The token attribute we're looking with @param plain_secret [#to_s] Plain secret value (any object that responds to `#to_s`) @return [Doorkeeper::AccessToken, nil] AccessToken object or nil if there is no record with such token  @example OauthAccessToken.find_by_fallback_token(:token, \"my_plain_token\") #=> #<OauthAccessToken:0x...> or nil  @note This method skips lookup for already hashed tokens to avoid unnecessary processing: - PBKDF2 hashed tokens (format: $pbkdf2-sha512$20000$$.c0G5XJV...) - SHA512 hashed tokens (128 hexadecimal characters)  @see #upgrade_fallback_value @see .fallback_strategies",
    "label": "",
    "id": "6887"
  },
  {
    "raw_code": "def secret_matches?(input)\n      # return false if either is nil, since secure_compare depends on strings\n      # but Application secrets MAY be nil depending on confidentiality.\n      return false if input.nil? || secret.nil?\n\n      # When matching the secret by comparer function, all is well.\n      return true if secret_strategy.secret_matches?(input, secret)\n\n      self.class.fallback_strategies.each do |fallback_strategy|\n        # When fallback lookup is enabled, ensure applications\n        # with plain secrets can still be found\n        return true if fallback_strategy.secret_matches?(input, secret)\n      end",
    "comment": "Check whether the given plain text secret matches our stored secret  @param input [#to_s] Plain secret provided by user (any object that responds to `#to_s`)  @return [Boolean] Whether the given secret matches the stored secret of this application. ",
    "label": "",
    "id": "6888"
  },
  {
    "raw_code": "def use_startup_query_for_index_page?\n    params[:before].nil? && params[:after].nil?\n  end",
    "comment": "For simplicity, only optimize non-paginated requests",
    "label": "",
    "id": "6889"
  },
  {
    "raw_code": "def clipboard_button(data = {})\n    css_class = data.delete(:class)\n    title = data.delete(:title) || _('Copy')\n    aria_keyshortcuts = data.delete(:aria_keyshortcuts) || nil\n    aria_label = data.delete(:aria_label) || title\n    button_text = data[:button_text] || nil\n    hide_tooltip = data[:hide_tooltip] || false\n    hide_button_icon = data[:hide_button_icon] || false\n    item_prop = data[:itemprop] || nil\n    variant = data[:variant] || :default\n    category = data[:category] || :tertiary\n    size = data[:size] || :small\n\n    # This supports code in app/assets/javascripts/copy_to_clipboard.js that\n    # works around ClipboardJS limitations to allow the context-specific copy/pasting of plain text or GFM.\n    if text = data.delete(:text)\n      data[:clipboard_text] =\n        if gfm = data.delete(:gfm)\n          { text: text, gfm: gfm }\n        else\n          text\n        end",
    "comment": "Output a \"Copy to Clipboard\" button  data  - Data attributes passed to `content_tag` (default: {}): :text   - Text to copy (optional) :gfm    - GitLab Flavored Markdown to copy, if different from `text` (optional) :target - Selector for target element to copy from (optional) :class  - CSS classes to be applied to the button (optional) :title  - Button's title attribute (used for the tooltip) (optional, default: Copy) :aria_label - Button's aria-label attribute (optional) :aria_keyshortcuts - Button's aria-keyshortcuts attribute (optional) :button_text - Button's displayed label (optional) :hide_tooltip - Whether the tooltip should be hidden (optional, default: false) :hide_button_icon - Whether the icon should be hidden (optional, default: false) :item_prop - itemprop attribute :variant - Button variant (optional, default: :default) :category - Button category (optional, default: :tertiary) :size - Button size (optional, default: :small)  Examples:  # Define the clipboard's text clipboard_button(text: \"Foo\") # => \"<button class='...' data-clipboard-text='Foo'>...</button>\"  # Define the target element clipboard_button(target: \"div#foo\") # => \"<button class='...' data-clipboard-target='div#foo'>...</button>\"  See http://clipboardjs.com/#usage",
    "label": "",
    "id": "6890"
  },
  {
    "raw_code": "def link_button_to(name = nil, href = nil, options = nil, &block)\n    if block\n      options = href\n      href = name\n    end",
    "comment": "Creates a link that looks like a button.  It renders a Pajamas::ButtonComponent.  It has the same API as `link_to`, but with some additional options specific to button rendering.  Examples: # Default button link_button_to _('Foo'), some_path  # Default button using a block link_button_to some_path do _('Foo') end  # Confirm variant link_button_to _('Foo'), some_path, variant: :confirm  # With icon link_button_to _('Foo'), some_path, icon: 'pencil'  # Icon-only # NOTE: The content must be `nil` in order to correctly render. Use aria-label # to ensure the link is accessible. link_button_to nil, some_path, icon: 'pencil', 'aria-label': _('Foo')  # Small button link_button_to _('Foo'), some_path, size: :small  # Secondary category danger button link_button_to _('Foo'), some_path, variant: :danger, category: :secondary  For accessibility, ensure that icon-only links have aria-label set.",
    "label": "",
    "id": "6891"
  },
  {
    "raw_code": "def link_to_markdown(body, url, html_options = {})\n    return '' if body.blank?\n\n    link_to_html(markdown(body, pipeline: :single_line), url, html_options)\n  end",
    "comment": "Use this in places where you would normally use link_to(gfm(...), ...).",
    "label": "",
    "id": "6892"
  },
  {
    "raw_code": "def link_to_html(redacted, url, html_options = {})\n    fragment = Nokogiri::HTML::DocumentFragment.parse(redacted)\n\n    if fragment.children.size == 1 && fragment.children[0].name == 'a'\n      # Fragment has only one node, and it's a link generated by `gfm`.\n      # Replace it with our requested link.\n      text = fragment.children[0].text\n      fragment.children[0].replace(link_to(text, url, html_options))\n    else\n      # Traverse the fragment's first generation of children looking for\n      # either pure text or emojis, wrapping anything found in the\n      # requested link\n      fragment.children.each do |node|\n        if node.text?\n          node.replace(link_to(node.text, url, html_options))\n        elsif node.name == 'gl-emoji'\n          node.replace(link_to(node.to_html.html_safe, url, html_options))\n        end",
    "comment": "It solves a problem occurring with nested links (i.e. \"<a>outer text <a>gfm ref</a> more outer text</a>\"). This will not be interpreted as intended. Browsers will parse something like \"<a>outer text </a><a>gfm ref</a> more outer text\" (notice the last part is not linked any more). link_to_html corrects that. It wraps all parts to explicitly produce the correct linking behavior (i.e. \"<a>outer text </a><a>gfm ref</a><a> more outer text</a>\").",
    "label": "",
    "id": "6893"
  },
  {
    "raw_code": "def first_line_in_markdown(object, attribute, max_chars = nil, **options)\n    md = markdown_field(object, attribute, options.merge(post_process: false))\n    return unless md.present?\n\n    tags = %w[a gl-emoji b strong i em pre code p span]\n\n    context = markdown_field_render_context(object, attribute, options)\n    context.reverse_merge!(truncate_visible_max_chars: max_chars || md.length)\n\n    text = prepare_for_rendering(md, context)\n    text = sanitize(\n      text,\n      tags: tags,\n      attributes: Rails::Html::WhiteListSanitizer.allowed_attributes +\n        %w[\n          style data-src data-name data-unicode-version data-html data-fallback-src\n          data-reference-type data-project-path data-iid data-mr-title\n          data-user\n        ]\n    )\n\n    render_links(text)\n  end",
    "comment": "Return the first line of +text+, up to +max_chars+, after parsing the line as Markdown.  HTML tags in the parsed output are not counted toward the +max_chars+ limit.  If the length limit falls within a tag's contents, then the tag contents are truncated without removing the closing tag.",
    "label": "",
    "id": "6894"
  },
  {
    "raw_code": "def cross_project_reference(project, entity)\n    if entity.respond_to?(:to_reference)\n      entity.to_reference(project, full: true)\n    else\n      ''\n    end",
    "comment": "Returns the text necessary to reference `entity` across projects  project - Project to reference entity  - Object that responds to `to_reference`  Examples:  cross_project_reference(project, project.issues.first) # => 'namespace1/project1#123'  cross_project_reference(project, project.merge_requests.first) # => 'namespace1/project1!345'  Returns a String",
    "label": "",
    "id": "6895"
  },
  {
    "raw_code": "def render_links(text)\n    scrubber = Loofah::Scrubber.new do |node|\n      next unless node.name == 'a'\n      next node.remove if node.children.empty?\n      next node.replace(node.children) if node['data-reference-type'] != 'user'\n\n      if defined?(current_user) && current_user && node['data-user'] == current_user.id.to_s\n        next node.append_class('current-user')\n      end",
    "comment": "Sanitize and style user references links  @param String text the string to be sanitized  1. Remove empty <a> tags which are caused by the <img> tags being stripped (as our markdown wraps images in links) 2. Strip all link tags, except user references, leaving just the link text 3. Add a highlight class for current user's references  @return sanitized HTML string",
    "label": "",
    "id": "6896"
  },
  {
    "raw_code": "def email_action(url)\n    name = action_title(url)\n    return unless name\n\n    gmail_goto_action(name, url)\n  end",
    "comment": "Google Actions https://developers.google.com/gmail/markup/reference/go-to-action",
    "label": "",
    "id": "6897"
  },
  {
    "raw_code": "def notification_reason_text(\n    reason: nil,\n    show_manage_notifications_link: false,\n    show_help_link: false,\n    manage_label_subscriptions_url: nil,\n    unsubscribe_url: nil,\n    format: :text\n  )\n    if unsubscribe_url && show_manage_notifications_link && show_help_link\n      notification_reason_text_with_unsubscribe_and_manage_notifications_and_help_links(\n        reason: reason,\n        unsubscribe_url: unsubscribe_url,\n        format: format\n      )\n    elsif !reason && manage_label_subscriptions_url && show_help_link\n      notification_reason_text_with_manage_label_subscriptions_and_help_links(\n        manage_label_subscriptions_url: manage_label_subscriptions_url,\n        format: format\n      )\n    elsif show_manage_notifications_link && show_help_link\n      notification_reason_text_with_manage_notifications_and_help_links(reason: reason, format: format)\n    else\n      notification_reason_text_without_links(reason: reason, format: format)\n    end",
    "comment": "\"You are receiving this email because ... on #{host}. ...\"",
    "label": "",
    "id": "6898"
  },
  {
    "raw_code": "def avatar_icon_for(user = nil, email = nil, size = nil, scale = 2, only_path: true)\n    if user\n      avatar_icon_for_user(user, size, scale, only_path: only_path)\n    elsif email\n      avatar_icon_for_email(email, size, scale, only_path: only_path)\n    else\n      default_avatar\n    end",
    "comment": "Takes both user and email and returns the avatar_icon by user (preferred) or email.",
    "label": "",
    "id": "6899"
  },
  {
    "raw_code": "def render_if_exists(partial = nil, **options)\n    return unless partial_exists?(partial || options[:partial])\n\n    if partial.nil?\n      render(**options)\n    else\n      render(partial, options)\n    end",
    "comment": "See https://docs.gitlab.com/ee/development/ee_features.html#code-in-appviews rubocop: disable CodeReuse/ActiveRecord We allow partial to be nil so that collection views can be passed in `render partial: 'some/view', collection: @some_collection`",
    "label": "",
    "id": "6900"
  },
  {
    "raw_code": "def error_css\n    Rails.application\n      .assets_manifest\n      .find_sources('errors.css')\n      .first\n      .to_s\n      .force_encoding('UTF-8') # See https://gitlab.com/gitlab-org/gitlab/-/merge_requests/145363\n      .html_safe # rubocop:disable Rails/OutputSafety -- No escaping needed\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "6901"
  },
  {
    "raw_code": "def current_controller?(*args)\n    args.any? do |v|\n      Gitlab::Utils.safe_downcase!(v.to_s) == controller.controller_name || Gitlab::Utils.safe_downcase!(v.to_s) == controller.controller_path\n    end",
    "comment": "Check if a particular controller is the current one  args - One or more controller names to check (using path notation when inside namespaces)  Examples  # On TreeController current_controller?(:tree)           # => true current_controller?(:commits)        # => false current_controller?(:commits, :tree) # => true  # On Admin::ApplicationController current_controller?(:application)         # => true current_controller?('admin/application')  # => true current_controller?('gitlab/application') # => false",
    "label": "",
    "id": "6902"
  },
  {
    "raw_code": "def current_action?(*args)\n    args.any? { |v| Gitlab::Utils.safe_downcase!(v.to_s) == action_name }\n  end",
    "comment": "Check if a particular action is the current one  args - One or more action names to check  Examples  # On Projects#new current_action?(:new)           # => true current_action?(:create)        # => false current_action?(:new, :create)  # => true",
    "label": "",
    "id": "6903"
  },
  {
    "raw_code": "def show_last_push_widget?(event)\n    # Skip if event is not about added or modified non-master branch\n    return false unless event && event.last_push_to_non_root? && !event.rm_ref?\n\n    project = event.project\n\n    # Skip if project repo is empty or MR disabled\n    return false unless project && !project.empty_repo? && project.feature_available?(:merge_requests, current_user)\n\n    # Skip if user already created appropriate MR\n    return false if project.merge_requests.where(source_branch: event.branch_name).opened.any?\n\n    # Skip if user removed branch right after that\n    return false unless project.repository.branch_exists?(event.branch_name)\n\n    true\n  end",
    "comment": "Define whenever show last push event with suggestion to create MR rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "6904"
  },
  {
    "raw_code": "def hexdigest(string)\n    Digest::SHA1.hexdigest string\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "6905"
  },
  {
    "raw_code": "def gitlab_config\n    Gitlab.config.gitlab\n  end",
    "comment": "shortcut for gitlab config",
    "label": "",
    "id": "6906"
  },
  {
    "raw_code": "def extra_config\n    Gitlab.config.extra\n  end",
    "comment": "shortcut for gitlab extra config",
    "label": "",
    "id": "6907"
  },
  {
    "raw_code": "def registry_config\n    Gitlab.config.registry\n  end",
    "comment": "shortcut for gitlab registry config",
    "label": "",
    "id": "6908"
  },
  {
    "raw_code": "def time_ago_with_tooltip(time, placement: 'top', html_class: '', short_format: false)\n    return \"\" if time.nil?\n\n    css_classes = [short_format ? 'js-short-timeago' : 'js-timeago']\n    css_classes << html_class unless html_class.blank?\n\n    content_tag :time, l(time, format: \"%b %d, %Y\"),\n      class: css_classes.join(' '),\n      title: l(time.to_time.in_time_zone, format: :timeago_tooltip),\n      datetime: time.to_time.getutc.iso8601,\n      tabindex: '0',\n      aria: { label: l(time.to_time.in_time_zone, format: :timeago_tooltip) },\n      data: {\n        toggle: 'tooltip',\n        placement: placement,\n        container: 'body'\n      }\n  end",
    "comment": "Render a `time` element with Javascript-based relative date and tooltip  time       - Time object placement  - Tooltip placement String (default: \"top\") html_class - Custom class for `time` element (default: \"time_ago\")  By default also includes a `script` element with Javascript necessary to initialize the `timeago` jQuery extension. If this method is called many times, for example rendering hundreds of commits, it's advisable to disable this behavior using the `skip_js` argument and re-initializing `timeago` manually once all of the elements have been rendered.  A `js-timeago` class is always added to the element, even when a custom `html_class` argument is provided.  Returns an HTML-safe String",
    "label": "",
    "id": "6909"
  },
  {
    "raw_code": "def self.community_forum\n    'https://forum.gitlab.com'\n  end",
    "comment": "This needs to be used outside of Rails",
    "label": "",
    "id": "6910"
  },
  {
    "raw_code": "def community_forum\n    ApplicationHelper.community_forum\n  end",
    "comment": "Convenient method for Rails helper",
    "label": "",
    "id": "6911"
  },
  {
    "raw_code": "def conditional_link_to(condition, options, html_options = {}, &block)\n    if condition\n      link_to options, html_options, &block\n    else\n      capture(&block)\n    end",
    "comment": "While similarly named to Rails's `link_to_if`, this method behaves quite differently. If `condition` is truthy, a link will be returned with the result of the block as its body. If `condition` is falsy, only the result of the block will be returned.",
    "label": "",
    "id": "6912"
  },
  {
    "raw_code": "def active_when(condition)\n    'active' if condition\n  end",
    "comment": "Returns active css class when condition returns true otherwise returns nil.  Example: %li{ class: active_when(params[:filter] == '1') }",
    "label": "",
    "id": "6913"
  },
  {
    "raw_code": "def read_only_message\n    return unless Gitlab::Database.read_only?\n\n    _('You are on a read-only GitLab instance.')\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "6914"
  },
  {
    "raw_code": "def load_max_project_member_accesses(projects)\n    # There are two different request store paradigms for max member access and\n    # we need to preload both of them. One is keyed User the other is keyed by\n    # Project. See https://gitlab.com/gitlab-org/gitlab/-/issues/396822\n\n    # rubocop: disable CodeReuse/ActiveRecord: `projects` can be array which also responds to pluck\n    project_ids = projects.pluck(:id)\n    # rubocop: enable CodeReuse/ActiveRecord\n\n    preload_project_associations(projects)\n\n    Preloaders::UserMaxAccessLevelInProjectsPreloader\n      .new(project_ids, current_user)\n      .execute\n\n    current_user&.max_member_access_for_project_ids(project_ids)\n  end",
    "comment": "Used to preload when you are rendering many projects and checking access",
    "label": "",
    "id": "6915"
  },
  {
    "raw_code": "def gl_tabs_nav(html_options = {}, &block)\n    gl_tabs_classes = %w[nav gl-tabs-nav]\n\n    html_options = html_options.merge(\n      role: 'tablist',\n      class: [*html_options[:class], gl_tabs_classes].join(' ')\n    )\n\n    content = capture(&block) if block\n    content_tag(:ul, content, html_options)\n  end",
    "comment": "Navigation tabs helper Create a <gl-tabs> container  Returns a `ul` element with classes that correspond to the <gl-tabs/> component. Can be populated by gl_tab_link_to elements.  See more at: https://gitlab-org.gitlab.io/gitlab-ui/?path=/story/base-tabs--default",
    "label": "",
    "id": "6916"
  },
  {
    "raw_code": "def gl_tab_link_to(name = nil, options = {}, html_options = {}, &block)\n    link_classes = %w[nav-link gl-tab-nav-item]\n    active_link_classes = %w[active gl-tab-nav-item-active]\n\n    if block\n      # Shift params to skip the omitted \"name\" param\n      html_options = options\n      options = name\n    end",
    "comment": "Create a <gl-tab> link  When a tab is active it gets highlighted to indicate this is currently viewed tab. Internally `current_page?` is called to determine if this is the current tab.  Usage is the same as \"link_to\", with the following additional options:  html_options - The html_options hash (default: {}) :item_active - Overrides the default state focing the \"active\" css classes (optional). ",
    "label": "",
    "id": "6917"
  },
  {
    "raw_code": "def gl_tab_counter_badge(count, html_options = {})\n    gl_badge_tag(\n      count,\n      html_options.merge(\n        class: ['gl-tab-counter-badge', *html_options[:class]]\n      )\n    )\n  end",
    "comment": "Creates a <gl-badge> for use inside tabs.  html_options - The html_options hash (default: {})",
    "label": "",
    "id": "6918"
  },
  {
    "raw_code": "def nav_link(options = {}, &block)\n    klass = active_nav_link?(options) ? 'active' : ''\n\n    # Add our custom class into the html_options, which may or may not exist\n    # and which may or may not already have a :class key\n    o = options.delete(:html_options) || {}\n    o[:class] = [*o[:class], klass].join(' ')\n    o[:class].strip!\n\n    if block\n      content_tag(:li, capture(&block), o)\n    else\n      content_tag(:li, nil, o)\n    end",
    "comment": "Navigation link helper  Returns an `li` element with an 'active' class if the supplied controller(s) and/or action(s) are currently active. The content of the element is the value passed to the block.  options - The options hash used to determine if the element is \"active\" (default: {}) :controller   - One or more controller names to check, use path notation when namespaced (optional). :action       - One or more action names to check (optional). :path         - A shorthand path, such as 'dashboard#index', to check (optional). :html_options - Extra options to be passed to the list element (optional). block   - An optional block that will become the contents of the returned `li` element.  When both :controller and :action are specified, BOTH must match in order to be marked as active. When only one is given, either can match.  Examples  # Assuming we're on TreeController#show  # Controller matches, but action doesn't nav_link(controller: [:tree, :refs], action: :edit) { \"Hello\" } # => '<li>Hello</li>'  # Controller matches nav_link(controller: [:tree, :refs]) { \"Hello\" } # => '<li class=\"active\">Hello</li>'  # Several paths nav_link(path: ['tree#show', 'profile#show']) { \"Hello\" } # => '<li class=\"active\">Hello</li>'  # Shorthand path nav_link(path: 'tree#show') { \"Hello\" } # => '<li class=\"active\">Hello</li>'  # Supplying custom options for the list element nav_link(controller: :tree, html_options: {class: 'home'}) { \"Hello\" } # => '<li class=\"home active\">Hello</li>'  # For namespaced controllers like Admin::AppearancesController#show  # Controller and namespace matches nav_link(controller: 'admin/appearances') { \"Hello\" } # => '<li class=\"active\">Hello</li>'  # Controller and namespace matches but action doesn't nav_link(controller: 'admin/appearances', action: :edit) { \"Hello\" } # => '<li>Hello</li>'  # Shorthand path with namespace nav_link(path: 'admin/appearances#show') { \"Hello\"} # => '<li class=\"active\">Hello</li>'  # When `TreeController#index` is requested # => '<li>Hello</li>'  # Paths, controller and actions can be used at the same time nav_link(path: 'tree#show', controller: 'admin/appearances') { \"Hello\" }  nav_link(path: 'foo#bar', controller: 'tree') { \"Hello\" } nav_link(path: 'foo#bar', controller: 'tree', action: 'show') { \"Hello\" } nav_link(path: 'foo#bar', action: 'show') { \"Hello\" }  Returns a list item element String",
    "label": "",
    "id": "6919"
  },
  {
    "raw_code": "def common_invite_group_modal_data(source, member_class)\n    {\n      id: source.id,\n      root_id: source.root_ancestor.id,\n      name: source.name,\n      default_access_level: Gitlab::Access::GUEST,\n      invalid_groups: source.related_group_ids,\n      help_link: help_page_url('user/permissions.md'),\n      is_project: source.is_a?(Project).to_s,\n      access_levels: member_class.permissible_access_level_roles(current_user, source).to_json,\n      full_path: source.full_path\n    }.merge(group_select_data(source))\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "6920"
  },
  {
    "raw_code": "def common_invite_modal_dataset(source)\n    {\n      id: source.id,\n      root_id: source.root_ancestor&.id,\n      name: source.name,\n      default_access_level: Gitlab::Access::GUEST,\n      full_path: source.full_path\n    }\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "6921"
  },
  {
    "raw_code": "def ide_data(project:, fork_info:, params:)\n    base_data = {\n      'new-web-ide-help-page-path' => help_page_path('user/project/web_ide/_index.md'),\n      'sign-in-path' => new_session_path(current_user),\n      'sign-out-path' => destroy_user_session_path,\n      'user-preferences-path' => profile_preferences_path\n    }.merge(extend_ide_data(project: project))\n\n    return base_data unless project\n\n    base_data.merge(\n      'fork-info' => fork_info&.to_json,\n      'branch-name' => params[:branch],\n      'file-path' => params[:path],\n      'merge-request' => params[:merge_request_id]\n    )\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "6922"
  },
  {
    "raw_code": "def default_autocomplete\n    [\n      { category: \"Settings\", label: _(\"User settings\"),    url: user_settings_profile_path },\n      { category: \"Settings\", label: _(\"SSH Keys\"),         url: user_settings_ssh_keys_path },\n      { category: \"Settings\", label: _(\"Dashboard\"),        url: root_path }\n    ]\n  end",
    "comment": "Autocomplete results for various settings pages",
    "label": "",
    "id": "6923"
  },
  {
    "raw_code": "def default_autocomplete_admin\n    return [] unless current_user.can_read_all_resources?\n\n    [\n      { category: \"Jump to\", label: _(\"Admin area / Dashboard\"), url: admin_root_path }\n    ]\n  end",
    "comment": "Autocomplete results for settings pages, for admins",
    "label": "",
    "id": "6924"
  },
  {
    "raw_code": "def help_autocomplete\n    [\n      { category: \"Help\", label: _(\"API Help\"),                     url: help_page_path(\"api/_index.md\") },\n      { category: \"Help\", label: _(\"Markdown Help\"),                url: help_page_path(\"user/markdown.md\") },\n      { category: \"Help\", label: _(\"Permissions Help\"),             url: help_page_path(\"user/permissions.md\") },\n      { category: \"Help\", label: _(\"Public Access Help\"),           url: help_page_path(\"user/public_access.md\") },\n      { category: \"Help\", label: _(\"Rake Tasks Help\"),\n        url: help_page_path(\"administration/raketasks/_index.md\") },\n      { category: \"Help\", label: _(\"SSH Keys Help\"), url: help_page_path(\"user/ssh.md\") },\n      {\n        category: \"Help\",\n        label: s_(\"Webhooks|System hooks help\"),\n        url: help_page_path(\"administration/system_hooks.md\")\n      },\n      {\n        category: \"Help\",\n        label: _(\"Webhooks help\"),\n        url: help_page_path(\"user/project/integrations/webhooks.md\")\n      }\n    ]\n  end",
    "comment": "Autocomplete results for internal help pages",
    "label": "",
    "id": "6925"
  },
  {
    "raw_code": "def project_autocomplete\n    if @project && @project.repository.root_ref\n      ref = @ref || @project.repository.root_ref\n\n      result = []\n\n      if can?(current_user, :read_code, @project)\n        result.concat([\n          { category: \"In this project\", label: _(\"Files\"),          url: project_tree_path(@project, ref) },\n          { category: \"In this project\", label: _(\"Commits\"),        url: project_commits_path(@project, ref) }\n        ])\n      end",
    "comment": "Autocomplete results for the current project, if it's defined",
    "label": "",
    "id": "6926"
  },
  {
    "raw_code": "def groups_autocomplete(term, limit = 5)\n    current_user\n      .search_on_authorized_groups(term, use_minimum_char_limit: false)\n      .order_id_desc\n      .limit(limit)\n      .map do |group|\n      {\n        category: \"Groups\",\n        id: group.id,\n        value: search_result_sanitize(group.name),\n        label: search_result_sanitize(group.full_name),\n        url: group_path(group),\n        avatar_url: group.avatar_url || ''\n      }\n    end",
    "comment": "Autocomplete results for the current user's groups",
    "label": "",
    "id": "6927"
  },
  {
    "raw_code": "def projects_autocomplete(term, limit = 5)\n    projects = current_user.authorized_projects.order_id_desc.search(\n      term,\n      include_namespace: true,\n      use_minimum_char_limit: false\n    ).sorted_by_stars_desc.non_archived.limit(limit)\n\n    projects.map do |p|\n      {\n        category: \"Projects\",\n        id: p.id,\n        value: search_result_sanitize(p.name),\n        label: search_result_sanitize(p.full_name),\n        url: project_path(p),\n        avatar_url: p.avatar_url || ''\n      }\n    end",
    "comment": "Autocomplete results for the current user's projects",
    "label": "",
    "id": "6928"
  },
  {
    "raw_code": "def search_truncate(source)\n    Truncato.truncate(\n      source,\n      count_tags: false,\n      count_tail: false,\n      filtered_tags: %w[img],\n      max_length: 200\n    )\n  end",
    "comment": "Sanitize a HTML field for search display. Most tags are stripped out and the maximum length is set to 200 characters.",
    "label": "",
    "id": "6929"
  },
  {
    "raw_code": "def highlight_and_truncate_issuable(issuable, search_term, _search_highlight)\n    simple_search_highlight_and_truncate(issuable.description, search_term)\n  end",
    "comment": "_search_highlight is used in EE override",
    "label": "",
    "id": "6930"
  },
  {
    "raw_code": "def safe_format(format, *args)\n    args = args.inject({}, &:merge)\n\n    # Use `Kernel.format` to avoid conflicts with ViewComponent's `format`.\n    Kernel.format(\n      ERB::Util.html_escape_once(format),\n      args.transform_values { |value| ERB::Util.html_escape(value) }\n    ).html_safe\n  end",
    "comment": "Returns a HTML-safe String.  @param [String] format is escaped via `ERB::Util.html_escape_once` @param [Array<Hash>] args are escaped via `ERB::Util.html_escape` if they are not marked as HTML-safe  @example safe_format('See %{user_input}', user_input: '<b>bold</b>') # => \"See &lt;b&gt;bold&lt;/b&gt\"  safe_format('In &lt; hour & more') # => \"In &lt; hour &amp; more\"  @example With +tag_pair+ support safe_format('Some %{open}bold%{close} text.', tag_pair(tag.strong, :open, :close)) # => \"Some <strong>bold</strong> text.\" safe_format('Some %{open}bold%{close} %{italicStart}text%{italicEnd}.', tag_pair(tag.strong, :open, :close), tag_pair(tag.i, :italicStart, :italicEnd)) # => \"Some <strong>bold</strong> <i>text</i>.",
    "label": "",
    "id": "6931"
  },
  {
    "raw_code": "def tag_pair(html_tag, open_name, close_name)\n    raise ArgumentError, 'Argument `tag` must be `html_safe`!' unless html_tag.html_safe?\n    return {} unless html_tag.start_with?('<')\n\n    # end of opening tag: <p>foo</p>\n    #                       ^\n    open_index = html_tag.index('>')\n    # start of closing tag: <p>foo</p>\n    #                             ^^\n    close_index = html_tag.rindex('</')\n\n    return {} unless open_index && close_index\n\n    {\n      open_name => html_tag[0, open_index + 1],\n      close_name => html_tag[close_index, html_tag.size]\n    }\n  end",
    "comment": "Returns a Hash containing a pair of +open+ and +close+ tag parts extracted from HTML-safe +tag+. The values are HTML-safe.  Returns an empty Hash if +tag+ is not a valid paired tag (e.g. <p>foo</p>). an empty Hash is returned.  @param [String] html_tag is a HTML-safe output from tag helper @param [Symbol,Object] open_name name of opening tag @param [Symbol,Object] close_name name of closing tag @raise [ArgumentError] if +tag+ is not HTML-safe  @example tag_pair(tag.strong, :open, :close) # => { open: '<strong>', close: '</strong>' } tag_pair(link_to('', '/'), :open, :close) # => { open: '<a href=\"/\">', close: '</a>' }",
    "label": "",
    "id": "6932"
  },
  {
    "raw_code": "def display_subscription_banner!; end\nend",
    "comment": "Overridden in EE",
    "label": "",
    "id": "6933"
  },
  {
    "raw_code": "def subject_snippets_path(subject = nil, opts = nil)\n    if subject.is_a?(Project)\n      project_snippets_path(subject, opts)\n    else # assume subject === User\n      dashboard_snippets_path(opts)\n    end",
    "comment": "Return the path of a snippets index for a user or for a project  @returns String, path to snippet index",
    "label": "",
    "id": "6934"
  },
  {
    "raw_code": "def has_missing_boards?\n    !multiple_boards_available? && current_board_parent.boards.size > 1\n  end",
    "comment": "Boards are hidden when extra boards were created but the license does not allow multiple boards",
    "label": "",
    "id": "6935"
  },
  {
    "raw_code": "def gl_redirect_listbox_tag(items, selected, html_options = {})\n    # Add script tag for app/assets/javascripts/entrypoints/behaviors/redirect_listbox.js\n    content_for :page_specific_javascripts do\n      webpack_bundle_tag 'redirect_listbox'\n    end",
    "comment": "Creates a listbox component with redirect behavior.  Use this for migrating existing deprecated dropdowns to become Pajamas-compliant. New features should use Vue components directly instead.  The `items` parameter must be an array of hashes, each with `value`, `text` and `href` keys, where `value` is a unique identifier for the item (e.g., the sort key), `text` is the user-facing string for the item, and `href` is the path to redirect to when that item is selected.  The `selected` parameter is the currently selected `value`, and should correspond to one of the `items`, or be `nil`. When `selected.nil?` or a value which does not correspond to one of the items, the first item is selected.  The final parameter `html_options` applies arbitrary attributes to the returned tag. Some of these are passed to the underlying Vue component as props, e.g., to right-align the menu of items, add `data: { placement: 'right' }`.  Examples: # Create a listbox with two items, with the first item selected - items = [{ value: 'foo', text: 'Name, ascending', href: '/foo' }, { value: 'bar', text: 'Name, descending', href: '/bar' }] = gl_redirect_listbox_tag items, 'foo'  # Create the same listbox, right-align the menu and add margin styling = gl_redirect_listbox_tag items, 'foo', class: 'gl-ml-3', data: { placement: 'right' }",
    "label": "",
    "id": "6936"
  },
  {
    "raw_code": "def instance_configuration_human_size_cell(value)\n    instance_configuration_cell_html(value) do |v|\n      number_to_human_size(v, strip_insignificant_zeros: true, significant: false)\n    end",
    "comment": "Value must be in bytes",
    "label": "",
    "id": "6937"
  },
  {
    "raw_code": "def commit_author_link(commit, options = {})\n    commit_person_link(commit, options.merge(source: :author))\n  end",
    "comment": "Returns a link to the commit author. If the author has a matching user and is a member of the current @project it will link to the team member page. Otherwise it will link to the author email as specified in the commit.  options: avatar: true will prepend the avatar image size:   size of the avatar image in px",
    "label": "",
    "id": "6938"
  },
  {
    "raw_code": "def commit_committer_link(commit, options = {})\n    commit_person_link(commit, options.merge(source: :committer))\n  end",
    "comment": "Just like #author_link but for the committer.",
    "label": "",
    "id": "6939"
  },
  {
    "raw_code": "def commits_breadcrumbs\n    return unless @project && @ref\n\n    # Add the root project link and the arrow icon\n    crumbs = content_tag(:li, class: 'breadcrumb-item') do\n      link_to(\n        @project.path,\n        project_commits_path(@project, @ref, ref_type: @ref_type)\n      )\n    end",
    "comment": "Breadcrumb links for a Project and, if applicable, a tree path",
    "label": "",
    "id": "6940"
  },
  {
    "raw_code": "def commit_partial_cache_key(commit, ref:, merge_request:, request:)\n    [\n      commit,\n      commit.author,\n      ref,\n      {\n        merge_request: merge_request&.cache_key,\n        pipeline_status: commit.detailed_status_for(ref)&.cache_key,\n        xhr: request.xhr?,\n        controller: controller.controller_path,\n        path: @path, # referred to in #link_to_browse_code\n        referenced_by: tag_checksum(commit.referenced_by)\n      }\n    ]\n  end",
    "comment": "This is used to calculate a cache key for the app/views/projects/commits/_commit.html.haml partial. It takes some of the same parameters as used in the partial and will hash the current pipeline status.  This includes a keyed hash for values that can be nil, to prevent invalid cache entries being served if the order should change in future.",
    "label": "",
    "id": "6941"
  },
  {
    "raw_code": "def commit_path_template(project)\n    project_commit_path(project, DEFAULT_SHA).sub(\"/#{DEFAULT_SHA}\", '/$COMMIT_SHA')\n  end",
    "comment": "Returns the template path for commit resources to be utilized by the client applications.",
    "label": "",
    "id": "6942"
  },
  {
    "raw_code": "def commit_person_link(commit, options = {})\n    user = commit.public_send(options[:source]) # rubocop:disable GitlabSecurity/PublicSend\n\n    source_name = clean(commit.public_send(:\"#{options[:source]}_name\")) # rubocop:disable GitlabSecurity/PublicSend\n    source_email = clean(commit.public_send(:\"#{options[:source]}_email\")) # rubocop:disable GitlabSecurity/PublicSend\n\n    person_name = user.try(:name) || source_name\n\n    text =\n      if options[:avatar]\n        content_tag(:span, person_name, class: \"commit-#{options[:source]}-name\")\n      else\n        person_name\n      end",
    "comment": "Private: Returns a link to a person. If the person has a matching user and is a member of the current @project it will link to the team member page. Otherwise it will link to the person email as specified in the commit.  options: source: one of :author or :committer avatar: true will prepend the avatar image size:   size of the avatar image in px",
    "label": "",
    "id": "6943"
  },
  {
    "raw_code": "def dropdown_tag(toggle_text, options: {}, &block)\n    content_tag :div, class: \"dropdown #{options[:wrapper_class] if options.key?(:wrapper_class)}\" do\n      data_attr = { toggle: \"dropdown\" }\n\n      if options.key?(:data)\n        data_attr = options[:data].merge(data_attr)\n      end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "6944"
  },
  {
    "raw_code": "def dropdown_toggle(toggle_text, data_attr, options = {})\n    default_label = data_attr[:default_label]\n    content_tag(:button, disabled: options[:disabled], class: \"dropdown-menu-toggle #{options[:toggle_class] if options.key?(:toggle_class)}\", id: (options[:id] if options.key?(:id)), type: \"button\", data: data_attr) do\n      output = content_tag(:span, toggle_text, class: \"dropdown-toggle-text #{'is-default' if toggle_text == default_label}\")\n      output << sprite_icon('chevron-down', css_class: \"dropdown-menu-toggle-icon\")\n      output.html_safe\n    end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "6945"
  },
  {
    "raw_code": "def visibility_level_description(level, form_model)\n    case form_model\n    when Project\n      project_visibility_level_description(level)\n    when Group\n      group_visibility_level_description(level, form_model)\n    end",
    "comment": "Return the description for the +level+ argument.  +level+       One of the Gitlab::VisibilityLevel constants +form_model+  Either a model object (Project, Snippet, etc.) or the name of a Project or Snippet class.",
    "label": "",
    "id": "6946"
  },
  {
    "raw_code": "def selected_visibility_level(form_model, requested_level)\n    requested_level =\n      if requested_level.present?\n        requested_level.to_i\n      else\n        default_project_visibility\n      end",
    "comment": "Visibility level can be restricted in two ways:  1. The group permissions (e.g. a subgroup is private, which requires all projects to be private) 2. The global allowed visibility settings, set by the admin",
    "label": "",
    "id": "6947"
  },
  {
    "raw_code": "def permitted_to_skip_email_otp_in_grace_period?(user)\n    Feature.enabled?(:email_based_mfa, user) &&\n      !user.two_factor_enabled? &&\n      trusted_ip_address?(user) &&\n      !treat_as_locked?(user) &&\n      in_email_otp_grace_period?(user)\n  end",
    "comment": "Used by frontend to decide if we should render the \"skip for now\" button",
    "label": "",
    "id": "6948"
  },
  {
    "raw_code": "def sort_options_hash\n    {\n      sort_value_created_date => sort_title_created_date,\n      sort_value_downvotes => sort_title_downvotes,\n      sort_value_due_date => sort_title_due_date,\n      sort_value_due_date_later => sort_title_due_date_later,\n      sort_value_due_date_soon => sort_title_due_date_soon,\n      sort_value_label_priority => sort_title_label_priority,\n      sort_value_largest_group => sort_title_largest_group,\n      sort_value_largest_repo => sort_title_largest_repo,\n      sort_value_milestone => sort_title_milestone,\n      sort_value_milestone_later => sort_title_milestone_later,\n      sort_value_milestone_soon => sort_title_milestone_soon,\n      sort_value_name => sort_title_name,\n      sort_value_name_desc => sort_title_name_desc,\n      sort_value_oldest_created => sort_title_oldest_created,\n      sort_value_oldest_signin => sort_title_oldest_signin,\n      sort_value_oldest_updated => sort_title_oldest_updated,\n      sort_value_recently_created => sort_title_recently_created,\n      sort_value_recently_signin => sort_title_recently_signin,\n      sort_value_recently_updated => sort_title_recently_updated,\n      sort_value_popularity => sort_title_popularity,\n      sort_value_priority => sort_title_priority,\n      sort_value_merged_date => sort_title_merged_date,\n      sort_value_merged_recently => sort_title_merged_recently,\n      sort_value_merged_earlier => sort_title_merged_earlier,\n      sort_value_closed_date => sort_title_closed_date,\n      sort_value_closed_recently => sort_title_closed_recently,\n      sort_value_closed_earlier => sort_title_closed_earlier,\n      sort_value_upvotes => sort_title_upvotes,\n      sort_value_contacted_date => sort_title_contacted_date,\n      sort_value_relative_position => sort_title_relative_position,\n      sort_value_size => sort_title_size,\n      sort_value_expire_date => sort_title_expire_date,\n      sort_value_title => sort_title_title\n    }\n  end",
    "comment": "rubocop: disable Metrics/AbcSize",
    "label": "",
    "id": "6949"
  },
  {
    "raw_code": "def projects_sort_options_hash\n    options = {\n      sort_value_latest_activity => sort_title_latest_activity,\n      sort_value_name => sort_title_name,\n      sort_value_name_desc => sort_title_name_desc,\n      sort_value_oldest_activity => sort_title_oldest_activity,\n      sort_value_oldest_created => sort_title_oldest_created,\n      sort_value_recently_created => sort_title_recently_created,\n      sort_value_stars_desc => sort_title_most_stars\n    }\n\n    options[sort_value_largest_repo] = sort_title_largest_repo if current_controller?('admin/projects')\n\n    options\n  end",
    "comment": "rubocop: enable Metrics/AbcSize",
    "label": "",
    "id": "6950"
  },
  {
    "raw_code": "def project_export_descriptions\n    [\n      _('Project and wiki repositories'),\n      _('Project uploads'),\n      _('Project configuration, excluding integrations'),\n      _('Issues with comments, merge requests with diffs and comments, labels, milestones, snippets, and other project entities'),\n      _('LFS objects'),\n      _('Issue Boards'),\n      _('Design Management files and data')\n    ]\n  end",
    "comment": "An EE-overwriteable list of descriptions",
    "label": "",
    "id": "6951"
  },
  {
    "raw_code": "def auth_active?(provider)\n    return current_user.atlassian_identity.present? if provider == :atlassian_oauth2\n\n    current_user.identities.exists?(provider: provider.to_s)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "6952"
  },
  {
    "raw_code": "def unlink_provider_allowed?(provider)\n    IdentityProviderPolicy.new(current_user, provider).can?(:unlink)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "6953"
  },
  {
    "raw_code": "def link_to_label(label, type: :issue, tooltip: true, css_class: nil, &block)\n    link = label.filter_path(type: type)\n\n    if block\n      link_to link, class: css_class, &block\n    else\n      render_label(label, link: link, tooltip: tooltip)\n    end",
    "comment": "Link to a Label  label   - LabelPresenter object to link to type    - The type of item the link will point to (:issue or :merge_request). If omitted, defaults to :issue. block   - An optional block that will be passed to `link_to`, forming the body of the link element. If omitted, defaults to `render_colored_label`.  Examples:  # Allow the generated link to use the label's own subject link_to_label(label)  # Force the generated link to use a provided group link_to_label(label, subject: Group.last)  # Force the generated link to use a provided project link_to_label(label, subject: Project.last)  # Force the generated link to point to merge requests instead of issues link_to_label(label, type: :merge_request)  # Customize link body with a block link_to_label(label) { \"My Custom Label Text\" }  Returns a String",
    "label": "",
    "id": "6954"
  },
  {
    "raw_code": "def wrap_label_html(label_html, label:)\n    wrapper_classes = %w[gl-label]\n\n    %(<span class=\"#{wrapper_classes.join(' ')}\">#{label_html}</span>).html_safe\n  end",
    "comment": "We need the `label` argument here for EE",
    "label": "",
    "id": "6955"
  },
  {
    "raw_code": "def approximate_fork_count_with_delimiters(count_data)\n    fork_network_count = count_data[ForkNetwork]\n    fork_network_member_count = count_data[ForkNetworkMember]\n    approximate_fork_count = fork_network_member_count - fork_network_count\n\n    number_with_delimiter(approximate_fork_count)\n  end",
    "comment": "This will approximate the fork count by checking all counting all fork network memberships, and deducting 1 for each root of the fork network. This might be inaccurate as the root of the fork network might have been deleted.  This makes querying this information a lot more efficient and it should be accurate enough for the instance wide statistics",
    "label": "",
    "id": "6956"
  },
  {
    "raw_code": "def safe_params\n    if params.respond_to?(:permit!)\n      params.except(*ActionDispatch::Routing::RouteSet::RESERVED_OPTIONS).permit!\n    else\n      params\n    end",
    "comment": "Rails 5.0 requires to permit `params` if they're used in url helpers. Use this helper when generating links with `params.merge(...)`",
    "label": "",
    "id": "6957"
  },
  {
    "raw_code": "def can_modify_blob?(blob, project = @project, ref = @ref)\n    !blob.stored_externally? && can_edit_tree?(project, ref)\n  end",
    "comment": "Used for single file Web Editor, Delete and Replace UI actions. can_edit_tree checks if ref is on top of the branch.",
    "label": "",
    "id": "6958"
  },
  {
    "raw_code": "def can_modify_blob_with_web_ide?(blob, project = @project)\n    !blob.stored_externally? && can_collaborate_with_project?(project)\n  end",
    "comment": "Used for WebIDE editor where editing is possible even if ref is not on top of the branch.",
    "label": "",
    "id": "6959"
  },
  {
    "raw_code": "def blob_icon(mode, name)\n    sprite_icon(file_type_icon_class('file', mode, name))\n  end",
    "comment": "Return an image icon depending on the file mode and extension  mode - File unix mode mode - File name",
    "label": "",
    "id": "6960"
  },
  {
    "raw_code": "def sanitize_svg_data(data)\n    Gitlab::Sanitizers::SVG.clean(data)\n  end",
    "comment": "SVGs can contain malicious JavaScript; only include allowlisted elements and attributes. Note that this allowlist is by no means complete and may omit some elements.",
    "label": "",
    "id": "6961"
  },
  {
    "raw_code": "def sort_title_created_date\n    s_('SortOptions|Created date')\n  end",
    "comment": "Titles.",
    "label": "",
    "id": "6962"
  },
  {
    "raw_code": "def sort_value_created_date\n    'created_date'\n  end",
    "comment": "Values.",
    "label": "",
    "id": "6963"
  },
  {
    "raw_code": "def javascript_include_tag(*sources)\n    options = { defer: true }.merge(sources.extract_options!)\n    options[:nonce] = true\n    super(*sources, **options)\n  end",
    "comment": "Override the default ActionView `javascript_include_tag` helper to support page specific deferred loading. PLEASE NOTE: `defer` is also critical so that we don't run JavaScript entrypoints before the DOM is ready. Please see https://gitlab.com/groups/gitlab-org/-/epics/4538#note_432159769. The helper also makes sure the `nonce` attribute is included in every script when the content security policy is enabled.",
    "label": "",
    "id": "6964"
  },
  {
    "raw_code": "def javascript_tag(content_or_options_with_block = nil, html_options = {})\n    if content_or_options_with_block.is_a?(Hash)\n      content_or_options_with_block[:nonce] = true\n    else\n      html_options[:nonce] = true\n    end",
    "comment": "The helper makes sure the `nonce` attribute is included in every script when the content security policy is enabled.",
    "label": "",
    "id": "6965"
  },
  {
    "raw_code": "def active_session_device_type_icon(active_session)\n    icon_name =\n      case active_session.device_type\n      when 'smartphone', 'feature phone', 'phablet'\n        'mobile'\n      when 'tablet'\n        'tablet'\n      when 'tv', 'smart display', 'camera', 'portable media player', 'console'\n        'media'\n      when 'car browser'\n        'car'\n      else\n        'monitor-o'\n      end",
    "comment": "Maps a device type as defined in `ActiveSession` to an svg icon name and outputs the icon html.  see `DeviceDetector::Device::DEVICE_NAMES` about the available device types",
    "label": "",
    "id": "6966"
  },
  {
    "raw_code": "def deprecated_attributes\n    [\n      :admin_notification_email,\n      :asset_proxy_whitelist\n    ]\n  end",
    "comment": "ok to remove in REST API v5",
    "label": "",
    "id": "6967"
  },
  {
    "raw_code": "def custom_admin_roles_available?\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "6968"
  },
  {
    "raw_code": "def scoped_labels_available?(parent)\n    false\n  end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "6969"
  },
  {
    "raw_code": "def timezone_data(format: :short)\n    attrs = TIME_ZONE_FORMAT_ATTRS.fetch(format) do\n      valid_formats = TIME_ZONE_FORMAT_ATTRS.keys.map { |k| \":#{k}\" }.join(\", \")\n      raise ArgumentError, \"Invalid format :#{format}. Valid formats are #{valid_formats}.\"\n    end",
    "comment": "format: * :full - all available fields * :short (default)  Example: timezone_data # :short by default timezone_data(format: :full) ",
    "label": "",
    "id": "6970"
  },
  {
    "raw_code": "def timezone_data_with_unique_identifiers(format: :short)\n    timezone_data(format: format)\n      .group_by { |entry| entry[:identifier] }\n      .map do |_identifier, entries|\n        names = entries.map { |entry| entry[:name] }.sort.join(', ') # rubocop:disable Rails/Pluck -- Not a ActiveRecord object\n        entries.first.merge({ name: names })\n      end",
    "comment": "The identifiers in `timezone_data` are not unique. Some cities (e.g. London and Edinburgh) have the same `identifier` value (e.g. \"Europe/London\"). This method merges such entries into one, joining the city names. This unique list is better suited for selectboxes etc.",
    "label": "",
    "id": "6971"
  },
  {
    "raw_code": "def image_tag(source, options = {})\n    source, options = prepare_dark_variant(source, options)\n    options = options.symbolize_keys\n\n    unless options.delete(:lazy) == false\n      options[:data] ||= {}\n      options[:data][:src] = path_to_image(source)\n\n      # options[:class] can be either String or Array.\n      klass_opts = Array.wrap(options[:class])\n      klass_opts << \"lazy\"\n\n      options[:class] = klass_opts.join(' ')\n      source = placeholder_image\n    end",
    "comment": "Override the default ActionView `image_tag` helper to support lazy-loading accept :auto_dark boolean to enable automatic dark variant of the image (see: https://gitlab.com/gitlab-org/gitlab-ui/-/merge_requests/2698) accept :dark_variant path to be used as a source when dark mode is enabled",
    "label": "",
    "id": "6972"
  },
  {
    "raw_code": "def submodule_links(submodule_item, ref = nil, repository = @repository, diff_file = nil)\n    repository.submodule_links.for(submodule_item, ref, diff_file)\n  end",
    "comment": "links to files listing for submodule if submodule is a project on this server",
    "label": "",
    "id": "6973"
  },
  {
    "raw_code": "def page_description(description = nil)\n    if description.present?\n      @page_description = description.squish\n    elsif @page_description.present?\n      sanitize(@page_description.truncate_words(30), tags: [])\n    end",
    "comment": "Define or get a description for the current page  description - String (default: nil)  If this helper is called multiple times with an argument, only the last description will be returned when called without an argument. Descriptions have newlines replaced with spaces and all HTML tags are sanitized.  Examples:  page_description # => \"GitLab Community Edition\" page_description(\"Foo\") page_description # => \"Foo\"  page_description(\"<b>Bar</b>\\nBaz\") page_description # => \"Bar Baz\"  Returns an HTML-safe String.",
    "label": "",
    "id": "6974"
  },
  {
    "raw_code": "def page_card_attributes(map = {})\n    raise ArgumentError, 'cannot provide more than two attributes' if map.length > 2\n\n    @page_card_attributes ||= {}\n    @page_card_attributes = map.reject { |_, v| v.blank? } if map.present?\n    @page_card_attributes\n  end",
    "comment": "Define or get attributes to be used as Twitter card metadata  map - Hash of label => data pairs. Keys become labels, values become data  Raises ArgumentError if given more than two attributes",
    "label": "",
    "id": "6975"
  },
  {
    "raw_code": "def search_context\n    strong_memoize(:search_context) do\n      next super if defined?(super)\n\n      Gitlab::SearchContext::Builder.new(controller.view_context).build!\n    end",
    "comment": "This helper ensures there is always a default `Gitlab::SearchContext` available to all controller that use the application layout.",
    "label": "",
    "id": "6976"
  },
  {
    "raw_code": "def reviewers_dropdown_options_for_suggested_reviewers\n    {}\n  end",
    "comment": "Overwritten",
    "label": "",
    "id": "6977"
  },
  {
    "raw_code": "def issue_supports_multiple_assignees?\n    false\n  end",
    "comment": "Overwritten",
    "label": "",
    "id": "6978"
  },
  {
    "raw_code": "def merge_request_supports_multiple_assignees?\n    false\n  end",
    "comment": "Overwritten",
    "label": "",
    "id": "6979"
  },
  {
    "raw_code": "def merge_request_supports_multiple_reviewers?\n    false\n  end",
    "comment": "Overwritten",
    "label": "",
    "id": "6980"
  },
  {
    "raw_code": "def send_git_blob(repository, blob, inline: true)\n    headers.store(*Gitlab::Workhorse.send_git_blob(repository, blob))\n\n    headers['Content-Disposition'] = content_disposition_for_blob(blob, inline)\n\n    # If enabled, this will override the values set above\n    workhorse_set_content_type!\n\n    render plain: \"\"\n  end",
    "comment": "Send a Git blob through Workhorse",
    "label": "",
    "id": "6981"
  },
  {
    "raw_code": "def send_git_diff(repository, diff_refs)\n    headers.store(*Gitlab::Workhorse.send_git_diff(repository, diff_refs))\n    headers['Content-Disposition'] = 'inline'\n    head :ok\n  end",
    "comment": "Send a Git diff through Workhorse",
    "label": "",
    "id": "6982"
  },
  {
    "raw_code": "def send_git_patch(repository, diff_refs)\n    headers.store(*Gitlab::Workhorse.send_git_patch(repository, diff_refs))\n    headers['Content-Disposition'] = 'inline'\n    head :ok\n  end",
    "comment": "Send a Git patch through Workhorse",
    "label": "",
    "id": "6983"
  },
  {
    "raw_code": "def send_git_archive(repository, **kwargs)\n    headers.store(*Gitlab::Workhorse.send_git_archive(repository, **kwargs))\n    head :ok\n  end",
    "comment": "Archive a Git repository and send it through Workhorse",
    "label": "",
    "id": "6984"
  },
  {
    "raw_code": "def send_artifacts_entry(file, entry)\n    headers.store(*Gitlab::Workhorse.send_artifacts_entry(file, entry))\n    headers.store(*Gitlab::Workhorse.detect_content_type)\n\n    head :ok\n  end",
    "comment": "Send an entry from artifacts through Workhorse and set safe content type",
    "label": "",
    "id": "6985"
  },
  {
    "raw_code": "def render_fork_suggestion\n    return unless current_user\n\n    strong_memoize(:fork_suggestion) do\n      render partial: \"projects/fork_suggestion\"\n    end",
    "comment": "As the fork suggestion button is identical every time, we cache it for a full page load",
    "label": "",
    "id": "6986"
  },
  {
    "raw_code": "def sorted_ancestors(group)\n    if group.root_ancestor.use_traversal_ids?\n      group.ancestors(hierarchy_order: :asc)\n    else\n      group.ancestors\n    end",
    "comment": "Ancestors sorted by hierarchy depth in bottom-top order.",
    "label": "",
    "id": "6987"
  },
  {
    "raw_code": "def localized_jobs_to_be_done_choices\n    {\n      basics: _('I want to learn the basics of Git'),\n      move_repository: _('I want to move my repository to GitLab from somewhere else'),\n      code_storage: _('I want to store my code'),\n      exploring: _('I want to explore GitLab to see if its worth switching to'),\n      ci: _('I want to use GitLab CI with my existing repository'),\n      other: _('A different reason')\n    }.with_indifferent_access.freeze\n  end",
    "comment": "Maps `jobs_to_be_done` values to option texts",
    "label": "",
    "id": "6988"
  },
  {
    "raw_code": "def paginate_collection(collection, remote: nil, total_pages: nil, event_tracking: nil)\n    if collection.is_a?(Kaminari::PaginatableWithoutCount)\n      paginate_without_count(collection, event_tracking: event_tracking)\n    elsif collection.respond_to?(:total_pages)\n      paginate_with_count(collection, remote: remote, total_pages: total_pages)\n    end",
    "comment": "total_pages will be inferred from the collection if nil. It is ignored if the collection is a Kaminari::PaginatableWithoutCount",
    "label": "",
    "id": "6989"
  },
  {
    "raw_code": "def ssh_key_expiration_tooltip(key)\n    key.errors.full_messages.join(', ') if key.errors.full_messages.any?\n  end",
    "comment": "Overridden in EE::ProfilesHelper#ssh_key_expiration_tooltip",
    "label": "",
    "id": "6990"
  },
  {
    "raw_code": "def ssh_key_expires_field_description\n    s_('Profiles|Optional but recommended. If set, key becomes invalid on the specified date.')\n  end",
    "comment": "Overridden in EE::ProfilesHelper#ssh_key_expires_field_description",
    "label": "",
    "id": "6991"
  },
  {
    "raw_code": "def ssh_key_expiration_policy_enabled?\n    false\n  end",
    "comment": "Overridden in EE::ProfilesHelper#ssh_key_expiration_policy_enabled?",
    "label": "",
    "id": "6992"
  },
  {
    "raw_code": "def prevent_delete_account?\n    false\n  end",
    "comment": "Overridden in EE::ProfilesHelper#prevent_delete_account?",
    "label": "",
    "id": "6993"
  },
  {
    "raw_code": "def breadcrumb(page_slug)\n    page_slug.split('/')\n      .map { |dir_or_page| WikiPage.unhyphenize(dir_or_page).capitalize }\n      .join(' / ')\n  end",
    "comment": "Produces a pure text breadcrumb for a given page.  page_slug - The slug of a WikiPage object.  Returns a String composed of the capitalized name of each directory and the capitalized name of the page itself.",
    "label": "",
    "id": "6994"
  },
  {
    "raw_code": "def work_items_data(resource_parent, current_user)\n    group = extract_group(resource_parent)\n\n    base_data(resource_parent, current_user, group).tap do |data|\n      if resource_parent.is_a?(Project)\n        data[:releases_path] = project_releases_path(resource_parent, format: :json)\n        data[:project_import_jira_path] = project_import_jira_path(resource_parent)\n        data[:can_import_work_items] = can?(current_user, :import_work_items, resource_parent).to_s\n        data[:export_csv_path] = export_csv_project_issues_path(resource_parent)\n      end",
    "comment": "overridden in EE",
    "label": "",
    "id": "6995"
  },
  {
    "raw_code": "def add_work_item_show_breadcrumb(resource_parent, _iid)\n    path = resource_parent.is_a?(Group) ? issues_group_path(resource_parent) : project_issues_path(resource_parent)\n\n    add_to_breadcrumbs(_('Issues'), path)\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "6996"
  },
  {
    "raw_code": "def instance_type_new_trial_path(_group)\n    self_managed_new_trial_url\n  end",
    "comment": "overridden in EE",
    "label": "",
    "id": "6997"
  },
  {
    "raw_code": "def any_projects?(projects)\n    return projects.any? if projects.is_a?(Array)\n\n    if projects.limit_value\n      projects.to_a.any?\n    else\n      projects.except(:offset).any?\n    end",
    "comment": "Returns true if any projects are present.  If the relation has a LIMIT applied we'll cast the relation to an Array since repeated any? checks would otherwise result in multiple COUNT queries being executed.  If no limit is applied we'll just issue a COUNT since the result set could be too large to load into memory.",
    "label": "",
    "id": "6998"
  },
  {
    "raw_code": "def delete_confirm_phrase(project)\n    project.path_with_namespace\n  end",
    "comment": "Returns the confirm phrase the user needs to type in order to delete the project  Thus the phrase should include the namespace to make it very clear to the user which project is subject to deletion. Relevant issue: https://gitlab.com/gitlab-org/gitlab/-/issues/343591",
    "label": "",
    "id": "6999"
  },
  {
    "raw_code": "def gl_badge_tag(*args, &block)\n    # Merge the options and html_options hashes if both are present,\n    # because the badge component wants a flat list of keyword args.\n    args.compact!\n    hashes, params = args.partition { |a| a.is_a? Hash }\n    options_hash = hashes.reduce({}, :merge)\n\n    if block\n      render Pajamas::BadgeComponent.new(**options_hash), &block\n    else\n      render Pajamas::BadgeComponent.new(*params, **options_hash)\n    end",
    "comment": "Creates a GitLab UI badge.  Examples: # Plain text badge gl_badge_tag(\"foo\")  # Danger variant gl_badge_tag(\"foo\", variant: :danger)  # With icon gl_badge_tag(\"foo\", icon: \"question-o\")  # Icon-only gl_badge_tag(\"foo\", icon: \"question-o\", icon_only: true)  # Badge link gl_badge_tag(\"foo\", nil, href: some_path)  # Custom classes gl_badge_tag(\"foo\", nil, class: \"foo-bar\")  # Block content gl_badge_tag({ variant: :danger }, { class: \"foo-bar\" }) do \"foo\" end  For accessibility, ensure that the given text or block is non-empty.  See also https://gitlab-org.gitlab.io/gitlab-ui/?path=/story/base-badge--default.",
    "label": "",
    "id": "7000"
  },
  {
    "raw_code": "def vue_readme_header_additional_data\n    {}\n  end",
    "comment": "@return [Hash]",
    "label": "",
    "id": "7001"
  },
  {
    "raw_code": "def user_dropdown_label(user_id, default_label)\n    return default_label if user_id.nil?\n    return \"Unassigned\" if user_id == \"0\"\n\n    user = User.find_by(id: user_id)\n\n    if user\n      user.name\n    else\n      default_label\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7002"
  },
  {
    "raw_code": "def project_dropdown_label(project_id, default_label)\n    return default_label if project_id.nil?\n    return \"Any project\" if project_id == \"0\"\n\n    project = Project.find_by(id: project_id)\n\n    if project\n      project.full_name\n    else\n      default_label\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7003"
  },
  {
    "raw_code": "def group_dropdown_label(group_id, default_label)\n    return default_label if group_id.nil?\n    return \"Any group\" if group_id == \"0\"\n\n    group = ::Group.find_by(id: group_id)\n\n    if group\n      group.full_name\n    else\n      default_label\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7004"
  },
  {
    "raw_code": "def issuables_state_counter_text(issuable_type, state, display_count)\n    titles = {\n      opened: _(\"Open\"),\n      closed: _(\"Closed\"),\n      merged: _(\"Merged\"),\n      all: _(\"All\")\n    }\n    state_title = titles[state] || state.to_s.humanize\n    html = content_tag(:span, state_title)\n\n    return html.html_safe unless display_count\n\n    count = issuables_count_for_state(issuable_type, state)\n    if count != -1\n      html << \" \" << gl_badge_tag(format_count(issuable_type, count, Gitlab::IssuablesCountForState::THRESHOLD), { variant: :neutral }, { class: \"gl-tab-counter-badge gl-hidden sm:gl-inline-flex\" })\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7005"
  },
  {
    "raw_code": "def integration_event_title(event)\n    case event\n    when \"push\", \"push_events\"\n      _(\"Push\")\n    when \"tag_push\", \"tag_push_events\"\n      _(\"Tag push\")\n    when \"note\", \"note_events\"\n      _(\"Note\")\n    when \"confidential_note\", \"confidential_note_events\"\n      _(\"Confidential note\")\n    when \"issue\", \"issue_events\"\n      _(\"Issue\")\n    when \"confidential_issue\", \"confidential_issue_events\"\n      _(\"Confidential issue\")\n    when \"merge_request\", \"merge_request_events\"\n      _(\"Merge request\")\n    when \"pipeline\", \"pipeline_events\"\n      _(\"Pipeline\")\n    when \"wiki_page\", \"wiki_page_events\"\n      _(\"Wiki page\")\n    when \"commit\", \"commit_events\"\n      _(\"Commit\")\n    when \"deployment\"\n      _(\"Deployment\")\n    when \"alert\"\n      _(\"Alert\")\n    when \"incident\"\n      _(\"Incident\")\n    end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "7006"
  },
  {
    "raw_code": "def integration_event_description(integration, event)\n    case integration\n    when Integrations::Jira\n      jira_integration_event_description(event)\n    when Integrations::Teamcity\n      teamcity_integration_event_description(event)\n    else\n      default_integration_event_description(event)\n    end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "7007"
  },
  {
    "raw_code": "def default_integration_event_description(event)\n    case event\n    when \"push\", \"push_events\"\n      s_(\"ProjectService|Trigger event for pushes to the repository.\")\n    when \"tag_push\", \"tag_push_events\"\n      s_(\"ProjectService|Trigger event for new tags pushed to the repository.\")\n    when \"note\", \"note_events\"\n      s_(\"ProjectService|Trigger event for new comments.\")\n    when \"confidential_note\", \"confidential_note_events\"\n      s_(\"ProjectService|Trigger event for new comments on confidential issues.\")\n    when \"issue\", \"issue_events\", \"issues_events\"\n      s_(\"ProjectService|Trigger event when an issue is created, updated, or closed.\")\n    when \"confidential_issue\", \"confidential_issue_events\", \"confidential_issues_events\"\n      s_(\"ProjectService|Trigger event when a confidential issue is created, updated, or closed.\")\n    when \"merge_request\", \"merge_request_events\", \"merge_requests_events\"\n      s_(\"ProjectService|Trigger event when a merge request is created, updated, or merged.\")\n    when \"pipeline\", \"pipeline_events\"\n      s_(\"ProjectService|Trigger event when a pipeline status changes.\")\n    when \"wiki_page\", \"wiki_page_events\"\n      s_(\"ProjectService|Trigger event when a wiki page is created or updated.\")\n    when \"commit\", \"commit_events\"\n      s_(\"ProjectService|Trigger event when a commit is created or updated.\")\n    when \"deployment\", \"deployment_events\"\n      s_(\"ProjectService|Trigger event when a deployment starts or finishes.\")\n    when \"alert\", \"alert_events\"\n      s_(\"ProjectService|Trigger event when a new, unique alert is recorded.\")\n    when \"incident\", \"incident_events\"\n      s_(\"ProjectService|Trigger event when an incident is created.\")\n    when \"build_events\"\n      s_(\"ProjectService|Trigger event when a build is created.\")\n    when \"archive_trace_events\"\n      s_('When enabled, job logs are collected by Datadog and displayed along with pipeline execution traces.')\n    end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "7008"
  },
  {
    "raw_code": "def trigger_events_for_integration(integration)\n    Integrations::EventSerializer.new(integration: integration).represent(integration.configurable_events).to_json\n  end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "7009"
  },
  {
    "raw_code": "def gl_loading_icon(inline: false, color: 'dark', size: 'sm', css_class: nil, data: nil)\n    render Pajamas::SpinnerComponent.new(\n      inline: inline,\n      color: color,\n      size: size,\n      class: css_class,\n      data: data\n    )\n  end",
    "comment": "Creates a GitLab UI loading icon/spinner.  Examples: # Default gl_loading_icon  # Sizes gl_loading_icon(size: 'md') gl_loading_icon(size: 'lg') gl_loading_icon(size: 'xl')  # Colors gl_loading_icon(color: 'light')  # Block/Inline gl_loading_icon(inline: true)  # Custom classes gl_loading_icon(css_class: \"foo-bar\")  See also https://gitlab-org.gitlab.io/gitlab-ui/?path=/story/base-loading-icon--default",
    "label": "",
    "id": "7010"
  },
  {
    "raw_code": "def tree_icon(type, mode, name)\n    sprite_icon(file_type_icon_class(type, mode, name))\n  end",
    "comment": "Return an image icon depending on the file type and mode  type - String type of the tree item; either 'folder' or 'file' mode - File unix mode name - File name",
    "label": "",
    "id": "7011"
  },
  {
    "raw_code": "def tree_join(...)\n    File.join(...)\n  end",
    "comment": "Simple shortcut to File.join",
    "label": "",
    "id": "7012"
  },
  {
    "raw_code": "def patch_branch_name(ref)\n    return unless current_user\n\n    username = current_user.username\n    epoch = time_in_milliseconds.to_s.last(5)\n\n    \"#{username}-#{ref}-patch-#{epoch}\"\n  end",
    "comment": "Generate a patch branch name that should look like: `username-branchname-patch-epoch` where `epoch` is the last 5 digits of the time since epoch (in milliseconds)",
    "label": "",
    "id": "7013"
  },
  {
    "raw_code": "def css_entrypoint_name(path)\n    \"styles/#{path}.css\"\n  end",
    "comment": "we must add `styles/` because ViteRuby prepends assets folder to the request if `/` is missing we must use `.css` extension, otherwise Vite will not detect this as a CSS entrypoint",
    "label": "",
    "id": "7014"
  },
  {
    "raw_code": "def dashboard_choices\n    dashboards = User.dashboards.keys\n\n    validate_dashboard_choices!(dashboards)\n    dashboards -= excluded_dashboard_choices\n\n    dashboards -= ['homepage'] unless Feature.enabled?(:personal_homepage, current_user)\n\n    # Move homepage to first position if it's available\n    # For homepage rollout with flipped mapping, homepage becomes their default (value 0)\n    dashboards.unshift('homepage') if dashboards.delete('homepage')\n\n    dashboards.map do |key|\n      {\n        # Use `fetch` so `KeyError` gets raised when a key is missing\n        text: localized_dashboard_choices_for_user.fetch(key),\n        value: key\n      }\n    end",
    "comment": "Returns an Array usable by a select field for more user-friendly option text",
    "label": "",
    "id": "7015"
  },
  {
    "raw_code": "def localized_dashboard_choices_for_user\n    return localized_dashboard_choices unless current_user.should_use_flipped_dashboard_mapping_for_rollout?\n\n    localized_dashboard_choices.dup.tap do |choices|\n      choices[:projects] = _(\"Your Contributed Projects\")\n      choices[:homepage] = _(\"Personal homepage (default)\")\n    end.freeze\n  end",
    "comment": "Maps `dashboard` values to more user-friendly option text for current user",
    "label": "",
    "id": "7016"
  },
  {
    "raw_code": "def localized_dashboard_choices\n    {\n      projects: _(\"Your Contributed Projects (default)\"),\n      stars: _(\"Starred Projects\"),\n      member_projects: _(\"Member Projects\"),\n      your_activity: _(\"Your Activity\"),\n      project_activity: _(\"Your Projects' Activity\"),\n      starred_project_activity: _(\"Starred Projects' Activity\"),\n      followed_user_activity: _(\"Followed Users' Activity\"),\n      groups: _(\"Your Groups\"),\n      todos: _(\"Your To-Do List\"),\n      issues: _(\"Assigned issues\"),\n      merge_requests: _(\"Assigned merge requests\"),\n      operations: _(\"Operations Dashboard\"),\n      homepage: _(\"Personal homepage\")\n    }.compact.with_indifferent_access.freeze\n  end",
    "comment": "Maps `dashboard` values to more user-friendly option text",
    "label": "",
    "id": "7017"
  },
  {
    "raw_code": "def validate_dashboard_choices!(user_dashboards)\n    if user_dashboards.size != localized_dashboard_choices.size\n      raise \"`User` defines #{user_dashboards.size} dashboard choices, \" \\\n        \"but `localized_dashboard_choices` defined #{localized_dashboard_choices.size}.\"\n    end",
    "comment": "Ensure that anyone adding new options updates `localized_dashboard_choices` too",
    "label": "",
    "id": "7018"
  },
  {
    "raw_code": "def excluded_dashboard_choices\n    ['operations']\n  end",
    "comment": "List of dashboard choice to be excluded from CE. EE would override this.",
    "label": "",
    "id": "7019"
  },
  {
    "raw_code": "def create_epic_menu_item(group)\n      nil\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "7020"
  },
  {
    "raw_code": "def batched_migration_progress(migration, completed_rows)\n      return 100 if migration.finished?\n      return 0 unless completed_rows.to_i > 0\n      return unless migration.total_tuple_count.to_i > 0\n\n      [100 * completed_rows / migration.total_tuple_count, 99].min\n    end",
    "comment": "The extra logic here is needed because total_tuple_count is just an estimate and completed_rows also does not account for last jobs whose batch size is likely larger than the actual number of rows processed",
    "label": "",
    "id": "7021"
  },
  {
    "raw_code": "def group_members_app_data(\n    group,\n    members:,\n    invited:,\n    access_requests:,\n    banned:,\n    include_relations:,\n    search:,\n    pending_members_count:,\n    placeholder_users:\n  )\n    {\n      user: group_members_list_data(group, members, { param_name: :page, params: { invited_members_page: nil, search_invited: nil } }),\n      group: group_group_links_list_data(group, include_relations, search),\n      invite: group_members_list_data(group, invited.nil? ? [] : invited, { param_name: :invited_members_page, params: { page: nil } }),\n      access_request: group_members_list_data(group, access_requests.nil? ? [] : access_requests),\n      source_id: group.id,\n      can_manage_members: can?(current_user, :admin_group_member, group),\n      can_manage_access_requests: can?(current_user, :admin_member_access_request, group),\n      group_name: group.name,\n      group_path: group.full_path,\n      can_approve_access_requests: true, # true for CE, overridden in EE\n      placeholder: placeholder_users,\n      available_roles: available_group_roles(group),\n      reassignment_csv_path: group_bulk_reassignment_file_path(group),\n      allow_inactive_placeholder_reassignment: allow_admin_bypass?.to_s,\n      allow_bypass_placeholder_confirmation: allow_bypass_placeholder_confirmation(group)\n    }\n  end",
    "comment": "rubocop:disable Metrics/ParameterLists -- all arguments needed",
    "label": "",
    "id": "7022"
  },
  {
    "raw_code": "def group_member_header_subtext(group)\n    ERB::Util.html_escape(_(\"You're viewing members of %{strong_start}%{group_name}%{strong_end}.\").html_safe) % {\n      group_name: group.name,\n      strong_start: '<strong>'.html_safe,\n      strong_end: '</strong>'.html_safe\n    }\n  end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "7023"
  },
  {
    "raw_code": "def allow_group_owner_enterprise_bypass?(group)\n    false\n  end",
    "comment": "Overriden in ee/app/helpers/ee/groups/group_members_helper.rb",
    "label": "",
    "id": "7024"
  },
  {
    "raw_code": "def group_members_list_data(group, members, pagination = {})\n    {\n      members: group_members_serialized(group, members),\n      pagination: members_pagination_data(members, pagination),\n      member_path: group_group_member_path(group, ':id')\n    }\n  end",
    "comment": "Overridden in `ee/app/helpers/ee/groups/group_members_helper.rb`",
    "label": "",
    "id": "7025"
  },
  {
    "raw_code": "def available_group_roles(group)\n    group.access_level_roles.sort_by { |_, access_level| access_level }.map do |name, access_level|\n      { title: name, value: \"static-#{access_level}\" }\n    end",
    "comment": "Overridden in `ee/app/helpers/ee/groups/group_members_helper.rb`",
    "label": "",
    "id": "7026"
  },
  {
    "raw_code": "def organizations_users_paths\n      {\n        admin_user: admin_user_path(:id)\n      }\n    end",
    "comment": "See UsersHelper#admin_users_paths for inspiration to this method",
    "label": "",
    "id": "7027"
  },
  {
    "raw_code": "def ci_icon_for_status(status, size: 24)\n      icon_name =\n        if detailed_status?(status)\n          status.icon\n        else\n          case status\n          when 'success'\n            'status_success'\n          when 'success-with-warnings'\n            'status_warning'\n          when 'failed'\n            'status_failed'\n          when 'pending'\n            'status_pending'\n          when 'waiting-for-resource'\n            'status_pending'\n          when 'preparing'\n            'status_preparing'\n          when 'running'\n            'status_running'\n          when 'play'\n            'play'\n          when 'created'\n            'status_created'\n          when 'skipped'\n            'status_skipped'\n          when 'manual'\n            'status_manual'\n          when 'scheduled'\n            'status_scheduled'\n          else\n            'status_canceled'\n          end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "7028"
  },
  {
    "raw_code": "def render_commit_status(commit, status, ref: nil, tooltip_placement: 'left')\n      project = commit.project\n      path = pipelines_project_commit_path(project, commit, ref: ref)\n\n      render_ci_icon(\n        status,\n        path,\n        tooltip_placement: tooltip_placement\n      )\n    end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity",
    "label": "",
    "id": "7029"
  },
  {
    "raw_code": "def topic_explore_projects_cleaned_path(topic_name:)\n      topic_name = URI.encode_www_form_component(topic_name) if Feature.enabled?(:explore_topics_cleaned_path)\n\n      topic_explore_projects_path(topic_name: topic_name)\n    end",
    "comment": "To ensure a route will always generate, we need to encode `topic_name`. Otherwise, various pages will encounter `No route matches` error.  This does mean some double encoding as Rails ActionDispatch also encodes segments but that is OK  Also, controllers that use `params[:topic_name]` will now need to perform decode_www_form_component.",
    "label": "",
    "id": "7030"
  },
  {
    "raw_code": "def show_integrated_tracking_disabled_alert?(project)\n    return false if ::Feature.enabled?(:integrated_error_tracking, project)\n\n    integrated_client_enabled?(project)\n  end",
    "comment": "Should show the alert if the FF was turned off after the integrated client has been configured.",
    "label": "",
    "id": "7031"
  },
  {
    "raw_code": "def available_project_roles(_)\n    Gitlab::Access.options_with_owner.map do |name, access_level|\n      { title: name, value: \"static-#{access_level}\" }\n    end",
    "comment": "Overridden in `ee/app/helpers/ee/projects/project_members_helper.rb`",
    "label": "",
    "id": "7032"
  },
  {
    "raw_code": "def fast_download_project_job_artifacts_path(project, job, params = {})\n      expose_fast_artifacts_path(project, job, :download, params)\n    end",
    "comment": "Rails path generators are slow because they need to do large regex comparisons against the arguments. We can speed this up 10x by generating the strings directly. /*namespace_id/:project_id/-/jobs/:job_id/artifacts/download(.:format)",
    "label": "",
    "id": "7033"
  },
  {
    "raw_code": "def fast_keep_project_job_artifacts_path(project, job)\n      expose_fast_artifacts_path(project, job, :keep)\n    end",
    "comment": "/*namespace_id/:project_id/-/jobs/:job_id/artifacts/keep(.:format)",
    "label": "",
    "id": "7034"
  },
  {
    "raw_code": "def fast_browse_project_job_artifacts_path(project, job)\n      expose_fast_artifacts_path(project, job, :browse)\n    end",
    "comment": "/*namespace_id/:project_id/-/jobs/:job_id/artifacts/browse(/*path)",
    "label": "",
    "id": "7035"
  },
  {
    "raw_code": "def self.organization_route?(route)\n        route.path.spec.to_s.include?(ORGANIZATION_PATH_PATTERN)\n      end",
    "comment": "Route name represents an Organization route.",
    "label": "",
    "id": "7036"
  },
  {
    "raw_code": "def self.build_route_pairs(organization_routes, global_routes)\n        org_route_names = organization_routes.map(&:name)\n        global_route_names = global_routes.map(&:name)\n\n        # Global route => Organization route\n        org_route_names.each_with_object({}) do |org_route_name, route_pairs|\n          global_route_name = extract_global_route_name(org_route_name)\n          next unless global_route_names.include?(global_route_name)\n\n          route_pairs[global_route_name] = org_route_name\n        end",
    "comment": "Build a Hash of global route => Organization route names.",
    "label": "",
    "id": "7037"
  },
  {
    "raw_code": "def self.extract_global_route_name(org_route_name)\n        return if org_route_name.nil?\n\n        # Handle organization patterns with proper underscore preservation\n        org_route_name.gsub(ORGANIZATION_PATH_REGEX, '')\n      end",
    "comment": "Map organization named route to global route.",
    "label": "",
    "id": "7038"
  },
  {
    "raw_code": "def self.build_override_module(route_pairs)\n        Module.new do\n          route_pairs.each do |global_route, org_route|\n            [PATH_SUFFIX, URL_SUFFIX].each do |suffix|\n              method_name = \"#{global_route}#{suffix}\"\n              org_method_name = \"#{org_route}#{suffix}\"\n\n              define_method(method_name) do |*args, **kwargs|\n                current_organization = Routing::OrganizationsHelper::MappedHelpers.current_organization\n\n                if current_organization && current_organization.scoped_paths?\n                  kwargs[:organization_path] ||= current_organization.path\n                end",
    "comment": "Build a module that overrides URL helpers with organization-aware versions",
    "label": "",
    "id": "7039"
  },
  {
    "raw_code": "def execute\n    eventable.resource_milestone_events.include_relations\n      .where(milestone_id: readable_milestone_ids) # rubocop: disable CodeReuse/ActiveRecord\n  end",
    "comment": "Returns the ResourceMilestoneEvents of the eventable visible to the user.  @return ResourceMilestoneEvent::ActiveRecord_AssociationRelation",
    "label": "",
    "id": "7040"
  },
  {
    "raw_code": "def events_milestones\n    @events_milestones ||= Milestone.where(id: unique_milestone_ids_from_events)\n                             .includes(:project, :group)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7041"
  },
  {
    "raw_code": "def relevant_milestone_parents\n    events_milestones.map(&:parent).uniq\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7042"
  },
  {
    "raw_code": "def unique_milestone_ids_from_events\n    eventable.resource_milestone_events.select(:milestone_id).distinct\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7043"
  },
  {
    "raw_code": "def key_for_parent(parent)\n    \"#{parent.class.name}_#{parent.id}\"\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7044"
  },
  {
    "raw_code": "def snippets_for_explore\n    Snippet.public_to_user(current_user).only_personal_snippets\n  end",
    "comment": "Produces a query that retrieves snippets for the Explore page  We only show personal snippets here because this page is meant for discovery, and project snippets are of limited interest here.",
    "label": "",
    "id": "7045"
  },
  {
    "raw_code": "def snippets_for_personal_and_multiple_projects\n    queries = []\n    queries << personal_snippets unless only_project?\n\n    if can?(current_user, :read_cross_project)\n      queries << snippets_of_visible_projects\n      queries << snippets_of_authorized_projects if current_user\n    end",
    "comment": "Produces a query that retrieves snippets from multiple projects.  The resulting query will, depending on the user's permissions, include the following collections of snippets:  1. Snippets that don't belong to any project. 2. Snippets of projects that are visible to the current user (e.g. snippets in public projects). 3. Snippets of projects that the current user is a member of.  Each collection is constructed in isolation, allowing for greater control over the resulting SQL query.",
    "label": "",
    "id": "7046"
  },
  {
    "raw_code": "def snippets_of_visible_projects\n    snippets_for_author_or_visible_to_user\n      .only_include_projects_visible_to(current_user)\n      .only_include_projects_with_snippets_enabled\n  end",
    "comment": "Returns the snippets that the current user (logged in or not) can view.",
    "label": "",
    "id": "7047"
  },
  {
    "raw_code": "def snippets_of_authorized_projects\n    base = author ? author.snippets : Snippet.all\n\n    base\n      .only_include_projects_with_snippets_enabled(include_private: true)\n      .only_include_authorized_projects(current_user)\n  end",
    "comment": "Returns the snippets that the currently logged in user has access to by being a member of the project the snippets belong to.  This method requires that `current_user` returns a `User` instead of `nil`, and is optimised for this specific scenario.",
    "label": "",
    "id": "7048"
  },
  {
    "raw_code": "def item_ids\n    item_ids = []\n\n    if project?\n      if project\n        if project.group.present?\n          labels_table = Label.arel_table\n          group_ids = group_ids_for(project.group)\n\n          item_ids << Label.where(\n            labels_table[:type].eq('GroupLabel').and(labels_table[:group_id].in(group_ids)).or(\n              labels_table[:type].eq('ProjectLabel').and(labels_table[:project_id].eq(project.id))\n            )\n          )\n        else\n          item_ids << project.labels\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7049"
  },
  {
    "raw_code": "def sort(items)\n    return items.reorder(title: :asc) unless params[:sort]\n\n    return items.sorted_by_similarity_desc(params[:search]) if params[:sort] == 'relevance' && params[:search].present?\n\n    items.order_by(params[:sort])\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7050"
  },
  {
    "raw_code": "def with_title(items)\n    return items if title.nil?\n    return items.none if title.blank?\n\n    items.where(title: title)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7051"
  },
  {
    "raw_code": "def by_search(labels)\n    return labels unless search?\n\n    labels.search(params[:search], search_in: params[:search_in])\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7052"
  },
  {
    "raw_code": "def ids_user_can_read_labels(projects)\n    Project.where(id: projects.select(:id)).ids_with_issuables_available_for(current_user)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7053"
  },
  {
    "raw_code": "def execute(current_user = nil)\n    return Project.none if @user.nil?\n    return Project.none unless can?(current_user, :read_user_profile, @user)\n\n    segments = all_projects(current_user)\n\n    find_union(segments, Project).with_namespace.sorted_by_activity\n  end",
    "comment": "Finds the projects belonging to the user in \"@user\", limited to either public projects or projects visible to the given user.  current_user - When given the list of projects is limited to those only visible by this user. params       - Optional query parameters min_access_level: integer  Returns an ActiveRecord::Relation.",
    "label": "",
    "id": "7054"
  },
  {
    "raw_code": "def find_by_id\n    User.find_by_id(@username_or_id)\n  end",
    "comment": "Tries to find a User by id, returning nil if none could be found.",
    "label": "",
    "id": "7055"
  },
  {
    "raw_code": "def find_by_id!\n    User.find(@username_or_id)\n  end",
    "comment": "Tries to find a User by id, raising a `ActiveRecord::RecordNotFound` if it could not be found.",
    "label": "",
    "id": "7056"
  },
  {
    "raw_code": "def find_by_username\n    User.find_by_username(@username_or_id)\n  end",
    "comment": "Tries to find a User by username, returning nil if none could be found.",
    "label": "",
    "id": "7057"
  },
  {
    "raw_code": "def find_by_username!\n    User.find_by_username!(@username_or_id)\n  end",
    "comment": "Tries to find a User by username, raising a `ActiveRecord::RecordNotFound` if it could not be found.",
    "label": "",
    "id": "7058"
  },
  {
    "raw_code": "def find_by_id_or_username\n    if input_is_id?\n      find_by_id\n    else\n      find_by_username\n    end",
    "comment": "Tries to find a User by username or id, returning nil if none could be found.",
    "label": "",
    "id": "7059"
  },
  {
    "raw_code": "def find_by_id_or_username!\n    if input_is_id?\n      find_by_id!\n    else\n      find_by_username!\n    end",
    "comment": "Tries to find a User by username or id, raising a `ActiveRecord::RecordNotFound` if it could not be found.",
    "label": "",
    "id": "7060"
  },
  {
    "raw_code": "def by_generated_ref_commit(items)\n    return items unless params[:project_id].present? && params[:commit_sha].present?\n\n    # Only perform generated ref commit lookup as fallback when no items found\n    return items unless items.empty?\n\n    ::MergeRequests::GeneratedRefCommit\n      .merge_request_for_sha(\n        params[:project_id],\n        params[:commit_sha]\n      )\n  end",
    "comment": "This method handles commit lookup for gitlab generated refs used to run pipelines and merge code and associated with merge requests. When merge requests are processed through merge trains, new commit SHAs are generated that are not stored in the original merge request diff. This method provides a way to find the merge request associated with GitLab-generated references for MR pipelines. These commits are merged/rebased into the target branch via MergeRequests::MergeStrategies::FromSourceBranch and MergeRequests::MergeStrategies::FromTrainRef. Lookup of Refs generated via `MergeRequests::MergeToRefService` is not currently supported See: https://gitlab.com/gitlab-org/gitlab/-/issues/421025",
    "label": "",
    "id": "7061"
  },
  {
    "raw_code": "def by_source_branch(items)\n    return items unless source_branch\n\n    items.where(source_branch: source_branch)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7062"
  },
  {
    "raw_code": "def target_branch\n    @target_branch ||= params[:target_branch].presence\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7063"
  },
  {
    "raw_code": "def by_target_branch(items)\n    return items unless target_branch\n\n    items.where(target_branch: target_branch)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7064"
  },
  {
    "raw_code": "def by_negated_target_branch(items)\n    return items unless not_params[:target_branch]\n\n    items.where.not(target_branch: not_params[:target_branch])\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7065"
  },
  {
    "raw_code": "def by_negated_approved_by(items)\n    return items unless not_params[:approved_by_usernames]\n\n    items.not_approved_by_users_with_usernames(not_params[:approved_by_usernames])\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7066"
  },
  {
    "raw_code": "def by_source_project_id(items)\n    return items unless source_project_id\n\n    items.where(source_project_id: source_project_id)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7067"
  },
  {
    "raw_code": "def by_resource_event_state(items)\n    return items unless params[:merged_without_event_source].present?\n\n    items.merged_without_state_event_source\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7068"
  },
  {
    "raw_code": "def by_draft(items)\n    draft_param = Gitlab::Utils.to_boolean(params.fetch(:draft) { params.fetch(:wip, nil) })\n    return items if draft_param.nil?\n\n    if draft_param\n      items.where(draft_match(items.arel_table))\n    else\n      items.where.not(draft_match(items.arel_table))\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7069"
  },
  {
    "raw_code": "def draft_match(table)\n    table[:title].matches('Draft - %')\n      .or(table[:title].matches('Draft:%'))\n      .or(table[:title].matches('[Draft]%'))\n      .or(table[:title].matches('(Draft)%'))\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7070"
  },
  {
    "raw_code": "def by_approvals(items)\n    MergeRequests::ByApprovalsFinder\n      .new(params[:approved_by_usernames], params[:approved_by_ids])\n      .execute(items)\n  end",
    "comment": "Filter by merge requests that had been approved by specific users rubocop: disable CodeReuse/Finder",
    "label": "",
    "id": "7071"
  },
  {
    "raw_code": "def by_deployments(items)\n    env = params[:environment]\n    before = parse_datetime(params[:deployed_before])\n    after = parse_datetime(params[:deployed_after])\n    id = params[:deployment_id]\n\n    return items if !env && !before && !after && !id\n\n    # Each filter depends on the same JOIN+WHERE. To prevent this JOIN+WHERE\n    # from being duplicated for every filter, we only produce it once. The\n    # filter methods in turn expect the JOIN+WHERE to already be present.\n    #\n    # This approach ensures that query performance doesn't degrade as the number\n    # of deployment related filters increases.\n    deploys = DeploymentMergeRequest.join_deployments_for_merge_requests\n    deploys = deploys.by_deployment_id(id) if id\n    deploys = deploys.deployed_to(env) if env\n    deploys = deploys.deployed_before(before) if before\n    deploys = deploys.deployed_after(after) if after\n\n    items.where_exists(deploys)\n  end",
    "comment": "rubocop: enable CodeReuse/Finder",
    "label": "",
    "id": "7072"
  },
  {
    "raw_code": "def preload_emails_enabled\n    group_ids_with_disabled_email = Group.ids_with_disabled_email(groups.to_a)\n\n    groups.each do |group|\n      group.emails_enabled_memoized = group_ids_with_disabled_email.exclude?(group.id) if group.parent_id\n    end",
    "comment": "This method preloads the `emails_enabled` strong memoized method for the given groups.  For each group, look up the ancestor hierarchy and look for any group where emails_enabled is false. The lookup is implemented with an EXISTS subquery, so we can look up the ancestor chain for each group individually. The query will return groups where at least one ancestor has the `emails_disabled` set to true.  After the query, we set the instance variable.",
    "label": "",
    "id": "7073"
  },
  {
    "raw_code": "def ancestors_of_groups(base_for_ancestors)\n    ancestors = Group.id_in(base_for_ancestors).self_and_ancestors(upto: parent_group.id)\n    ancestors = ancestors.self_or_ancestors_inactive if inactive?\n    ancestors\n  end",
    "comment": "When filtering we want all to preload all the ancestors upto the specified parent group.  - root - subgroup - nested-group - project  So when searching 'project', on the 'subgroup' page we want to preload 'nested-group' but not 'subgroup' or 'root'",
    "label": "",
    "id": "7074"
  },
  {
    "raw_code": "def descendant_projects\n    projects_nested_in_group = Project.in_namespace(parent_group.self_and_descendants.as_ids)\n\n    finder_params = params.dup\n    finder_params[:search] = params[:filter] if params[:filter]\n\n    ProjectsFinder.new( # rubocop:disable CodeReuse/Finder\n      params: finder_params,\n      current_user: current_user,\n      project_ids_relation: projects_nested_in_group\n    ).execute\n  end",
    "comment": "Finds all projects nested under `parent_group` or any of its descendant groups",
    "label": "",
    "id": "7075"
  },
  {
    "raw_code": "def by_visible_to_users(descendants)\n    groups_table = Group.arel_table\n    visible_to_user = groups_table[:visibility_level]\n      .in(Gitlab::VisibilityLevel.levels_for_user(current_user))\n\n    if current_user\n      authorized_groups = GroupsFinder.new(current_user, all_available: false) # rubocop: disable CodeReuse/Finder\n        .execute.arel.as('authorized')\n      authorized_to_user = groups_table.project(1).from(authorized_groups)\n        .where(authorized_groups[:id].eq(groups_table[:id]))\n        .exists\n      visible_to_user = visible_to_user.or(authorized_to_user)\n    end",
    "comment": "Filters group descendants to only include those visible to the current user.  This method applies visibility filtering based on two criteria: 1. Groups with visibility level accessible to the current user 2. Groups where the user has explicit authorization (if authenticated)  @param descendants [ActiveRecord::Relation<Group>] The collection of group descendants to filter @return [ActiveRecord::Relation<Group>] Filtered descendants visible to the current user rubocop: disable CodeReuse/ActiveRecord -- Needs specialized queries for optimization",
    "label": "",
    "id": "7076"
  },
  {
    "raw_code": "def by_active(descendants)\n    return descendants if params[:active].nil?\n\n    params[:active] ? descendants.self_and_ancestors_active : descendants.self_or_ancestors_inactive\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7077"
  },
  {
    "raw_code": "def execute_multi\n    users = []\n    @target_user.each do |user|\n      users.append(user.id) if can?(current_user, :read_user_profile, user)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7078"
  },
  {
    "raw_code": "def target_events\n    Event.where(author: target_user)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7079"
  },
  {
    "raw_code": "def limit\n    return DEFAULT_LIMIT unless params[:limit].present?\n\n    [params[:limit].to_i, MAX_LIMIT].min\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7080"
  },
  {
    "raw_code": "def initialize(params = {})\n    @params = params\n\n    # To prevent N+1 queries when fetching the users of the PendingTodos.\n    @preload_user_association = params.fetch(:preload_user_association, false)\n  end",
    "comment": "users - The list of users to retrieve the todos for. If nil is passed, it won't filter todos based on users params - A Hash containing columns and values to use for filtering todos.",
    "label": "",
    "id": "7081"
  },
  {
    "raw_code": "def groups_with_min_access_level\n    inner_query = current_user\n      .groups\n      .where('members.access_level >= ?', params[:min_access_level])\n      .self_and_descendants\n    cte = Gitlab::SQL::CTE.new(:groups_with_min_access_level_cte, inner_query)\n    cte.apply_to(Group.where({}))\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7082"
  },
  {
    "raw_code": "def authorized_groups\n    return unless current_user\n\n    if params.fetch(:include_ancestors, true)\n      current_user.authorized_groups.self_and_ancestors\n    else\n      current_user.authorized_groups\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7083"
  },
  {
    "raw_code": "def execute(*params)\n    group = Group.find_by(*params)\n\n    if can?(@current_user, :read_group, group)\n      group\n    else\n      nil\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7084"
  },
  {
    "raw_code": "def negatable_scalar_params\n      @negatable_scalar_params ||= scalar_params - %i[search in]\n    end",
    "comment": "This should not be used in controller strong params!",
    "label": "",
    "id": "7085"
  },
  {
    "raw_code": "def negatable_array_params\n      @negatable_array_params ||= array_params.keys.append(:iids)\n    end",
    "comment": "This should not be used in controller strong params!",
    "label": "",
    "id": "7086"
  },
  {
    "raw_code": "def negatable_params\n      @negatable_params ||= negatable_scalar_params + negatable_array_params\n    end",
    "comment": "This should not be used in controller strong params!",
    "label": "",
    "id": "7087"
  },
  {
    "raw_code": "def filter_negated_items(items)\n    items = by_negated_milestone(items)\n    items = by_negated_release(items)\n    items = by_negated_my_reaction_emoji(items)\n    by_negated_iids(items)\n  end",
    "comment": "Negates all params found in `negatable_params`",
    "label": "",
    "id": "7088"
  },
  {
    "raw_code": "def count_by_state\n    count_params = params.merge(state: nil, sort: nil)\n    finder = self.class.new(current_user, count_params)\n\n    state_counts = finder\n      .execute\n      .without_order\n      .group(:state_id)\n      .count\n\n    counts = Hash.new(0)\n\n    state_counts.each do |key, value|\n      counts[count_key(key)] += value\n    end",
    "comment": "We often get counts for each state by running a query per state, and counting those results. This is typically slower than running one query (even if that query is slower than any of the individual state queries) and grouping and counting within that query.  rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7089"
  },
  {
    "raw_code": "def search\n    params[:search].presence\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7090"
  },
  {
    "raw_code": "def by_scope(items)\n    return items.none if params.current_user_related? && !current_user\n\n    case params[:scope]\n    when 'created_by_me', 'authored'\n      items.where(author_id: current_user.id)\n    when 'assigned_to_me'\n      items.assigned_to(current_user)\n    when 'reviews_for_me'\n      items.review_requested_to(current_user)\n    else\n      items\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7091"
  },
  {
    "raw_code": "def by_closed_at(items)\n    items = items.closed_after(params[:closed_after]) if params[:closed_after].present?\n    items = items.closed_before(params[:closed_before]) if params[:closed_before].present?\n\n    items\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7092"
  },
  {
    "raw_code": "def filter_by_namespace(items)\n    if use_join_strategy_for_project?\n      # When finding issues for multiple projects it's more efficient\n      # to use a JOIN instead of running a sub-query\n      # See https://gitlab.com/gitlab-org/gitlab/-/commit/8591cc02be6b12ed60f763a5e0147f2cbbca99e1\n      items.join_project_through_namespace.merge(accessible_projects.reorder(nil))\n    else\n      items.in_namespaces(accessible_projects.map(&:project_namespace_id)).references_project\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7093"
  },
  {
    "raw_code": "def accessible_projects\n    params.projects\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7094"
  },
  {
    "raw_code": "def by_search(items)\n    return items unless search\n    return items if items.null_relation?\n\n    return filter_by_full_text_search(items) if use_full_text_search?\n\n    if use_cte_for_search?\n      cte = Gitlab::SQL::CTE.new(klass.table_name, items)\n\n      items = klass.with(cte.to_arel).from(klass.table_name)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7095"
  },
  {
    "raw_code": "def use_full_text_search?\n    klass.try(:pg_full_text_searchable_columns).present? &&\n      params[:search] =~ FULL_TEXT_SEARCH_TERM_REGEX\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7096"
  },
  {
    "raw_code": "def by_negated_iids(items)\n    not_params[:iids].present? ? items.where.not(iid: not_params[:iids]) : items\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7097"
  },
  {
    "raw_code": "def sort(items)\n    # Ensure we always have an explicit sort order (instead of inheriting\n    # multiple orders when combining ActiveRecord::Relation objects).\n    if params[:sort]\n      items.sort_by_attribute(\n        params[:sort],\n        excluded_labels: label_filter.label_names_excluded_from_priority_sort\n      )\n    else\n      items.reorder(id: :desc)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7098"
  },
  {
    "raw_code": "def by_author(items)\n    Issuables::AuthorFilter.new(\n      current_user: current_user,\n      params: original_params\n    ).filter(items)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7099"
  },
  {
    "raw_code": "def by_closed_by(items)\n    return items if params[:closed_by_id].blank?\n\n    items.where(closed_by_id: params[:closed_by_id])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7100"
  },
  {
    "raw_code": "def by_label(items)\n    label_filter.filter(items)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7101"
  },
  {
    "raw_code": "def by_milestone(items)\n    return items unless params.milestones?\n\n    if params.filter_by_no_milestone?\n      items.left_joins_milestones.where(milestone_id: [-1, nil])\n    elsif params.filter_by_any_milestone?\n      items.any_milestone\n    elsif params.filter_by_upcoming_milestone?\n      upcoming_ids = Milestone.upcoming_ids(accessible_projects, params.related_groups,\n        legacy_filtering_logic: use_legacy_milestone_filtering?)\n      items.left_joins_milestones.where(milestone_id: upcoming_ids)\n    elsif params.filter_by_started_milestone?\n      items.left_joins_milestones\n           .merge(Milestone.started(legacy_filtering_logic: use_legacy_milestone_filtering?))\n    else\n      items.with_milestone(params[:milestone_title])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7102"
  },
  {
    "raw_code": "def by_negated_milestone(items)\n    return items unless not_params.milestones?\n\n    if not_params.filter_by_upcoming_milestone?\n      items.joins(:milestone)\n           .merge(Milestone.not_upcoming(legacy_filtering_logic: use_legacy_milestone_filtering?))\n    elsif not_params.filter_by_started_milestone?\n      items.joins(:milestone).merge(Milestone.not_started)\n    else\n      items.without_particular_milestones(not_params[:milestone_title])\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7103"
  },
  {
    "raw_code": "def by_release(items)\n    return items unless params.releases?\n    return items if params.group? # don't allow release filtering at group level\n\n    if params.filter_by_no_release?\n      items.without_release\n    elsif params.filter_by_any_release?\n      items.any_release\n    else\n      items.with_release(params[:release_tag], params[:project_id])\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7104"
  },
  {
    "raw_code": "def filter_projects(collection)\n    collection = by_namespace_path(collection)\n    collection = by_deleted_status(collection)\n    collection = by_ids(collection)\n    collection = by_full_paths(collection)\n    collection = by_personal(collection)\n    collection = by_starred(collection)\n    collection = by_trending(collection)\n    collection = by_visibility_level(collection)\n    collection = by_topics(collection)\n    collection = by_topic_id(collection)\n    collection = by_search(collection)\n    collection = by_active(collection)\n    collection = by_archived(collection)\n    collection = by_custom_attributes(collection)\n    collection = by_not_aimed_for_deletion(collection)\n    collection = by_last_activity_after(collection)\n    collection = by_last_activity_before(collection)\n    collection = by_language(collection)\n    collection = by_feature_availability(collection)\n    collection = by_updated_at(collection)\n    collection = by_marked_for_deletion_on(collection)\n    collection = by_aimed_for_deletion(collection)\n    collection = by_last_repository_check_failed(collection)\n    by_repository_storage(collection)\n  end",
    "comment": "EE would override this to add more filters",
    "label": "",
    "id": "7105"
  },
  {
    "raw_code": "def collection_without_user\n    if private_only? || owned_projects? || min_access_level?\n      Project.none\n    else\n      Project.public_to_user\n    end",
    "comment": "Builds a collection for an anonymous user.",
    "label": "",
    "id": "7106"
  },
  {
    "raw_code": "def impossible_visibility_level?\n    return unless params[:visibility_level].present?\n\n    public_visibility_levels = Gitlab::VisibilityLevel.levels_for_user(current_user)\n\n    public_visibility_levels.exclude?(params[:visibility_level].to_i)\n  end",
    "comment": "This is an optimization - surprisingly PostgreSQL does not optimize for this.  If the default visibility level and desired visibility level filter cancels each other out, don't use the SQL clause for visibility level in `Project.public_or_visible_to_user`. In fact, this then becomes equivalent to just authorized projects for the user.  E.g. (EXISTS(<authorized_projects>) OR projects.visibility_level IN (10,20)) AND \"projects\".\"visibility_level\" = 0  is essentially EXISTS(<authorized_projects>) AND \"projects\".\"visibility_level\" = 0  See https://gitlab.com/gitlab-org/gitlab/issues/37007",
    "label": "",
    "id": "7107"
  },
  {
    "raw_code": "def by_ids(items)\n    items = items.where(id: project_ids_relation) if project_ids_relation\n    items = items.where('projects.id > ?', params[:id_after]) if params[:id_after]\n    items = items.where('projects.id < ?', params[:id_before]) if params[:id_before]\n    items\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7108"
  },
  {
    "raw_code": "def by_full_paths(items)\n    params[:full_paths].present? ? items.where_full_path_in(params[:full_paths], preload_routes: false) : items\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7109"
  },
  {
    "raw_code": "def by_visibility_level(items)\n    params[:visibility_level].present? ? items.where(visibility_level: params[:visibility_level]) : items\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7110"
  },
  {
    "raw_code": "def by_topics(items)\n    return items unless params[:topic].present?\n\n    topics = params[:topic].instance_of?(String) ? params[:topic].split(',') : params[:topic]\n    sanitized_topics = topics.map(&:strip).uniq.reject(&:empty?)\n\n    items.contains_all_topic_names(sanitized_topics)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7111"
  },
  {
    "raw_code": "def initialize(project, params: {}, current_user: nil)\n    project_ids = project.forks.includes(:creator).select(:id)\n    super(params: params, current_user: current_user, project_ids_relation: project_ids)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7112"
  },
  {
    "raw_code": "def use_snooze_custom_sort?\n    filter_pending_only?\n  end",
    "comment": "We only need to surface snoozed to-dos when querying pending items. The special sort order is unnecessary in the `Done` and `All` tabs where we can simply sort by ID (= creation date).",
    "label": "",
    "id": "7113"
  },
  {
    "raw_code": "def ancestor_clusters\n    strong_memoize(:ancestor_clusters) do\n      Clusters::Cluster.ancestor_clusters_for_clusterable(clusterable)\n    end",
    "comment": "This unfortunately returns an Array, not a Relation!",
    "label": "",
    "id": "7114"
  },
  {
    "raw_code": "def initialize(params = {})\n    @source = params.delete(:source)\n    @current_user = params.delete(:current_user)\n    @scope = params.delete(:scope)\n    @params = params\n  end",
    "comment": "Used to filter Events  Arguments: source - which user or project to looks for events on current_user - only return events for projects visible to this user scope - return all events across a user's projects params: action: string target_type: string before: datetime after: datetime per_page: integer (max. 100) page: integer with_associations: boolean sort: 'asc' or 'desc'",
    "label": "",
    "id": "7115"
  },
  {
    "raw_code": "def by_current_user_access(events)\n    events.merge(Project.public_or_visible_to_user(current_user))\n      .joins(:project)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7116"
  },
  {
    "raw_code": "def by_action(events)\n    safe_action = Event.actions[params[:action]]\n    return events unless safe_action\n\n    events.where(action: safe_action)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7117"
  },
  {
    "raw_code": "def by_target_type(events)\n    return events unless Event::TARGET_TYPES[params[:target_type]]\n\n    events.where(target_type: Event::TARGET_TYPES[params[:target_type]].name)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7118"
  },
  {
    "raw_code": "def by_created_at_before(events)\n    return events unless params[:before]\n\n    events.where('events.created_at < ?', params[:before].beginning_of_day)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7119"
  },
  {
    "raw_code": "def by_created_at_after(events)\n    return events unless params[:after]\n\n    events.where('events.created_at > ?', params[:after].end_of_day)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7120"
  },
  {
    "raw_code": "def cannot_access_private_profile?\n    source.is_a?(User) && !Ability.allowed?(current_user, :read_user_profile, source)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7121"
  },
  {
    "raw_code": "def execute(current_user = nil)\n    @user\n      .authorized_groups\n      .with_non_archived_projects\n      .with_non_invite_group_members\n      .public_or_visible_to_user(current_user)\n      .with_route\n      .order_id_desc\n  end",
    "comment": "Finds the groups of the source user, optionally limited to those visible to the current user.",
    "label": "",
    "id": "7122"
  },
  {
    "raw_code": "def preload_associations(scope)\n    scope.includes(\n      :user,\n      environment: [],\n      deployable: {\n        job_artifacts: [],\n        user: [],\n        metadata: [],\n        job_definition: [],\n        job_environment: [],\n        pipeline: {\n          project: {\n            route: [],\n            namespace: :route\n          }\n        },\n        project: {\n          namespace: :route\n        }\n      }\n    )\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7123"
  },
  {
    "raw_code": "def initialize(container, secret, file_path)\n    @container = container\n    @secret = secret\n    @file_path = file_path\n  end",
    "comment": "Instantiates a new FileUploader or NamespaceFileUploader based on container type FileUploader can be opened via .open agnostic of storage type Arguments correspond to Upload.secret, Upload.model_type and Upload.file_path Returns a FileUploader with uploaded file retrieved into the object state  container - project, project namespace or group secret - secret string in path to the file, based on FileUploader::MARKDOWN_PATTERN regex file_path - relative path to the file based on FileUploader::MARKDOWN_PATTERN regex",
    "label": "",
    "id": "7124"
  },
  {
    "raw_code": "def include_subgroups?\n    options.fetch(:include_subgroups, false)\n  end",
    "comment": "subgroups are supported only for owned projects not for shared",
    "label": "",
    "id": "7125"
  },
  {
    "raw_code": "def include_ancestor_groups?\n    options.fetch(:include_ancestor_groups, false)\n  end",
    "comment": "ancestor groups are supported only for owned projects not for shared",
    "label": "",
    "id": "7126"
  },
  {
    "raw_code": "def filter_negated_items(items)\n    issues = super\n    by_negated_issue_types(issues)\n  end",
    "comment": "Negates all params found in `negatable_params`",
    "label": "",
    "id": "7127"
  },
  {
    "raw_code": "def initialize(current_user, params = {})\n    @project = params[:project]\n    @current_user = current_user\n    @params = params.dup\n    @target_type = @params[:target_type]\n  end",
    "comment": "Used to filter Notes When used with target_type and target_id this returns notes specifically for the controller  Arguments: current_user - which user check authorizations with project - which project to look for notes on params: target: noteable target_type: string target_id: integer last_fetched_at: time search: string sort: string ",
    "label": "",
    "id": "7128"
  },
  {
    "raw_code": "def notes_of_any_type\n    types = %w[commit issue merge_request snippet]\n    note_relations = types.map { |t| notes_for_type(t) }\n    note_relations.map! { |notes| search(notes) }\n    UnionFinder.new.find_union(note_relations, Note.includes(:author)) # rubocop: disable CodeReuse/Finder\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7129"
  },
  {
    "raw_code": "def noteables_for_type(noteable_type)\n    case noteable_type\n    when \"issue\"\n      IssuesFinder.new(@current_user, project_id: @project.id).execute # rubocop: disable CodeReuse/Finder\n    when \"merge_request\"\n      MergeRequestsFinder.new(@current_user, project_id: @project.id).execute # rubocop: disable CodeReuse/Finder\n    when \"snippet\", \"project_snippet\"\n      SnippetsFinder.new(@current_user, project: @project).execute # rubocop: disable CodeReuse/Finder\n    when \"personal_snippet\"\n      SnippetsFinder.new(@current_user, only_personal: true).execute # rubocop: disable CodeReuse/Finder\n    when \"wiki_page/meta\"\n      WikiPage::Meta.for_project(@project)\n    else\n      raise \"invalid target_type '#{noteable_type}'\"\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7130"
  },
  {
    "raw_code": "def notes_for_type(noteable_type)\n    if noteable_type == \"commit\"\n      if Ability.allowed?(@current_user, :read_code, @project)\n        @project.notes.where(noteable_type: 'Commit')\n      else\n        Note.none\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7131"
  },
  {
    "raw_code": "def notes_on_target\n    if target.respond_to?(:related_notes)\n      target.related_notes\n    else\n      target.notes\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7132"
  },
  {
    "raw_code": "def search(notes)\n    query = @params[:search]\n    return notes unless query\n\n    notes.search(query)\n  end",
    "comment": "Searches for notes matching the given query.  This method uses ILIKE on PostgreSQL. ",
    "label": "",
    "id": "7133"
  },
  {
    "raw_code": "def since_fetch_at(notes)\n    return notes unless @params[:last_fetched_at]\n\n    # Default to 0 to remain compatible with old clients\n    last_fetched_at = @params.fetch(:last_fetched_at, Time.at(0))\n\n    # Use overlapping intervals to avoid worrying about race conditions\n    last_fetched_at -= FETCH_OVERLAP\n\n    notes.updated_after(last_fetched_at)\n  end",
    "comment": "Notes changed since last fetch",
    "label": "",
    "id": "7134"
  },
  {
    "raw_code": "def by_search(items, search)\n    items.joins(:route).fuzzy_search(search, [Route.arel_table[:path], Route.arel_table[:name], :description])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7135"
  },
  {
    "raw_code": "def initialize(source)\n    @source = source\n  end",
    "comment": "Arguments: source - a Group or Project",
    "label": "",
    "id": "7136"
  },
  {
    "raw_code": "def execute\n    items = Project.without_deleted.with_statistics.with_route\n    items = by_namespace_id(items)\n    items = by_visibility_level(items)\n    items = by_with_push(items)\n    items = by_abandoned(items)\n    items = by_last_repository_check_failed(items)\n    items = by_archived(items)\n    items = by_personal(items)\n    items = by_name(items)\n    items = items.includes(namespace: [:owner, :route])\n    sort(items).page(params[:page])\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7137"
  },
  {
    "raw_code": "def by_visibility_level(items)\n    params[:visibility_level].present? ? items.where(visibility_level: params[:visibility_level]) : items\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7138"
  },
  {
    "raw_code": "def by_with_push(items)\n    params[:with_push].present? ? items.with_push : items\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7139"
  },
  {
    "raw_code": "def groups_with_guest_access_plus\n      groups = GroupsFinder.new(current_user, min_access_level: Gitlab::Access::GUEST).execute\n\n      # We move the result into a materialized CTE to improve query performance during text search.\n      union_query = ::Group.from_union([groups])\n      cte = Gitlab::SQL::CTE.new(:my_union_cte, union_query)\n      Group.with(cte.to_arel).from(cte.alias_to(Group.arel_table)) # rubocop: disable CodeReuse/ActiveRecord -- CTE use\n    end",
    "comment": "rubocop: disable CodeReuse/Finder",
    "label": "",
    "id": "7140"
  },
  {
    "raw_code": "def can_share_project?\n      Ability.allowed?(current_user, :admin_project, project_to_be_shared) &&\n        project_to_be_shared.allowed_to_share_with_group?\n    end",
    "comment": "rubocop: enable CodeReuse/Finder",
    "label": "",
    "id": "7141"
  },
  {
    "raw_code": "def execute(commits)\n      mapping = {}\n      shas = commits.map(&:id)\n\n      # To include merge requests by the merged/merge/squash SHA, we don't need\n      # to go through any diff rows.\n      #\n      # We can't squeeze all this into a single query, as the diff based data\n      # relies on a GROUP BY. On the other hand, retrieving MRs by their merge\n      # SHAs separately is much easier, and plenty fast.\n      @project\n        .merge_requests\n        .preload_target_project\n        .by_merged_or_merge_or_squash_commit_sha(shas)\n        .each do |mr|\n          # SHAs for merge commits, squash commits, and rebased source SHAs,\n          # can't be in the merge request source branch. It _is_ possible a\n          # newer merge request includes the commit, but in that case we still\n          # want the oldest merge request.\n          #\n          # It's also possible that a merge request produces both a squashed\n          # commit and a merge commit. In that case we want to store the mapping\n          # for both the SHAs.\n          mapping[mr.squash_commit_sha] = mr if mr.squash_commit_sha\n          mapping[mr.merge_commit_sha] = mr if mr.merge_commit_sha\n          mapping[mr.merged_commit_sha] = mr if mr.merged_commit_sha\n        end",
    "comment": "Returns a Hash that maps a commit ID to the oldest merge request that introduced that commit.",
    "label": "",
    "id": "7142"
  },
  {
    "raw_code": "def initialize(usernames, ids)\n      # rubocop:disable CodeReuse/ActiveRecord\n      @usernames = Array(usernames).map(&:to_s).uniq.take(MAX_FILTER_ELEMENTS)\n      @ids = Array(ids).uniq.take(MAX_FILTER_ELEMENTS)\n      # rubocop:enable CodeReuse/ActiveRecord\n    end",
    "comment": "Initialize the finder  @param [Array<String>] usernames @param [Array<Integers>] ids",
    "label": "",
    "id": "7143"
  },
  {
    "raw_code": "def execute(items)\n      if by_no_approvals?\n        without_approvals(items)\n      elsif by_any_approvals?\n        with_any_approvals(items)\n      elsif ids.present?\n        find_approved_by_ids(items)\n      elsif usernames.present?\n        find_approved_by_names(items)\n      else\n        items\n      end",
    "comment": "Filter MergeRequest collections by approvers  @param [ActiveRecord::Relation] items the activerecord relation",
    "label": "",
    "id": "7144"
  },
  {
    "raw_code": "def by_no_approvals?\n      includes_special_label?(IssuableFinder::Params::FILTER_NONE)\n    end",
    "comment": "Is param using special condition: \"None\" ?  @return [Boolean] whether special condition \"None\" is being used",
    "label": "",
    "id": "7145"
  },
  {
    "raw_code": "def by_any_approvals?\n      includes_special_label?(IssuableFinder::Params::FILTER_ANY)\n    end",
    "comment": "Is param using special condition: \"Any\" ?  @return [Boolean] whether special condition \"Any\" is being used",
    "label": "",
    "id": "7146"
  },
  {
    "raw_code": "def includes_special_label?(label)\n      ids.first.to_s.downcase == label || usernames.map(&:downcase).include?(label)\n    end",
    "comment": "Check if we have the special label in ids or usernames field  @param [String] label the special label @return [Boolean] whether ids or usernames includes the special label",
    "label": "",
    "id": "7147"
  },
  {
    "raw_code": "def without_approvals(items)\n      items.without_approvals\n    end",
    "comment": "Merge requests without any approval  @param [ActiveRecord::Relation] items",
    "label": "",
    "id": "7148"
  },
  {
    "raw_code": "def with_any_approvals(items)\n      items.select_from_union([items.with_approvals])\n    end",
    "comment": "Merge requests with any number of approvals  @param [ActiveRecord::Relation] items the activerecord relation",
    "label": "",
    "id": "7149"
  },
  {
    "raw_code": "def find_approved_by_names(items)\n      items.approved_by_users_with_usernames(*usernames)\n    end",
    "comment": "Merge requests approved by given usernames  @param [ActiveRecord::Relation] items the activerecord relation",
    "label": "",
    "id": "7150"
  },
  {
    "raw_code": "def find_approved_by_ids(items)\n      items.approved_by_users_with_ids(*ids)\n    end",
    "comment": "Merge requests approved by given user IDs  @param [ActiveRecord::Relation] items the activerecord relation",
    "label": "",
    "id": "7151"
  },
  {
    "raw_code": "def initialize(organization:, current_user:)\n      @organization = organization\n      @current_user = current_user\n    end",
    "comment": "@param organization [Organizations::Organization] @param current_user [User]",
    "label": "",
    "id": "7152"
  },
  {
    "raw_code": "def initialize(organization:, current_user:)\n      @organization = organization\n      @current_user = current_user\n    end",
    "comment": "@param organization [Organizations::Organization] @param current_user [User]",
    "label": "",
    "id": "7153"
  },
  {
    "raw_code": "def get_releases\n      Gitlab::Pagination::Keyset::InOperatorOptimization::QueryBuilder.new(\n        scope: releases_scope,\n        array_scope: Project.for_group_and_its_subgroups(parent).select(:id),\n        array_mapping_scope: ->(project_id_expression) {\n          Release.where(Release.arel_table[:project_id].eq(project_id_expression))\n        },\n        finder_query: ->(order_by, id_expression) { Release.where(Release.arel_table[:id].eq(id_expression)) }\n      )\n      .execute\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7154"
  },
  {
    "raw_code": "def merge(other)\n      self.class.new(params.merge(other), current_user, klass)\n    end",
    "comment": "We use Hash#merge in a few places, so let's support it",
    "label": "",
    "id": "7155"
  },
  {
    "raw_code": "def merge!(other)\n      params.merge!(other)\n    end",
    "comment": "Just for symmetry, and in case someone tries to use it",
    "label": "",
    "id": "7156"
  },
  {
    "raw_code": "def by_crm_organization(issuables)\n      return issuables if params[:crm_organization_id].blank?\n      return issuables unless current_user&.can?(:read_crm_organization, parent&.crm_group)\n\n      condition = CustomerRelations::IssueContact\n        .joins(:contact)\n        .where(contact: { organization_id: params[:crm_organization_id] })\n        .where(Arel.sql(\"issue_id = issues.id\"))\n      issuables.where(condition.arel.exists)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7157"
  },
  {
    "raw_code": "def by_crm_contact(issuables)\n      return issuables if params[:crm_contact_id].blank?\n      return issuables unless current_user&.can?(:read_crm_contact, parent&.crm_group)\n\n      condition = CustomerRelations::IssueContact\n        .where(contact_id: params[:crm_contact_id])\n        .where(Arel.sql(\"issue_id = issues.id\"))\n      issuables.where(condition.arel.exists)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7158"
  },
  {
    "raw_code": "def extract_group_member_ids(username_param)\n      filter_param = Array(username_param)\n      return unless username_param_is_a_group_handle?(filter_param)\n\n      reference_extractor = ::Gitlab::ReferenceExtractor.new(nil, current_user)\n      reference_extractor.analyze(filter_param.first, { skip_project_check: true })\n\n      # Extract references when group is readable by user\n      group = reference_extractor.references(:mentioned_group).first # rubocop: disable CodeReuse/ActiveRecord -- not an ActiveRecord model\n      return unless group\n\n      # Limit the filtering only for groups with less than 100 members\n      # to avoid database performance issues\n      if group.group_members.limit(MAX_GROUP_MEMBERS_COUNT + 1).count > MAX_GROUP_MEMBERS_COUNT\n        raise TooManyGroupMembersError,\n          \"Group has too many members (limit is #{MAX_GROUP_MEMBERS_COUNT}).\"\n      end",
    "comment": "Returns active record relation if search parameter is a group handle, eg '@group/subgroup'",
    "label": "",
    "id": "7159"
  },
  {
    "raw_code": "def label_link_query(issuables, label_ids: nil, label_names: nil)\n      target_model = issuables.klass\n      base_target_model = issuables.base_class\n\n      # passing the original target_model just to avoid running the labels union query on group level issues pages\n      # as the query becomes more expensive at group level. This is to be removed altogether as we migrate labels off\n      # Epic altogether, planned as a high priority follow-up for Epic to WorkItem migration:\n      # re https://gitlab.com/gitlab-org/gitlab/-/issues/465725\n      relation = target_label_links_query(target_model, base_target_model, label_ids)\n      relation = relation.joins(:label).where(labels: { name: label_names }) if label_names\n\n      relation\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7160"
  },
  {
    "raw_code": "def by_label(issuables)\n      return issuables unless label_names_from_params.present?\n\n      if filter_by_no_label?\n        issuables.where(label_link_query(issuables).arel.exists.not)\n      elsif filter_by_any_label?\n        issuables.where(label_link_query(issuables).arel.exists)\n      else\n        issuables_with_selected_labels(issuables, label_names_from_params)\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7161"
  },
  {
    "raw_code": "def by_label_union(issuables)\n      return issuables unless label_names_from_or_params.present?\n\n      if root_namespace\n        all_label_ids = find_label_ids(label_names_from_or_params).flatten\n        issuables.where(label_link_query(issuables, label_ids: all_label_ids).arel.exists)\n      else\n        issuables.where(label_link_query(issuables, label_names: label_names_from_or_params).arel.exists)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7162"
  },
  {
    "raw_code": "def by_negated_label(issuables)\n      return issuables unless label_names_from_not_params.present?\n\n      issuables_without_selected_labels(issuables, label_names_from_not_params)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7163"
  },
  {
    "raw_code": "def issuables_with_selected_labels(issuables, label_names)\n      if root_namespace\n        all_label_ids = find_label_ids(label_names)\n        # Found less labels in the DB than we were searching for. Return nothing.\n        return issuables.none if all_label_ids.size != label_names.size\n\n        all_label_ids.each do |label_ids|\n          issuables = issuables.where(label_link_query(issuables, label_ids: label_ids).arel.exists)\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7164"
  },
  {
    "raw_code": "def issuables_without_selected_labels(issuables, label_names)\n      if root_namespace\n        label_ids = find_label_ids(label_names).flatten(1)\n\n        return issuables if label_ids.empty?\n\n        issuables.where(label_link_query(issuables, label_ids: label_ids).arel.exists.not)\n      else\n        issuables.where(label_link_query(issuables, label_names: label_names).arel.exists.not)\n      end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7165"
  },
  {
    "raw_code": "def find_label_ids(label_names)\n      find_label_ids_uncached(label_names)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7166"
  },
  {
    "raw_code": "def find_label_ids_uncached(label_names)\n      return [] if label_names.empty?\n\n      group_labels = group_labels_for_root_namespace.where(title: label_names)\n      project_labels = project_labels_for_root_namespace.where(title: label_names)\n\n      Label\n        .from_union([group_labels, project_labels], remove_duplicates: false)\n        .without_order\n        .pluck(:title, :id)\n        .group_by(&:first)\n        .values\n        .map { |labels| labels.map(&:last) }\n    end",
    "comment": "This returns an array of label IDs per label name. It is possible for a label name to have multiple IDs because we allow labels with the same name if they are on a different project or group.  For example, if we pass in `['bug', 'feature']`, this will return something like: `[ [1, 2], [3] ]`  rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7167"
  },
  {
    "raw_code": "def group_labels_for_root_namespace\n      Label.where(project_id: nil).where(group_id: root_namespace.self_and_descendant_ids)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7168"
  },
  {
    "raw_code": "def project_labels_for_root_namespace\n      Label.where(group_id: nil)\n           .where(project_id: Project.select(:id).where(namespace_id: root_namespace.self_and_descendant_ids))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7169"
  },
  {
    "raw_code": "def target_label_links_query(_target_model, base_target_model, label_ids)\n      LabelLink.by_target_for_exists_query(base_target_model.name, base_target_model.arel_table['id'], label_ids)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord overridden in EE",
    "label": "",
    "id": "7170"
  },
  {
    "raw_code": "def filter_by_availability\n      # Re-find by id so subsequent filters don't expose unavailable records\n      @collection = collection.id_in(collection\n        .select('DISTINCT ON (type_identifier) id')\n        .ordered_by_type_and_id\n        .limit(TYPE_IDENTIFIERS.length))\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "7171"
  },
  {
    "raw_code": "def self.counts_by_status(current_user, project, params = {})\n      new(current_user, project, params).execute.counts_by_status\n    end",
    "comment": "@return [Hash<Symbol,Integer>] Mapping of status id to count ex) { triggered: 6, ...etc }",
    "label": "",
    "id": "7172"
  },
  {
    "raw_code": "def limited_users\n      # When changing the order of these method calls, make sure that\n      # reorder_by_name() is called _before_ optionally_search(), otherwise\n      # reorder_by_name will break the ORDER BY applied in optionally_search().\n      find_users\n        .where(state: states)\n        .with_duo_code_review_bot\n        .reorder_by_name\n        .optionally_search(search, use_minimum_char_limit: use_minimum_char_limit)\n        .limit_to_todo_authors(\n          user: current_user,\n          with_todos: todo_filter,\n          todo_state: todo_state_filter\n        )\n        .limit(LIMIT)\n        .to_a\n    end",
    "comment": "Returns the users based on the input parameters, as an Array.  This method is separate so it is easier to extend in EE. rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7173"
  },
  {
    "raw_code": "def prepend_current_user?\n      filter_by_current_user.present? && current_user\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7174"
  },
  {
    "raw_code": "def initialize(current_user = nil, params = {})\n      @current_user = current_user\n      @project_id = params[:project_id]\n    end",
    "comment": "current_user - The currently logged in user, if any. params - A Hash containing parameters to use for finding the project.  The following parameters are supported:  * project_id: The ID of the project to find.",
    "label": "",
    "id": "7175"
  },
  {
    "raw_code": "def execute\n      return if project_id.blank?\n\n      project = Project.find(project_id)\n\n      # This removes the need for using `return render_404` and similar patterns\n      # in controllers that use this finder.\n      unless Ability.allowed?(current_user, :read_project, project)\n        raise ActiveRecord::RecordNotFound, \"Could not find a Project with ID #{project_id}\"\n      end",
    "comment": "Attempts to find a Project based on the current project ID.",
    "label": "",
    "id": "7176"
  },
  {
    "raw_code": "def initialize(current_user, params = {})\n      @current_user = current_user\n      @search = params[:search]\n      @project_id = params[:project_id]\n    end",
    "comment": "current_user - The User object of the user that wants to view the list of projects.  params - A Hash containing additional parameters to set.  The following parameters can be set (as Symbols):  * search: An optional search query to apply to the list of projects. * project_id: The ID of a project to exclude from the returned relation.",
    "label": "",
    "id": "7177"
  },
  {
    "raw_code": "def initialize(current_user = nil, project = nil, params = {})\n      @current_user = current_user\n      @project = project\n      @group_id = params[:group_id]\n    end",
    "comment": "current_user - The currently logged in user, if any. project - The Project (if any) to use for the autocomplete data sources. params - A Hash containing parameters to use for finding the project.  The following parameters are supported:  * group_id: The ID of the group to find.",
    "label": "",
    "id": "7178"
  },
  {
    "raw_code": "def execute\n      return unless project.blank? && group_id.present?\n\n      group = Group.find(group_id)\n\n      # This removes the need for using `return render_404` and similar patterns\n      # in controllers that use this finder.\n      unless Ability.allowed?(current_user, :read_group, group)\n        raise ActiveRecord::RecordNotFound, \"Could not find a Group with ID #{group_id}\"\n      end",
    "comment": "Attempts to find a Group based on the current group ID.",
    "label": "",
    "id": "7179"
  },
  {
    "raw_code": "def find_by!(...)\n    raise_not_found_unless_authorized execute.reorder(nil).find_by!(...)\n  end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7180"
  },
  {
    "raw_code": "def find_by(...)\n    if_authorized execute.reorder(nil).find_by(...)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7181"
  },
  {
    "raw_code": "def find(...)\n    raise_not_found_unless_authorized execute.reorder(nil).find(...)\n  end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7182"
  },
  {
    "raw_code": "def group_ids_for(group)\n    strong_memoize(:group_ids) do\n      groups = groups_to_include(group)\n\n      # Because we are sure that all groups are in the same hierarchy tree\n      # we can preset root group for all of them to optimize permission checks\n      Group.preset_root_ancestor_for(groups)\n\n      preload_associations(groups) if !skip_authorization && current_user\n\n      groups_user_can_read_items(groups).map(&:id)\n    end",
    "comment": "Gets redacted array of group ids which can include the ancestors and descendants of the requested group.",
    "label": "",
    "id": "7183"
  },
  {
    "raw_code": "def by_custom_attributes(items)\n    return items unless params[:custom_attributes].is_a?(Hash)\n    return items unless Ability.allowed?(current_user, :read_custom_attribute)\n\n    association = items.reflect_on_association(:custom_attributes)\n    attributes_table = association.klass.arel_table\n    attributable_table = items.model.arel_table\n\n    custom_attributes = association.klass.select('true').where(\n      attributes_table[association.foreign_key]\n        .eq(attributable_table[association.association_primary_key])\n    )\n\n    # perform a subquery for each attribute to be filtered\n    params[:custom_attributes].inject(items) do |scope, (key, value)|\n      scope.where('EXISTS (?)', custom_attributes.where(key: key, value: value))\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7184"
  },
  {
    "raw_code": "def filter_by_package_type(packages)\n      # Only filter by package_type when using the base `Packages::Package` class\n      # Format-specific classes like `Packages::TerraformModule::Package` don't need this filter\n      return packages unless packages_class == ::Packages::Package\n      return packages.without_package_type(:terraform_module) unless package_type\n      raise InvalidPackageTypeError unless ::Packages::Package.package_types.key?(package_type)\n\n      packages.with_package_type(package_type)\n    end",
    "comment": "TODO: Remove with the rollout of the FF packages_refactor_group_packages_finder https://gitlab.com/gitlab-org/gitlab/-/issues/568923",
    "label": "",
    "id": "7185"
  },
  {
    "raw_code": "def ref_and_associated_reserved_refs(container, ref, pipeline_sources = nil)\n      return [] unless ref\n\n      normalized_sources = Array.wrap(pipeline_sources || Pipeline.sources.keys).map(&:to_s)\n\n      if normalized_sources.include?('merge_request_event')\n        [ref, *merge_request_reserved_refs_for_container(container, ref)]\n      else\n        [ref]\n      end",
    "comment": "Returns an array containing the original ref and any associated reserved refs from merge requests that use the given ref as source branch.  @param container [Namespace, Project] The container used for requests @param ref [String, nil] The git reference to filter by @param pipeline_sources [Array<String>, String, nil] The pipeline sources to consider @return [Array<String>] Array of refs including the original and any reserved refs",
    "label": "",
    "id": "7186"
  },
  {
    "raw_code": "def initialize(\n      current_user, group, params = { exclude_subgroups: false,\n                                      exact_name: false,\n                                      order_by: 'created_at',\n                                      sort: 'asc',\n                                      packages_class: ::Packages::Package }\n    )\n      @current_user = current_user\n      @group = group\n      @params = params\n    end",
    "comment": "TODO: Remove `packages_class` with the rollout of the FF packages_refactor_group_packages_finder https://gitlab.com/gitlab-org/gitlab/-/issues/568923",
    "label": "",
    "id": "7187"
  },
  {
    "raw_code": "def projects_visible_to_reporters(user, within_group:, _within_public_package_registry: false)\n        return user.accessible_projects if user.is_a?(DeployToken)\n\n        access = if Feature.enabled?(:allow_guest_plus_roles_to_pull_packages, within_group.root_ancestor)\n                   ::Gitlab::Access::GUEST\n                 else\n                   ::Gitlab::Access::REPORTER\n                 end",
    "comment": "This overrides projects_visible_to_reporters in app/finders/concerns/packages/finder_helper.rb to implement npm-specific optimizations",
    "label": "",
    "id": "7188"
  },
  {
    "raw_code": "def pipeline_for_ref_subquery(items)\n      where_query = Arel.sql(\"#{Ci::Pipeline.table_name}.ref = refs.ref\")\n\n      items\n        .where(where_query)\n        .order(id: :desc)\n        .limit(1) # Limit to 1 because we only want the latest pipeline per ref\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7189"
  },
  {
    "raw_code": "def refs_values(refs)\n      # Create list of form `[['main'], ['branch1']]`\n      list = refs.map { |ref| Array(ref) }\n\n      # Create values list of form `(VALUES ('main'), ('branch1'))`\n      values_list = Arel::Nodes::Grouping.new(Arel::Nodes::ValuesList.new(list))\n\n      # (VALUES ('main'), ('branch1')) AS refs(ref)\n      Arel::Nodes::As.new(values_list, Arel.sql('refs(ref)'))\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7190"
  },
  {
    "raw_code": "def from_derived_table(derived_pipelines_table, refs_values_table)\n      pipelines\n        .unscoped\n        .from([\n          refs_values_table,\n          derived_pipelines_table.arel.lateral.as(Ci::Pipeline.table_name)\n        ])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7191"
  },
  {
    "raw_code": "def branches\n      project.repository.branch_names\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7192"
  },
  {
    "raw_code": "def by_yaml_errors(items)\n      case Gitlab::Utils.to_boolean(params[:yaml_errors])\n      when true\n        items.where.not(yaml_errors: nil)\n      when false\n        items.where(yaml_errors: nil)\n      else\n        items\n      end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7193"
  },
  {
    "raw_code": "def by_name(items)\n      return items unless params[:name].present?\n\n      items.for_name(params[:name])\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7194"
  },
  {
    "raw_code": "def sort_items(items)\n      order_by = if ALLOWED_INDEXED_COLUMNS.include?(params[:order_by])\n                   params[:order_by]\n                 else\n                   :id\n                 end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7195"
  },
  {
    "raw_code": "def filter_by_source\n      relation\n        .from(\"(#{build_source_scope.to_sql}) p_ci_build_sources, LATERAL (#{ci_builds_query.to_sql}) p_ci_builds\")\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- Need specialized queries for database optimizations",
    "label": "",
    "id": "7196"
  },
  {
    "raw_code": "def filter_builds(builds)\n      builds = filter_by_with_artifacts(builds)\n      builds = filter_by_runner_types(builds)\n      filter_by_scope(builds)\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "7197"
  },
  {
    "raw_code": "def use_runner_type_filter?\n      params[:runner_type].present? && Feature.enabled?(:admin_jobs_filter_runner_type, project, type: :ops)\n    end",
    "comment": "Overridden in EE",
    "label": "",
    "id": "7198"
  },
  {
    "raw_code": "def filter_by_name(build_relation)\n      build_name_relation = Ci::BuildName\n        .where(project_id: project.id)\n        .pg_full_text_search_in_model(limited_name_search_terms)\n\n      build_relation.where(\"(id, partition_id) IN (?)\", build_name_relation.select(:build_id, :partition_id))\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord -- Need specialized queries for database optimizations",
    "label": "",
    "id": "7199"
  },
  {
    "raw_code": "def execute\n      if can_read_pipeline_in_target_project? && can_read_pipeline_in_source_project?\n        all\n      elsif can_read_pipeline_in_source_project?\n        all.for_project(merge_request.source_project)\n      elsif can_read_pipeline_in_target_project?\n        all.for_project(merge_request.target_project)\n      else\n        Ci::Pipeline.none\n      end",
    "comment": "Fetch all pipelines that the user can read.",
    "label": "",
    "id": "7200"
  },
  {
    "raw_code": "def all\n      strong_memoize(:all_pipelines) do\n        next Ci::Pipeline.none unless source_project\n\n        pipelines =\n          if merge_request.persisted?\n            all_pipelines_for_merge_request\n          else\n            triggered_for_branch.for_sha(commit_shas)\n          end",
    "comment": "Fetch all pipelines without permission check.",
    "label": "",
    "id": "7201"
  },
  {
    "raw_code": "def triggered_by_merge_request\n      Ci::Pipeline.triggered_by_merge_request(merge_request)\n    end",
    "comment": "NOTE: this method returns only parent merge request pipelines. Child merge request pipelines have a different source.",
    "label": "",
    "id": "7202"
  },
  {
    "raw_code": "def by_status(items)\n      return items unless Ci::HasStatus::AVAILABLE_STATUSES.include?(params[:status])\n\n      items.where(status: params[:status])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7203"
  },
  {
    "raw_code": "def sort_items(items)\n      return items unless ALLOWED_INDEXED_COLUMNS.include?(params[:order_by])\n\n      order_by = params[:order_by]\n      sort = if /\\A(ASC|DESC)\\z/i.match?(params[:sort])\n               params[:sort]\n             else\n               :desc\n             end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7204"
  },
  {
    "raw_code": "def execute\n      deployments =\n        if ref\n          Deployment.where(ref: ref.to_s)\n        elsif sha\n          Deployment.where(sha: sha)\n        else\n          Deployment.none\n        end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7205"
  },
  {
    "raw_code": "def apply_min_access_level(groups)\n      return groups unless params[:shared_min_access_level]\n\n      groups.where('project_group_links.group_access >= ?', params[:shared_min_access_level])\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7206"
  },
  {
    "raw_code": "def exclude_group_ids(groups)\n      return groups unless params[:skip_groups]\n\n      groups.id_not_in(params[:skip_groups])\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7207"
  },
  {
    "raw_code": "def user_ids_and_access_levels_from_all_memberships\n        strong_memoize(:user_ids_and_access_levels_from_all_memberships) do\n          all_possible_avenues_of_membership.flat_map do |members|\n            apply_scopes(members).pluck(*USER_ID_AND_ACCESS_LEVEL) # rubocop: disable CodeReuse/ActiveRecord\n          end",
    "comment": "@return [Array<[user_id, access_level]>]",
    "label": "",
    "id": "7208"
  },
  {
    "raw_code": "def project_owner\n        user_id = project.namespace.owner.id\n        access_level = Gitlab::Access::OWNER\n\n        Member\n          .from(generate_from_statement([[user_id, access_level]])) # rubocop: disable CodeReuse/ActiveRecord\n          .select(:user_id, :access_level)\n          .limit(1)\n      end",
    "comment": "workaround until we migrate Project#owners to have membership with OWNER access level",
    "label": "",
    "id": "7209"
  },
  {
    "raw_code": "def user_id_and_access_level_for_project_group_shares(link)\n        least_access_level_among_group_membership_and_project_share =\n          smallest_value_arel([link.group_access, GroupMember.arel_table[:access_level]], 'access_level')\n\n        [\n          :user_id,\n          least_access_level_among_group_membership_and_project_share\n        ]\n      end",
    "comment": "methods for `select` options",
    "label": "",
    "id": "7210"
  },
  {
    "raw_code": "def assemble_version(matches)\n      return if matches.blank?\n\n      major = matches[:major]\n      minor = matches[:minor]\n      patch = matches[:patch]\n      build = matches[:meta]\n\n      return unless major && minor && patch\n\n      version = \"#{major}.#{minor}.#{patch}\"\n      version += \"+#{build}\" if build\n\n      version\n    end",
    "comment": "Builds a version string based on regex matcher's output",
    "label": "",
    "id": "7211"
  },
  {
    "raw_code": "def initialize(project:, from:, to:, per_page: COMMITS_PER_PAGE)\n      @project = project\n      @from = from\n      @to = to\n      @per_page = per_page\n    end",
    "comment": "The `project` argument specifies the project for which to obtain the commits.  The `from` and `to` arguments specify the range of commits to include. The commit specified in `from` won't be included itself. The commit specified in `to` _is_ included.  The `per_page` argument specifies how many commits are retrieved in a single Gitaly API call.",
    "label": "",
    "id": "7212"
  },
  {
    "raw_code": "def each_page(trailer)\n      return to_enum(__method__, trailer) unless block_given?\n\n      offset = 0\n      reverted = Set.new\n      response = fetch_commits\n\n      while response.any?\n        commits = []\n\n        response.each do |commit|\n          # If the commit is reverted in the same range (by a newer commit), we\n          # won't include it. This works here because commits are processed in\n          # reverse order (= newer first).\n          next if reverted.include?(commit.id)\n\n          if (sha = revert_commit_sha(commit))\n            reverted << sha\n          end",
    "comment": "Fetches all commits that have the given trailer set.  The commits are yielded to the supplied block in batches. This allows other code to process these commits in batches too, instead of first having to load all commits into memory.  Example:  ChangelogCommitsFinder.new(...).each_page('Changelog') do |commits| commits.each do |commit| ... end end",
    "label": "",
    "id": "7213"
  },
  {
    "raw_code": "def project_authorizations\n            namespace_ids = project.group ? all_namespace_ids : project.namespace_id\n\n            query = Clusters::Agents::Authorizations::CiAccess::ProjectAuthorization\n              .where(project_id: project.id)\n              .joins(agent: :project)\n              .preload(agent: :project)\n              .with_available_ci_access_fields(project)\n\n            unless organization_agents_enabled?\n              query = query.where(cluster_agents: { projects: { namespace_id: namespace_ids } })\n            end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7214"
  },
  {
    "raw_code": "def all_namespace_ids\n            project.root_ancestor.self_and_descendants.select(:id)\n          end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7215"
  },
  {
    "raw_code": "def initialize(design_or_collection, current_user, params = {})\n      @design_or_collection = design_or_collection\n      @current_user = current_user\n      @params = params\n    end",
    "comment": "The `design_or_collection` argument should be either a:  - DesignManagement::Design, or - DesignManagement::DesignCollection  The object will have `#versions` called on it to set up the initial scope of the versions.  valid params: - earlier_or_equal_to: Version - sha: String - version_id: Integer ",
    "label": "",
    "id": "7216"
  },
  {
    "raw_code": "def initialize(issue, current_user, params = {})\n      @issue = issue\n      @current_user = current_user\n      @params = params\n    end",
    "comment": "Params: ids: integer[] filenames: string[] visible_at_version: ?version filenames: String[]",
    "label": "",
    "id": "7217"
  },
  {
    "raw_code": "def by_visible_at_version(items)\n      items.visible_at_version(params[:visible_at_version])\n    end",
    "comment": "Returns all designs that existed at a particular design version, where `nil` means `at-current-version`.",
    "label": "",
    "id": "7218"
  },
  {
    "raw_code": "def record_upload(_tempfile = nil)\n      return unless model\n      return unless file && file.exists?\n\n      Upload.transaction { readd_upload }\n    end",
    "comment": "After storing an attachment, create a corresponding Upload record  NOTE: We're ignoring the argument passed to this callback because we want the `SanitizedFile` object from `CarrierWave::Uploader::Base#file`, not the `Tempfile` object the callback gets.  Called `after :store` rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7219"
  },
  {
    "raw_code": "def upload_path\n      File.join(store_dir, filename.to_s)\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7220"
  },
  {
    "raw_code": "def uploads\n      Upload.order(id: :desc).where(uploader: self.class.to_s)\n    end",
    "comment": "rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7221"
  },
  {
    "raw_code": "def build_upload\n      Upload.new(\n        uploader: self.class.to_s,\n        size: file.size,\n        path: upload_path,\n        model: model,\n        mount_point: mounted_as,\n        version: VERSION\n      )\n    end",
    "comment": "rubocop: enable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7222"
  },
  {
    "raw_code": "def destroy_upload(*args)\n      return unless file && file.exists?\n\n      self.upload = nil\n      uploads.where(path: upload_path).delete_all\n    end",
    "comment": "Before removing an attachment, destroy any Upload records at the same path  Called `before :remove` rubocop: disable CodeReuse/ActiveRecord",
    "label": "",
    "id": "7223"
  },
  {
    "raw_code": "def check_content_type_whitelist!(new_file)\n      if content_type_whitelist\n        content_type = mime_magic_content_type(new_file.path)\n\n        unless whitelisted_content_type?(content_type)\n          message = I18n.t(:\"errors.messages.content_type_whitelist_error\", allowed_types: Array(content_type_whitelist).join(\", \"))\n          raise CarrierWave::IntegrityError, message\n        end",
    "comment": "CarrierWave calls this method as part of it's before :cache callbacks. Here we override and extend CarrierWave's method that does not parse the magic headers.",
    "label": "",
    "id": "7224"
  },
  {
    "raw_code": "def workhorse_authorize(\n        has_length:,\n        maximum_size: nil,\n        use_final_store_path: false,\n        final_store_path_config: {})\n        {}.tap do |hash|\n          if self.direct_upload_to_object_store?\n            hash[:RemoteObject] = workhorse_remote_upload_options(\n              has_length: has_length,\n              maximum_size: maximum_size,\n              use_final_store_path: use_final_store_path,\n              final_store_path_config: final_store_path_config\n            )\n          else\n            hash[:TempPath] = workhorse_local_upload_path\n          end",
    "comment": "final_store_path_config is only used if use_final_store_path is set to true Two keys are available: - :root_hash. The root hash used in Gitlab::HashedPath for the path generation. - :override_path. If set, the path generation is skipped and this value is used instead. Make sure that this value is unique for each upload.",
    "label": "",
    "id": "7225"
  },
  {
    "raw_code": "def original_filename\n        return File.basename(file_path) if file_path.present?\n\n        nil\n      end",
    "comment": "CarrierWave#cache! calls filename, which calls original_filename",
    "label": "",
    "id": "7226"
  },
  {
    "raw_code": "def filename\n      @filename || super || file&.filename # rubocop:disable Gitlab/ModuleWithInstanceVariables\n    end",
    "comment": "allow to configure and overwrite the filename",
    "label": "",
    "id": "7227"
  },
  {
    "raw_code": "def object_store=(value)\n      @object_store = value || Store::LOCAL\n      model[store_serialization_column] = @object_store if sync_model_object_store? && persist_object_store?\n      @storage = storage_for(object_store)\n    end",
    "comment": "rubocop:disable Gitlab/ModuleWithInstanceVariables",
    "label": "",
    "id": "7228"
  },
  {
    "raw_code": "def persist_object_store?\n      model.respond_to?(:\"#{store_serialization_column}=\")\n    end",
    "comment": "rubocop:enable Gitlab/ModuleWithInstanceVariables Return true if the current file is part or the model (i.e. is mounted in the model) ",
    "label": "",
    "id": "7229"
  },
  {
    "raw_code": "def persist_object_store!\n      return unless persist_object_store?\n\n      updated = model.update_column(store_serialization_column, object_store)\n      raise 'Failed to update object store' unless updated\n    end",
    "comment": "Save the current @object_store to the model <mounted_as>_store column",
    "label": "",
    "id": "7230"
  },
  {
    "raw_code": "def migrate!(new_store)\n      with_exclusive_lease do\n        unsafe_migrate!(new_store)\n      end",
    "comment": " Move the file to another store  new_store: Enum (Store::LOCAL, Store::REMOTE) ",
    "label": "",
    "id": "7231"
  },
  {
    "raw_code": "def fog_public\n      nil\n    end",
    "comment": "Set ACL of uploaded objects to not-public (fog-aws)[1] or no ACL at all (fog-google).  Value is ignored by fog-aliyun [1]: https://github.com/fog/fog-aws/blob/daa50bb3717a462baf4d04d0e0cbfc18baacb541/lib/fog/aws/models/storage/file.rb#L152-L159",
    "label": "",
    "id": "7232"
  },
  {
    "raw_code": "def upload_paths(identifier)\n      store_dirs.map { |store, path| File.join(path, identifier) }\n    end",
    "comment": "Returns all the possible paths for an upload. the `upload.path` is a lookup parameter, and it may change depending on the `store` param.",
    "label": "",
    "id": "7233"
  },
  {
    "raw_code": "def file=(file)\n      @file = file # rubocop:disable Gitlab/ModuleWithInstanceVariables\n    end",
    "comment": "this is a hack around CarrierWave. The #migrate method needs to be able to force the current file to the migrated file upon success.",
    "label": "",
    "id": "7234"
  },
  {
    "raw_code": "def store_serialization_column\n      [serialization_column, 'store'].compact.join('_').to_sym\n    end",
    "comment": "Returns the column where the 'store' is saved defaults to 'store'",
    "label": "",
    "id": "7235"
  },
  {
    "raw_code": "def unsafe_migrate!(new_store)\n      return unless object_store != new_store\n      return unless file\n\n      new_file = nil\n      file_to_delete = file\n      from_object_store = object_store\n      self.object_store = new_store # changes the storage and file\n\n      cache_stored_file! if file_storage?\n\n      with_callbacks(:migrate, file_to_delete) do\n        with_callbacks(:store, file_to_delete) do # for #store_versions!\n          new_file = storage.store!(file)\n          persist_object_store!\n          self.file = new_file\n        end",
    "comment": " Move the file to another store  new_store: Enum (Store::LOCAL, Store::REMOTE) ",
    "label": "",
    "id": "7236"
  },
  {
    "raw_code": "def self.root\n    options.storage_path\n  end",
    "comment": "Re-Override",
    "label": "",
    "id": "7237"
  },
  {
    "raw_code": "def model_valid?\n    true\n  end",
    "comment": "model_path_segment does not require a model to be passed, so we can always generate a path, even when there's no model.",
    "label": "",
    "id": "7238"
  },
  {
    "raw_code": "def store_dir\n    store_dirs[object_store]\n  end",
    "comment": "Revert-Override",
    "label": "",
    "id": "7239"
  },
  {
    "raw_code": "def upload_paths(identifier)\n    [\n      local_storage_path(identifier),\n      File.join(remote_storage_base_path, identifier)\n    ]\n  end",
    "comment": "A personal snippet path is stored using FileUploader#upload_path.  The format for the path:  Local storage: :random_hex/:filename. Object storage: personal_snippet/:id/:random_hex/:filename.  upload_paths represent the possible paths for a given identifier, which will vary depending on whether the file is stored in local or object storage. upload_path should match an element in upload_paths.  base_dir represents the path seen by the user in Markdown, and it should always be prefixed with uploads/-/system.  store_dirs represent the paths that are actually used on disk. For object storage, this should omit the prefix /uploads/-/system.  For example, consider the requested path /uploads/-/system/personal_snippet/172/ff4ad5c2e40b39ae57cda51577317d20/file.png.  For local storage:  File on disk: /opt/gitlab/embedded/service/gitlab-rails/public/uploads/-/system/personal_snippet/172/ff4ad5c2e40b39ae57cda51577317d20/file.png.  base_dir: uploads/-/system/personal_snippet/172 upload_path: ff4ad5c2e40b39ae57cda51577317d20/file.png upload_paths: [\"ff4ad5c2e40b39ae57cda51577317d20/file.png\", \"personal_snippet/172/ff4ad5c2e40b39ae57cda51577317d20/file.png\"]. store_dirs: => {1=>\"uploads/-/system/personal_snippet/172/ff4ad5c2e40b39ae57cda51577317d20\", 2=>\"personal_snippet/172/ff4ad5c2e40b39ae57cda51577317d20\"}  For object storage:  upload_path: personal_snippet/172/ff4ad5c2e40b39ae57cda51577317d20/file.png",
    "label": "",
    "id": "7240"
  },
  {
    "raw_code": "def remote_storage_base_path\n    File.join(self.class.model_path_segment(model), dynamic_segment)\n  end",
    "comment": "To avoid prefacing the remote storage path with `/uploads/-/system`, we just drop that part so that the destination path will be personal_snippet/:id/:random_hex/:filename.",
    "label": "",
    "id": "7241"
  },
  {
    "raw_code": "def storage_location(location)\n      self.storage_location_identifier = location\n      _ = options # Ensures that we have a valid storage_location_identifier\n    end",
    "comment": "DSL setter",
    "label": "",
    "id": "7242"
  },
  {
    "raw_code": "def base_dir\n      options.fetch('base_dir', '')\n    end",
    "comment": "represent the directory namespacing at the class level",
    "label": "",
    "id": "7243"
  },
  {
    "raw_code": "def replace_file_without_saving!(file)\n    raise ArgumentError, 'should be a CarrierWave::SanitizedFile' unless file.is_a? CarrierWave::SanitizedFile\n\n    storage.store!(file)\n  end",
    "comment": "Used to replace an existing upload with another +file+ without modifying stored metadata Use this method only to repair/replace an existing upload, or to upload to a Geo secondary node  @param [CarrierWave::SanitizedFile] file that will replace existing upload @return CarrierWave::SanitizedFile",
    "label": "",
    "id": "7244"
  },
  {
    "raw_code": "def dynamic_segment\n    raise(NotImplementedError)\n  end",
    "comment": "Designed to be overridden by child uploaders that have a dynamic path segment -- that is, a path that changes based on mutable attributes of its associated model  For example, `FileUploader` builds the storage path based on the associated project model's `path_with_namespace` value, which can change when the project or its containing namespace is moved or renamed.  When implementing this method, raise `ObjectNotReadyError` if the model does not yet exist, as it will be tested in `#protect_from_path_traversal!`",
    "label": "",
    "id": "7245"
  },
  {
    "raw_code": "def workfile_path(for_file = original_filename)\n    # To be safe, keep this directory outside of the the cache directory\n    # because calling CarrierWave.clean_cache_files! will remove any files in\n    # the cache directory.\n    File.join(work_dir, cache_id, version_name.to_s, for_file)\n  end",
    "comment": "To prevent files from moving across filesystems, override the default implementation: http://github.com/carrierwaveuploader/carrierwave/blob/v1.0.0/lib/carrierwave/uploader/cache.rb#L181-L183",
    "label": "",
    "id": "7246"
  },
  {
    "raw_code": "def protect_from_path_traversal!(file)\n    PROTECTED_METHODS.each do |method|\n      Gitlab::PathTraversal.check_path_traversal!(self.send(method)) # rubocop: disable GitlabSecurity/PublicSend\n\n    rescue ObjectNotReadyError\n      # Do nothing. This test was attempted before the file was ready for that method\n    end",
    "comment": "Protect against path traversal attacks This takes a list of methods to test for path traversal, e.g. ../../ and checks each of them. This uses `.send` so that any potential errors don't block the entire set from being tested.  @param [CarrierWave::SanitizedFile] @return [Nil] @raise [Gitlab::PathTraversal::PathTraversalAttackError]",
    "label": "",
    "id": "7247"
  },
  {
    "raw_code": "def self.root\n    options.storage_path\n  end",
    "comment": "Re-Override",
    "label": "",
    "id": "7248"
  },
  {
    "raw_code": "def store_dir\n    store_dirs[object_store]\n  end",
    "comment": "Re-Override",
    "label": "",
    "id": "7249"
  },
  {
    "raw_code": "def self.absolute_base_dir(model)\n    File.join(root, base_dir(model))\n  end",
    "comment": "used in migrations and import/exports",
    "label": "",
    "id": "7250"
  },
  {
    "raw_code": "def self.model_path_segment(model)\n    case model\n    when Storage::Hashed then model.disk_path\n    else\n      model.hashed_storage?(:attachments) ? model.disk_path : model.full_path\n    end",
    "comment": "Returns the part of `store_dir` that can change based on the model's current path  This is used to build Upload paths dynamically based on the model's current namespace and path, allowing us to ignore renames or transfers.  model - Object that responds to `full_path` and `disk_path`  Returns a String without a trailing slash",
    "label": "",
    "id": "7251"
  },
  {
    "raw_code": "def base_dir(store = nil)\n    self.class.base_dir(@model, store || object_store)\n  end",
    "comment": "enforce the usage of Hashed storage when storing to remote store as the FileMover doesn't support OS",
    "label": "",
    "id": "7252"
  },
  {
    "raw_code": "def absolute_path\n    self.class.absolute_path(@upload)\n  end",
    "comment": "we don't need to know the actual path, an uploader instance should be able to yield the file content on demand, so we should build the digest",
    "label": "",
    "id": "7253"
  },
  {
    "raw_code": "def self.copy_to(uploader, to_container)\n    moved = self.new(to_container)\n    moved.object_store = uploader.object_store\n    moved.filename = uploader.filename\n\n    moved.copy_file(uploader.file)\n    moved\n  end",
    "comment": "return a new uploader with a file copy on another container",
    "label": "",
    "id": "7254"
  },
  {
    "raw_code": "def self.workhorse_local_upload_path\n      Rails.root.join('public/uploads/tmp/terraform_state').to_s\n    end",
    "comment": "On Cloud Native GitLab, /srv/gitlab/public/uploads/tmp is a shared mount. Use a subpath from that directory to ensure the gitlab-workhorse and webservice containers can both access this directory.",
    "label": "",
    "id": "7255"
  },
  {
    "raw_code": "def direct_upload_enabled?\n        false\n      end",
    "comment": "direct upload is disabled since the file must always be encrypted",
    "label": "",
    "id": "7256"
  },
  {
    "raw_code": "def set_content_type(file)\n    return unless model.instance_of?(DependencyProxy::Manifest)\n\n    file.content_type = model.content_type\n  end",
    "comment": "Docker manifests return a custom content type GCP will only use the content-type that is stored with the file and will not allow it to be overwritten when downloaded so we must store the custom content type in object storage. This does not apply to DependencyProxy::Blob uploads.",
    "label": "",
    "id": "7257"
  },
  {
    "raw_code": "def content_type_whitelist\n      MIME_TYPE_ALLOWLIST\n    end",
    "comment": "Allow CarrierWave to reject files without correct mimetypes.",
    "label": "",
    "id": "7258"
  },
  {
    "raw_code": "def move_to_cache\n      false\n    end",
    "comment": "Override `GitlabUploader` and always return false, otherwise local `LfsObject` files would be deleted. https://github.com/carrierwaveuploader/carrierwave/blob/f84672a/lib/carrierwave/uploader/cache.rb#L131-L135",
    "label": "",
    "id": "7259"
  },
  {
    "raw_code": "def trim_filename_if_needed(filename)\n      return if filename.nil?\n      return filename if filename.length <= 60\n\n      # Take the last 60 characters\n      filename[-60..]\n    end",
    "comment": "Trims filename to 60 characters from the front if it exceeds that length",
    "label": "",
    "id": "7260"
  },
  {
    "raw_code": "def base_dir\n      \"@hashed\"\n    end",
    "comment": "@hashed is chosen to avoid conflict with namespace name because we use the same directory for storage @ is not valid character for namespace",
    "label": "",
    "id": "7261"
  },
  {
    "raw_code": "def move_to_cache\n      false\n    end",
    "comment": "override GitlabUploader if set to true it erases the original file when uploading and we copy from the artifacts archive, so artifacts end up without the file",
    "label": "",
    "id": "7262"
  },
  {
    "raw_code": "def direct_upload_enabled?\n        false\n      end",
    "comment": "we only upload this files from the rails background job so we don't need direct upload for pages deployments this method is here to ignore any user setting",
    "label": "",
    "id": "7263"
  },
  {
    "raw_code": "def build_description(feature_flag, latest_feature_flag_status)\n      ready_for_removal = has_ready_for_removal_label?(feature_flag)\n\n      introduction_text = if ready_for_removal\n                            \"This feature flag was introduced in #{feature_flag.milestone} and has been \" \\\n                              \"marked as ready for removal with the `~\\\"feature flag::ready for removal\\\"` \" \\\n                              \"label, bypassing the standard milestone cutoff requirements.\"\n                          else\n                            cutoff_count = if latest_feature_flag_status == :enabled\n                                             CUTOFF_MILESTONE_FOR_ENABLED_FLAG\n                                           else\n                                             CUTOFF_MILESTONE_FOR_DISABLED_FLAG\n                                           end",
    "comment": "rubocop:disable Gitlab/DocumentationLinks/HardcodedUrl -- Not running inside rails application",
    "label": "",
    "id": "7264"
  },
  {
    "raw_code": "def build_ff_identifiers(feature_flag)\n      [self.class.name.demodulize, feature_flag.name]\n    end",
    "comment": "rubocop:enable Gitlab/DocumentationLinks/HardcodedUrl",
    "label": "",
    "id": "7265"
  },
  {
    "raw_code": "def def_node_matcher(method_name, pattern)\n          RuboCop::AST::NodePattern.new(pattern).def_node_matcher(RuboCop::AST::Node, method_name)\n\n          define_method method_name do\n            source.ast.public_send(method_name) # rubocop:disable GitlabSecurity/PublicSend -- it's used to evaluate the node matcher at instance level\n          end",
    "comment": "Define a node matcher method in the +RuboCop::AST::Node+, which all other node types inherits from.",
    "label": "",
    "id": "7266"
  },
  {
    "raw_code": "def strip_comments\n        source.comments.each do |comment|\n          next if comment.text.include?('frozen_string_literal: true')\n\n          rewriter.remove(comment_range(comment))\n        end",
    "comment": "Strip comments from the source file, except the for frozen_string_literal: true",
    "label": "",
    "id": "7267"
  },
  {
    "raw_code": "def comment_range(comment)\n        range = comment.loc.expression\n        adjusted_range = range.adjust(begin_pos: -1)\n\n        return range if comment.document?\n\n        adjusted_range.source.start_with?(' ') ? adjusted_range : range\n      end",
    "comment": "Finds the proper range for the comment.  @Note inline comments can cause trailing whitespaces. For such cases, the extra whitespace needs to be removed",
    "label": "",
    "id": "7268"
  },
  {
    "raw_code": "def text_in_node(node)\n        texts = node.children.map do |child|\n          child.text.strip if text_node?(child)\n        end",
    "comment": "Build an array of all strings in child text nodes. non text nodes are nil, where we'll split the sentences.",
    "label": "",
    "id": "7269"
  },
  {
    "raw_code": "def inline_text(node)\n        text = node.source_code.gsub(\"%#{node.tag_name}\", '')\n\n        attributes = node.attributes_source.map(&:last)\n        attributes.each { |attribute| text = text.gsub(attribute, '') }\n\n        text = strip_html_entities(text)\n        text.strip\n      end",
    "comment": "Removes a node's attributes and tag from the source code, returning the inline text of a node.",
    "label": "",
    "id": "7270"
  },
  {
    "raw_code": "def settings_value(target)\n      case target\n      when 'puma' then ::Settings.monitoring.web_exporter\n      when 'sidekiq' then ::Settings.monitoring.sidekiq_exporter\n      else ensure_valid_target!(target)\n      end",
    "comment": "We need to use `.` (dot) notation to access the updates we did in `config/initializers/1_settings.rb` For that reason, avoid using `[]` (\"optional/dynamic settings notation\") to resolve it dynamically. Refer to https://gitlab.com/gitlab-org/gitlab/-/issues/386865",
    "label": "",
    "id": "7271"
  },
  {
    "raw_code": "def path_matches?(pattern, path)\n      # `FNM_DOTMATCH` makes sure we also match files starting with a `.`\n      # `FNM_PATHNAME` makes sure ** matches path separators\n      flags = ::File::FNM_DOTMATCH | ::File::FNM_PATHNAME\n\n      # BEGIN extension\n      flags |= ::File::FNM_EXTGLOB\n      # END extension\n\n      ::File.fnmatch?(normalize_pattern(pattern), path, flags)\n    end",
    "comment": "Copied and modified from ee/lib/gitlab/code_owners/file.rb",
    "label": "",
    "id": "7272"
  },
  {
    "raw_code": "def normalize_pattern(pattern)\n      # Remove `\\` when escaping `\\#`\n      pattern = pattern.sub(/\\A\\\\#/, '#')\n      # Replace all whitespace preceded by a \\ with a regular whitespace\n      pattern = pattern.gsub(/\\\\\\s+/, ' ')\n\n      return '/**/*' if pattern == '*'\n\n      unless pattern.start_with?('/')\n        pattern = \"/**/#{pattern}\"\n      end",
    "comment": "Copied from ee/lib/gitlab/code_owners/file.rb",
    "label": "",
    "id": "7273"
  },
  {
    "raw_code": "def each_releases_page(args, &block)\n      return to_enum(__method__, args) unless block\n\n      page = 0\n      final_args = args.dup\n\n      begin\n        collection = raw_releases(page, final_args)\n\n        yield Page.new(collection, page += 1)\n      end while collection.any?\n    end",
    "comment": "Fetches data from Helm and yields a Page object for every page of data, without loading all of them into memory.  method - The Octokit method to use for getting the data. args - Arguments to pass to the `helm list` command.",
    "label": "",
    "id": "7274"
  },
  {
    "raw_code": "def each_release(args, &block)\n      return to_enum(__method__, args) unless block\n\n      each_releases_page(args) do |page|\n        page.releases.each do |release|\n          yield release\n        end",
    "comment": "Iterates over all of the releases.  args - Any arguments to pass to the `helm list` command.",
    "label": "",
    "id": "7275"
  },
  {
    "raw_code": "def generate_pot\n      super([])\n    end",
    "comment": "Overrides method from GetText::Tools::XGetText This makes a method public and passes in an empty array of paths, as our overidden \"parse\" method needs no paths",
    "label": "",
    "id": "7276"
  },
  {
    "raw_code": "def header_content\n      super.gsub(/^POT?-(?:Creation|Revision)-Date:.*\\n/, '')\n    end",
    "comment": "Overrides method from GetText::Tools::XGetText in order to remove revision dates, as we check in our locale/gitlab.pot",
    "label": "",
    "id": "7277"
  },
  {
    "raw_code": "def self.run(rspec_args: nil, filter_tests_file: nil)\n      new(rspec_args: rspec_args, filter_tests_file: filter_tests_file).run\n    end",
    "comment": "rubocop:disable Gitlab/Json -- standard JSON is sufficient",
    "label": "",
    "id": "7278"
  },
  {
    "raw_code": "def filter_files\n        @_filter_files ||= changed_files.select do |filename|\n          filename.start_with?(*view_base_folders) &&\n            File.basename(filename).end_with?('.html.haml') &&\n            File.exist?(filename)\n        end",
    "comment": "Keep the views files that are in the @view_base_folders folder",
    "label": "",
    "id": "7279"
  },
  {
    "raw_code": "def to_feature_spec_folder(view_file)\n        view_file.sub(%r{(ee/|jh/)?app/views}, '\\1spec/features')\n      end",
    "comment": "e.g. go from app/views/groups/merge_requests.html.haml to spec/features/groups/merge_requests.html.haml",
    "label": "",
    "id": "7280"
  },
  {
    "raw_code": "def filter_files\n        selected_files = changed_files.select do |filename|\n          filename.start_with?(*@js_base_folders) && File.exist?(filename)\n        end",
    "comment": "Keep the files that are in the @js_base_folders folders  Returns a hash, where the key is the GitLab edition, and the values the JS specs",
    "label": "",
    "id": "7281"
  },
  {
    "raw_code": "def construct_js_keywords(js_files)\n        js_files.map do |js_file|\n          filename = js_file.scan(@first_js_folder_extract_regexp).flatten.first\n          singularize(filename)\n        end.uniq\n      end",
    "comment": "Extract keywords in the JS filenames to be used for searching matching system specs",
    "label": "",
    "id": "7282"
  },
  {
    "raw_code": "def singularize(string)\n        if string.end_with?('ies')\n          string.sub(/ies$/, 'y')\n        # e.g. branches -> branch, protected branches -> protected branch\n        elsif string.end_with?('hes')\n          string.sub(/hes$/, 'h')\n        elsif string.end_with?('s')\n          string.sub(/s$/, '')\n        else\n          string\n        end",
    "comment": "We don't want to use active_support for this method, and our singularization cases are much simpler than what the active_support method would need.",
    "label": "",
    "id": "7283"
  },
  {
    "raw_code": "def extract_partial_keyword(partial_filename)\n        File.basename(partial_filename).delete_prefix('_').delete_suffix('.html.haml')\n      end",
    "comment": "e.g. if app/views/clusters/clusters/_sidebar.html.haml was modified, the partial keyword is `sidebar`.",
    "label": "",
    "id": "7284"
  },
  {
    "raw_code": "def view_includes_modified_partial?(view_file, included_partial_name)\n        view_file_parent_folder        = File.dirname(view_file)\n        included_partial_filename      = reconstruct_partial_filename(included_partial_name)\n        included_partial_relative_path = File.join(view_file_parent_folder, included_partial_filename)\n\n        # We do this because in render (or render_if_exists)\n        # apparently looks for partials in other GitLab editions\n        #\n        # Example:\n        #\n        # ee/app/views/events/_epics_filter.html.haml is used in app/views/shared/_event_filter.html.haml\n        # with render_if_exists 'events/epics_filter'\n        included_partial_absolute_paths = view_base_folders.map do |view_base_folder|\n          File.join(view_base_folder, included_partial_filename)\n        end",
    "comment": "Why do we need this method?  Assume app/views/clusters/clusters/_sidebar.html.haml was modified in the MR.  Suppose now you find = render 'sidebar' in a view. Is this view including the sidebar partial that was modified, or another partial called \"_sidebar.html.haml\" somewhere else?",
    "label": "",
    "id": "7285"
  },
  {
    "raw_code": "def types_hierarchies\n        return @types_hierarchies if @types_hierarchies\n\n        @types_hierarchies = {}\n        GRAPHQL_TYPES_FOLDERS.each_key do |edition|\n          @types_hierarchies[edition] = Hash.new { |h, k| h[k] = [] }\n\n          graphql_files_for_edition_glob = File.join(\"{#{GRAPHQL_TYPES_FOLDERS[edition].join(',')}}\", '**', '*.rb')\n          Dir[graphql_files_for_edition_glob].each do |graphql_file|\n            graphql_base_types = File.read(graphql_file).scan(GRAPHQL_IMPLEMENTS_REGEXP)\n            next if graphql_base_types.empty?\n\n            graphql_base_classes = graphql_base_types.flatten.map { |class_name| class_name.split('::').last }\n            graphql_base_classes.each do |graphql_base_class|\n              @types_hierarchies[edition][graphql_base_class] += [graphql_file]\n            end",
    "comment": "Regroup all GraphQL types (by edition) that are implementing another GraphQL type.  The key is the type that is being implemented (e.g. NoteableInterface, TodoableInterface below) The value is an array of GraphQL type files that are implementing those types.  Example output:  { nil => { \"NoteableInterface\" => [ \"app/graphql/types/alert_management/alert_type.rb\", \"app/graphql/types/design_management/design_type.rb\" , \"TodoableInterface\" => [...] }, \"ee\" => { \"NoteableInterface\" => [ \"app/graphql/types/alert_management/alert_type.rb\", \"app/graphql/types/design_management/design_type.rb\", \"ee/app/graphql/types/epic_type.rb\"], \"TodoableInterface\"=> [...] } }",
    "label": "",
    "id": "7286"
  },
  {
    "raw_code": "def camelize(str)\n        str.split('_').collect(&:capitalize).join\n      end",
    "comment": "We don't want to use active_support for this method, so we're making it ourselves",
    "label": "",
    "id": "7287"
  },
  {
    "raw_code": "def filter_files\n        @_filter_files ||= changed_files.select do |filename|\n          filename.start_with?(*@view_base_folders) &&\n            File.exist?(filename)\n        end",
    "comment": "Keep the files that are in the @view_base_folders folder",
    "label": "",
    "id": "7288"
  },
  {
    "raw_code": "def find_partials(file)\n        partial_paths = find_pattern_in_file(file, RAILS_PARTIAL_INVOCATION_REGEXP)\n        partial_paths.flat_map do |partial_path|\n          view_file_folder        = File.dirname(file)\n          partial_relative_folder = File.dirname(partial_path)\n\n          dirname =\n            if partial_relative_folder == '.' # The partial is in the same folder as the HTML file\n              view_file_folder\n            else\n              File.join(view_file_folder, partial_relative_folder)\n            end",
    "comment": "Note: We only search for partials with depth 1. We don't do recursive search, as it is probably not necessary for a first iteration.",
    "label": "",
    "id": "7289"
  },
  {
    "raw_code": "def send_event(event_name, label:, value: nil, property: nil, extra_properties: {})\n        return log(:error, \"Error: Cannot send event '#{event_name}'. Missing project access token.\") unless api_token\n\n        properties = { label:, value:, property:, **extra_properties }.compact\n        body = {\n          event: event_name,\n          send_to_snowplow: true,\n          namespace_id: namespace_id,\n          project_id: project_id,\n          additional_properties: properties\n        }.to_json\n\n        log(:info, \"Sending data for event: #{event_name}\")\n        response = client.request_post(\"/api/v4/usage_data/track_event\", body, headers)\n\n        if response.code.to_i == 200\n          log(:info, \"Successfully sent data with properties: #{properties}\")\n        else\n          log(:error, \"Failed event tracking: #{response.code}, body: #{response.body}\")\n        end",
    "comment": "Send tracking event to usage_data API  @param event_name [String] the name of the event to track @param label [String] Event attribute @param value [Number] Numeric event attribute @param property [String] Optional event attribute @param extra_properties [Hash] Additional custom properties @return [Net::HTTPResponse]",
    "label": "",
    "id": "7290"
  },
  {
    "raw_code": "def log(level, message)\n        return logger.public_send(level, message) if logger.respond_to?(level) # rubocop:disable GitlabSecurity/PublicSend -- CI usage only\n\n        %i[warn error].include?(level) ? warn(message) : puts(message)\n      end",
    "comment": "Print to stdout/stderr or use logger if defined  @param level [Symbol] @param message [String] @return [void]",
    "label": "",
    "id": "7291"
  },
  {
    "raw_code": "def client\n        @client ||= Net::HTTP.new(uri.host, uri.port).tap do |http|\n          http.use_ssl = true\n        end",
    "comment": "Http client  @return [Net::HTTP]",
    "label": "",
    "id": "7292"
  },
  {
    "raw_code": "def uri\n        @uri ||= URI.parse(ENV['CI_SERVER_URL'])\n      end",
    "comment": "CI server uri  @return [Uri]",
    "label": "",
    "id": "7293"
  },
  {
    "raw_code": "def headers\n        @headers ||= {\n          \"PRIVATE-TOKEN\" => api_token,\n          \"Content-Type\" => \"application/json\"\n        }\n      end",
    "comment": "Default request headers  @return [Hash]",
    "label": "",
    "id": "7294"
  },
  {
    "raw_code": "def namespace_id\n        @namespace_id ||= ENV[\"CI_PROJECT_NAMESPACE_ID\"].to_i\n      end",
    "comment": "Project namespace ID  @return [Integer]",
    "label": "",
    "id": "7295"
  },
  {
    "raw_code": "def project_id\n        @project_id ||= ENV[\"CI_PROJECT_ID\"].to_i\n      end",
    "comment": "Project ID  @return [Integer]",
    "label": "",
    "id": "7296"
  },
  {
    "raw_code": "def execute\n        logger.info(\"Running metrics export for test type: #{test_type}\")\n\n        case test_type\n        when :backend\n          export_rspec_metrics\n        when :frontend\n          export_jest_metrics\n        end",
    "comment": "Execute metrics export  @return [Boolean]",
    "label": "",
    "id": "7297"
  },
  {
    "raw_code": "def export_rspec_metrics\n        export_all_strategies(TEST_TYPES[:backend]) do |strategy|\n          generate_and_record_metrics(strategy, rspec_matching_tests(strategy))\n        end",
    "comment": "Export rspec test metrics  @return [Boolean]",
    "label": "",
    "id": "7298"
  },
  {
    "raw_code": "def export_jest_metrics\n        export_all_strategies(TEST_TYPES[:frontend]) do |strategy|\n          generate_and_record_metrics(strategy, jest_matching_tests)\n        end",
    "comment": "Export jest test metrics  @return [Boolean]",
    "label": "",
    "id": "7299"
  },
  {
    "raw_code": "def export_all_strategies(strategies)\n        results = strategies.map do |strategy|\n          logger.info(\"Running export for '#{strategy}' strategy\")\n          yield(strategy)\n          true\n        rescue StandardError => e\n          logger.error(\"Failed to export test metrics for strategy '#{strategy}': #{e.message}\")\n          logger.error(e.backtrace.select { |entry| entry.include?(project_root) }.join(\"\\n\")) if e.backtrace\n          false\n        end",
    "comment": "Export metrics for all defined strategies  @param strategies [Array] @return [Boolean]",
    "label": "",
    "id": "7300"
  },
  {
    "raw_code": "def project_root\n        @project_root ||= File.expand_path(\"../../../..\", __dir__)\n      end",
    "comment": "Project root folder  @return [String]",
    "label": "",
    "id": "7301"
  },
  {
    "raw_code": "def output_path\n        @output_path ||= File.join(@output_dir, test_type.to_s).tap { |path| FileUtils.mkdir_p(path) }\n      end",
    "comment": "Path for specific test type output  @return [String]",
    "label": "",
    "id": "7302"
  },
  {
    "raw_code": "def tracker\n        @tracker ||= Tooling::Events::TrackPipelineEvents.new(logger: logger)\n      end",
    "comment": "Internal event tracker  @return [TrackPipelineEvents]",
    "label": "",
    "id": "7303"
  },
  {
    "raw_code": "def changed_files\n        @changed_files ||= ChangedFiles.fetch(\n          changes: Tooling::FindChanges.new(\n            from: :api,\n            frontend_fixtures_mapping_pathname: frontend_fixtures_mapping_file\n          ).execute\n        )\n      end",
    "comment": "MR changed files  @return [String]",
    "label": "",
    "id": "7304"
  },
  {
    "raw_code": "def mapping_fetcher\n        @mapping_fetcher ||= Tooling::PredictiveTests::MappingFetcher.new(logger: logger)\n      end",
    "comment": "Mapping file fetcher  @return [MappingFetcher]",
    "label": "",
    "id": "7305"
  },
  {
    "raw_code": "def frontend_fixtures_mapping_file\n        @frontend_fixtures_mapping_file ||= File.join(Dir.tmpdir, \"frontend_fixtures_mapping.json\").tap do |file|\n          mapping_fetcher.fetch_frontend_fixtures_mappings(file)\n        end",
    "comment": "Frontend fixtures mapping file  @return [String]",
    "label": "",
    "id": "7306"
  },
  {
    "raw_code": "def rspec_matching_tests(strategy)\n        mapping_file = fetch_crystalball_mappings(strategy)\n        test_selector(mapping_file).rspec_spec_list\n      end",
    "comment": "Matching rspec tests generated via test selector  @param strategy [Symbol] @return [Array]",
    "label": "",
    "id": "7307"
  },
  {
    "raw_code": "def jest_matching_tests\n        return @jest_matching_tests if @jest_matching_tests\n\n        script = File.join(project_root, JEST_PREDICTIVE_TESTS_SCRIPT_PATH)\n        result_path = File.join(Dir.tmpdir, \"predictive_jest_matching_tests.txt\")\n        ruby_files = changed_files.reject do |f|\n          Tooling::PredictiveTests::ChangedFiles::JS_FILE_FILTER_REGEX.match?(f)\n        end",
    "comment": "Matching jest tests generated via native js script  @return [Array]",
    "label": "",
    "id": "7308"
  },
  {
    "raw_code": "def backend_mapping_file_path(strategy)\n        File.join(Dir.tmpdir, \"#{strategy}_mapping.json\")\n      end",
    "comment": "Mapping file path for specific strategy  @param strategy [Symbol] @return [String]",
    "label": "",
    "id": "7309"
  },
  {
    "raw_code": "def path_for_strategy(strategy, *args)\n        File.join(output_path, strategy.to_s, *args)\n      end",
    "comment": "Full path within strategy specific folder  @param strategy [Symbol] @param *args [Array] optional extra path parts to append @return [String]",
    "label": "",
    "id": "7310"
  },
  {
    "raw_code": "def test_selector(rspec_test_mapping_path = nil)\n        Tooling::PredictiveTests::TestSelector.new(\n          changed_files: changed_files,\n          rspec_test_mapping_path: rspec_test_mapping_path,\n          logger: logger,\n          rspec_mappings_limit_percentage: nil # always return all tests in the mapping,\n        )\n      end",
    "comment": "Predictive spec list selector  @param strategy [Symbol] @return [TestSelector]",
    "label": "",
    "id": "7311"
  },
  {
    "raw_code": "def generate_and_record_metrics(strategy, predicted_test_files)\n        logger.info(\"Generating metrics for mapping strategy '#{strategy}' ...\")\n\n        metrics = generate_metrics_data(\n          changed_files,\n          predicted_test_files,\n          strategy\n        )\n\n        save_metrics(metrics, strategy)\n        send_metrics_events(metrics, strategy)\n\n        logger.info(\"Metrics generation completed for strategy '#{strategy}'\")\n      end",
    "comment": "Create, save and export metrics for selected RSpec tests for specific strategy  @param strategy [Symbol] @return [void]",
    "label": "",
    "id": "7312"
  },
  {
    "raw_code": "def fetch_crystalball_mappings(strategy)\n        backend_mapping_file_path(strategy).tap do |file|\n          mapping_fetcher.fetch_rspec_mappings(file, type: strategy)\n        end",
    "comment": "Fetch crystalball mappings and return file location  @param strategy [Symbol] @return [String]",
    "label": "",
    "id": "7313"
  },
  {
    "raw_code": "def generate_metrics_data(changed_files, predicted_test_files, strategy)\n        {\n          timestamp: Time.now.iso8601,\n          test_type: test_type,\n          strategy: strategy,\n          core_metrics: {\n            changed_files_count: changed_files.size,\n            predicted_test_files_count: predicted_test_files.size,\n            missed_failing_test_files: (failed_test_files - predicted_test_files).size,\n            predicted_failing_test_files: (failed_test_files & predicted_test_files).size,\n            failed_test_files_count: failed_test_files.size,\n            # rspec tests have runtime information provided via knapsack report\n            # frontend tests don't have a runtime report yet, so we skip them\n            runtime_metrics: runtime_metrics(predicted_test_files)\n          }.compact\n        }\n      end",
    "comment": "Create metrics hash with all calculated metrics  @param changed_files [Array] @param predicted_test_files [Array] @param strategy [Symbol] @return [Hash]",
    "label": "",
    "id": "7314"
  },
  {
    "raw_code": "def save_metrics(metrics, strategy)\n        File.write(File.join(output_path, \"metrics_#{strategy}.json\"), JSON.pretty_generate(metrics))\n      end",
    "comment": "Save metrics hash as json file  @param metrics [Hash] @param strategy [Symbol] @return [void]",
    "label": "",
    "id": "7315"
  },
  {
    "raw_code": "def send_event(label, value, strategy)\n        extra_properties = { ci_job_id: ENV[\"CI_JOB_ID\"], ci_pipeline_id: ENV[\"CI_PIPELINE_ID\"], test_type: test_type }\n        tracker.send_event(\n          PREDICTIVE_TEST_METRICS_EVENT,\n          label: label,\n          value: value,\n          property: strategy.to_s,\n          extra_properties: extra_properties\n        )\n      end",
    "comment": "Send event with specific metrics via internal events @param label [String] @param value [Integer|Float] @param strategy [Symbol]",
    "label": "",
    "id": "7316"
  },
  {
    "raw_code": "def send_metrics_events(metrics, strategy)\n        core = metrics[:core_metrics]\n\n        send_event(\"changed_files_count\", core[:changed_files_count], strategy)\n        send_event(\"predicted_test_files_count\", core[:predicted_test_files_count], strategy)\n        send_event(\"missed_failing_test_files\", core[:missed_failing_test_files], strategy)\n        send_event(\"predicted_failing_test_files\", core[:predicted_failing_test_files], strategy)\n\n        return unless test_type == :backend\n\n        runtime = core[:runtime_metrics]\n\n        send_event(\"projected_test_runtime_seconds\", runtime[:projected_test_runtime_seconds], strategy)\n        send_event(\"test_files_missing_runtime_count\", runtime[:test_files_missing_runtime_count], strategy)\n      end",
    "comment": "Send events containing calculated predictive tests metrics  @param metrics [Hash] @param strategy [Symbol] @return [void]",
    "label": "",
    "id": "7317"
  },
  {
    "raw_code": "def runtime_metrics(predicted_test_files)\n        return if knapsack_report.empty? || test_type == :frontend\n\n        specs_missing_runtime = []\n        predicted_test_runtime_seconds = predicted_test_files.sum do |spec|\n          if knapsack_report[spec]\n            # round the value to 4 digits after to avoid very big floats in the output\n            knapsack_report[spec].round(4)\n          else\n            specs_missing_runtime << spec\n            DEFAULT_SPEC_RUNTIME_SECONDS\n          end",
    "comment": "Create projected test runtime metrics hash for rspec tests based on knapsack report  @param predicted_test_files [Array] @return [Hash]",
    "label": "",
    "id": "7318"
  },
  {
    "raw_code": "def knapsack_report\n        return @knapsack_report if @knapsack_report\n        return @knapsack_report = {} unless test_runtime_report_file && File.exist?(test_runtime_report_file)\n\n        @knapsack_report = JSON.parse(File.read(test_runtime_report_file)) # rubocop:disable Gitlab/Json -- not in Rails environment\n      rescue JSON::ParserError, Errno::ENOENT, Errno::EACCES => e\n        logger.error(\"Failed to parse knapsack report #{e.message}\")\n        logger.error(e.backtrace.select { |entry| entry.include?(project_root) }) if e.backtrace\n        @knapsack_report = {}\n      end",
    "comment": "Knapsack report from CI environment which maps specs to runtime Used to create project test runtime metric for predictive rspec tests  @return [Hash]",
    "label": "",
    "id": "7319"
  },
  {
    "raw_code": "def rspec_spec_list\n        logger.info \"Creating predictive rspec test files specs list ...\"\n        specs = {\n          test_file_finder_specs: specs_from_mapping,\n          graphql_type_mapping_specs: specs_from_graphql_base_types,\n          js_changes_specs: system_specs_from_js_changes,\n          view_changes_specs: system_specs_from_view_changes\n        }\n\n        logger.info(\"Generated following rspec specs list: #{JSON.pretty_generate(specs)}\")\n        specs.values.flatten\n      end",
    "comment": "Predictive rspec test files specs list  @return [Array]",
    "label": "",
    "id": "7320"
  },
  {
    "raw_code": "def specs_from_mapping\n        @specs_from_mapping ||= Tooling::FindTests.new(\n          changed_files,\n          mappings_file: rspec_test_mapping_path,\n          mappings_limit_percentage: rspec_mappings_limit_percentage\n        ).execute\n      end",
    "comment": "Add specs based on crystalball mapping or static tests.yml file  @return [void]",
    "label": "",
    "id": "7321"
  },
  {
    "raw_code": "def system_specs_from_js_changes\n        @system_specs_from_js_changes ||= Tooling::Mappings::JsToSystemSpecsMappings.new(changed_files).execute\n      end",
    "comment": "Add system specs based on changes to JS files.  @return [void]",
    "label": "",
    "id": "7322"
  },
  {
    "raw_code": "def specs_from_graphql_base_types\n        @specs_from_graphql_base_types ||= Tooling::Mappings::GraphqlBaseTypeMappings.new(changed_files).execute\n      end",
    "comment": "Add specs based on potential changes to the GraphQL base types  @return [void]",
    "label": "",
    "id": "7323"
  },
  {
    "raw_code": "def system_specs_from_view_changes\n        @system_specs_from_view_changes ||= Tooling::Mappings::ViewToSystemSpecsMappings.new(changed_files).execute\n      end",
    "comment": "Add system specs based on changes to views.  @return [void]",
    "label": "",
    "id": "7324"
  },
  {
    "raw_code": "def get(api_token, uri)\n          http = Net::HTTP.new(uri.host, uri.port)\n          http.use_ssl = true\n\n          request = Net::HTTP::Get.new(uri)\n          request['PRIVATE-TOKEN'] = api_token\n\n          response = http.request(request)\n\n          return response unless block_given?\n\n          # Yield the first page of results, and then yield successive pages.\n          yield response\n\n          # Continue to loop over pages until there are no more.\n          next_page_url = get_next_page_url(response)\n          while next_page_url\n            uri = URI(next_page_url)\n            response = get(api_token, uri)\n\n            yield response\n\n            next_page_url = get_next_page_url(response)\n          end",
    "comment": "Pass a block to this method to be called with each page of results.",
    "label": "",
    "id": "7325"
  },
  {
    "raw_code": "def folders_for_available_editions(base_folder)\n        foss_prefix        = base_folder\n        extension_prefixes = ::GitlabEdition.extensions.map { |prefix| \"#{prefix}/#{foss_prefix}\" }\n        [foss_prefix, *extension_prefixes]\n      end",
    "comment": "Input: A folder Output: An array of folders, each prefixed with a GitLab edition",
    "label": "",
    "id": "7326"
  },
  {
    "raw_code": "def render_full_field(field, heading_level: 3, owner: nil)\n          conn = connection?(field)\n          args = field[:arguments].reject { |arg| conn && CONNECTION_ARGS.include?(arg[:name]) }\n          arg_owner = [owner, field[:name]]\n\n          chunks = [\n            render_name_and_description(field, level: heading_level, owner: owner),\n            render_return_type(field),\n            render_input_type(field),\n            render_connection_note(field),\n            render_argument_table(heading_level, args, arg_owner),\n            render_return_fields(field, owner: owner)\n          ]\n\n          join(:block, chunks)\n        end",
    "comment": "Template methods: Methods that return chunks of Markdown for insertion into the document",
    "label": "",
    "id": "7327"
  },
  {
    "raw_code": "def connection_object_types\n          objects.select { |t| t[:is_edge] || t[:is_connection] }\n        end",
    "comment": "QUERIES: Methods that return parts of the schema, or related information:",
    "label": "",
    "id": "7328"
  },
  {
    "raw_code": "def mutations\n          @mutations ||= sorted_by_name(graphql_mutation_types).map do |t|\n            inputs = t[:input_fields]\n            input = inputs.first\n            name = t[:name]\n\n            assert!(inputs.one?, \"Expected exactly 1 input field named #{name}. Found #{inputs.count} instead.\")\n            assert!(input[:name] == 'input', \"Expected the input of #{name} to be named 'input'\")\n\n            input_type_name = input[:type][:name]\n            input_type = graphql_input_object_types.find { |t| t[:name] == input_type_name }\n            assert!(input_type.present?, \"Cannot find #{input_type_name} for #{name}.input\")\n\n            arguments = input_type[:input_fields]\n            seen_type!(input_type_name)\n            t.merge(arguments: arguments)\n          end",
    "comment": "Place the arguments of the input types on the mutation itself. see: `#input_types` - this method must not call `#input_types` to avoid mutual recursion",
    "label": "",
    "id": "7329"
  },
  {
    "raw_code": "def input_types\n          mutations # ensure that mutations have seen their inputs first\n          graphql_input_object_types.reject { |t| seen_type?(t[:name]) }\n        end",
    "comment": "We assume that the mutations have been processed first, marking their inputs as `seen_type?`",
    "label": "",
    "id": "7330"
  },
  {
    "raw_code": "def enums\n          graphql_enum_types\n            .reject { |type| type[:values].empty? }\n            .reject { |enum_type| enum_type[:name].start_with?('__') }\n            .map { |type| type.merge(values: sorted_by_name(type[:values])) }\n        end",
    "comment": "We ignore the built-in enum types, and sort values by name",
    "label": "",
    "id": "7331"
  },
  {
    "raw_code": "def render_return_type(query)\n          return unless query[:type] # for example, mutations\n\n          \"Returns #{render_field_type(query[:type])}.\"\n        end",
    "comment": "Template methods",
    "label": "",
    "id": "7332"
  },
  {
    "raw_code": "def render_description(object, owner = nil, context = :block)\n          if deprecated?(object, owner)\n            render_deprecation(object, owner, context)\n          else\n            render_description_of(object, owner, context)\n          end",
    "comment": "Returns the object description. If the object has been deprecated, the deprecation reason will be returned in place of the description.",
    "label": "",
    "id": "7333"
  },
  {
    "raw_code": "def sorted_by_name(objects)\n          return [] unless objects.present?\n\n          objects.sort_by { |o| o[:name] }\n        end",
    "comment": "Queries",
    "label": "",
    "id": "7334"
  },
  {
    "raw_code": "def objects\n          strong_memoize(:objects) do\n            mutations = schema.mutation&.fields&.keys&.to_set || []\n\n            graphql_object_types\n              .reject { |object_type| object_type[:name][\"__\"] || object_type[:name] == 'Subscription' } # We ignore introspection and subscription types.\n              .map do |type|\n                name = type[:name]\n                type.merge(\n                  is_edge: name.ends_with?('Edge'),\n                  is_connection: name.ends_with?('Connection'),\n                  is_payload: name.ends_with?('Payload') && mutations.include?(name.chomp('Payload').camelcase(:lower)),\n                  fields: type[:fields] + type[:connections]\n                )\n              end",
    "comment": "We are ignoring connections and built in types for now, they should be added when queries are generated.",
    "label": "",
    "id": "7335"
  },
  {
    "raw_code": "def schema_deprecation(type_name, field_name)\n          key = [*Array.wrap(type_name), field_name].join('.')\n          deprecations[key]\n        end",
    "comment": "returns the deprecation information for a field or argument See: Gitlab::Graphql::Deprecation",
    "label": "",
    "id": "7336"
  },
  {
    "raw_code": "def get_changed_files_in_merged_results_pipeline\n      `git diff --name-only --diff-filter=d HEAD~..HEAD`.split(\"\\n\")\n    end",
    "comment": "See: https://gitlab.com/groups/gitlab-org/-/epics/16845#note_2370956250 for why we use `HEAD~` to compare",
    "label": "",
    "id": "7337"
  },
  {
    "raw_code": "def database_dictionary_files(change_type:)\n        files = helper.public_send(\"#{change_type}_files\") # rubocop:disable GitlabSecurity/PublicSend\n\n        files.filter_map { |path| Found.new(path) if DICTIONARY_PATH_REGEXP.match?(path) }\n      end",
    "comment": "`change_type` can be: - :added - :modified - :deleted",
    "label": "",
    "id": "7338"
  },
  {
    "raw_code": "def find_line_number(file_lines, searched_line, exclude_indexes: [])\n        lines_to_search = up_method_lines(file_lines)\n\n        _, index = file_lines.each_with_index.find do |file_line, index|\n          next unless lines_to_search.include?(index)\n\n          file_line == searched_line && !exclude_indexes.include?(index)\n        end",
    "comment": "This method was overwritten to make use of +up_method_lines+. It's necessary to only match lines that are inside the +up+ block in a migration file.  @return [Integer, Nil] the line number - only if the line is from within a +up+ method",
    "label": "",
    "id": "7339"
  },
  {
    "raw_code": "def up_method_lines(file_lines)\n        capture_up_block = false\n        up_method_content_lines = []\n\n        file_lines.each_with_index do |content, line_number|\n          capture_up_block = false if capture_up_block && END_METHOD_REGEX.match?(content)\n          up_method_content_lines << line_number if capture_up_block\n          capture_up_block = true if UP_METHOD_REGEX.match?(content)\n        end",
    "comment": "Return the line numbers from within the up method  @example line 0 def up line 1   cleanup_conversion_of_integer_to_bigint():my_table, :my_column) line 2   remove_column(:my_table, :my_column) line 3   other_method line 4 end  => [1, 2, 3]",
    "label": "",
    "id": "7340"
  },
  {
    "raw_code": "def categorize_batched_background_migrations(migration_class_names)\n        finalized_migrations = {}\n        unremovable_migrations = []\n\n        yaml_files = DB_DOC_BBM_DIRECTORY.flat_map { |dir| Dir.glob(File.join(dir, \"*.yml\")) }\n\n        yaml_files.each do |file_path|\n          yaml_content_raw = File.read(file_path)\n          yaml_content = YAML.safe_load(yaml_content_raw, permitted_classes: [], aliases: false)\n          next unless yaml_content.is_a?(Hash)\n\n          migration_job_name = yaml_content[\"migration_job_name\"]\n          next unless migration_class_names.key?(migration_job_name)\n\n          finalized_by = yaml_content[\"finalized_by\"]\n\n          if finalized_by.to_s.match?(/\\A\\d{14}\\z/)\n            # Migration is finalized with a timestamp\n            finalized_migrations[migration_job_name] ||= []\n            finalized_migrations[migration_job_name] << {\n              background_migration_class_name: migration_class_names[migration_job_name],\n              finalized_migration_timestamp: finalized_by.to_s\n            }\n          else\n            # Migration is not finalized yet\n            unremovable_migrations << {\n              batched_background_migration_file: migration_class_names[migration_job_name],\n              comment: MISSING_FINALIZED_MIGRATION\n            }\n          end",
    "comment": "Search through the YAML files and categorize BBM migrations as finalized or unremovable In the case of finalized migrations, we're expecting the finalized_by field in db/docs/batched_background_migrations to have a timestamp: Example: ... finalized_by: '20241103232325'",
    "label": "",
    "id": "7341"
  },
  {
    "raw_code": "def display_unremovable_migrations(unremovable_migrations)\n        return if unremovable_migrations.empty?\n\n        failure_messages = unremovable_migrations.map { |migration| format_migration_message(migration) }\n        error_message = format(ERROR_MESSAGE, DOCUMENTATION, failure_messages.join(\"\\n\\n\"))\n        fail(error_message)\n      end",
    "comment": "Format and display error messages",
    "label": "",
    "id": "7342"
  },
  {
    "raw_code": "def format_migration_message(migration)\n        message = \"**#{migration[:batched_background_migration_file]}**: #{migration[:comment]}\"\n\n        if migration[:finalized_migration_file]\n          additional_info = []\n          additional_info << \"Finalized migration: #{migration[:finalized_migration_file]}\"\n          additional_info << \"Finalized migration milestone: #{migration[:finalized_migration_milestone]}\"\n\n          if migration[:current_gitlab_version]\n            additional_info << \"Current GitLab version: #{migration[:current_gitlab_version]}\"\n          end",
    "comment": "Helper method to format a migration message",
    "label": "",
    "id": "7343"
  },
  {
    "raw_code": "def extract_class_or_module_name\n        namespace_names = collect_module_and_class_names\n\n        # Find the module/class that comes after BackgroundMigration\n        bg_index = namespace_names.index(BACKGROUND_MIGRATION_MODULE)\n        return unless bg_index && bg_index < namespace_names.length - 1\n\n        namespace_names[bg_index + 1]\n      end",
    "comment": "Extracts both class and module name after the BackgroundMigration module ex: Gitlab => BackgroundMigration => ClassName ex: EE => Gitlab => BackgroundMigration => ModuleName",
    "label": "",
    "id": "7344"
  },
  {
    "raw_code": "def has_ensure_batched_background_migration_is_finished_call?\n        find_method_call(:ensure_batched_background_migration_is_finished)\n      end",
    "comment": "Checks if the code contains a call to ensure_batched_background_migration_is_finished",
    "label": "",
    "id": "7345"
  },
  {
    "raw_code": "def contains_class_name_assignment?(class_name)\n        has_constant_assignment?(class_name) || has_job_class_name_argument?(class_name)\n      end",
    "comment": "Checks if the code contains any assignment or reference to a specific class name Looks for patterns like: MIGRATION = 'ClassName' method(job_class_name: 'ClassName')",
    "label": "",
    "id": "7346"
  },
  {
    "raw_code": "def extract_milestone\n        node = ast.each_descendant(:send).find do |n|\n          n.method_name == :milestone &&\n            n.arguments.first&.type == :str &&\n            !n.parenthesized?\n        end",
    "comment": "Extracts milestone info Looking for milestone '15.0' / milestone \"14.5\"",
    "label": "",
    "id": "7347"
  },
  {
    "raw_code": "def collect_module_and_class_names\n        namespace_names = []\n        ast.each_descendant(:module, :class) do |node|\n          name_node = node.children.first\n          namespace_names << name_node.const_name.to_s if name_node&.type == :const\n        end",
    "comment": "Collect all module and class names in the file",
    "label": "",
    "id": "7348"
  },
  {
    "raw_code": "def find_method_call(method_name)\n        ast.each_descendant(:send).any? { |node| node.method_name == method_name }\n      end",
    "comment": "Find if a specific method is called anywhere in the file",
    "label": "",
    "id": "7349"
  },
  {
    "raw_code": "def has_constant_assignment?(string_value)\n        ast.each_descendant(:casgn).any? do |node|\n          value_node = node.children[2]\n          value_node&.type == :str && value_node.str_content == string_value\n        end",
    "comment": "Check if a constant is assigned the specified string value ex. MIGRATION = 'ClassName'",
    "label": "",
    "id": "7350"
  },
  {
    "raw_code": "def has_job_class_name_argument?(class_name)\n        ast.each_descendant(:send).any? do |node|\n          node.arguments.any? do |arg|\n            next false unless arg.type == :hash\n\n            arg.pairs.any? do |pair|\n              key_node, value_node = pair.children\n              key_node.type == :sym &&\n                key_node.value == JOB_CLASS_NAME_KEY &&\n                value_node.type == :str &&\n                value_node.str_content == class_name\n            end",
    "comment": "Check if a method is called with job_class_name: 'ClassName' argument",
    "label": "",
    "id": "7351"
  },
  {
    "raw_code": "def feature_flag_files(danger_helper:, change_type:)\n        files = danger_helper.public_send(\"#{change_type}_files\") # rubocop:disable GitlabSecurity/PublicSend -- we allow calling danger_helper.added_files & danger_helper.modified_files\n\n        files.select { |path| %r{\\A(ee/)?config/feature_flags/.*\\.yml\\z}.match?(path) }.map { |path| Found.build(path) }\n      end",
    "comment": "`change_type` can be: - :added - :modified - :deleted",
    "label": "",
    "id": "7352"
  },
  {
    "raw_code": "def files(change_type:)\n        files = helper.public_send(\"#{change_type}_files\") # rubocop:disable GitlabSecurity/PublicSend\n\n        files.filter_map { |path| path.start_with?('ee/config/saas_features/') && Found.new(path) }\n      end",
    "comment": "`change_type` can be: - :added - :modified - :deleted",
    "label": "",
    "id": "7353"
  },
  {
    "raw_code": "def add_suggestion(filename:, regex:, replacement: nil, comment_text: nil, exclude: nil, once_per_file: false)\n        added_lines = added_lines_matching(filename, regex)\n\n        return if added_lines.empty?\n\n        file_lines = project_helper.file_lines(filename)\n\n        added_lines.each_with_object([]) do |added_line, processed_line_numbers|\n          break if once_per_file && processed_line_numbers.any?\n\n          line_number = find_line_number(file_lines, added_line.delete_prefix('+'),\n            exclude_indexes: processed_line_numbers)\n\n          next unless line_number\n          next if !exclude.nil? && added_line.include?(exclude)\n\n          processed_line_numbers << line_number\n\n          if replacement\n            suggestion_text = file_lines[line_number]\n            suggestion_text = suggestion_text.gsub(regex, replacement)\n          end",
    "comment": "For file lines matching `regex` adds suggestion `replacement` with `comment_text` added.",
    "label": "",
    "id": "7354"
  },
  {
    "raw_code": "def create_unique_internal(scope, username, email_pattern, &creation_block)\n    FactoryBot.create(:common_organization)\n\n    super\n  end",
    "comment": "TODO: Until https://gitlab.com/groups/gitlab-org/-/epics/18745 is resolved we're creating internal users in the first organization as a temporary workaround. Many specs lack an organization in the database, causing foreign key constraint violations when creating internal users. We're not seeding organizations before all specs for performance.",
    "label": "",
    "id": "7355"
  },
  {
    "raw_code": "def self.slower_app_requires\n    require 'active_support/all'\n    require 'pry'\n\n    nil\n  end",
    "comment": "@return [void]",
    "label": "",
    "id": "7356"
  },
  {
    "raw_code": "def self.app_requires\n    require_relative 'deprecation_warnings'\n    require 'gitlab/utils/all'\n    require_relative 'rails_autoload'\n    ENV['IN_MEMORY_APPLICATION_SETTINGS'] = 'true'\n    require_relative '../config/settings'\n    require_relative '../lib/gitlab'\n\n    nil\n  end",
    "comment": "@return [void]",
    "label": "",
    "id": "7357"
  },
  {
    "raw_code": "def self.slower_spec_requires\n    require 'rspec-parameterized'\n    require_relative 'support/rspec'\n\n    nil\n  end",
    "comment": "@return [void]",
    "label": "",
    "id": "7358"
  },
  {
    "raw_code": "def self.spec_requires_and_configuration\n    require 'gitlab/rspec/next_instance_of'\n    require 'hashdiff'\n    require_relative 'support/patches/rspec_mocks_doubles_fast_spec_helper_patch'\n    require_relative 'support/matchers/result_matchers'\n    require_relative 'support/railway_oriented_programming'\n    require_relative 'simplecov_env'\n\n    # NOTE: Consider making any common RSpec configuration tweaks in `spec/support/rspec.rb` instead of here,\n    # because it is also used by `spec/spec_helper.rb`.\n    RSpec.configure do |config|\n      config.include NextInstanceOf\n      config.disable_monkey_patching! # Enable zero monkey patching mode before loading any other RSpec code.\n      config.mock_with :rspec do |mocks|\n        mocks.verify_doubled_constant_names = false # Allow mocking of non-lib module/class names from Rails\n      end",
    "comment": "@return [void]",
    "label": "",
    "id": "7359"
  },
  {
    "raw_code": "def self.domain_specific_spec_helper_support\n    # If you want to extensively use `fast_spec_helper` for your domain or\n    # bounded context (https://handbook.gitlab.com/handbook/engineering/architecture/design-documents/modular_monolith/bounded_contexts/),\n    # but don't want to have to repeat the same require statement or configuration across multiple spec files, you can\n    # add a custom fast_spec_helper for your domain and require it here.\n    # Just make sure your additions don't do anything to noticably increase the runtime of `fast_spec_helper`!\n\n    # Remote Development domain\n    require_relative('../ee/spec/support/fast_spec/remote_development/fast_spec_helper_support') if Gitlab.ee?\n\n    # Web IDE domain\n    require_relative 'support/fast_spec/web_ide/fast_spec_helper_support'\n\n    nil\n  end",
    "comment": "@return [void]",
    "label": "",
    "id": "7360"
  },
  {
    "raw_code": "def self.post_require_configuration\n    SimpleCovEnv.start!\n    ActiveSupport::XmlMini.backend = 'Nokogiri'\n\n    nil\n  end",
    "comment": "@return [void]",
    "label": "",
    "id": "7361"
  },
  {
    "raw_code": "def self.with_slow_execution_warning(max_allowed:)\n    data = Benchmark.measure do\n      yield\n    end",
    "comment": "@param [Float] max_allowed @return [void]",
    "label": "",
    "id": "7362"
  },
  {
    "raw_code": "def self.run\n    # NOTE: These max_allowed times are generally 2-4 times higher than the actual average\n    #       execution times, to avoid false warnings on slower machines or CI runners.\n    with_slow_execution_warning(max_allowed: 2.0) { slower_app_requires }\n    with_slow_execution_warning(max_allowed: 0.2) { app_requires }\n    with_slow_execution_warning(max_allowed: 1.0) { slower_spec_requires }\n    with_slow_execution_warning(max_allowed: 0.2) { spec_requires_and_configuration }\n    with_slow_execution_warning(max_allowed: 1.0) { domain_specific_spec_helper_support }\n    with_slow_execution_warning(max_allowed: 0.2) { post_require_configuration }\n\n    nil\n  end",
    "comment": "@return [void]",
    "label": "",
    "id": "7363"
  },
  {
    "raw_code": "def trigger(test, current_deprecations, recorded_deprecations)\n        if selected_for_raise?(current_deprecations)\n          raise RaiseDisallowedDeprecation.new(test, current_deprecations)\n        elsif ENV['RECORD_DEPRECATIONS']\n          record(test, current_deprecations, recorded_deprecations)\n        end",
    "comment": "Note: trigger does not get called if the current_deprecations matches recorded_deprecations See https://github.com/Shopify/deprecation_toolkit/blob/2398f38acb62220fb79a6cd720f61d9cea26bc06/lib/deprecation_toolkit/test_triggerer.rb#L8-L11",
    "label": "",
    "id": "7364"
  },
  {
    "raw_code": "def self.kwargs_warning\n    %r{warning: (?:Using the last argument (?:for `.+' )?as keyword parameters is deprecated; maybe \\*\\* should be added to the call|Passing the keyword argument (?:for `.+' )?as the last hash parameter is deprecated|Splitting the last argument (?:for `.+' )?into positional and keyword parameters is deprecated|The called method (?:`.+' )?is defined here)\\n\\z}\n  end",
    "comment": "Taken from https://github.com/jeremyevans/ruby-warning/blob/1.1.0/lib/warning.rb#L18 Note: When a spec fails due to this warning, please update the spec to address the deprecation.",
    "label": "",
    "id": "7365"
  },
  {
    "raw_code": "def self.allowed_kwarg_warning_paths\n    %w[]\n  end",
    "comment": "Note: No new exceptions should be added here, unless they are in external dependencies. In this case, we recommend to add a silence together with an issue to patch or update the dependency causing the problem. See https://gitlab.com/gitlab-org/gitlab/-/commit/aea37f506bbe036378998916d374966c031bf347#note_647515736",
    "label": "",
    "id": "7366"
  },
  {
    "raw_code": "def render_component\n    raise \"Override render_component in the including spec\"\n  end",
    "comment": "This should be overridden in the including spec",
    "label": "",
    "id": "7367"
  },
  {
    "raw_code": "def added_and_removed_lines\n      hunk = \"\n        --- a/app/views/layouts/preview/rapid_diffs.html.haml\t(revision eaba934a0bc6eed56cfd1f082e9fa3f5409f2938)\n        +++ b/app/views/layouts/preview/rapid_diffs.html.haml\t(date 1718119822001)\n        @@ -1,7 +1,6 @@\n        -= universal_stylesheet_link_tag 'application'\n        -= universal_stylesheet_link_tag 'application_utilities'\n         = universal_stylesheet_link_tag 'preview/rapid_diffs'\n         = webpack_bundle_tag 'javascripts/entrypoints/preview/rapid_diffs'\n        += webpack_bundle_tag 'javascripts/entrypoints/preview'\n\n        -%div{ style: 'padding: 20px' }\n        +%div{ style: 'padding: 20px', class: 'container-fluid' }\n           = yield\n      \"\n      render(::RapidDiffs::DiffFileComponent.new(diff_file: diff_file_from_hunk(hunk)))\n    end",
    "comment": "@!group Code",
    "label": "",
    "id": "7368"
  },
  {
    "raw_code": "def binary_file_added\n      hunk = \"\n        --- /dev/null\n        +++ b/binary\n          Binary files /dev/null and b/binary differ\n      \"\n      diff = raw_diff(diff_content(hunk), new_file: true)\n      render(::RapidDiffs::DiffFileComponent.new(diff_file: diff_file(diff, binary: true)))\n    end",
    "comment": "@!endgroup @!group Binary",
    "label": "",
    "id": "7369"
  },
  {
    "raw_code": "def moved_text_file\n      hunk = \"\n        --- a/old_text_file\n        +++ b/new_text_file\n         Line 1\n      \"\n      diff = raw_diff(diff_content(hunk), new_file: false, renamed_file: true, a_mode: '100644')\n      render(::RapidDiffs::DiffFileComponent.new(diff_file: diff_file(diff)))\n    end",
    "comment": "@!endgroup @!group NoPreview",
    "label": "",
    "id": "7370"
  },
  {
    "raw_code": "def default(\n      icon: :group,\n      href: \"gitlab.com\",\n      description: \"Groups are the best way to manage projects and members\",\n      title: \"Create a group\")\n      render Onboarding::ActionCardComponent.new(\n        title: title,\n        description: description,\n        icon: icon,\n        href: href\n      )\n    end",
    "comment": "Action card ---  @param icon select [~, star-o, issue-closed, group] @param href url @param description text @param title text",
    "label": "",
    "id": "7371"
  },
  {
    "raw_code": "def default(\n      heading: 'Settings block heading',\n      description: 'Settings block description',\n      body: 'Settings block content',\n      id: 'settings-block-id'\n    )\n      render(::Layouts::SettingsBlockComponent.new(heading, description: description, id: id, expanded: nil)) do |c|\n        c.with_description { description }\n        c.with_body { body }\n      end",
    "comment": "@param heading text @param description text @param body text @param id text",
    "label": "",
    "id": "7372"
  },
  {
    "raw_code": "def default(\n      heading: 'Settings section heading',\n      description: 'Settings section description',\n      body: 'Settings section content',\n      id: 'settings-section-id',\n      testid: 'settings-section-testid',\n      options: { data: { test: 'value' } }\n    )\n      render(::Layouts::SettingsSectionComponent.new(\n        heading, description: description,\n        id: id, testid: testid, options: options\n      )) do |c|\n        c.with_description { description }\n        c.with_body { body }\n      end",
    "comment": "@param heading text @param description text @param body text @param id text @param testid text @param options",
    "label": "",
    "id": "7373"
  },
  {
    "raw_code": "def default(type: :search)\n      render(::Layouts::EmptyResultComponent.new(\n        type: type\n      ))\n    end",
    "comment": "@param type select {{ Layouts::EmptyResultComponent::TYPE_OPTIONS }}",
    "label": "",
    "id": "7374"
  },
  {
    "raw_code": "def default(\n      border: true,\n      title: 'Naming, visibility',\n      description: 'Update your group name, description, avatar, and visibility.',\n      body: 'Settings fields here.'\n    )\n      render(::Layouts::HorizontalSectionComponent.new(border: border, options: { class: 'gl-mb-6 gl-pb-3' })) do |c|\n        c.with_title { title }\n        c.with_description { description }\n        c.with_body { body }\n      end",
    "comment": "@param border toggle @param title text @param description text @param body text",
    "label": "",
    "id": "7375"
  },
  {
    "raw_code": "def default(\n      heading: 'Page heading',\n      actions: 'Page actions go here',\n      description: 'Page description goes here'\n    )\n      render(::Layouts::PageHeadingComponent.new(heading)) do |c|\n        c.with_actions { actions }\n        c.with_description { description }\n      end",
    "comment": "@param heading text @param actions text @param description text",
    "label": "",
    "id": "7376"
  },
  {
    "raw_code": "def default(\n      title: 'CRUD Component title',\n      description: 'Description',\n      count: 99,\n      icon: 'rocket',\n      icon_class: 'gl-text-success',\n      toggle_text: 'Add action',\n      actions: 'Custom actions',\n      body: 'Body slot',\n      form: 'Form slot',\n      footer: 'Footer slot',\n      pagination: 'Pagination slot',\n      is_collapsible: false\n    )\n      render(::Layouts::CrudComponent.new(\n        title,\n        description: description,\n        count: count,\n        icon: icon,\n        icon_class: icon_class,\n        toggle_text: toggle_text,\n        is_collapsible: is_collapsible)) do |c|\n        c.with_description { description }\n        c.with_actions { actions }\n        c.with_body { body }\n        c.with_form { form }\n        c.with_footer { footer }\n        c.with_pagination { pagination }\n      end",
    "comment": "@param title text @param description text @param count number @param icon text @param toggle_text text @param is_collapsible rubocop:disable Metrics/ParameterLists -- allow all params",
    "label": "",
    "id": "7377"
  },
  {
    "raw_code": "def default(\n      message: 'Message for the broadcast banner',\n      id: '99',\n      theme: 'light-indigo',\n      dismissable: true,\n      expire_date: Time.now.next_year.iso8601,\n      cookie_key: 'my_cookie',\n      dismissal_path: '/my-path'\n    )\n      render(Pajamas::BroadcastBannerComponent.new(\n        message: message,\n        id: id,\n        theme: theme,\n        dismissable: dismissable,\n        expire_date: expire_date,\n        cookie_key: cookie_key,\n        dismissal_path: dismissal_path\n      ))\n    end",
    "comment": "@param message text @param id text @param theme text @param dismissable toggle @param expire_date text @param cookie_key text @param dismissal_path text",
    "label": "",
    "id": "7378"
  },
  {
    "raw_code": "def default(header: nil, body: \"Every card has a body.\", footer: nil)\n      render(Pajamas::CardComponent.new) do |c|\n        c.with_header { header } if header\n\n        c.with_body do\n          content_tag(:p, body)\n        end",
    "comment": "Card ---- See its design reference [here](https://design.gitlab.com/components/card).  @param header text @param body textarea @param footer text",
    "label": "",
    "id": "7379"
  },
  {
    "raw_code": "def with_collection(collection: COLLECTION)\n      render(Pajamas::CardComponent.with_collection(collection, card_options: { class: 'gl-mb-5' }))\n    end",
    "comment": "@param collection [Array] \"collection of cards as an array of hashes with header, body, footer\"",
    "label": "",
    "id": "7380"
  },
  {
    "raw_code": "def default(icon: :tanuki, icon_only: false, href: nil, text: \"Tanuki\", variant: :neutral)\n      render Pajamas::BadgeComponent.new(\n        text,\n        icon: icon,\n        icon_only: icon_only,\n        href: href,\n        variant: variant\n      )\n    end",
    "comment": "Badge ---  See its design reference [here](https://design.gitlab.com/components/badge).  @param icon select [~, star-o, issue-closed, tanuki] @param icon_only toggle @param href url @param text text @param variant select {{ Pajamas::BadgeComponent::VARIANT_OPTIONS }}",
    "label": "",
    "id": "7381"
  },
  {
    "raw_code": "def slot\n      render Pajamas::BadgeComponent.new(variant: :info) do\n        \"!ereht olleh\".reverse.capitalize\n      end",
    "comment": "Using the content slot ---  Use the content slot instead of the `text` param when things get more complicated than a plain string. All other options (`icon`, etc.) work as usual.",
    "label": "",
    "id": "7382"
  },
  {
    "raw_code": "def custom\n      render Pajamas::BadgeComponent.new(\n        \"I'm special.\",\n        class: \"js-special-badge\",\n        data: { count: 1 },\n        icon: :tanuki,\n        icon_classes: [\"js-special-badge-icon\"],\n        id: \"special-badge-22\",\n        variant: :success\n      )\n    end",
    "comment": "Custom HTML attributes and icon classes ---  Any extra options passed into the component are treated as HTML attributes. This makes adding data or an id easy.  CSS classes provided with the `class:` option are combined with the component classes.  It is also possible to set custom `icon_classes:`.  The order in which you provide these keywords doesn't matter.",
    "label": "",
    "id": "7383"
  },
  {
    "raw_code": "def circular_icons\n      render Pajamas::BadgeComponent.new(variant: :success, icon: 'issue-open-m') do\n        'With status open'\n      end",
    "comment": "Circular issuable status icons ---  Circular icons 'issue-open-m' and 'issue-close'",
    "label": "",
    "id": "7384"
  },
  {
    "raw_code": "def icon_only\n      render Pajamas::BadgeComponent.new(variant: :success, icon: 'calendar')\n    end",
    "comment": "Icon only ---  Uses an icon only.",
    "label": "",
    "id": "7385"
  },
  {
    "raw_code": "def default(inline: false, label: \"Loading\", size: :md)\n      render Pajamas::SpinnerComponent.new(\n        inline: inline,\n        label: label,\n        size: size\n      )\n    end",
    "comment": "Spinner ---- See its design reference [here](https://design.gitlab.com/components/spinner).  @param inline toggle @param label text @param size select {{ Pajamas::SpinnerComponent::SIZE_OPTIONS }}",
    "label": "",
    "id": "7386"
  },
  {
    "raw_code": "def light\n      render(Pajamas::SpinnerComponent.new(color: :light))\n    end",
    "comment": "Use a light spinner on dark backgrounds.  @display bg_dark true",
    "label": "",
    "id": "7387"
  },
  {
    "raw_code": "def extra_attributes\n      render Pajamas::SpinnerComponent.new(\n        class: \"js-do-something\",\n        data: { foo: \"bar\" },\n        id: \"my-special-spinner\"\n      )\n    end",
    "comment": "Any extra HTML attributes like `class`, `data` or `id` get automatically applied to the spinner container element. ",
    "label": "",
    "id": "7388"
  },
  {
    "raw_code": "def default(\n      title: \"This state is empty\",\n      description: \"The title and message should be clear, concise, and explain why the user is seeing this screen.\n        The actions should help the user on what to do to get the real feature.\",\n      compact: false,\n      svg_path: \"illustrations/empty-state/empty-projects-deleted-md.svg\",\n      primary_button_text: \"Do primary action\",\n      primary_button_link: \"#learn-more-primary\",\n      secondary_button_text: \"Do secondary action\",\n      secondary_button_link: \"#learn-more-secondary\")\n      render(Pajamas::EmptyStateComponent.new(\n        title: title,\n        svg_path: svg_path,\n        primary_button_text: primary_button_text,\n        primary_button_link: primary_button_link,\n        secondary_button_text: secondary_button_text,\n        secondary_button_link: secondary_button_link,\n        compact: compact\n      )) do |c|\n        c.with_description { description } if description\n      end",
    "comment": "@param title text @param description textarea @param compact toggle @param svg_path text @param primary_button_text text @param primary_button_link text @param secondary_button_text text @param secondary_button_link text",
    "label": "",
    "id": "7389"
  },
  {
    "raw_code": "def default(\n      button_text: \"Learn more\",\n      button_link: \"https://about.gitlab.com/\",\n      content: \"Add your message here.\",\n      variant: :promotion\n    )\n      render(Pajamas::BannerComponent.new(\n        button_text: button_text,\n        button_link: button_link,\n        svg_path: \"illustrations/devops-sm.svg\",\n        variant: variant\n      )) do |c|\n        content_tag :p, content\n      end",
    "comment": "Banner ---- See its design reference [here](https://design.gitlab.com/components/banner).  @param button_text text @param button_link text @param content textarea @param variant select {{ Pajamas::BannerComponent::VARIANT_OPTIONS }}",
    "label": "",
    "id": "7390"
  },
  {
    "raw_code": "def with_primary_action_slot\n      render(Pajamas::BannerComponent.new) do |c|\n        c.with_primary_action do\n          # You could also `render` another partial here.\n          tag.button \"I'm special\", class: \"btn btn-md btn-confirm gl-button\"\n        end",
    "comment": "Use the `primary_action` slot instead of `button_text` and `button_link` if you need something more special, like rendering a partial that holds your button.",
    "label": "",
    "id": "7391"
  },
  {
    "raw_code": "def with_illustration_slot\n      render(Pajamas::BannerComponent.new) do |c|\n        c.with_illustration do\n          '<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"white\" stroke=\"white\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"feather feather-thumbs-up\"><path d=\"M14 9V5a3 3 0 0 0-3-3l-4 9v11h11.28a2 2 0 0 0 2-1.7l1.38-9a2 2 0 0 0-2-2.3zM7 22H4a2 2 0 0 1-2-2v-7a2 2 0 0 1 2-2h3\"></path></svg>'.html_safe # rubocop:disable Layout/LineLength, Lint/RedundantCopDisableDirective\n        end",
    "comment": "Use the `illustration` slot instead of `svg_path` if your illustration is not part or the asset pipeline, but for example, an inline SVG via `custom_icon`.",
    "label": "",
    "id": "7392"
  },
  {
    "raw_code": "def morph(variant: :current, is_on: false)\n      todo_component = Pajamas::AnimatedIconComponent.new(\n        icon: :todo,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      star_component = Pajamas::AnimatedIconComponent.new(\n        icon: :star,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      sort_component = Pajamas::AnimatedIconComponent.new(\n        icon: :sort,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      smile_component = Pajamas::AnimatedIconComponent.new(\n        icon: :smile,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      sidebar_component = Pajamas::AnimatedIconComponent.new(\n        icon: :sidebar,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      notification_component = Pajamas::AnimatedIconComponent.new(\n        icon: :notifications,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      chevron_right_down_component = Pajamas::AnimatedIconComponent.new(\n        icon: :chevron_right_down,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      chevron_lg_right_down_component = Pajamas::AnimatedIconComponent.new(\n        icon: :chevron_lg_right_down,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      chevron_down_up_component = Pajamas::AnimatedIconComponent.new(\n        icon: :chevron_down_up,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      chevron_lg_down_up_component = Pajamas::AnimatedIconComponent.new(\n        icon: :chevron_lg_down_up,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n\n      render_with_template(template: 'animated_icon_component_preview/morph', locals: {\n        todo_component: todo_component,\n        star_component: star_component,\n        sort_component: sort_component,\n        smile_component: smile_component,\n        sidebar_component: sidebar_component,\n        notification_component: notification_component,\n        chevron_right_down_component: chevron_right_down_component,\n        chevron_lg_right_down_component: chevron_lg_right_down_component,\n        chevron_down_up_component: chevron_down_up_component,\n        chevron_lg_down_up_component: chevron_lg_down_up_component\n      })\n    end",
    "comment": "Render morphing example icon @param variant select {{ Pajamas::AnimatedIconComponent::VARIANT_CLASSES.keys }}",
    "label": "",
    "id": "7393"
  },
  {
    "raw_code": "def infinite(icon: :upload, variant: :current, is_on: true)\n      upload_component = Pajamas::AnimatedIconComponent.new(\n        icon: icon.to_sym,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      duo_chat_component = Pajamas::AnimatedIconComponent.new(\n        icon: :duo_chat,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n      loader_component = Pajamas::AnimatedIconComponent.new(\n        icon: :loader,\n        variant: variant.to_sym,\n        is_on: is_on\n      )\n\n      render_with_template(template: 'animated_icon_component_preview/infinite', locals: {\n        upload_component: upload_component,\n        duo_chat_component: duo_chat_component,\n        loader_component: loader_component\n      })\n    end",
    "comment": "Render infinite example icon @param variant select {{ Pajamas::AnimatedIconComponent::VARIANT_CLASSES.keys }}",
    "label": "",
    "id": "7394"
  },
  {
    "raw_code": "def default(value: 50, variant: :primary)\n      render Pajamas::ProgressComponent.new(value: value, variant: variant)\n    end",
    "comment": "Progress ---  See its design reference [here](https://design.gitlab.com/components/progress-bar).  @param value range { min: 0, max: 100, step: 1 } @param variant select {{ Pajamas::ProgressComponent::VARIANT_OPTIONS }}",
    "label": "",
    "id": "7395"
  },
  {
    "raw_code": "def default(title: \"Alert title (optional)\", body: \"Alert message goes here.\", dismissible: true, variant: :info)\n      render(Pajamas::AlertComponent.new(\n        title: title,\n        dismissible: dismissible,\n        variant: variant.to_sym\n      )) do |c|\n        if body\n          c.with_body { body }\n        end",
    "comment": "@param title text @param body text @param dismissible toggle @param variant select {{ Pajamas::AlertComponent::VARIANT_ICONS.keys }}",
    "label": "",
    "id": "7396"
  },
  {
    "raw_code": "def default( # rubocop:disable Metrics/ParameterLists\n      category: :primary,\n      variant: :default,\n      size: :medium,\n      type: :button,\n      disabled: false,\n      loading: false,\n      block: false,\n      label: false,\n      selected: false,\n      icon: nil,\n      text: \"Edit\"\n    )\n      render(Pajamas::ButtonComponent.new(\n        category: category,\n        variant: variant,\n        size: size,\n        type: type,\n        disabled: disabled,\n        loading: loading,\n        block: block,\n        label: label,\n        selected: selected,\n        icon: icon\n      )) do\n        text.presence\n      end",
    "comment": "Button ---- See its design reference [here](https://design.gitlab.com/components/button).  @param category select {{ Pajamas::ButtonComponent::CATEGORY_OPTIONS }} @param variant select {{ Pajamas::ButtonComponent::VARIANT_OPTIONS }} @param size select {{ Pajamas::ButtonComponent::SIZE_OPTIONS }} @param type select {{ Pajamas::ButtonComponent::TYPE_OPTIONS }} @param disabled toggle @param loading toggle @param block toggle @param label toggle @param selected toggle @param icon select [~, star-o, issue-closed, tanuki] @param text text",
    "label": "",
    "id": "7397"
  },
  {
    "raw_code": "def link(target: nil)\n      render(Pajamas::ButtonComponent.new(\n        href: \"https://gitlab.com\",\n        target: target\n      )) do\n        \"This is a link\"\n      end",
    "comment": "The component can also be used to create links that look and feel like buttons. Just provide a `href` and optionally a `target` to create an `<a>` tag. For links with target=\"_blank\", the component automatically adds rel=\"noopener noreferrer\".  @param target select {{ Pajamas::ButtonComponent::TARGET_OPTIONS }}",
    "label": "",
    "id": "7398"
  },
  {
    "raw_code": "def default(title: \"Accordion title (open)\", state: :opened)\n      render(Pajamas::AccordionItemComponent.new(\n        title: title,\n        state: state\n      ))\n    end",
    "comment": "@param title text @param state",
    "label": "",
    "id": "7399"
  },
  {
    "raw_code": "def default(src: ActionController::Base.helpers.image_path('logo.svg'), size: 64)\n      render(Pajamas::AvatarComponent.new(src, size: size))\n    end",
    "comment": "Avatar ---- See its Pajamas design reference [here](https://design.gitlab.com/components/avatar).  The avatar component takes a single `item` param and a couple of optional arguments: - If the `item` is a plain `String`, this string will become the image `src`. In this case, also provide the `alt:` option, otherwise the resulting avatar image won't have an alt attribute. - If the `item` is a `User` object, the avatar will have a round shape. - For any other object (`Group`, `Project` etc) the shape will be rectangular with rounded corners. @param src text @param size select {{ Pajamas::AvatarComponent::SIZE_OPTIONS }}",
    "label": "",
    "id": "7400"
  },
  {
    "raw_code": "def user(size: 64)\n      render(Pajamas::AvatarComponent.new(User.first, size: size))\n    end",
    "comment": "We show user avatars in a circle. @param size select {{ Pajamas::AvatarComponent::SIZE_OPTIONS }}",
    "label": "",
    "id": "7401"
  },
  {
    "raw_code": "def project(size: 64)\n      render(Pajamas::AvatarComponent.new(Project.first, size: size))\n    end",
    "comment": "@param size select {{ Pajamas::AvatarComponent::SIZE_OPTIONS }}",
    "label": "",
    "id": "7402"
  },
  {
    "raw_code": "def group(size: 64)\n      render(Pajamas::AvatarComponent.new(Group.first, size: size))\n    end",
    "comment": "@param size select {{ Pajamas::AvatarComponent::SIZE_OPTIONS }}",
    "label": "",
    "id": "7403"
  },
  {
    "raw_code": "def default(\n      title: 'Single stat',\n      stat_value: '9,001',\n      unit: '',\n      title_icon: 'chart',\n      meta_text: '',\n      meta_icon: 'check-circle',\n      variant: :default\n    )\n      render Pajamas::SingleStatComponent.new(\n        title: title,\n        stat_value: stat_value,\n        unit: unit,\n        title_icon: title_icon,\n        meta_text: meta_text,\n        meta_icon: meta_icon,\n        variant: variant\n      )\n    end",
    "comment": "SingleStat ---  See its design reference [here](https://design.gitlab.com/data-visualization/single-stat).  @param title text @param stat_value text @param unit text @param title_icon text @param meta_text text @param meta_icon text @param variant select {{ Pajamas::BadgeComponent::VARIANT_OPTIONS }}",
    "label": "",
    "id": "7404"
  },
  {
    "raw_code": "def default(text: 'My Project', href: '#')\n      render Pajamas::BreadcrumbComponent.new do |c|\n        c.with_item(text: 'Home', href: '/')\n        c.with_item(text: 'My Group', href: '#')\n        c.with_item(text: text, href: href)\n        c.with_item(text: 'Issues', href: '#')\n        c.with_item(text: '#1234', href: '#')\n      end",
    "comment": "Breadcrumb ---- See its design reference [here](https://design.gitlab.com/components/breadcrumb).  @param text text @param href url",
    "label": "",
    "id": "7405"
  },
  {
    "raw_code": "def default_url_options\n    { controller: 'projects/blob', action: 'show', namespace_id: @project.namespace.path, project_id: @project.path }\n  end",
    "comment": "Make url_for work",
    "label": "",
    "id": "7406"
  },
  {
    "raw_code": "def parse_service_ping_keys(object, key_path = [])\n          if object.is_a?(Hash)\n            object.each_with_object([]) do |(key, value), result|\n              result.append parse_service_ping_keys(value, key_path + [key])\n            end",
    "comment": "Recursively traverse nested Hash of a generated Service Ping to return an Array of key paths in the dotted format used in metric definition YAML files, e.g.: 'count.category.metric_name'",
    "label": "",
    "id": "7407"
  },
  {
    "raw_code": "def without_format(path)\n    path.split('(', 2)[0]\n  end",
    "comment": "Pass in a full path to remove the format segment: `/ci/lint(.:format)` -> `/ci/lint`",
    "label": "",
    "id": "7408"
  },
  {
    "raw_code": "def path_before_wildcard(path)\n    path = path.gsub(starting_with_namespace, \"\")\n    path_segments = path.split('/').reject(&:empty?)\n    wildcard_index = path_segments.index { |segment| parameter?(segment) }\n\n    segments_before_wildcard = path_segments[0..wildcard_index - 1]\n\n    segments_before_wildcard.join('/')\n  end",
    "comment": "Pass in a full path and get the last segment before a wildcard That's not a parameter `/*namespace_id/:project_id/builds/artifacts/*ref_name_and_path` -> 'builds/artifacts'",
    "label": "",
    "id": "7409"
  },
  {
    "raw_code": "def wildcards_include?(path)\n    described_class::PROJECT_WILDCARD_ROUTES.include?(path) ||\n      described_class::PROJECT_WILDCARD_ROUTES.include?(path.split('/').first)\n  end",
    "comment": "If the path is reserved. Then no conflicting paths can# be created for any route using this reserved word.  Both `builds/artifacts` & `build` are covered by reserving the word `build`",
    "label": "",
    "id": "7410"
  },
  {
    "raw_code": "def get_attribute(name)\n          get_class_attribute(name)\n        end",
    "comment": "get_class_attribute and set_class_attribute are protected, hence those methods are for testing purpose",
    "label": "",
    "id": "7411"
  },
  {
    "raw_code": "def querify(params)\n      params.map { |k, v| \"#{k}=#{v}\" }.join('&')\n    end",
    "comment": "Can't use params.to_query as #to_query will encode values",
    "label": "",
    "id": "7412"
  },
  {
    "raw_code": "def sql(query, comments: true)\n      if comments\n        \"/*application:web,controller:badges,action:pipeline,correlation_id:01EYN39K9VMJC56Z7808N7RSRH*/ #{query}\"\n      else\n        query\n      end",
    "comment": "Emulate Marginalia pre-pending comments",
    "label": "",
    "id": "7413"
  },
  {
    "raw_code": "def sql(query, comments: true)\n      if comments\n        \"/*application:web,controller:badges,action:pipeline,correlation_id:01EYN39K9VMJC56Z7808N7RSRH*/ #{query}\"\n      else\n        query\n      end",
    "comment": "Emulate Marginalia pre-pending comments",
    "label": "",
    "id": "7414"
  },
  {
    "raw_code": "def long_email_local_part\n        \"longemail\" * 300\n      end",
    "comment": "Email with unicode characters",
    "label": "",
    "id": "7415"
  },
  {
    "raw_code": "def long_email_local_part\n        \"email\" * 24\n      end",
    "comment": "Email with unicode characters that normalize to multiple characters",
    "label": "",
    "id": "7416"
  },
  {
    "raw_code": "def self.execute(message)\n        generate_error_response_from_message(message: message, reason: :does_not_matter)\n      end",
    "comment": "@param [Gitlab::Fp::Message] message @return [Hash]",
    "label": "",
    "id": "7417"
  },
  {
    "raw_code": "def generate_and_cache_issues_ids(count:, position_offset: 0, position_direction: 1)\n    issues = []\n\n    count.times do |idx|\n      id = idx + 1\n      issues << double(relative_position: position_direction * ((id * 10) + position_offset), id: id)\n    end",
    "comment": "count - how many issue ids to generate, issue ids will start at 1 position_offset - if you'd want to offset generated relative_position for the issue ids, relative_position is generated as = issue id * 10 + position_offset position_direction - (1) for positive relative_positions, (-1) for negative relative_positions",
    "label": "",
    "id": "7418"
  },
  {
    "raw_code": "def username_regexp\n              default_regexp\n            end",
    "comment": "Ensure there is no match line header here",
    "label": "",
    "id": "7419"
  },
  {
    "raw_code": "def public_query_for_row_tracking_the_file(file_path)\n        query_for_row_tracking_the_file(file_path)\n      end",
    "comment": "Expose private methods for testing",
    "label": "",
    "id": "7420"
  },
  {
    "raw_code": "def generate_sidekiq_hash(worker)\n      job_hash = { 'payload' => ::Gitlab::Json.dump({\n        'class' => worker,\n        'created_at' => Time.now.to_f - described_class::TRACKING_KEY_TTL\n      }) }\n\n      Sidekiq.dump_json(job_hash)\n    end",
    "comment": "Format from https://github.com/sidekiq/sidekiq/blob/v7.3.9/lib/sidekiq/api.rb#L1209 The tid field in the `{pid}:work` hash contains a hash of 'payload' -> job hash.",
    "label": "",
    "id": "7421"
  },
  {
    "raw_code": "def setup_models\n    model_names.index_with do |model_name|\n      associations_for(relation_class_for_name(model_name)) - ['project']\n    end",
    "comment": "List of current models between models, in the format of {model: [model_2, model3], ...}",
    "label": "",
    "id": "7422"
  },
  {
    "raw_code": "def save_lfs_data\n        %w[project wiki].each do |repository_type|\n          create(\n            :lfs_objects_project,\n            project: project,\n            repository_type: repository_type,\n            lfs_object: lfs_object\n          )\n        end",
    "comment": "Use the LfsSaver to save data to be restored",
    "label": "",
    "id": "7423"
  },
  {
    "raw_code": "def setup_project\n    release = create(:release)\n\n    project = create(:project,\n      :public,\n      :repository,\n      :issues_disabled,\n      :wiki_enabled,\n      :builds_private,\n      description: 'description',\n      releases: [release],\n      group: group,\n      approvals_before_merge: 1,\n      merge_commit_template: 'merge commit message template',\n      squash_commit_template: 'squash commit message template')\n\n    issue = create(:issue, :task, assignees: [user], project: project)\n    snippet = create(:project_snippet, project: project)\n    project_label = create(:label, project: project)\n    group_label = create(:group_label, group: group)\n    create(:label_link, label: project_label, target: issue)\n    create(:label_link, label: group_label, target: issue)\n    create(:label_priority, label: group_label, priority: 1)\n    milestone = create(:milestone, project: project)\n    merge_request = create(:merge_request, source_project: project, milestone: milestone, assignees: [user], reviewers: [user])\n    create(:approval, merge_request: merge_request, user: user)\n    create(:diff_note_on_merge_request, project: project, author: user, noteable: merge_request)\n\n    ci_build = create(:ci_build, project: project, when: nil)\n    ci_build.pipeline.update!(project: project)\n    create(:commit_status, project: project, pipeline: ci_build.pipeline)\n    create(:generic_commit_status, pipeline: ci_build.pipeline, ci_stage: ci_build.ci_stage, project: project)\n    create(:ci_bridge, pipeline: ci_build.pipeline, ci_stage: ci_build.ci_stage, project: project)\n\n    create(:milestone, project: project)\n    discussion_note = create(:discussion_note, noteable: issue, project: project)\n    mr_note = create(:note, noteable: merge_request, project: project)\n    create(:system_note, noteable: merge_request, project: project, author: user, note: 'merged')\n    private_system_note = \"mentioned in merge request #{private_mr.to_reference(project)}\"\n    create(:system_note, noteable: merge_request, project: project, author: user, note: private_system_note)\n    create(:note, noteable: snippet, project: project)\n    create(:note_on_commit,\n      author: user,\n      project: project,\n      commit_id: ci_build.pipeline.sha)\n\n    create(:system_note_metadata, action: 'description', note: discussion_note)\n    create(:system_note_metadata, commit_count: 1, action: 'commit', note: mr_note)\n\n    create(:resource_label_event, label: project_label, issue: issue)\n    create(:resource_label_event, label: group_label, merge_request: merge_request)\n\n    create(:event, :created, target: milestone, project: project, author: user)\n\n    create(:project_custom_attribute, project: project)\n    create(:project_custom_attribute, project: project)\n\n    create(:project_badge, project: project)\n    create(:project_badge, project: project)\n\n    board = create(:board, project: project, name: 'TestBoard')\n    create(:list, board: board, position: 0, label: project_label)\n\n    design = create(:design, :with_file, versions_count: 2, issue: issue)\n    create(:diff_note_on_design, noteable: design, project: project, author: user)\n    create(:ci_pipeline_schedule, project: project, owner: user)\n\n    project\n  end",
    "comment": "rubocop: disable Metrics/AbcSize",
    "label": "",
    "id": "7424"
  },
  {
    "raw_code": "def drop_table_if_exists(table_name)\n    Gitlab::Database.database_base_models.each_value do |model|\n      model.connection.execute(\"DROP TABLE IF EXISTS #{table_name}\")\n    end",
    "comment": "To drop the test tables that have been created in the test migrations",
    "label": "",
    "id": "7425"
  },
  {
    "raw_code": "def self.next_partition_if_wrapper(...)\n          next_partition?(...)\n        end",
    "comment": "method().call cannot be detected by rspec, so we add a layer of indirection here",
    "label": "",
    "id": "7426"
  },
  {
    "raw_code": "def execute(sql)\n    connection.execute(sql)\n  end",
    "comment": "Needed by track_record_deletions",
    "label": "",
    "id": "7427"
  },
  {
    "raw_code": "def create_issue(title:, namespace:, author: user_1)\n    issues.create!(\n      title: title,\n      author_id: author.id,\n      namespace_id: namespace.id,\n      work_item_type_id: 1\n    )\n  end",
    "comment": "Helpers",
    "label": "",
    "id": "7428"
  },
  {
    "raw_code": "def create_members\n    [\n      create_invalid_project_member(id: 3, user_id: user3.id),\n      create_invalid_project_member(id: 4, user_id: user4.id),\n      create_invalid_group_member(id: 7, user_id: user7.id),\n      create_invalid_group_member(id: 8, user_id: user8.id)\n    ]\n  end",
    "comment": "create invalid project and group member records",
    "label": "",
    "id": "7429"
  },
  {
    "raw_code": "def unescape(html)\n    %w([ ] { }).each do |cgi_escape|\n      html.sub!(CGI.escape(cgi_escape), cgi_escape)\n    end",
    "comment": "Rinku does not escape these characters in HTML attributes, but content_tag does. We don't care about that difference for these specs, though.",
    "label": "",
    "id": "7430"
  },
  {
    "raw_code": "def convert_markdown(text, context = {})\n    Banzai::Pipeline::FullPipeline.to_html(text, { project: project, no_sourcepos: true }.merge(context))\n  end",
    "comment": "Since we're truncating nodes of an html document, actually use the full pipeline to generate full documents.",
    "label": "",
    "id": "7431"
  },
  {
    "raw_code": "def fast_auto_devops_stages\n    auto_devops_template = YAML.safe_load(File.read('lib/gitlab/ci/templates/Auto-DevOps.gitlab-ci.yml'))\n    auto_devops_template['stages']\n  end",
    "comment": "stubbing this method allows this spec file to use fast_spec_helper",
    "label": "",
    "id": "7432"
  },
  {
    "raw_code": "def fast_auto_devops_stages\n    auto_devops_template = YAML.safe_load(File.read('lib/gitlab/ci/templates/Auto-DevOps.gitlab-ci.yml'))\n    auto_devops_template['stages']\n  end",
    "comment": "stubbing this method allows this spec file to use fast_spec_helper",
    "label": "",
    "id": "7433"
  },
  {
    "raw_code": "def fast_auto_devops_stages\n    auto_devops_template = YAML.safe_load(File.read('lib/gitlab/ci/templates/Auto-DevOps.gitlab-ci.yml'))\n    auto_devops_template['stages']\n  end",
    "comment": "stubbing this method allows this spec file to use fast_spec_helper",
    "label": "",
    "id": "7434"
  },
  {
    "raw_code": "def fast_auto_devops_stages\n    auto_devops_template = YAML.safe_load(File.read('lib/gitlab/ci/templates/Auto-DevOps.gitlab-ci.yml'))\n    auto_devops_template['stages']\n  end",
    "comment": "stubbing this method allows this spec file to use fast_spec_helper",
    "label": "",
    "id": "7435"
  },
  {
    "raw_code": "def fast_auto_devops_stages\n    auto_devops_template = YAML.safe_load(File.read('lib/gitlab/ci/templates/Auto-DevOps.gitlab-ci.yml'))\n    auto_devops_template['stages']\n  end",
    "comment": "stubbing this method allows this spec file to use fast_spec_helper",
    "label": "",
    "id": "7436"
  },
  {
    "raw_code": "def present(obj)\n    described_class.new(obj).presented\n  end",
    "comment": "This mimics the behavior of the `Grape::Entity` serializer",
    "label": "",
    "id": "7437"
  },
  {
    "raw_code": "def current_request\n            nil\n          end",
    "comment": "It doesn't matter what this returns as long as the method is defined",
    "label": "",
    "id": "7438"
  },
  {
    "raw_code": "def namespace_inheritable(key, value = nil)\n            return unless key == :authentication\n\n            if value\n              @authentication = value\n            else\n              @authentication\n            end",
    "comment": "Spoof Grape's namespace inheritable system",
    "label": "",
    "id": "7439"
  },
  {
    "raw_code": "def task_executor_service\n              ^^^^^^^^^^^^^^^^^^^^^^^^^ Methods defined in rake tasks share the same namespace and can cause collisions. Please define it in a bounded contexts module in a separate Ruby file. For example, Search::RakeTask::<Namespace>. See https://github.com/rubocop/rubocop-rake/issues/42\n                Search::RakeTaskExecutorService.new(logger: stdout_logger)\n              end",
    "comment": "{'    '}",
    "label": "",
    "id": "7440"
  },
  {
    "raw_code": "def top_level_method\n          ^^^^^^^^^^^^^^^^^^^^ Methods defined in rake tasks share the same namespace and can cause collisions. Please define it in a bounded contexts module in a separate Ruby file. For example, Search::RakeTask::<Namespace>. See https://github.com/rubocop/rubocop-rake/issues/42\n            'top level'\n          end",
    "comment": "Top-level method - also gets an offense",
    "label": "",
    "id": "7441"
  },
  {
    "raw_code": "def inside_namespace_method\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^ Methods defined in rake tasks share the same namespace and can cause collisions. Please define it in a bounded contexts module in a separate Ruby file. For example, Search::RakeTask::<Namespace>. See https://github.com/rubocop/rubocop-rake/issues/42\n              'bad practice'\n            end",
    "comment": "Method inside namespace - offense",
    "label": "",
    "id": "7442"
  },
  {
    "raw_code": "def build_group(project, visibility: :public)\n    group = create_nested_group(visibility)\n    project.update!(namespace_id: group.id)\n\n    # Group member: global=disabled, group=watch\n    @g_watcher ||= create_user_with_notification(:watch, 'group_watcher', project.group)\n    @g_watcher.notification_settings_for(nil).disabled!\n\n    # Group member: global=watch, group=global\n    @g_global_watcher ||= create_global_setting_for(create(:user), :watch)\n    group.add_members([@g_watcher, @g_global_watcher], :maintainer)\n\n    group\n  end",
    "comment": "Users in the project's group but not part of project's team with different notification settings",
    "label": "",
    "id": "7443"
  },
  {
    "raw_code": "def execute_service(\n      source: :push,\n      before: '00000000',\n      after: project.commit.id,\n      ref: ref_name,\n      variables_attributes: nil,\n      merge_request: nil,\n      external_pull_request: nil,\n      push_options: nil,\n      source_sha: nil,\n      target_sha: nil,\n      partition_id: nil,\n      save_on_errors: true,\n      pipeline_creation_request: nil)\n      params = { ref: ref,\n                 before: before,\n                 after: after,\n                 variables_attributes: variables_attributes,\n                 push_options: push_options,\n                 source_sha: source_sha,\n                 target_sha: target_sha,\n                 partition_id: partition_id,\n                 pipeline_creation_request: pipeline_creation_request }\n\n      described_class.new(project, user, params).execute(source,\n        save_on_errors: save_on_errors,\n        merge_request: merge_request,\n        external_pull_request: external_pull_request) do |pipeline|\n        yield(pipeline) if block_given?\n      end",
    "comment": "rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "7444"
  },
  {
    "raw_code": "def build(name)\n    statuses.latest.find_by(name: name)\n  end",
    "comment": "The method name can be confusing because this can actually return both Ci::Build and Ci::Bridge",
    "label": "",
    "id": "7445"
  },
  {
    "raw_code": "def mock_play_jobs_during_processing(jobs)\n    collection = Ci::PipelineProcessing::AtomicProcessingService::StatusCollection.new(pipeline)\n\n    allow(collection).to receive(:stopped_job_names).and_return(jobs.map(&:name), [])\n\n    # Return the same collection object for every instance of StatusCollection\n    allow(Ci::PipelineProcessing::AtomicProcessingService::StatusCollection).to receive(:new)\n      .and_return(collection)\n  end",
    "comment": "A status collection is initialized at the start of pipeline processing and then again at the end of processing.  Here we simulate \"playing\" the given jobs during pipeline processing by stubbing stopped_job_names so that they appear to have been stopped at the beginning of processing and then later changed to alive status at the end.",
    "label": "",
    "id": "7446"
  },
  {
    "raw_code": "def nullify_in_batches_regexp(table, column, user, batch_size: 100)\n        %r{^UPDATE \"#{table}\" SET \"#{column}\" = NULL WHERE \\(\"#{table}\".\"id\"\\) IN \\(SELECT \"#{table}\".\"id\" FROM \"#{table}\" WHERE \"#{table}\".\"#{column}\" = #{user.id} LIMIT #{batch_size}\\)}\n      end",
    "comment": "rubocop:disable Layout/LineLength",
    "label": "",
    "id": "7447"
  },
  {
    "raw_code": "def delete_all_in_batches_regexp(table, column, user, batch_size: 1000)\n        %r{^DELETE FROM \"#{table}\" WHERE \\(\"#{table}\".\"id\"\\) IN \\(SELECT \"#{table}\".\"id\" FROM \"#{table}\" WHERE \"#{table}\".\"#{column}\" = #{user.id} LIMIT #{batch_size}\\)}\n      end",
    "comment": "rubocop:enable Layout/LineLength rubocop:disable Layout/LineLength -- long regex",
    "label": "",
    "id": "7448"
  },
  {
    "raw_code": "def touch_files(files_to_touch = nil)\n    design_files = files_to_touch || files\n\n    design_files.each do |f|\n      f.tempfile.write(SecureRandom.random_bytes)\n    end",
    "comment": "Randomly alter the content of files. This allows the files to be updated by the service, as unmodified files are rejected.",
    "label": "",
    "id": "7449"
  },
  {
    "raw_code": "def run_service(delenda = nil)\n    service = described_class.new(project, user, issue: issue, designs: delenda || designs)\n    service.execute\n  end",
    "comment": "Defined as a method so that the response is not cached. We also construct a new service executor each time to avoid the intermediate cached values it constructs during its execution.",
    "label": "",
    "id": "7450"
  },
  {
    "raw_code": "def current_user\n      @feat.user\n    end",
    "comment": "Fake a `current_user` helper",
    "label": "",
    "id": "7451"
  },
  {
    "raw_code": "def write_markdown(filename = 'markdown_spec')\n    File.open(Rails.root.join(\"tmp/capybara/#{filename}.html\"), 'w') do |file|\n      file.puts @html\n    end",
    "comment": "Sometimes it can be useful to see the parsed output of the Markdown document for debugging. Call this method to write the output to `tmp/capybara/<filename>.html`.",
    "label": "",
    "id": "7452"
  },
  {
    "raw_code": "def current_user\n    @feat.user\n  end",
    "comment": "Fake a `current_user` helper",
    "label": "",
    "id": "7453"
  },
  {
    "raw_code": "def remove_branch_with_hooks(project, user, branch)\n      params = {\n        change: {\n          oldrev: project.commit(branch).id,\n          newrev: Gitlab::Git::SHA1_BLANK_SHA,\n          ref: \"refs/heads/#{branch}\"\n        }\n      }\n\n      yield\n\n      Git::BranchPushService.new(project, user, params).execute\n    end",
    "comment": " This is a workaround for problem described in #24543 ",
    "label": "",
    "id": "7454"
  },
  {
    "raw_code": "def skip_auth_posts\n        object[:skip_auth_posts].keep_if do |post|\n          Ability.allowed?(context[:current_user], :read_post, post)\n        end",
    "comment": "make this act as a resolver",
    "label": "",
    "id": "7455"
  },
  {
    "raw_code": "def skip_auth_posts_collection\n        ::Gitlab::Graphql::Lazy.new do\n          object[:skip_auth_posts_collection].keep_if do |post|\n            Ability.allowed?(context[:current_user], :read_post, post)\n          end",
    "comment": "make this act as a resolver",
    "label": "",
    "id": "7456"
  },
  {
    "raw_code": "def with_auth_posts_collection\n        ::Gitlab::Graphql::Lazy.new do\n          Gitlab::Graphql::ExternallyPaginatedArray.new(nil, nil, *object[:with_auth_posts_collection])\n        end",
    "comment": "make this act as a resolver",
    "label": "",
    "id": "7457"
  },
  {
    "raw_code": "def ci_partitioned_foreign_key?(foreign_key)\n    target = foreign_key.to_table.split('.').last\n    schema = Gitlab::Database::GitlabSchema.table_schema!(target)\n    schema == :gitlab_ci &&\n      Array.wrap(foreign_key.column).many? &&\n      foreign_key.column.first.end_with?('partition_id')\n  end",
    "comment": "For partitioned CI references we do not require a composite index starting with `partition_id` as each partition only contains records with a single `partition_id`. As such the index on the other id in the foreign key will be sufficient.",
    "label": "",
    "id": "7458"
  },
  {
    "raw_code": "def object_metadata_errors(title, field, objects)\n    lines = objects.map do |object_name|\n      <<~EOM\n        #{object_metadata_file(object_name)}\n          #{metadata[object_name][field]}\n      EOM\n    end",
    "comment": "rubocop:disable Naming/HeredocDelimiterNaming",
    "label": "",
    "id": "7459"
  },
  {
    "raw_code": "def ref_param_name\n              'FOO_BAR_BRANCH'\n            end",
    "comment": "Must be overridden",
    "label": "",
    "id": "7460"
  },
  {
    "raw_code": "def trigger_web_hooks\n    params = { merge_request: { description: FFaker::Lorem.sentence } }\n    put project_merge_request_path(project, merge_request), params: params, headers: headers\n  end",
    "comment": "Trigger a change to the merge request to fire the webhooks.",
    "label": "",
    "id": "7461"
  },
  {
    "raw_code": "def send_request\n      get(\n        discussions_namespace_project_merge_request_path(namespace_id: project.namespace, project_id: project, id: merge_request.iid),\n        headers: { 'If-None-Match' => @etag }\n      )\n\n      @etag = response.etag\n    end",
    "comment": "rubocop:disable RSpec/InstanceVariable",
    "label": "",
    "id": "7462"
  },
  {
    "raw_code": "def add_projects_to_group(group, share_with: nil)\n      projects = {\n        public: create(:project, :public, namespace: group),\n        internal: create(:project, :internal, namespace: group),\n        private: create(:project, :private,  namespace: group)\n      }\n\n      if share_with\n        create(:project_group_link, project: projects[:public],   group: share_with)\n        create(:project_group_link, project: projects[:internal], group: share_with)\n        create(:project_group_link, project: projects[:private],  group: share_with)\n      end",
    "comment": "Given a group, create one project for each visibility level  group      - Group to add projects to share_with - If provided, each project will be shared with this Group  Returns a Hash of visibility_level => Project pairs",
    "label": "",
    "id": "7463"
  },
  {
    "raw_code": "def authorize_permissions_table\n      false | :developer  | :private | true  | :job_token             | :unauthorized\n      false | :developer  | :private | true  | :personal_access_token | :unauthorized\n      false | :developer  | :public  | true  | :job_token             | :unauthorized\n      false | :developer  | :public  | true  | :personal_access_token | :unauthorized\n      false | :guest      | :private | true  | :job_token             | :unauthorized\n      false | :guest      | :private | true  | :personal_access_token | :unauthorized\n      false | :guest      | :public  | true  | :job_token             | :unauthorized\n      false | :guest      | :public  | true  | :personal_access_token | :unauthorized\n      true  | :anonymous  | :private | false | :personal_access_token | :unauthorized\n      true  | :anonymous  | :public  | false | :personal_access_token | :unauthorized\n      true  | :developer  | :private | true  | :job_token             | :success\n      true  | :developer  | :private | true  | :personal_access_token | :success\n      true  | :developer  | :public  | true  | :job_token             | :success\n      true  | :developer  | :public  | true  | :personal_access_token | :success\n      true  | :guest      | :private | true  | :job_token             | :forbidden\n      true  | :guest      | :private | true  | :personal_access_token | :forbidden\n      true  | :guest      | :public  | true  | :job_token             | :forbidden\n      true  | :guest      | :public  | true  | :personal_access_token | :forbidden\n      true  | :reporter   | :private | true  | :job_token             | :forbidden\n      true  | :reporter   | :private | true  | :personal_access_token | :forbidden\n      true  | :reporter   | :public  | true  | :job_token             | :forbidden\n      true  | :reporter   | :public  | true  | :personal_access_token | :forbidden\n    end",
    "comment": "rubocop:disable Metrics/AbcSize :valid_token, :user_role, :visibility, :member, :token_type, :expected_status",
    "label": "",
    "id": "7464"
  },
  {
    "raw_code": "def download_permissions_tables\n      false |  :developer  | :private | true  | :job_token             | :unauthorized\n      false |  :developer  | :private | true  | :personal_access_token | :unauthorized\n      false |  :developer  | :public  | true  | :job_token             | :unauthorized\n      false |  :developer  | :public  | true  | :personal_access_token | :unauthorized\n      false |  :guest      | :private | true  | :job_token             | :unauthorized\n      false |  :guest      | :private | true  | :personal_access_token | :unauthorized\n      false |  :guest      | :public  | true  | :job_token             | :unauthorized\n      false |  :guest      | :public  | true  | :personal_access_token | :unauthorized\n      true  |  :anonymous  | :private | false | :personal_access_token | :not_found\n      true  |  :anonymous  | :public  | false | :personal_access_token | :success\n      true  |  :developer  | :private | true  | :job_token             | :success\n      true  |  :developer  | :private | true  | :personal_access_token | :success\n      true  |  :developer  | :public  | true  | :job_token             | :success\n      true  |  :developer  | :public  | true  | :personal_access_token | :success\n      true  |  :guest      | :private | true  | :job_token             | :success\n      true  |  :guest      | :private | true  | :personal_access_token | :success\n      true  |  :guest      | :public  | true  | :job_token             | :success\n      true  |  :guest      | :public  | true  | :personal_access_token | :success\n      true  |  :reporter   | :private | true  | :job_token             | :success\n      true  |  :reporter   | :private | true  | :personal_access_token | :success\n      true  |  :reporter   | :public  | true  | :job_token             | :success\n      true  |  :reporter   | :public  | true  | :personal_access_token | :success\n    end",
    "comment": "::valid_token, :user_role, visibility, :member, :token_type, :expected_status",
    "label": "",
    "id": "7465"
  },
  {
    "raw_code": "def commit_messages(response)\n          Gitlab::Json.parse(response.body)[\"commits\"].map do |commit|\n            commit[\"message\"]\n          end",
    "comment": "Parse the commits ourselves because json_response is cached",
    "label": "",
    "id": "7466"
  },
  {
    "raw_code": "def design_nodes\n        design_response.map do |response|\n          response['node']\n        end",
    "comment": "Filters just design nodes from the larger `design_response`",
    "label": "",
    "id": "7467"
  },
  {
    "raw_code": "def version_nodes\n        design_response.map do |response|\n          response.dig('node', 'versions', 'edges')\n        end",
    "comment": "Filters just version nodes from the larger `design_response`",
    "label": "",
    "id": "7468"
  },
  {
    "raw_code": "def simulate_puma_worker\n        # Called in https://github.com/rails/rails/blob/6-1-stable/activerecord/lib/active_record/connection_adapters/pool_config.rb#L73\n        ActiveRecord::ConnectionAdapters::PoolConfig.discard_pools!\n\n        # Called in config/puma.rb\n        Gitlab::Cluster::LifecycleEvents.do_worker_start\n\n        yield\n      end",
    "comment": "We tried using Process.fork for a more realistic simulation but run into bugs where GPRC cannot be used before forking processes. See https://gitlab.com/gitlab-org/gitlab/-/issues/333184#note_1081658113",
    "label": "",
    "id": "7469"
  },
  {
    "raw_code": "def permissions_abilities(role)\n      case role\n      when :admin\n        if project_visibility == :private || access_level == ProjectFeature::PRIVATE\n          maintainer_operations_permissions - admin_excluded_permissions\n        else\n          maintainer_operations_permissions\n        end",
    "comment": "Overrides `permissions_abilities` defined below to be suitable for container_image policies",
    "label": "",
    "id": "7470"
  },
  {
    "raw_code": "def permission_table_for_guest\n    :read_merge_request            | true\n    :create_todo                   | true\n    :create_note                   | true\n    :update_subscription           | true\n    :create_merge_request_in       | true\n    :create_merge_request_from     | false\n    :approve_merge_request         | false\n    :update_merge_request          | false\n    :reset_merge_request_approvals | false\n    :mark_note_as_internal         | false\n  end",
    "comment": ":policy, :is_allowed",
    "label": "",
    "id": "7471"
  },
  {
    "raw_code": "def permission_table_for_reporter\n    :read_merge_request            | true\n    :create_todo                   | true\n    :create_note                   | true\n    :update_subscription           | true\n    :create_merge_request_in       | true\n    :create_merge_request_from     | false\n    :approve_merge_request         | false\n    :update_merge_request          | false\n    :reset_merge_request_approvals | false\n    :mark_note_as_internal         | true\n  end",
    "comment": ":policy, :is_allowed",
    "label": "",
    "id": "7472"
  },
  {
    "raw_code": "def permission_table_for_planner(public_merge_request: false)\n    :read_merge_request            | true\n    :create_todo                   | true\n    :create_note                   | true\n    :update_subscription           | true\n    :create_merge_request_in       | public_merge_request\n    :create_merge_request_from     | false\n    :approve_merge_request         | false\n    :update_merge_request          | false\n    :reset_merge_request_approvals | false\n    :mark_note_as_internal         | true\n  end",
    "comment": ":policy, :is_allowed",
    "label": "",
    "id": "7473"
  },
  {
    "raw_code": "def self.name\n                'Gitlab::TestUsageWorker'\n              end",
    "comment": "`include ApplicationWorker` raises errors for unnamed classes",
    "label": "",
    "id": "7474"
  },
  {
    "raw_code": "def norm(query)\n    query.tr(\"\\n\", ' ').gsub(/\\s+/, ' ').strip\n  end",
    "comment": "Normalize irrelevant whitespace to make comparison easier",
    "label": "",
    "id": "7475"
  },
  {
    "raw_code": "def dynamic_segment\n        File.join(model.class.underscore, model.id.to_s)\n      end",
    "comment": "user/:id",
    "label": "",
    "id": "7476"
  },
  {
    "raw_code": "def microseconds(time)\n    (time.to_i * 1_000_000) + time.usec\n  end",
    "comment": "Convert a time to an integer number of microseconds",
    "label": "",
    "id": "7477"
  },
  {
    "raw_code": "def etag(action)\n        ActionDispatch::TestResponse.new.send(:generate_weak_etag, [action.cache_key])\n      end",
    "comment": "The design images generated by Factorybot are identical, so refer to the `ETag` header, which is uniquely generated from the Action (the record that represents the design at a specific version), to verify that the correct file is being returned.",
    "label": "",
    "id": "7478"
  },
  {
    "raw_code": "def load_partitioned_mrdf(mrd_id)\n    ActiveRecord::Base.connection.execute(\n      \"SELECT * FROM merge_request_diff_files_99208b8fac WHERE merge_request_diff_id = #{mrd_id}\"\n    )\n  end",
    "comment": "rubocop:disable Database/MultipleDatabases -- This is a test for a partitioned table, which doesn't have an ActiveRecord model",
    "label": "",
    "id": "7479"
  },
  {
    "raw_code": "def load_schema!\n      expect(model).to receive(:load_schema!).and_call_original\n\n      model.new\n    end",
    "comment": "load_schema! is not a documented class method, so use a documented method that we know will call load_schema!",
    "label": "",
    "id": "7480"
  },
  {
    "raw_code": "def save!\n        # noop\n        changes_applied\n        true\n      end",
    "comment": "Stub ActiveRecord::Persistence behavior",
    "label": "",
    "id": "7481"
  },
  {
    "raw_code": "def reload\n        # noop\n        clear_changes_information\n        self\n      end",
    "comment": "Stub both ActiveModel::Dirty and ActiveRecord::Persistence behavior",
    "label": "",
    "id": "7482"
  },
  {
    "raw_code": "def name=(val)\n        name_will_change! unless val == @name\n        @name = val\n      end",
    "comment": "Stub ActiveModel::Dirty behavior",
    "label": "",
    "id": "7483"
  },
  {
    "raw_code": "def build_filter_text(pipeline, initial_text)\n    filter_source = {}\n    input_text    = initial_text\n    result        = nil\n\n    pipeline.filters.each do |filter_klass|\n      # store inputs for current filter_klass\n      filter_source[filter_klass] = { input_text: input_text, input_result: result }\n\n      filter = filter_klass.new(input_text, context, result)\n      output = filter.call\n\n      # save these for the next filter_klass\n      input_text = output\n      result = filter.result\n    end",
    "comment": "build up the source text for each filter",
    "label": "",
    "id": "7484"
  },
  {
    "raw_code": "def current_user\n    feature.user\n  end",
    "comment": "Fake a `current_user` helper",
    "label": "",
    "id": "7485"
  },
  {
    "raw_code": "def stub_languages_translation_percentage(list = {})\n    return if list.blank?\n\n    allow(Gitlab::I18n)\n      .to receive(:percentage_translated_for)\n      .and_wrap_original do |_original, code|\n        list.with_indifferent_access[code].to_i\n      end",
    "comment": "Stubs the translation percentage of the i18n languages - When a `blank?` list is given no stubbing is done; - When the list is not empty, the languages in the list are stubbed with the given values, any other language will have the translation percent set to 0;",
    "label": "",
    "id": "7486"
  },
  {
    "raw_code": "def ability_found?(policy, ability)\n        # NilPolicy has no abilities. Ignore it.\n        return true if policy.is_a?(DeclarativePolicy::NilPolicy)\n\n        # Search in current policy first\n        return true if policy.class.ability_map.map.key?(ability)\n\n        # Search recursively in all delegations otherwise.\n        # This is potentially slow.\n        # Stolen from:\n        # https://gitlab.com/gitlab-org/ruby/gems/declarative-policy/-/blob/d691e/lib/declarative_policy/base.rb#L360-369\n        policy.class.delegations.any? do |_, block|\n          new_subject = policy.instance_eval(&block)\n          new_policy = policy.policy_for(new_subject)\n\n          ability_found?(new_policy, ability)\n        end",
    "comment": "Use Policy#has_ability? instead after it has been accepted and released. See https://gitlab.com/gitlab-org/ruby/gems/declarative-policy/-/issues/25",
    "label": "",
    "id": "7487"
  },
  {
    "raw_code": "def example_group_started(notification)\n        order = notification.group.metadata[:order]\n\n        output.puts \"  # order #{order}\" if order\n      end",
    "comment": "See https://github.com/rspec/rspec-core/blob/v3.11.0/lib/rspec/core/formatters/documentation_formatter.rb#L24-L29",
    "label": "",
    "id": "7488"
  },
  {
    "raw_code": "def self.add_formatter_to(config)\n        documentation_formatter = config.formatters\n          .find { |formatter| formatter.is_a?(RSpec::Core::Formatters::DocumentationFormatter) }\n        return unless documentation_formatter\n\n        config.add_formatter self, documentation_formatter.output\n      end",
    "comment": "Print order information only with `--format documentation`.",
    "label": "",
    "id": "7489"
  },
  {
    "raw_code": "def webmock_enable_with_http_connect_on_start!\n  webmock_enable!(net_http_connect_on_start: true)\nend",
    "comment": "This prevents Selenium/WebMock from spawning thousands of connections while waiting for an element to appear via Capybara's find: https://github.com/teamcapybara/capybara/issues/2322#issuecomment-619321520",
    "label": "",
    "id": "7490"
  },
  {
    "raw_code": "def stub_member_access_level(object, **access_levels)\n    expectation = case object\n                  when Project\n                    ->(user) { expect(object.team).to receive(:max_member_access).with(user.id) }\n                  when Group\n                    ->(user) { expect(object).to receive(:max_member_access_for_user).with(user) }\n                  else\n                    raise ArgumentError,\n                      \"Stubbing member access level unsupported for #{object.inspect} (#{object.class})\"\n                  end",
    "comment": "Stubs access level of a member of +object+.  The following types are supported: * `Project` - stubs `project.team.max_member_access(user.id)` * `Group` - stubs `group.max_member_access_for_user(user)`  @example  stub_member_access_level(project, maintainer: user) project.team.max_member_access(user.id) # => Gitlab::Access::MAINTAINER  stub_member_access_level(group, developer: user) group.max_member_access_for_user(user) # => Gitlab::Access::DEVELOPER  stub_member_access_level(project, reporter: user, guest: [guest1, guest2]) project.team.max_member_access(user.id) # => Gitlab::Access::REPORTER project.team.max_member_access(guests.first.id) # => Gitlab::Access::GUEST project.team.max_member_access(guests.last.id) # => Gitlab::Access::GUEST  @param object [Project, Group] Object to be stubbed. @param access_levels [Hash<Symbol, User>, Hash<Symbol, [User]>] Map of access level to users",
    "label": "",
    "id": "7491"
  },
  {
    "raw_code": "def fixture_file_upload(*args, **kwargs)\n    Rack::Test::UploadedFile.new(*args, **kwargs)\n  end",
    "comment": "FactoryBot doesn't allow yet to add a helper that can be used in factories While the fixture_file_upload helper is reasonable to be used there:  https://github.com/thoughtbot/factory_bot/issues/564#issuecomment-389491577",
    "label": "",
    "id": "7492"
  },
  {
    "raw_code": "def self.unique_file(name)\n    upload = new(Rack::Test::UploadedFile.new(\"spec/fixtures/#{name}\"))\n    ext = File.extname(name)\n    new_name = File.basename(FactoryBot.generate(:filename), '.*')\n    upload.original_filename = new_name + ext\n\n    upload\n  end",
    "comment": "Get a fixture file with a new unique name, and the same extension",
    "label": "",
    "id": "7493"
  },
  {
    "raw_code": "def update_column_regex(column)\n  /UPDATE.+SET.+#{column}[^=*]=.+FROM.*/m\nend",
    "comment": "frozen_string_literal: true",
    "label": "",
    "id": "7494"
  },
  {
    "raw_code": "def validate_traversal_ids(current_node, parent = null_node)\n    expect(current_node.traversal_ids).to be_present\n    expect(current_node.traversal_ids).to eq parent.traversal_ids + [current_node.id]\n\n    current_node.children.each do |child|\n      validate_traversal_ids(child, current_node)\n    end",
    "comment": "Walk the tree to assert that the current_node's traversal_id is always present and equal to it's parent's traversal_ids plus it's own ID.",
    "label": "",
    "id": "7495"
  },
  {
    "raw_code": "def debug_print_outstanding_lfk_records\n    Gitlab::Database::SharedModel.using_connection(parent.connection) do\n      lfk_deleted_records = []\n      Gitlab::Database::LooseForeignKeys.definitions_by_table.each_key do |table|\n        fully_qualified_table_name = \"#{parent.connection.current_schema}.#{table}\"\n        lfk_deleted_records << LooseForeignKeys::DeletedRecord.load_batch_for_table(fully_qualified_table_name, 1000)\n      end",
    "comment": "+ Debug code to be removed.",
    "label": "",
    "id": "7496"
  },
  {
    "raw_code": "def any_outstanding_lfk_records?(parent)\n    Gitlab::Database::SharedModel.using_connection(parent.connection) do\n      Gitlab::Database::LooseForeignKeys.definitions_by_table.each_key do |table|\n        fully_qualified_table_name = \"#{parent.connection.current_schema}.#{table}\"\n        return true if LooseForeignKeys::DeletedRecord.load_batch_for_table(fully_qualified_table_name, 1000).any?\n      end",
    "comment": "- Debug code to be removed. rubocop:disable Cop/AvoidReturnFromBlocks -- Intentional Short Circuit",
    "label": "",
    "id": "7497"
  },
  {
    "raw_code": "def parse_usage_ping_keys(object, key_path = [])\n    if object.is_a?(Hash) && !object_with_schema?(key_path.join('.'))\n      object.each_with_object([]) do |(key, value), result|\n        result.append parse_usage_ping_keys(value, key_path + [key])\n      end",
    "comment": "Recursively traverse nested Hash of a generated Usage Ping to return an Array of key paths in the dotted format used in metric definition YAML files, e.g.: 'count.category.metric_name'",
    "label": "",
    "id": "7498"
  },
  {
    "raw_code": "def raw_repo_without_container(repository)\n  Gitlab::Git::Repository.new(\n    repository.shard,\n    \"#{repository.disk_path}.git\",\n    repository.repo_type.identifier_for_container(repository.container),\n    repository.container.full_path\n  )\nend",
    "comment": "frozen_string_literal: true",
    "label": "",
    "id": "7499"
  },
  {
    "raw_code": "def stub_email_setting(key_value_pairs)\n  case setting_name\n  when :incoming_email\n    stub_incoming_email_setting(key_value_pairs)\n  when :service_desk_email\n    stub_service_desk_email_setting(key_value_pairs)\n  end",
    "comment": "frozen_string_literal: true Set the particular setting as a key-value pair Setting method is different depending on klass and must be defined in the calling spec",
    "label": "",
    "id": "7500"
  },
  {
    "raw_code": "def process_changes\n    base_sha = current_sha\n    yield\n    create_service(base_sha).execute\n  end",
    "comment": "In order to construct the correct GitPostReceive object that represents the changes we are applying, we need to describe the changes between old-ref and new-ref. Old ref (the base sha) we have to capture before we perform any changes. Once the changes have been applied, we can execute the service to process them.",
    "label": "",
    "id": "7501"
  },
  {
    "raw_code": "def write_new_page\n    generate(:wiki_page_title).tap do |t|\n      repository.create_file(\n        current_user, ::Wiki.sluggified_full_path(t, 'md'), 'Hello',\n        **commit_details\n      )\n    end",
    "comment": "It is important not to re-use the WikiPage services here, since they create events - these helper methods below are intended to simulate actions on the repo that have not gone through our services.",
    "label": "",
    "id": "7502"
  },
  {
    "raw_code": "def write_non_page\n    params = {\n      file_name: 'attachment.log',\n      file_content: 'some stuff',\n      branch_name: 'master'\n    }\n    ::Wikis::CreateAttachmentService.new(container: wiki.container, current_user: current_user, params: params).execute\n  end",
    "comment": "We write something to the wiki-repo that is not a page - as, for example, an attachment. This will appear as a raw-diff change, but wiki.find_page will return nil.",
    "label": "",
    "id": "7503"
  },
  {
    "raw_code": "def nested_internal_id_query(root_field, parent, field, args, selection: :iid)\n      graphql_query_for(root_field, { full_path: parent.full_path },\n        query_nodes(field, selection, args: args, include_pagination_info: true)\n      )\n    end",
    "comment": "Convenience helper for the large number of queries defined as a projection from some root value indexed by full_path to a collection of objects with IID",
    "label": "",
    "id": "7504"
  },
  {
    "raw_code": "def run_query(query)\n    post_graphql(query, current_user: current_user)\n    expect(graphql_errors).not_to be_present\n  end",
    "comment": "Run a known good query with the current user",
    "label": "",
    "id": "7505"
  },
  {
    "raw_code": "def get_issue\n  json_response.is_a?(Array) ? json_response.detect { |issue| issue['id'] == target_issue.id } : json_response\nend",
    "comment": "frozen_string_literal: true",
    "label": "",
    "id": "7506"
  },
  {
    "raw_code": "def exclude_field?(integration, field)\n    integration.is_a?(Integrations::Jira) && %w[\n      jira_auth_type jira_check_enabled jira_exists_check_enabled\n      jira_assignee_check_enabled jira_status_check_enabled jira_allowed_statuses_as_string\n    ].include?(field[:name])\n  end",
    "comment": "Fields that have specific handling on the frontend",
    "label": "",
    "id": "7507"
  },
  {
    "raw_code": "def editable?(integration)\n    integration.editable?\n  end",
    "comment": "Some integrations are only editable when active, otherwise their fields are disabled",
    "label": "",
    "id": "7508"
  },
  {
    "raw_code": "def assign_session_token(provider)\n  session[:\"#{provider}_access_token\"] = 'asdasd12345'\nend",
    "comment": "frozen_string_literal: true Specifications for behavior common to all objects with an email attribute. Takes a list of email-format attributes and requires: - subject { \"the object with a attribute= setter\"  } Note: You have access to `email_value` which is the email address value being currently tested).",
    "label": "",
    "id": "7509"
  },
  {
    "raw_code": "def create_visit_records\n    [\n      [1, Time.current],\n\n      [2, 1.week.ago],\n      [2, 1.week.ago],\n\n      [2, 2.weeks.ago],\n      [3, 2.weeks.ago],\n      [3, 2.weeks.ago],\n      [4, 2.weeks.ago],\n      [5, 2.weeks.ago],\n      [6, 2.weeks.ago],\n      [7, 2.weeks.ago],\n      [8, 2.weeks.ago],\n\n      [1, 3.weeks.ago],\n      [1, 3.weeks.ago],\n      [3, 3.weeks.ago],\n      [3, 3.weeks.ago],\n\n      [1, 4.weeks.ago],\n      [2, 4.weeks.ago],\n      [2, 4.weeks.ago],\n\n      [3, 7.weeks.ago],\n      [3, 7.weeks.ago],\n\n      [1, 8.weeks.ago],\n      [1, 8.weeks.ago],\n      [1, 8.weeks.ago],\n      [1, 8.weeks.ago],\n\n      [2, 9.weeks.ago],\n      [2, 9.weeks.ago],\n      [2, 9.weeks.ago]\n    ].each do |id, datetime|\n      described_class.create!(entity_id: id, user_id: user.id, visited_at: datetime)\n    end",
    "comment": "rubocop: disable Metrics/AbcSize -- Despite being long, this method is quite straightforward. Splitting it in smaller chunks would likely harm readability more than anything.",
    "label": "",
    "id": "7510"
  },
  {
    "raw_code": "def call_before_action\n        before_action.call if before_action\n      end",
    "comment": "Pass a block as :before_action if issuable state needs to be changed before the quick action is executed.",
    "label": "",
    "id": "7511"
  },
  {
    "raw_code": "def json_response\n    Gitlab::Json.parse(response.body)\n  end",
    "comment": "This should _not_ be a let block, otherwise it is memoized, and breaks the contract of #response, which is to always return the response of the last request. Request specs may do multiple requests in a single example, and making this a let would cause stale responses to be returned.",
    "label": "",
    "id": "7512"
  },
  {
    "raw_code": "def retry_in(klass, time, args = 0)\n    message = Gitlab::Json.generate(\n      'class' => klass.name,\n      'args' => [args],\n      'retry' => true\n    )\n\n    allow(klass).to receive(:sidekiq_retry_in_block).and_return(proc { time.to_i })\n\n    begin\n      Sidekiq::JobRetry.new(Sidekiq).local(klass, message, klass.queue) { raise 'boom' }\n    rescue Sidekiq::JobRetry::Handled\n      # Sidekiq scheduled the retry\n    end",
    "comment": "Try to mimic as closely as possible what Sidekiq will actually do to retry a job.",
    "label": "",
    "id": "7513"
  },
  {
    "raw_code": "def sender(author_id, params = {})\n        author_id\n      end",
    "comment": "this method is implemented in Notify class, we don't need to test it",
    "label": "",
    "id": "7514"
  },
  {
    "raw_code": "def mail_new_thread(issue, options)\n        # we need to rewrite this in order to look up templates in the correct directory\n        self.class.mailer_name = 'notify'\n\n        # this is needed for default layout\n        @unsubscribe_url = 'http://unsubscribe.example.com'\n\n        mail(options)\n      end",
    "comment": "this method is implemented in Notify class  We do not need to test the Notify method, it is already tested in notify_spec",
    "label": "",
    "id": "7515"
  },
  {
    "raw_code": "def permission_table_for_reporter_feature_access\n    :public   | :enabled  | :admin      | true  | 1\n    :public   | :enabled  | :admin      | false | 1\n    :public   | :enabled  | :reporter   | nil   | 1\n    :public   | :enabled  | :guest      | nil   | 1\n    :public   | :enabled  | :non_member | nil   | 1\n    :public   | :enabled  | :anonymous  | nil   | 1\n\n    :public   | :private  | :admin      | true  | 1\n    :public   | :private  | :admin      | false | 0\n    :public   | :private  | :reporter   | nil   | 1\n    :public   | :private  | :guest      | nil   | 0\n    :public   | :private  | :non_member | nil   | 0\n    :public   | :private  | :anonymous  | nil   | 0\n\n    :public   | :disabled | :reporter   | nil   | 0\n    :public   | :disabled | :guest      | nil   | 0\n    :public   | :disabled | :non_member | nil   | 0\n    :public   | :disabled | :anonymous  | nil   | 0\n\n    :internal | :enabled  | :admin      | true  | 1\n    :internal | :enabled  | :admin      | false | 1\n    :internal | :enabled  | :reporter   | nil   | 1\n    :internal | :enabled  | :guest      | nil   | 1\n    :internal | :enabled  | :non_member | nil   | 1\n    :internal | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :private  | :admin      | true  | 1\n    :internal | :private  | :admin      | false | 0\n    :internal | :private  | :reporter   | nil   | 1\n    :internal | :private  | :guest      | nil   | 0\n    :internal | :private  | :non_member | nil   | 0\n    :internal | :private  | :anonymous  | nil   | 0\n\n    :internal | :disabled | :reporter   | nil   | 0\n    :internal | :disabled | :guest      | nil   | 0\n    :internal | :disabled | :non_member | nil   | 0\n    :internal | :disabled | :anonymous  | nil   | 0\n\n    :private  | :private  | :admin      | true  | 1\n    :private  | :private  | :admin      | false | 0\n    :private  | :private  | :reporter   | nil   | 1\n    :private  | :private  | :guest      | nil   | 0\n    :private  | :private  | :non_member | nil   | 0\n    :private  | :private  | :anonymous  | nil   | 0\n\n    :private  | :disabled | :reporter   | nil   | 0\n    :private  | :disabled | :guest      | nil   | 0\n    :private  | :disabled | :non_member | nil   | 0\n    :private  | :disabled | :anonymous  | nil   | 0\n  end",
    "comment": "rubocop:disable Metrics/AbcSize project_level, :feature_access_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7516"
  },
  {
    "raw_code": "def permission_table_for_guest_feature_access\n    :public   | :enabled  | :admin      | true  | 1\n    :public   | :enabled  | :admin      | false | 1\n    :public   | :enabled  | :reporter   | nil   | 1\n    :public   | :enabled  | :guest      | nil   | 1\n    :public   | :enabled  | :non_member | nil   | 1\n    :public   | :enabled  | :anonymous  | nil   | 1\n\n    :public   | :private  | :admin      | true  | 1\n    :public   | :private  | :admin      | false | 0\n    :public   | :private  | :reporter   | nil   | 1\n    :public   | :private  | :guest      | nil   | 1\n    :public   | :private  | :non_member | nil   | 0\n    :public   | :private  | :anonymous  | nil   | 0\n\n    :public   | :disabled | :reporter   | nil   | 0\n    :public   | :disabled | :guest      | nil   | 0\n    :public   | :disabled | :non_member | nil   | 0\n    :public   | :disabled | :anonymous  | nil   | 0\n\n    :internal | :enabled  | :admin      | true  | 1\n    :internal | :enabled  | :admin      | false | 1\n    :internal | :enabled  | :reporter   | nil   | 1\n    :internal | :enabled  | :guest      | nil   | 1\n    :internal | :enabled  | :non_member | nil   | 1\n    :internal | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :private  | :admin      | true  | 1\n    :internal | :private  | :admin      | false | 0\n    :internal | :private  | :reporter   | nil   | 1\n    :internal | :private  | :guest      | nil   | 1\n    :internal | :private  | :non_member | nil   | 0\n    :internal | :private  | :anonymous  | nil   | 0\n\n    :internal | :disabled | :reporter   | nil   | 0\n    :internal | :disabled | :guest      | nil   | 0\n    :internal | :disabled | :non_member | nil   | 0\n    :internal | :disabled | :anonymous  | nil   | 0\n\n    :private  | :private  | :admin      | true  | 1\n    :private  | :private  | :admin      | false | 0\n    :private  | :private  | :reporter   | nil   | 1\n    :private  | :private  | :guest      | nil   | 1\n    :private  | :private  | :non_member | nil   | 0\n    :private  | :private  | :anonymous  | nil   | 0\n\n    :private  | :disabled | :reporter   | nil   | 0\n    :private  | :disabled | :guest      | nil   | 0\n    :private  | :disabled | :non_member | nil   | 0\n    :private  | :disabled | :anonymous  | nil   | 0\n  end",
    "comment": "project_level, :feature_access_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7517"
  },
  {
    "raw_code": "def permission_table_for_notes_feature_access\n    :public   | :enabled  | :admin      | true  | 2\n    :public   | :enabled  | :admin      | false | 1\n    :public   | :enabled  | :reporter   | nil   | 2\n    :public   | :enabled  | :guest      | nil   | 1\n    :public   | :enabled  | :non_member | nil   | 1\n    :public   | :enabled  | :anonymous  | nil   | 1\n\n    :public   | :private  | :admin      | true  | 2\n    :public   | :private  | :admin      | false | 0\n    :public   | :private  | :reporter   | nil   | 2\n    :public   | :private  | :guest      | nil   | 1\n    :public   | :private  | :non_member | nil   | 0\n    :public   | :private  | :anonymous  | nil   | 0\n\n    :public   | :disabled | :reporter   | nil   | 0\n    :public   | :disabled | :guest      | nil   | 0\n    :public   | :disabled | :non_member | nil   | 0\n    :public   | :disabled | :anonymous  | nil   | 0\n\n    :internal | :enabled  | :admin      | true  | 2\n    :internal | :enabled  | :admin      | false | 1\n    :internal | :enabled  | :reporter   | nil   | 2\n    :internal | :enabled  | :guest      | nil   | 1\n    :internal | :enabled  | :non_member | nil   | 1\n    :internal | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :private  | :admin      | true  | 2\n    :internal | :private  | :admin      | false | 0\n    :internal | :private  | :reporter   | nil   | 2\n    :internal | :private  | :guest      | nil   | 1\n    :internal | :private  | :non_member | nil   | 0\n    :internal | :private  | :anonymous  | nil   | 0\n\n    :internal | :disabled | :reporter   | nil   | 0\n    :internal | :disabled | :guest      | nil   | 0\n    :internal | :disabled | :non_member | nil   | 0\n    :internal | :disabled | :anonymous  | nil   | 0\n\n    :private  | :private  | :admin      | true  | 2\n    :private  | :private  | :admin      | false | 0\n    :private  | :private  | :reporter   | nil   | 2\n    :private  | :private  | :guest      | nil   | 1\n    :private  | :private  | :non_member | nil   | 0\n    :private  | :private  | :anonymous  | nil   | 0\n\n    :private  | :disabled | :reporter   | nil   | 0\n    :private  | :disabled | :guest      | nil   | 0\n    :private  | :disabled | :non_member | nil   | 0\n    :private  | :disabled | :anonymous  | nil   | 0\n  end",
    "comment": "This table is based on permission_table_for_guest_feature_access, but takes into account note confidentiality. It is required on the context to have one regular note and one confidential note.  project_level, :feature_access_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7518"
  },
  {
    "raw_code": "def permission_table_for_guest_feature_access_and_non_private_project_only\n    :public   | :enabled  | :admin      | true  | 1\n    :public   | :enabled  | :admin      | false | 1\n    :public   | :enabled  | :reporter   | nil   | 1\n    :public   | :enabled  | :guest      | nil   | 1\n    :public   | :enabled  | :non_member | nil   | 1\n    :public   | :enabled  | :anonymous  | nil   | 1\n\n    :public   | :private  | :admin      | true  | 1\n    :public   | :private  | :admin      | false | 0\n    :public   | :private  | :reporter   | nil   | 1\n    :public   | :private  | :guest      | nil   | 1\n    :public   | :private  | :non_member | nil   | 0\n    :public   | :private  | :anonymous  | nil   | 0\n\n    :public   | :disabled | :reporter   | nil   | 0\n    :public   | :disabled | :guest      | nil   | 0\n    :public   | :disabled | :non_member | nil   | 0\n    :public   | :disabled | :anonymous  | nil   | 0\n\n    :internal | :enabled  | :admin      | true  | 1\n    :internal | :enabled  | :admin      | false | 1\n    :internal | :enabled  | :reporter   | nil   | 1\n    :internal | :enabled  | :guest      | nil   | 1\n    :internal | :enabled  | :non_member | nil   | 1\n    :internal | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :private  | :admin      | true  | 1\n    :internal | :private  | :admin      | false | 0\n    :internal | :private  | :reporter   | nil   | 1\n    :internal | :private  | :guest      | nil   | 1\n    :internal | :private  | :non_member | nil   | 0\n    :internal | :private  | :anonymous  | nil   | 0\n\n    :internal | :disabled | :reporter   | nil   | 0\n    :internal | :disabled | :guest      | nil   | 0\n    :internal | :disabled | :non_member | nil   | 0\n    :internal | :disabled | :anonymous  | nil   | 0\n\n    :private  | :private  | :admin      | true  | 1\n    :private  | :private  | :admin      | false | 0\n    :private  | :private  | :reporter   | nil   | 1\n    :private  | :private  | :guest      | nil   | 0\n    :private  | :private  | :non_member | nil   | 0\n    :private  | :private  | :anonymous  | nil   | 0\n\n    :private  | :disabled | :reporter   | nil   | 0\n    :private  | :disabled | :guest      | nil   | 0\n    :private  | :disabled | :non_member | nil   | 0\n    :private  | :disabled | :anonymous  | nil   | 0\n  end",
    "comment": "This table is based on permission_table_for_guest_feature_access, but with a slight twist. Some features can be hidden away to GUEST, when project is private. (see ProjectFeature::PRIVATE_FEATURES_MIN_ACCESS_LEVEL_FOR_PRIVATE_PROJECT) This is the table for such features.  e.g. `repository` feature has minimum requirement of GUEST, but a GUEST are prohibited from reading code if project is private.  project_level, :feature_access_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7519"
  },
  {
    "raw_code": "def permission_table_for_milestone_access\n    :public   | :enabled  | :enabled  | :admin      | true  | 1\n    :public   | :enabled  | :enabled  | :admin      | false | 1\n    :public   | :enabled  | :enabled  | :reporter   | nil   | 1\n    :public   | :enabled  | :enabled  | :guest      | nil   | 1\n    :public   | :enabled  | :enabled  | :non_member | nil   | 1\n    :public   | :enabled  | :enabled  | :anonymous  | nil   | 1\n\n    :public   | :enabled  | :private  | :admin      | true  | 1\n    :public   | :enabled  | :private  | :admin      | false | 1\n    :public   | :enabled  | :private  | :reporter   | nil   | 1\n    :public   | :enabled  | :private  | :guest      | nil   | 1\n    :public   | :enabled  | :private  | :non_member | nil   | 1\n    :public   | :enabled  | :private  | :anonymous  | nil   | 1\n\n    :public   | :enabled  | :disabled | :admin      | true  | 1\n    :public   | :enabled  | :disabled | :admin      | false | 1\n    :public   | :enabled  | :disabled | :reporter   | nil   | 1\n    :public   | :enabled  | :disabled | :guest      | nil   | 1\n    :public   | :enabled  | :disabled | :non_member | nil   | 1\n    :public   | :enabled  | :disabled | :anonymous  | nil   | 1\n\n    :public   | :private  | :enabled  | :admin      | true  | 1\n    :public   | :private  | :enabled  | :admin      | false | 1\n    :public   | :private  | :enabled  | :reporter   | nil   | 1\n    :public   | :private  | :enabled  | :guest      | nil   | 1\n    :public   | :private  | :enabled  | :non_member | nil   | 1\n    :public   | :private  | :enabled  | :anonymous  | nil   | 1\n\n    :public   | :private  | :private  | :admin      | true  | 1\n    :public   | :private  | :private  | :admin      | false | 0\n    :public   | :private  | :private  | :reporter   | nil   | 1\n    :public   | :private  | :private  | :guest      | nil   | 1\n    :public   | :private  | :private  | :non_member | nil   | 0\n    :public   | :private  | :private  | :anonymous  | nil   | 0\n\n    :public   | :private  | :disabled | :admin      | true  | 1\n    :public   | :private  | :disabled | :admin      | false | 0\n    :public   | :private  | :disabled | :reporter   | nil   | 1\n    :public   | :private  | :disabled | :guest      | nil   | 1\n    :public   | :private  | :disabled | :non_member | nil   | 0\n    :public   | :private  | :disabled | :anonymous  | nil   | 0\n\n    :public   | :disabled | :enabled  | :admin      | true  | 1\n    :public   | :disabled | :enabled  | :admin      | false | 1\n    :public   | :disabled | :enabled  | :reporter   | nil   | 1\n    :public   | :disabled | :enabled  | :guest      | nil   | 1\n    :public   | :disabled | :enabled  | :non_member | nil   | 1\n    :public   | :disabled | :enabled  | :anonymous  | nil   | 1\n\n    :public   | :disabled | :private  | :admin      | true  | 1\n    :public   | :disabled | :private  | :admin      | false | 0\n    :public   | :disabled | :private  | :reporter   | nil   | 1\n    :public   | :disabled | :private  | :guest      | nil   | 0\n    :public   | :disabled | :private  | :non_member | nil   | 0\n    :public   | :disabled | :private  | :anonymous  | nil   | 0\n\n    :public   | :disabled | :disabled | :reporter   | nil   | 0\n    :public   | :disabled | :disabled | :guest      | nil   | 0\n    :public   | :disabled | :disabled | :non_member | nil   | 0\n    :public   | :disabled | :disabled | :anonymous  | nil   | 0\n\n    :internal | :enabled  | :enabled  | :admin      | true  | 1\n    :internal | :enabled  | :enabled  | :admin      | false | 1\n    :internal | :enabled  | :enabled  | :reporter   | nil   | 1\n    :internal | :enabled  | :enabled  | :guest      | nil   | 1\n    :internal | :enabled  | :enabled  | :non_member | nil   | 1\n    :internal | :enabled  | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :enabled  | :private  | :admin      | true  | 1\n    :internal | :enabled  | :private  | :admin      | false | 1\n    :internal | :enabled  | :private  | :reporter   | nil   | 1\n    :internal | :enabled  | :private  | :guest      | nil   | 1\n    :internal | :enabled  | :private  | :non_member | nil   | 1\n    :internal | :enabled  | :private  | :anonymous  | nil   | 0\n\n    :internal | :enabled  | :disabled | :admin      | true  | 1\n    :internal | :enabled  | :disabled | :admin      | false | 1\n    :internal | :enabled  | :disabled | :reporter   | nil   | 1\n    :internal | :enabled  | :disabled | :guest      | nil   | 1\n    :internal | :enabled  | :disabled | :non_member | nil   | 1\n    :internal | :enabled  | :disabled | :anonymous  | nil   | 0\n\n    :internal | :private  | :enabled  | :admin      | true  | 1\n    :internal | :private  | :enabled  | :admin      | false | 1\n    :internal | :private  | :enabled  | :reporter   | nil   | 1\n    :internal | :private  | :enabled  | :guest      | nil   | 1\n    :internal | :private  | :enabled  | :non_member | nil   | 1\n    :internal | :private  | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :private  | :private  | :admin      | true  | 1\n    :internal | :private  | :private  | :admin      | false | 0\n    :internal | :private  | :private  | :reporter   | nil   | 1\n    :internal | :private  | :private  | :guest      | nil   | 1\n    :internal | :private  | :private  | :non_member | nil   | 0\n    :internal | :private  | :private  | :anonymous  | nil   | 0\n\n    :internal | :private  | :disabled | :admin      | true  | 1\n    :internal | :private  | :disabled | :admin      | false | 0\n    :internal | :private  | :disabled | :reporter   | nil   | 1\n    :internal | :private  | :disabled | :guest      | nil   | 1\n    :internal | :private  | :disabled | :non_member | nil   | 0\n    :internal | :private  | :disabled | :anonymous  | nil   | 0\n\n    :internal | :disabled | :enabled  | :admin      | true  | 1\n    :internal | :disabled | :enabled  | :admin      | false | 1\n    :internal | :disabled | :enabled  | :reporter   | nil   | 1\n    :internal | :disabled | :enabled  | :guest      | nil   | 1\n    :internal | :disabled | :enabled  | :non_member | nil   | 1\n    :internal | :disabled | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :disabled | :private  | :admin      | true  | 1\n    :internal | :disabled | :private  | :admin      | false | 0\n    :internal | :disabled | :private  | :reporter   | nil   | 1\n    :internal | :disabled | :private  | :guest      | nil   | 0\n    :internal | :disabled | :private  | :non_member | nil   | 0\n    :internal | :disabled | :private  | :anonymous  | nil   | 0\n\n    :internal | :disabled | :disabled | :reporter   | nil   | 0\n    :internal | :disabled | :disabled | :guest      | nil   | 0\n    :internal | :disabled | :disabled | :non_member | nil   | 0\n    :internal | :disabled | :disabled | :anonymous  | nil   | 0\n\n    :private  | :private  | :private  | :admin      | true  | 1\n    :private  | :private  | :private  | :admin      | false | 0\n    :private  | :private  | :private  | :reporter   | nil   | 1\n    :private  | :private  | :private  | :guest      | nil   | 1\n    :private  | :private  | :private  | :non_member | nil   | 0\n    :private  | :private  | :private  | :anonymous  | nil   | 0\n\n    :private  | :private  | :disabled | :admin      | true  | 1\n    :private  | :private  | :disabled | :admin      | false | 0\n    :private  | :private  | :disabled | :reporter   | nil   | 1\n    :private  | :private  | :disabled | :guest      | nil   | 1\n    :private  | :private  | :disabled | :non_member | nil   | 0\n    :private  | :private  | :disabled | :anonymous  | nil   | 0\n\n    :private  | :disabled | :private  | :admin      | true  | 1\n    :private  | :disabled | :private  | :admin      | false | 0\n    :private  | :disabled | :private  | :reporter   | nil   | 1\n    :private  | :disabled | :private  | :guest      | nil   | 0\n    :private  | :disabled | :private  | :non_member | nil   | 0\n    :private  | :disabled | :private  | :anonymous  | nil   | 0\n\n    :private  | :disabled | :disabled | :reporter   | nil   | 0\n    :private  | :disabled | :disabled | :guest      | nil   | 0\n    :private  | :disabled | :disabled | :non_member | nil   | 0\n    :private  | :disabled | :disabled | :anonymous  | nil   | 0\n  end",
    "comment": ":project_level, :issues_access_level, :merge_requests_access_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7520"
  },
  {
    "raw_code": "def permission_table_for_project_access\n    :public   | :admin      | true  | 1\n    :public   | :admin      | false | 1\n    :public   | :reporter   | nil   | 1\n    :public   | :guest      | nil   | 1\n    :public   | :non_member | nil   | 1\n    :public   | :anonymous  | nil   | 1\n\n    :internal | :admin      | true  | 1\n    :internal | :admin      | false | 1\n    :internal | :reporter   | nil   | 1\n    :internal | :guest      | nil   | 1\n    :internal | :non_member | nil   | 1\n    :internal | :anonymous  | nil   | 0\n\n    :private  | :admin      | true  | 1\n    :private  | :admin      | false | 0\n    :private  | :reporter   | nil   | 1\n    :private  | :guest      | nil   | 1\n    :private  | :non_member | nil   | 0\n    :private  | :anonymous  | nil   | 0\n  end",
    "comment": ":project_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7521"
  },
  {
    "raw_code": "def permission_table_for_project_snippet_access\n    :public   | :public   | :enabled  | :admin      | true  | 1\n    :public   | :public   | :enabled  | :admin      | false | 1\n    :public   | :public   | :enabled  | :reporter   | nil   | 1\n    :public   | :public   | :enabled  | :guest      | nil   | 1\n    :public   | :public   | :enabled  | :non_member | nil   | 1\n    :public   | :public   | :enabled  | :anonymous  | nil   | 1\n\n    :public   | :public   | :private  | :admin      | true  | 1\n    :public   | :public   | :private  | :admin      | false | 0\n    :public   | :public   | :private  | :reporter   | nil   | 1\n    :public   | :public   | :private  | :guest      | nil   | 1\n    :public   | :public   | :private  | :non_member | nil   | 0\n    :public   | :public   | :private  | :anonymous  | nil   | 0\n\n    :public   | :public   | :disabled | :admin      | true  | 1\n    :public   | :public   | :disabled | :admin      | false | 0\n    :public   | :public   | :disabled | :reporter   | nil   | 0\n    :public   | :public   | :disabled | :guest      | nil   | 0\n    :public   | :public   | :disabled | :non_member | nil   | 0\n    :public   | :public   | :disabled | :anonymous  | nil   | 0\n\n    :public   | :internal | :enabled  | :admin      | true  | 1\n    :public   | :internal | :enabled  | :admin      | false | 1\n    :public   | :internal | :enabled  | :reporter   | nil   | 1\n    :public   | :internal | :enabled  | :guest      | nil   | 1\n    :public   | :internal | :enabled  | :non_member | nil   | 1\n    :public   | :internal | :enabled  | :anonymous  | nil   | 0\n\n    :public   | :internal | :private  | :admin      | true  | 1\n    :public   | :internal | :private  | :admin      | false | 0\n    :public   | :internal | :private  | :reporter   | nil   | 1\n    :public   | :internal | :private  | :guest      | nil   | 1\n    :public   | :internal | :private  | :non_member | nil   | 0\n    :public   | :internal | :private  | :anonymous  | nil   | 0\n\n    :public   | :internal | :disabled | :admin      | true  | 1\n    :public   | :internal | :disabled | :admin      | false | 0\n    :public   | :internal | :disabled | :reporter   | nil   | 0\n    :public   | :internal | :disabled | :guest      | nil   | 0\n    :public   | :internal | :disabled | :non_member | nil   | 0\n    :public   | :internal | :disabled | :anonymous  | nil   | 0\n\n    :public   | :private  | :private  | :admin      | true  | 1\n    :public   | :private  | :private  | :admin      | false | 0\n    :public   | :private  | :private  | :reporter   | nil   | 1\n    :public   | :private  | :private  | :guest      | nil   | 1\n    :public   | :private  | :private  | :non_member | nil   | 0\n    :public   | :private  | :private  | :anonymous  | nil   | 0\n\n    :public   | :private  | :disabled | :reporter   | nil   | 0\n    :public   | :private  | :disabled | :guest      | nil   | 0\n    :public   | :private  | :disabled | :non_member | nil   | 0\n    :public   | :private  | :disabled | :anonymous  | nil   | 0\n\n    :internal | :public   | :enabled  | :admin      | true  | 1\n    :internal | :public   | :enabled  | :admin      | false | 1\n    :internal | :public   | :enabled  | :reporter   | nil   | 1\n    :internal | :public   | :enabled  | :guest      | nil   | 1\n    :internal | :public   | :enabled  | :non_member | nil   | 1\n    :internal | :public   | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :public   | :private  | :admin      | true  | 1\n    :internal | :public   | :private  | :admin      | false | 0\n    :internal | :public   | :private  | :reporter   | nil   | 1\n    :internal | :public   | :private  | :guest      | nil   | 1\n    :internal | :public   | :private  | :non_member | nil   | 0\n    :internal | :public   | :private  | :anonymous  | nil   | 0\n\n    :internal | :public   | :disabled | :admin      | true  | 1\n    :internal | :public   | :disabled | :admin      | false | 0\n    :internal | :public   | :disabled | :reporter   | nil   | 0\n    :internal | :public   | :disabled | :guest      | nil   | 0\n    :internal | :public   | :disabled | :non_member | nil   | 0\n    :internal | :public   | :disabled | :anonymous  | nil   | 0\n\n    :internal | :internal | :enabled  | :admin      | true  | 1\n    :internal | :internal | :enabled  | :admin      | false | 1\n    :internal | :internal | :enabled  | :reporter   | nil   | 1\n    :internal | :internal | :enabled  | :guest      | nil   | 1\n    :internal | :internal | :enabled  | :non_member | nil   | 1\n    :internal | :internal | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :internal | :private  | :admin      | true  | 1\n    :internal | :internal | :private  | :admin      | false | 0\n    :internal | :internal | :private  | :reporter   | nil   | 1\n    :internal | :internal | :private  | :guest      | nil   | 1\n    :internal | :internal | :private  | :non_member | nil   | 0\n    :internal | :internal | :private  | :anonymous  | nil   | 0\n\n    :internal | :internal | :disabled | :admin      | true  | 1\n    :internal | :internal | :disabled | :admin      | false | 0\n    :internal | :internal | :disabled | :reporter   | nil   | 0\n    :internal | :internal | :disabled | :guest      | nil   | 0\n    :internal | :internal | :disabled | :non_member | nil   | 0\n    :internal | :internal | :disabled | :anonymous  | nil   | 0\n\n    :internal | :private  | :private  | :admin      | true  | 1\n    :internal | :private  | :private  | :admin      | false | 0\n    :internal | :private  | :private  | :reporter   | nil   | 1\n    :internal | :private  | :private  | :guest      | nil   | 1\n    :internal | :private  | :private  | :non_member | nil   | 0\n    :internal | :private  | :private  | :anonymous  | nil   | 0\n\n    :internal | :private  | :disabled | :admin      | true  | 1\n    :internal | :private  | :disabled | :admin      | false | 0\n    :internal | :private  | :disabled | :reporter   | nil   | 0\n    :internal | :private  | :disabled | :guest      | nil   | 0\n    :internal | :private  | :disabled | :non_member | nil   | 0\n    :internal | :private  | :disabled | :anonymous  | nil   | 0\n\n    :private  | :public   | :enabled  | :admin      | true  | 1\n    :private  | :public   | :enabled  | :admin      | false | 0\n    :private  | :public   | :enabled  | :reporter   | nil   | 1\n    :private  | :public   | :enabled  | :guest      | nil   | 1\n    :private  | :public   | :enabled  | :non_member | nil   | 0\n    :private  | :public   | :enabled  | :anonymous  | nil   | 0\n\n    :private  | :public   | :private  | :admin      | true  | 1\n    :private  | :public   | :private  | :admin      | false | 0\n    :private  | :public   | :private  | :reporter   | nil   | 1\n    :private  | :public   | :private  | :guest      | nil   | 1\n    :private  | :public   | :private  | :non_member | nil   | 0\n    :private  | :public   | :private  | :anonymous  | nil   | 0\n\n    :private  | :public   | :disabled | :admin      | true  | 1\n    :private  | :public   | :disabled | :admin      | false | 0\n    :private  | :public   | :disabled | :reporter   | nil   | 0\n    :private  | :public   | :disabled | :guest      | nil   | 0\n    :private  | :public   | :disabled | :non_member | nil   | 0\n    :private  | :public   | :disabled | :anonymous  | nil   | 0\n\n    :private  | :internal | :enabled  | :admin      | true  | 1\n    :private  | :internal | :enabled  | :admin      | false | 0\n    :private  | :internal | :enabled  | :reporter   | nil   | 1\n    :private  | :internal | :enabled  | :guest      | nil   | 1\n    :private  | :internal | :enabled  | :non_member | nil   | 0\n    :private  | :internal | :enabled  | :anonymous  | nil   | 0\n\n    :private  | :internal | :private  | :admin      | true  | 1\n    :private  | :internal | :private  | :admin      | false | 0\n    :private  | :internal | :private  | :reporter   | nil   | 1\n    :private  | :internal | :private  | :guest      | nil   | 1\n    :private  | :internal | :private  | :non_member | nil   | 0\n    :private  | :internal | :private  | :anonymous  | nil   | 0\n\n    :private  | :internal | :disabled | :admin      | true  | 1\n    :private  | :internal | :disabled | :admin      | false | 0\n    :private  | :internal | :disabled | :reporter   | nil   | 0\n    :private  | :internal | :disabled | :guest      | nil   | 0\n    :private  | :internal | :disabled | :non_member | nil   | 0\n    :private  | :internal | :disabled | :anonymous  | nil   | 0\n\n    :private  | :private  | :private  | :admin      | true  | 1\n    :private  | :private  | :private  | :admin      | false | 0\n    :private  | :private  | :private  | :reporter   | nil   | 1\n    :private  | :private  | :private  | :guest      | nil   | 1\n    :private  | :private  | :private  | :non_member | nil   | 0\n    :private  | :private  | :private  | :anonymous  | nil   | 0\n\n    :private  | :private  | :disabled | :admin      | true  | 1\n    :private  | :private  | :disabled | :admin      | false | 0\n    :private  | :private  | :disabled | :reporter   | nil   | 0\n    :private  | :private  | :disabled | :guest      | nil   | 0\n    :private  | :private  | :disabled | :non_member | nil   | 0\n    :private  | :private  | :disabled | :anonymous  | nil   | 0\n  end",
    "comment": ":snippet_level, :project_level, :feature_access_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7522"
  },
  {
    "raw_code": "def permission_table_for_personal_snippet_access\n    :public   | :admin      | true  | 1\n    :public   | :admin      | false | 1\n    :public   | :author     | nil   | 1\n    :public   | :non_member | nil   | 1\n    :public   | :anonymous  | nil   | 1\n\n    :internal | :admin      | true  | 1\n    :internal | :admin      | false | 1\n    :internal | :author     | nil   | 1\n    :internal | :non_member | nil   | 1\n    :internal | :anonymous  | nil   | 0\n\n    :private  | :admin      | true  | 1\n    :private  | :admin      | false | 0\n    :private  | :author     | nil   | 1\n    :private  | :non_member | nil   | 0\n    :private  | :anonymous  | nil   | 0\n  end",
    "comment": ":snippet_level, :membership, :expected_count",
    "label": "",
    "id": "7523"
  },
  {
    "raw_code": "def permission_table_for_reporter_issue_access\n    :public   | :enabled  | :admin      | true  | 1\n    :public   | :enabled  | :admin      | false | 0\n    :public   | :enabled  | :reporter   | nil   | 1\n    :public   | :enabled  | :guest      | nil   | 0\n    :public   | :enabled  | :non_member | nil   | 0\n    :public   | :enabled  | :anonymous  | nil   | 0\n\n    :public   | :private  | :admin      | true  | 1\n    :public   | :private  | :admin      | false | 0\n    :public   | :private  | :reporter   | nil   | 1\n    :public   | :private  | :guest      | nil   | 0\n    :public   | :private  | :non_member | nil   | 0\n    :public   | :private  | :anonymous  | nil   | 0\n\n    :public   | :disabled | :reporter   | nil   | 0\n    :public   | :disabled | :guest      | nil   | 0\n    :public   | :disabled | :non_member | nil   | 0\n    :public   | :disabled | :anonymous  | nil   | 0\n\n    :internal | :enabled  | :admin      | true  | 1\n    :internal | :enabled  | :admin      | false | 0\n    :internal | :enabled  | :reporter   | nil   | 1\n    :internal | :enabled  | :guest      | nil   | 0\n    :internal | :enabled  | :non_member | nil   | 0\n    :internal | :enabled  | :anonymous  | nil   | 0\n\n    :internal | :private  | :admin      | true  | 1\n    :internal | :private  | :admin      | false | 0\n    :internal | :private  | :reporter   | nil   | 1\n    :internal | :private  | :guest      | nil   | 0\n    :internal | :private  | :non_member | nil   | 0\n    :internal | :private  | :anonymous  | nil   | 0\n\n    :internal | :disabled | :reporter   | nil   | 0\n    :internal | :disabled | :guest      | nil   | 0\n    :internal | :disabled | :non_member | nil   | 0\n    :internal | :disabled | :anonymous  | nil   | 0\n\n    :private  | :private  | :admin      | true  | 1\n    :private  | :private  | :admin      | false | 0\n    :private  | :private  | :reporter   | nil   | 1\n    :private  | :private  | :guest      | nil   | 0\n    :private  | :private  | :non_member | nil   | 0\n    :private  | :private  | :anonymous  | nil   | 0\n\n    :private  | :disabled | :reporter   | nil   | 0\n    :private  | :disabled | :guest      | nil   | 0\n    :private  | :disabled | :non_member | nil   | 0\n    :private  | :disabled | :anonymous  | nil   | 0\n  end",
    "comment": "Based on the permission_table_for_reporter_feature_access table, but for issue features where public and internal projects with issues enabled only allow access to reporters and above (excluding admins if admin mode is disabled)  project_level, :feature_access_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7524"
  },
  {
    "raw_code": "def permission_table_for_epics_access # rubocop:disable Metrics/AbcSize -- needed for visibility specs\n    :public   | :admin      | true  | 1\n    :public   | :admin      | false | 1\n    :public   | :admin      | true  | 1\n    :public   | :admin      | false | 1\n    :public   | :planner    | nil   | 1\n    :public   | :reporter   | nil   | 1\n    :public   | :guest      | nil   | 1\n    :public   | :non_member | nil   | 1\n    :public   | :anonymous  | nil   | 1\n\n    :internal | :admin      | true  | 1\n    :internal | :admin      | false | 1\n    :internal | :reporter   | nil   | 1\n    :internal | :planner    | nil   | 1\n    :internal | :guest      | nil   | 1\n    :internal | :non_member | nil   | 1\n    :internal | :anonymous  | nil   | 0\n\n    :private  | :admin      | true  | 1\n    :private  | :admin      | false | 0\n    :private  | :reporter   | nil   | 1\n    :private  | :planner    | nil   | 1\n    :private  | :guest      | nil   | 1\n    :private  | :non_member | nil   | 0\n    :private  | :anonymous  | nil   | 0\n  end",
    "comment": "group_visibility_level, :membership, :admin_mode, :expected_count",
    "label": "",
    "id": "7525"
  },
  {
    "raw_code": "def allow_gitaly_n_plus_1\n    Gitlab::GitalyClient.allow_n_plus_1_calls do\n      yield\n    end",
    "comment": "We need to explicitly permit Gitaly N+1s because of the specs that use :request_store. Gitaly N+1 detection is only enabled when :request_store is, but we don't care about potential N+1s when we're just creating several projects in the setup phase.",
    "label": "",
    "id": "7526"
  },
  {
    "raw_code": "def file_path_rerun_argument\n        loaded_spec_files = RSpec.configuration.loaded_spec_files\n\n        RSpec::Core::Metadata.ascending(metadata) do |meta|\n          break meta[:file_path] if loaded_spec_files.include?(meta[:absolute_file_path])\n        end",
    "comment": "Based on https://github.com/rspec/rspec-core/blob/d57c371ee92b16211b80ac7b0b025968438f5297/lib/rspec/core/example.rb#L96-L104, Same as location_rerun_argument but with line number",
    "label": "",
    "id": "7527"
  },
  {
    "raw_code": "def self.setup(test)\n      @tests ||= Hash.new { |h, k| h[k] = [] }.tap(&:compare_by_identity)\n      @tests[test] = FakeGitlabEventStore.new\n    end",
    "comment": "Ensure to use the same stubbed Gitlab::EventStore within the test example, this way the list of published events is shared within the example enabling us to use .(not_)to publish_event multiple times and also mix it with .[to/and] not_publish_event",
    "label": "",
    "id": "7528"
  },
  {
    "raw_code": "def find_or_init_instance_spy(expected_klass, &)\n    existing_spies = RSpec::Mocks.space.proxies.values.filter_map do |proxy|\n      klass = proxy.instance_variable_get(:@doubled_module)&.send(:object)\n      spy = proxy.instance_variable_get(:@object)\n\n      spy if klass == expected_klass\n    end",
    "comment": "Reuse the same spy instance across any chained matchers so that expected `receive` counts are accurately tracked and reported",
    "label": "",
    "id": "7529"
  },
  {
    "raw_code": "def expect_data_attributes(node, negate: false)\n    # ensure assertions work for Capybara::Node::Simple inputs\n    node = node.native if node.respond_to?(:native)\n\n    check_negated_chain_methods_for_node! if negate\n    check_chain_methods_for_node!\n    check_negated_events_limit_for_node! if negate\n    check_events_limit_for_node!\n\n    expect_data_attribute(node, 'tracking', @event_names.first)\n    expect_data_attribute(node, 'label', @additional_properties.try(:[], :label))\n    expect_data_attribute(node, 'property', @additional_properties.try(:[], :property))\n    expect_data_attribute(node, 'value', @additional_properties.try(:[], :value))\n    expect_data_attribute(node, 'tracking-load', @on_load)\n\n    true\n  rescue RSpec::Expectations::ExpectationNotMetError => e\n    @failure_message = e.message\n    false\n  end",
    "comment": "All `node` inputs should be compatible with the have_css matcher https://www.rubydoc.info/gems/capybara/Capybara/RSpecMatchers#have_css-instance_method",
    "label": "",
    "id": "7530"
  },
  {
    "raw_code": "def expect_data_attribute(node, attribute, value)\n    if value\n      expect(node).to have_css(\"[data-event-#{attribute}=\\\"#{value}\\\"]\")\n    else\n      expect(node).not_to have_css(\"[data-event-#{attribute}]\")\n    end",
    "comment": "Keep this in sync with the constants in app/assets/javascripts/tracking/constants.js",
    "label": "",
    "id": "7531"
  },
  {
    "raw_code": "def setup_context(key_paths, default_chained_methods = [])\n    @key_paths = key_paths.flatten\n    @values ||= {}\n    @chained_methods ||= default_chained_methods\n\n    check_if_params_provided!(:key_paths, key_paths)\n    stub_metric_timeframes\n  end",
    "comment": "Init instance vars and validate inputs",
    "label": "",
    "id": "7532"
  },
  {
    "raw_code": "def build_matcher\n    @key_paths.reduce(nil) do |matcher, key_path|\n      metric_definition = get_metric_definition(key_path)\n      value_tracker = metric_value_tracker(key_path, metric_definition)\n      change_matcher = yield(value_tracker)\n      chained_matcher = apply_chain_methods(change_matcher, @chained_methods)\n\n      matcher ? matcher.and(chained_matcher) : chained_matcher\n    end",
    "comment": "Builds a single change matcher for verifying all provided metric values, including chained expected counts",
    "label": "",
    "id": "7533"
  },
  {
    "raw_code": "def metric_value_tracker(key_path, metric_definition)\n    proc do\n      stub_usage_data_connections if metric_definition.data_source == 'database'\n\n      metric = Gitlab::Usage::Metric.new(metric_definition)\n      instrumentation_object = metric.send(:instrumentation_object)\n\n      instrumentation_object.value.tap do |value|\n        @values[key_path] ||= []\n        @values[key_path] << value\n      end",
    "comment": "Returns a proc that reads the current value of given metric, to be passed to a change matcher",
    "label": "",
    "id": "7534"
  },
  {
    "raw_code": "def format_buffer(string)\n    line_deletion_counts = string.lines.map { |line| line.scan(\"\\e[1A\").length }\n\n    buffer_length = line_deletion_counts.each.with_index.reduce(0) do |buffer, (deleted_lines, idx)|\n      excess_lines = idx + buffer - deleted_lines\n      excess_lines < 0 ? buffer + excess_lines.abs : buffer\n    end",
    "comment": "Truncated outputs may include more line deletions than already printed lines, so we don't want to overwrite previous lines",
    "label": "",
    "id": "7535"
  },
  {
    "raw_code": "def reject_groups_with_different_parameters(suffs)\n      return suffs if suffs.size != 2\n\n      counts_a, counts_b = suffs.values\n      return {} if counts_a == counts_b.reverse && counts_a.include?(0)\n\n      suffs\n    end",
    "comment": "Eliminates groups that differ only in parameters, to make it easier to debug the output.  For example, if we have a group `SELECT * FROM users...`, with the following suffixes `WHERE id = 1` (counts: N, 0) `WHERE id = 2` (counts: 0, N)",
    "label": "",
    "id": "7536"
  },
  {
    "raw_code": "def count_queries(query_recorder)\n    strip_marginalia_annotations(query_log(query_recorder))\n      .map { |q| query_group_key(q) }\n      .group_by { |k| k[:prefix] }\n      .transform_values { |keys| frequencies(:suffix, keys) }\n  end",
    "comment": "Take a query recorder and tabulate the frequencies of suffixes for each prefix.  @return Hash[String, Hash[String, Int]]  Example:  r = ActiveRecord::QueryRecorder.new do SomeTable.create(x: 1, y: 2, z: 3) SomeOtherTable.where(id: 1).first SomeTable.create(x: 4, y: 5, z: 6) SomeOtherTable.all end count_queries(r) #=> { 'INSERT INTO \"some_table\" VALUES' => { '(1,2,3)' => 1, '(4,5,6)' => 1 }, 'SELECT * FROM \"some_other_table\"' => { 'WHERE id = 1 LIMIT 1' => 1, '' => 2 } }",
    "label": "",
    "id": "7537"
  },
  {
    "raw_code": "def normalize_elements(elem)\n      case elem\n      when Hash\n        elem.to_h do |key, value|\n          if ignore_key?(key, value)\n            [key, :ignored]\n          else\n            [key, normalize_elements(value)]\n          end",
    "comment": "Helper that traverses a project tree and normalizes data that we know to vary in the process of importing (such as list order or row IDs)",
    "label": "",
    "id": "7538"
  },
  {
    "raw_code": "def ignore_key?(key, value)\n      id?(key) || # IDs are known to be replaced during imports\n        key == 'updated_at' || # these get changed frequently during imports\n        key == 'next_run_at' || # these values change based on wall clock\n        key == 'notes' # the importer attaches an extra \"by user XYZ\" at the end of a note\n    end",
    "comment": "We currently need to ignore certain entries when checking for equivalence because we know them to change between imports/exports either by design or because of bugs; this helper filters out these problematic nodes.",
    "label": "",
    "id": "7539"
  },
  {
    "raw_code": "def in_directory_with_expanded_export(project, user)\n    Dir.mktmpdir do |tmpdir|\n      export_file = project.export_file(user).path\n      _output, exit_status = Gitlab::Popen.popen(%W[tar -zxf #{export_file} -C #{tmpdir}])\n\n      yield(exit_status, tmpdir)\n    end",
    "comment": "Expands the compressed file for an exported project into +tmpdir+",
    "label": "",
    "id": "7540"
  },
  {
    "raw_code": "def deep_find_with_parent(sensitive_key_word, object, found = nil)\n    sensitive_key_found = object_contains_key?(object, sensitive_key_word)\n\n    # Returns the parent object and the object found containing a sensitive word as part of the key\n    if sensitive_key_found && object[sensitive_key_found]\n      ObjectWithParent.new(object[sensitive_key_found], object, sensitive_key_found)\n    elsif object.is_a?(Enumerable)\n      # Recursively lookup for keys containing sensitive words in a Hash or Array\n      object_with_parent = nil\n\n      object.find do |*hash_or_array|\n        object_with_parent = deep_find_with_parent(sensitive_key_word, hash_or_array.last, found)\n      end",
    "comment": "Recursively finds key/values including +key+ as part of the key, inside a nested hash",
    "label": "",
    "id": "7541"
  },
  {
    "raw_code": "def object_contains_key?(object, sensitive_key_word)\n    return false unless object.is_a?(Hash)\n\n    object.keys.find { |key| key.include?(sensitive_key_word) }\n  end",
    "comment": "Return true if the hash has a key containing a sensitive word",
    "label": "",
    "id": "7542"
  },
  {
    "raw_code": "def find_sensitive_attributes(sensitive_word, project_hash)\n    loop do\n      object_with_parent = deep_find_with_parent(sensitive_word, project_hash)\n\n      return unless object_with_parent && object_with_parent.object\n\n      if is_safe_hash?(object_with_parent.parent, sensitive_word)\n        # It's in the safe list, remove hash and keep looking\n        object_with_parent.parent.delete(object_with_parent.key_found)\n      else\n        return object_with_parent\n      end",
    "comment": "Returns the offended ObjectWithParent object if a sensitive word is found inside a hash, excluding the allowlisted safe hashes.",
    "label": "",
    "id": "7543"
  },
  {
    "raw_code": "def is_safe_hash?(parent, sensitive_word)\n    return false unless parent && safe_list[sensitive_word.to_sym]\n\n    # Extra attributes that appear in a model but not in the exported hash.\n    excluded_attributes = ['type']\n\n    safe_list[sensitive_word.to_sym].each do |model|\n      # Check whether this is a hash attribute inside a model\n      if model.is_a?(Symbol)\n        return true if (safe_hashes[model] - parent.keys).empty?\n      elsif safe_model?(model, excluded_attributes, parent)\n        return true\n      end",
    "comment": "Returns true if it's one of the excluded models in +safe_list+",
    "label": "",
    "id": "7544"
  },
  {
    "raw_code": "def safe_model?(model, excluded_attributes, parent)\n    excluded_attributes += associations_for(model)\n    parsed_model_attributes = parsed_attributes(model.name.underscore, model.attribute_names)\n\n    (parsed_model_attributes - parent.keys - excluded_attributes).empty?\n  end",
    "comment": "Compares model attributes with those found in the hash and returns true if there is a match, ignoring some excluded attributes.",
    "label": "",
    "id": "7545"
  },
  {
    "raw_code": "def names_from_tree(project_tree)\n    project_tree.map do |branch_or_model|\n      branch_or_model = branch_or_model.to_s if branch_or_model.is_a?(Symbol)\n\n      branch_or_model.is_a?(String) ? branch_or_model : names_from_tree(branch_or_model)\n    end",
    "comment": "Returns a list of models from hashes/arrays contained in +project_tree+",
    "label": "",
    "id": "7546"
  },
  {
    "raw_code": "def stub_metric_timeframes\n    [\n      :weekly_time_range,\n      :monthly_time_range,\n      :weekly_time_range_db_params,\n      :monthly_time_range_db_params\n    ].each do |method|\n      allow_any_instance_of(Gitlab::Usage::TimeFrame)\n        .to receive(method)\n        .and_wrap_original { |_, **args| ClassWithStubbedTimeframe.new.send(method, **args) }\n    end",
    "comment": "Override metric timeframe from within specs rubocop:disable RSpec/AnyInstanceOf -- Gitlab::Usage::TimeFrame is initialized multiple times from many classes",
    "label": "",
    "id": "7547"
  },
  {
    "raw_code": "def get_current_service_ping_payload\n      override_timeframe_from_dev_console!\n\n      Gitlab::Usage::ServicePingReport.for(output: :all_metrics_values)\n    end",
    "comment": "Generates a full service ping report from rails console",
    "label": "",
    "id": "7548"
  },
  {
    "raw_code": "def get_current_usage_metric_value(metric_key_path)\n      override_timeframe_from_dev_console!\n\n      metric_definition = Gitlab::Usage::MetricDefinition.definitions[metric_key_path]\n\n      raise ArgumentError, \"Metric not found for key path #{metric_key_path}\" unless metric_definition\n\n      unless metric_definition.instrumentation_class\n        raise ArgumentError,\n          \"Can't read metric value!\\n  \" \\\n          \"This legacy metric is only available from the full service ping report. \\n  \" \\\n          \"You can generate the report & read the value like this:\\n\\n  \" \\\n          \"ServicePingHelpers.get_current_service_ping_payload.dig(*'#{metric_key_path}'.split('.'))\\n\"\n      end",
    "comment": "Reads the current value for a metric from rails console @metric_key_path [String] key_path field from metric definition yml ex) 'usage_activity_by_stage_monthly.manage.issue_imports.csv'",
    "label": "",
    "id": "7549"
  },
  {
    "raw_code": "def stub_ldap_config(messages)\n    allow_any_instance_of(::Gitlab::Auth::Ldap::Config).to receive_messages(to_settings(messages))\n  end",
    "comment": "Accepts a hash of Gitlab::Auth::Ldap::Config keys and values.  Example: stub_ldap_config( group_base: 'ou=groups,dc=example,dc=com', admin_group: 'my-admin-group' )",
    "label": "",
    "id": "7550"
  },
  {
    "raw_code": "def stub_ldap_person_find_by_uid(uid, entry, provider = 'ldapmain')\n    return_value = ::Gitlab::Auth::Ldap::Person.new(entry, provider) if entry.present?\n\n    allow(::Gitlab::Auth::Ldap::Person)\n      .to receive(:find_by_uid).with(uid, any_args).and_return(return_value)\n  end",
    "comment": "Stub an LDAP person search and provide the return entry. Specify `nil` for `entry` to simulate when an LDAP person is not found  Example: adapter = ::Gitlab::Auth::Ldap::Adapter.new('ldapmain', double(:ldap)) ldap_user_entry = ldap_user_entry('john_doe')  stub_ldap_person_find_by_uid('john_doe', ldap_user_entry, adapter)",
    "label": "",
    "id": "7551"
  },
  {
    "raw_code": "def ldap_user_entry(uid)\n    entry = Net::LDAP::Entry.new\n    entry['dn'] = user_dn(uid)\n    entry['uid'] = uid\n\n    entry\n  end",
    "comment": "Create a simple LDAP user entry.",
    "label": "",
    "id": "7552"
  },
  {
    "raw_code": "def over_limit?\n      @delete_count_by_table.values.sum >= max_deletes ||\n        @update_count_by_table.values.sum >= max_updates\n    end",
    "comment": "Redefine over_limit? to not have time limit as this has been found to be too slow and flaky in the inconsistent CI performance. Context: https://gitlab.com/gitlab-org/gitlab/-/merge_requests/187891#note_2448115940",
    "label": "",
    "id": "7553"
  },
  {
    "raw_code": "def sample_blob\n    OpenStruct.new(\n      oid: '5f53439ca4b009096571d3c8bc3d09d30e7431b3',\n      path: \"files/js/commit.js.coffee\",\n      data: <<EOS\nclass Commit\n  constructor: ->\n    $('.files .diff-file').each ->\n      new CommitFile(this)\n\n@Commit = Commit\nEOS\n    )\n  end",
    "comment": "Text file in repo  Ex.  # Get object blob = RepoHelpers.text_blob  blob.path # => 'files/js/commit.js.coffee' blob.data # => 'class Commit...'  Build the options hash that's passed to Rugged::Commit#create",
    "label": "",
    "id": "7554"
  },
  {
    "raw_code": "def rewritten_fields_hash(hash)\n    if mode == :remote\n      # For remote uploads, workhorse still submits rewritten_fields,\n      # but all the values are empty strings.\n      hash.keys.each { |k| hash[k] = '' }\n    end",
    "comment": "This function assumes a `mode` variable to be set",
    "label": "",
    "id": "7555"
  },
  {
    "raw_code": "def expect_uploaded_file(file, expectation)\n    expect(file).to be_a(::UploadedFile)\n    expect(file.original_filename).to eq(expectation[:original_filename])\n    expect(file.sha256).to eq('1234567890')\n\n    case mode\n    when :local\n      expect(file.path).to eq(File.realpath(expectation[:filepath]))\n      expect(file.remote_id).to be_nil\n      expect(file.size).to eq(expectation[:size])\n    when :remote\n      expect(file.remote_id).to eq(expectation[:remote_id])\n      expect(file.path).to be_nil\n      expect(file.size).to eq(3.megabytes)\n    else\n      raise ArgumentError, \"can't handle #{mode} mode\"\n    end",
    "comment": "This function assumes a `mode` variable to be set",
    "label": "",
    "id": "7556"
  },
  {
    "raw_code": "def get_params(env)\n    req = ActionDispatch::Request.new(env)\n    req.GET.merge(req.POST)\n  end",
    "comment": "Rails doesn't combine the GET/POST parameters in ActionDispatch::HTTP::Parameters if action_dispatch.request.parameters is set: https://github.com/rails/rails/blob/aea6423f013ca48f7704c70deadf2cd6ac7d70a1/actionpack/lib/action_dispatch/http/parameters.rb#L41",
    "label": "",
    "id": "7557"
  },
  {
    "raw_code": "def enable_project_studio!(user)\n    stub_feature_flags(tailwind_container_queries: user, paneled_view: user)\n\n    user.project_studio_enabled = true\n  end",
    "comment": "This can't run in a `before_all` block as long as we set all feature flags to be `false` in the `spec_helper`",
    "label": "",
    "id": "7558"
  },
  {
    "raw_code": "def stub_session(session_data:, user_id: nil)\n    unless RSpec.current_example.metadata[:clean_gitlab_redis_sessions]\n      raise 'Add :clean_gitlab_redis_sessions to your spec!'\n    end",
    "comment": "Stub a session in Redis, for use in request specs where we can't mock the session directly. This also needs the :clean_gitlab_redis_sessions tag on the spec.",
    "label": "",
    "id": "7559"
  },
  {
    "raw_code": "def allowing_for_delay(interval: 0.5, retries: 5)\n    tries = 0\n\n    begin\n      sleep interval\n\n      yield\n    rescue Capybara::ExpectationNotMet => ex\n      if tries <= retries\n        tries += 1\n        sleep interval\n        retry\n      else\n        raise ex\n      end",
    "comment": "Execute a block a certain number of times before considering it a failure  The given block is called, and if it raises a `Capybara::ExpectationNotMet` error, we wait `interval` seconds and then try again, until `retries` is met.  This allows for better handling of timing-sensitive expectations in a sketchy CI environment, for example.  interval - Delay between retries in seconds (default: 0.5) retries  - Number of times to execute before failing (default: 5)",
    "label": "",
    "id": "7560"
  },
  {
    "raw_code": "def refresh\n    url = current_url\n    visit_blank_page\n    visit url\n  end",
    "comment": "Refresh the page. Calling `visit current_url` doesn't seem to work consistently. ",
    "label": "",
    "id": "7561"
  },
  {
    "raw_code": "def clear_browser_session\n    page.driver.browser.manage.delete_cookie('_gitlab_session')\n  end",
    "comment": "Simulate a browser restart by clearing the session cookie.",
    "label": "",
    "id": "7562"
  },
  {
    "raw_code": "def expect_snowplow_event(category:, action:, context: nil, tracking_method: :event, **kwargs)\n    if context\n      if context.is_a?(Array)\n        kwargs[:context] = []\n        context.each do |c|\n          expect(SnowplowTracker::SelfDescribingJson).to have_received(:new)\n            .with(c[:schema], c[:data]).at_least(:once)\n          kwargs[:context] << an_instance_of(SnowplowTracker::SelfDescribingJson)\n        end",
    "comment": "Asserts call for one snowplow event from `Gitlab::Tracking#event`.  @param [Hash]  Examples:  describe '#show' do it 'tracks snowplow events' do get :show  expect_snowplow_event(category: 'Experiment', action: 'start') end end  describe '#create' do it 'tracks snowplow events' do post :create  expect_snowplow_event( category: 'Experiment', action: 'created', ) expect_snowplow_event( category: 'Experiment', action: 'accepted', property: 'property', label: 'label' ) end end  Passing context:  Simply provide a hash that has the schema and data expected.  expect_snowplow_event( category: 'Experiment', action: 'created', context: [ { schema: 'iglu:com.gitlab/.../0-3-0', data: { key: 'value' } } ] )",
    "label": "",
    "id": "7563"
  },
  {
    "raw_code": "def expect_no_snowplow_event(category: nil, action: nil, tracking_method: :event, **kwargs)\n    if category && action\n      expect(Gitlab::Tracking).not_to have_received(tracking_method).with(category, action, **kwargs)\n    else\n      expect(Gitlab::Tracking).not_to have_received(tracking_method)\n    end",
    "comment": "Asserts that no call to `Gitlab::Tracking#event` was made.  Example:  describe '#show', :snowplow do it 'does not track any snowplow events' do get :show  expect_no_snowplow_event end end",
    "label": "",
    "id": "7564"
  },
  {
    "raw_code": "def wait_for(condition_name, max_wait_time: Capybara.default_max_wait_time, polling_interval: 0.01, reload: false)\n    wait_until = Time.now + max_wait_time.seconds\n    loop do\n      result = yield\n      break result if result\n\n      page.refresh if reload\n\n      if Time.now > wait_until\n        raise \"Condition not met: #{condition_name}\"\n      else\n        sleep(polling_interval)\n      end",
    "comment": "Waits until the passed block returns true",
    "label": "",
    "id": "7565"
  },
  {
    "raw_code": "def stub_application_setting_enum(setting, value)\n    stub_application_setting(setting.to_sym => value)\n\n    ApplicationSetting.send(setting.pluralize.to_sym).each_key do |key|\n      stub_application_setting(\"#{setting}_#{key}\".to_sym => key == value)\n    end",
    "comment": "For enums with `_prefix: true`, this allows us to stub the application setting properly",
    "label": "",
    "id": "7566"
  },
  {
    "raw_code": "def add_predicates(messages)\n    # Only modify keys that aren't already predicates\n    keys = messages.keys.map(&:to_s).reject { |k| k.end_with?('?') }\n\n    keys.each do |key|\n      predicate = key + '?'\n      messages[predicate.to_sym] = messages[key.to_sym]\n    end",
    "comment": "Modifies stubbed messages to also stub possible predicate versions  Examples:  add_predicates(foo: true) # => {foo: true, foo?: true}  add_predicates(signup_enabled?: false) # => {signup_enabled? false}",
    "label": "",
    "id": "7567"
  },
  {
    "raw_code": "def to_settings(hash)\n    hash.transform_values do |value|\n      if value.is_a? Hash\n        ::GitlabSettings::Options.build(value)\n      else\n        value\n      end",
    "comment": "Support nested hashes by converting all values into GitlabSettings::Objects objects",
    "label": "",
    "id": "7568"
  },
  {
    "raw_code": "def self.fieldnamerize(underscored_field_name)\n    # Skip transformation for a field with leading underscore\n    return underscored_field_name.to_s if underscored_field_name.start_with?('_')\n\n    underscored_field_name.to_s.camelize(:lower)\n  end",
    "comment": "makes an underscored string look like a fieldname \"merge_request\" => \"mergeRequest\"",
    "label": "",
    "id": "7569"
  },
  {
    "raw_code": "def self.deep_transform_args(args, field)\n    args.to_h do |k, v|\n      argument = field.arguments[k.to_s.camelize(:lower)]\n      [argument.keyword, v.is_a?(Hash) ? deep_transform_args(v, argument.type) : v]\n    end",
    "comment": "Some arguments use `as:` to expose a different name internally. Transform the args to use those names",
    "label": "",
    "id": "7570"
  },
  {
    "raw_code": "def self.as_graphql_argument_literals(args)\n    args.transform_values { |value| transform_arg_value(value) }\n  end",
    "comment": "Convert incoming args into the form usually passed in from the client, all strings, etc.",
    "label": "",
    "id": "7571"
  },
  {
    "raw_code": "def resolve(\n    resolver_class, # [Class[<= BaseResolver]] The resolver at test.\n    obj: nil, # [Any] The BaseObject#object for the resolver (available as `#object` in the resolver).\n    args: {}, # [Hash] The arguments to the resolver (using client names).\n    ctx: {},  # [#to_h] The current context values.\n    schema: GitlabSchema, # [GraphQL::Schema] Schema to use during execution.\n    parent: :not_given, # A GraphQL query node to be passed as the `:parent` extra.\n    lookahead: :not_given, # A GraphQL lookahead object to be passed as the `:lookahead` extra.\n    arg_style: :internal_prepared, # Args are in internal format, but should use more rigorous processing,\n    field_opts: {}\n  )\n    # All resolution goes through fields, so we need to create one here that\n    # uses our resolver. Thankfully, apart from the field name, resolvers\n    # contain all the configuration needed to define one.\n    field = ::Types::BaseField.new(\n      resolver_class: resolver_class,\n      owner: resolver_parent,\n      name: 'field_value',\n      calls_gitaly: field_opts[:calls_gitaly],\n      **(field_opts[:connection_extension] ? { connection_extension: field_opts[:connection_extension] } : {})\n    )\n\n    # All mutations accept a single `:input` argument. Wrap arguments here.\n    args = { input: args } if resolver_class <= ::Mutations::BaseMutation && !args.key?(:input)\n\n    resolve_field(\n      field,\n      obj,\n      args: args,\n      ctx: ctx,\n      schema: schema,\n      object_type: resolver_parent,\n      extras: { parent: parent, lookahead: lookahead },\n      arg_style: arg_style\n    )\n  end",
    "comment": "DEPRECATED: This method of testing is too coupled to gem internals making upgrades painful, and bypasses much of the validation of the framework.  Use full integration tests instead.  See https://gitlab.com/gitlab-org/gitlab/-/issues/363121  rubocop: disable Metrics/ParameterLists -- This was disabled to add `field_opts`, needed for :calls_gitaly",
    "label": "",
    "id": "7572"
  },
  {
    "raw_code": "def resolve_field(\n    field,                        # An instance of `BaseField`, or the name of a field on the current described_class\n    object,                       # The current object of the `BaseObject` this field 'belongs' to\n    args:   {},                   # Field arguments (keys will be fieldnamerized)\n    ctx:    {},                   # Context values (important ones are :current_user)\n    extras: {},                   # Stub values for field extras (parent and lookahead)\n    current_user: :not_given,     # The current user (specified explicitly, overrides ctx[:current_user])\n    schema: GitlabSchema,         # A specific schema instance\n    object_type: described_class, # The `BaseObject` type this field belongs to\n    arg_style: :internal_prepared, # Args are in internal format, but should use more rigorous processing\n    query: nil                     # Query to evaluate the field\n  )\n    field = to_base_field(field, object_type).ensure_loaded\n    ctx[:current_user] = current_user unless current_user == :not_given\n    query ||= GraphQL::Query.new(schema, context: ctx.to_h)\n    extras[:lookahead] = negative_lookahead if extras[:lookahead] == :not_given && field.extras.include?(:lookahead)\n    query_ctx = query.context\n\n    mock_extras(query_ctx, **extras)\n\n    parent = object_type.authorized_new(object, query_ctx)\n    raise UnauthorizedObject unless parent\n\n    # we enable the request store so we can track gitaly calls.\n    ::Gitlab::SafeRequestStore.ensure_request_store do\n      prepared_args = case arg_style\n                      when :internal_prepared\n                        args_internal_prepared(field, args: args, query_ctx: query_ctx, parent: parent, extras: extras, query: query)\n                      else\n                        args_internal(field, args: args, query_ctx: query_ctx, parent: parent, extras: extras, query: query)\n                      end",
    "comment": "Resolve the value of a field on an object.  Use this method to test individual fields within type specs.  e.g.  issue = create(:issue) user = issue.author project = issue.project  resolve_field(:author, issue, current_user: user, object_type: ::Types::IssueType) resolve_field(:issue, project, args: { iid: issue.iid }, current_user: user, object_type: ::Types::ProjectType)  The `object_type` defaults to the `described_class`, so when called from type specs, the above can be written as:  # In project_type_spec.rb resolve_field(:author, issue, current_user: user)  # In issue_type_spec.rb resolve_field(:issue, project, args: { iid: issue.iid }, current_user: user)  NB: Arguments are passed from the client's perspective. If there is an argument `foo` aliased as `bar`, then we would pass `args: { bar: the_value }`, and types are checked before resolution.",
    "label": "",
    "id": "7573"
  },
  {
    "raw_code": "def query_context(user: current_user, request: {})\n    query = GraphQL::Query.new(empty_schema, document: nil, context: {}, variables: {})\n    GraphQL::Query::Context.new(query: query, values: { current_user: user, request: request })\n  end",
    "comment": "create a valid query context object",
    "label": "",
    "id": "7574"
  },
  {
    "raw_code": "def args_internal(field, args:, query_ctx:, parent:, extras:, query:)\n    arguments = GraphqlHelpers.deep_transform_args(args, field)\n    arguments.merge!(extras.reject { |k, v| v == :not_given })\n  end",
    "comment": "rubocop:enable Metrics/ParameterLists Pros: - Original way we handled arguments  Cons: - the `prepare` method of a type is not called.  Whether as a proc or as a method on the type, it's not called. For example `:cluster_id` in ee/app/graphql/resolvers/vulnerabilities_resolver.rb, or `prepare` in app/graphql/types/range_input_type.rb, used by Types::TimeframeInputType",
    "label": "",
    "id": "7575"
  },
  {
    "raw_code": "def args_internal_prepared(field, args:, query_ctx:, parent:, extras:, query:)\n    arguments = GraphqlHelpers.as_graphql_argument_literals(args)\n    arguments.merge!(extras.reject { |k, v| v == :not_given })\n\n    # Use public API to properly prepare the args for use by the resolver.\n    # It uses `coerce_arguments` under the covers\n    prepared_args = nil\n    query.arguments_cache.dataload_for(GraphqlHelpers.deep_fieldnamerize(arguments), field, parent) do |kwarg_arguments|\n      prepared_args = kwarg_arguments\n    end",
    "comment": "Pros: - Allows the use of ruby types, without having to pass in strings - All args are converted into strings just like if it was called from a client - Much stronger argument verification  Cons: - Some values, such as enums, would need to be changed in the specs to use the external values, because there is no easy way to handle them.  take internal style args, and force them into client style args",
    "label": "",
    "id": "7576"
  },
  {
    "raw_code": "def resolver_parent\n    @resolver_parent ||= fresh_object_type('ResolverParent')\n  end",
    "comment": "a synthetic BaseObject type to be used in resolver specs. See `GraphqlHelpers#resolve`",
    "label": "",
    "id": "7577"
  },
  {
    "raw_code": "def eager_resolve(resolver_class, **opts)\n    sync(resolve(resolver_class, **opts))\n  end",
    "comment": "Eagerly run a loader's named resolver (syncs any lazy values returned by resolve)",
    "label": "",
    "id": "7578"
  },
  {
    "raw_code": "def batch(max_queries: nil, &blk)\n    wrapper = -> { with_clean_batchloader_executor(&blk) }\n\n    if max_queries\n      result = nil\n      expect { result = wrapper.call }.not_to exceed_query_limit(max_queries)\n      result\n    else\n      wrapper.call\n    end",
    "comment": "Runs a block inside a BatchLoader::Executor wrapper",
    "label": "",
    "id": "7579"
  },
  {
    "raw_code": "def run_with_clean_state(query, **args)\n    ::Gitlab::SafeRequestStore.ensure_request_store do\n      with_clean_rails_cache do\n        with_clean_batchloader_executor do\n          ::GitlabSchema.execute(query, **args)\n        end",
    "comment": "Use this when writing N+1 tests.  It does not use the controller, so it avoids confounding factors due to authentication (token set-up, license checks) It clears the request store, rails cache, and BatchLoader Executor between runs.",
    "label": "",
    "id": "7580"
  },
  {
    "raw_code": "def with_clean_rails_cache(&blk)\n    caching_store = Rails.cache\n    Rails.cache = ActiveSupport::Cache::MemoryStore.new\n\n    ActiveRecord::Base.cache(&blk)\n  ensure\n    Rails.cache = caching_store\n  end",
    "comment": "Basically a combination of use_sql_query_cache and use_clean_rails_memory_store_caching, but more fine-grained, suitable for comparing two runs in the same example.",
    "label": "",
    "id": "7581"
  },
  {
    "raw_code": "def batch_sync(max_queries: nil, &blk)\n    batch(max_queries: max_queries) { sync_all(&blk) }\n  end",
    "comment": "BatchLoader::GraphQL returns a wrapper, so we need to :sync in order to get the actual values",
    "label": "",
    "id": "7582"
  },
  {
    "raw_code": "def prepare_variables(input)\n    return input.map { prepare_variables(_1) } if input.is_a?(Array)\n    return input.to_s if input.is_a?(GlobalID) || input.is_a?(Symbol)\n    return input unless input.is_a?(Hash)\n\n    input.to_h do |name, value|\n      [GraphqlHelpers.fieldnamerize(name), prepare_variables(value)]\n    end",
    "comment": "Recursively convert any ruby object we can pass as a variable value to an object we can serialize with JSON, using fieldname-style keys  prepare_variables({ 'my_key' => 1 }) => { 'myKey' => 1 } prepare_variables({ enums: [:FOO, :BAR], user_id: global_id_of(user) }) => { 'enums' => ['FOO', 'BAR'], 'userId' => \"gid://User/123\" } prepare_variables({ nested: { hash_values: { are_supported: true } } }) => { 'nested' => { 'hashValues' => { 'areSupported' => true } } }",
    "label": "",
    "id": "7583"
  },
  {
    "raw_code": "def query_graphql_path(segments, fields = nil)\n    # we really want foldr here...\n    segments.reverse.reduce(fields) do |tail, segment|\n      name, args = Array.wrap(segment)\n      query_graphql_field(name, args, tail)\n    end",
    "comment": "e.g: query_graphql_path(%i[foo bar baz], all_graphql_fields_for('Baz')) => foo { bar { baz { x y z } } }",
    "label": "",
    "id": "7584"
  },
  {
    "raw_code": "def find_uploads(paths, path, value)\n    case value\n    when Rack::Test::UploadedFile\n      paths << path\n    when Hash\n      value.each do |k, v|\n        find_uploads(paths, path + [k], v)\n      end",
    "comment": "Depth first search for UploadedFile values",
    "label": "",
    "id": "7585"
  },
  {
    "raw_code": "def mutation_to_apollo_uploads_param(mutation, files: [])\n    operations = { 'query' => mutation.query, 'variables' => mutation.variables }\n    map = {}\n    extracted_files = {}\n\n    files.each_with_index do |file_path, idx|\n      apollo_idx = (idx + 1).to_s\n      parent_dig_path = file_path[0..-2]\n      file_key = file_path[-1]\n\n      parent = operations['variables']\n      parent = parent.dig(*parent_dig_path) unless parent_dig_path.empty?\n\n      extracted_files[apollo_idx] = parent[file_key]\n      parent[file_key] = nil\n\n      map[apollo_idx] = [\"variables.#{file_path.join('.')}\"]\n    end",
    "comment": "this implements GraphQL multipart request v2 https://github.com/jaydenseric/graphql-multipart-request-spec/tree/v2.0.0-alpha.2 this is simplified and do not support file deduplication",
    "label": "",
    "id": "7586"
  },
  {
    "raw_code": "def graphql_data(body = fresh_response_data)\n    body['data'] || (raise NoData, graphql_errors(body))\n  end",
    "comment": "Raises an error if no data is found NB: We use fresh_response_data to support tests that make multiple requests.",
    "label": "",
    "id": "7587"
  },
  {
    "raw_code": "def graphql_dig_at(data, *path)\n    keys = path.map { |segment| segment.is_a?(Integer) ? segment : GraphqlHelpers.fieldnamerize(segment) }\n\n    # Allows for array indexing, like this\n    # ['project', 'boards', 'edges', 0, 'node', 'lists']\n    keys.reduce(data) do |memo, key|\n      if memo.is_a?(Array) && key.is_a?(Integer)\n        memo[key]\n      elsif memo.is_a?(Array)\n        memo.compact.flat_map do |e|\n          x = e[key]\n          x.nil? ? [x] : Array.wrap(x)\n        end",
    "comment": "Slightly more powerful than just `dig`: - also supports implicit flat-mapping (.e.g. :foo :nodes :bar :nodes)",
    "label": "",
    "id": "7588"
  },
  {
    "raw_code": "def expect_graphql_error_to_be_created(error_class, match_message = '')\n    resolved = yield\n\n    expect(resolved).to be_instance_of(error_class)\n    expect(resolved.message).to match(match_message)\n  end",
    "comment": "Helps migrate to the new GraphQL interpreter, https://gitlab.com/gitlab-org/gitlab/-/issues/210556",
    "label": "",
    "id": "7589"
  },
  {
    "raw_code": "def graphql_mutation_response(mutation_name)\n    graphql_data.fetch(GraphqlHelpers.fieldnamerize(mutation_name))\n  end",
    "comment": "Raises an error if no response is found",
    "label": "",
    "id": "7590"
  },
  {
    "raw_code": "def required_arguments?(field)\n    return field.requires_argument? if field.is_a?(::Types::BaseField)\n\n    if (meta = field.try(:metadata)) && meta[:type_class]\n      required_arguments?(meta[:type_class])\n    elsif args = field.try(:arguments)\n      args.values.any? { |argument| argument.type.non_null? }\n    else\n      false\n    end",
    "comment": "There are a few non BaseField fields in our schema (pageInfo for one). None of them require arguments.",
    "label": "",
    "id": "7591"
  },
  {
    "raw_code": "def allow_unlimited_graphql_complexity\n    allow_any_instance_of(GitlabSchema).to receive(:max_complexity).and_return nil\n    allow(GitlabSchema).to receive(:max_query_complexity).with(any_args).and_return nil\n  end",
    "comment": "for most tests, we want to allow unlimited complexity",
    "label": "",
    "id": "7592"
  },
  {
    "raw_code": "def execute_query(query_type = Types::QueryType, schema: empty_schema, graphql: query_string, raise_on_error: false, variables: {})\n    schema.query(query_type)\n\n    r = schema.execute(\n      graphql,\n      context: { current_user: user },\n      variables: variables\n    )\n\n    if raise_on_error && r.to_h['errors'].present?\n      raise NoData, r.to_h['errors']\n    end",
    "comment": "assumes query_string and user to be let-bound in the current context",
    "label": "",
    "id": "7593"
  },
  {
    "raw_code": "def a_graphql_entity_for(model = nil, *fields, **attrs)\n    raise ArgumentError, 'model is nil' if model.nil? && fields.any?\n\n    attrs.transform_keys! { GraphqlHelpers.fieldnamerize(_1) }\n    attrs['id'] = global_id_of(model).to_s if model\n    fields.each do |name|\n      attrs[GraphqlHelpers.fieldnamerize(name)] = model.public_send(name)\n    end",
    "comment": "Construct a matcher for GraphQL entity response objects, of the form `{ \"id\" => \"some-gid\" }`.  Usage:  ```ruby expect(graphql_data_at(:path, :to, :entity)).to match a_graphql_entity_for(user) ```  This can be called as:  ```ruby a_graphql_entity_for(project, :full_path) # also checks that `entity['fullPath'] == project.full_path a_graphql_entity_for(project, full_path: 'some/path') # same as above, with explicit values a_graphql_entity_for(user, :username, foo: 'bar') # combinations of the above a_graphql_entity_for(foo: 'bar') # if properties are defined, the model is not necessary ```  Note that the model instance must not be nil, unless some properties are explicitly passed in. The following are rejected with `ArgumentError`:  ``` a_graphql_entity_for(nil, :username) a_graphql_entity_for(:username) a_graphql_entity_for ``` ",
    "label": "",
    "id": "7594"
  },
  {
    "raw_code": "def positive_lookahead\n    double(selected?: true, selects?: true).tap do |selection|\n      allow(selection).to receive(:selection).and_return(selection)\n      allow(selection).to receive(:selections).and_return(selection)\n      allow(selection).to receive(:map).and_return(double(include?: true))\n      allow(selection).to receive_message_chain(:field, :type, :list?).and_return(false)\n    end",
    "comment": "A lookahead that selects everything",
    "label": "",
    "id": "7595"
  },
  {
    "raw_code": "def negative_lookahead\n    double(selected?: false, selects?: false, selections: []).tap do |selection|\n      allow(selection).to receive(:selection).and_return(selection)\n    end",
    "comment": "A lookahead that selects nothing",
    "label": "",
    "id": "7596"
  },
  {
    "raw_code": "def expect_rendered\n    render\n    expect(rendered)\n  end",
    "comment": "Wraps the `rendered` in `expect` to make it the target of an expectation. Designed to read nicely for one-liners. rubocop:disable RSpec/VoidExpect",
    "label": "",
    "id": "7597"
  },
  {
    "raw_code": "def set_devise_mapping(context:)\n    env = env_from_context(context)\n\n    env['devise.mapping'] = Devise.mappings[:user] if env\n  end",
    "comment": "explicitly tells Devise which mapping to use this is needed when we are testing a Devise controller bypassing the router",
    "label": "",
    "id": "7598"
  },
  {
    "raw_code": "def update_custom_notification(event, user, resource: nil, value: true)\n    setting = user.notification_settings_for(resource)\n    setting.update!(event => value)\n  end",
    "comment": "Create custom notifications When resource is nil it means global notification",
    "label": "",
    "id": "7599"
  },
  {
    "raw_code": "def insert_ci_builds_to_click_house(builds)\n    result = clickhouse_fixture(:ci_finished_builds, builds.map do |build|\n      build.slice(\n        %i[id project_id pipeline_id status finished_at created_at started_at queued_at runner_id name\n          stage_id]).symbolize_keys\n          .merge(\n            runner_run_untagged: build.runner&.run_untagged,\n            runner_type: Ci::Runner.runner_types[build.runner&.runner_type],\n            runner_owner_namespace_id: build.runner&.owner_runner_namespace&.namespace_id,\n            runner_manager_system_xid: build.runner_manager&.system_xid,\n            runner_manager_version: build.runner_manager&.version || '',\n            runner_manager_revision: build.runner_manager&.revision || '',\n            runner_manager_platform: build.runner_manager&.platform || '',\n            runner_manager_architecture: build.runner_manager&.architecture || ''\n          )\n    end)\n\n    expect(result).to eq(true)\n  end",
    "comment": "rubocop:disable Metrics/CyclomaticComplexity -- the method is straightforward, just a lot of &. rubocop:disable Metrics/PerceivedComplexity -- same",
    "label": "",
    "id": "7600"
  },
  {
    "raw_code": "def insert_ci_pipelines_to_click_house(pipelines)\n    result = clickhouse_fixture(:ci_finished_pipelines, pipelines.map do |pipeline|\n      pipeline.slice(\n        %i[id duration status source ref committed_at created_at started_at finished_at]).symbolize_keys\n           .merge(\n             path: pipeline.project&.project_namespace&.traversal_path || '0/'\n           )\n    end)\n\n    expect(result).to eq(true)\n  end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity rubocop:enable Metrics/PerceivedComplexity",
    "label": "",
    "id": "7601"
  },
  {
    "raw_code": "def enable_admin_mode!(user, use_ui: false)\n    if use_ui\n      visit new_admin_session_path\n\n      # When JavaScript is enabled, wait for the password field, with class `.js-password`,\n      # to be replaced by the Vue passsword component,\n      # `app/assets/javascripts/authentication/password/components/password_input.vue`.\n      expect(page).not_to have_selector('.js-password') if javascript_test?\n\n      fill_in 'user_password', with: user.password\n      click_button 'Enter admin mode'\n\n      wait_for_requests\n    else\n      fake_user_mode = instance_double(Gitlab::Auth::CurrentUserMode)\n\n      allow(Gitlab::Auth::CurrentUserMode).to receive(:new).and_call_original\n\n      allow(Gitlab::Auth::CurrentUserMode).to receive(:new).with(user).and_return(fake_user_mode)\n      allow(fake_user_mode).to receive(:admin_mode?).and_return(user&.can_access_admin_area?)\n    end",
    "comment": "Administrators are logged in by default in user mode and have to switch to admin mode for accessing any administrative functionality. This helper lets a user access the admin area in two different ways:  * Fast (use_ui: false) and suitable form the most use cases: fakes calls and grants access to the admin area without requiring a second authentication step (provided the user is an admin) * Slow (use_ui: true): visits the admin UI and enters the users password. A second authentication step may be needed.  See also tag :enable_admin_mode in spec/spec_helper.rb for a spec-wide alternative",
    "label": "",
    "id": "7602"
  },
  {
    "raw_code": "def permit_postgresql!\n    db_hosts.each do |host|\n      next if host.start_with?('/') # Exclude UNIX sockets\n\n      # https://github.com/ged/ruby-pg/blob/252512608a814de16bbad55911f9bbcef0e73cb9/lib/pg/connection.rb#L720\n      allow(Addrinfo).to receive(:getaddrinfo).with(host, anything, anything, :STREAM, any_args).and_call_original\n    end",
    "comment": "pg v1.4.0, unlike v1.3.5, uses AddrInfo.getaddrinfo to resolve IPv4 and IPv6 addresses: https://github.com/ged/ruby-pg/pull/459",
    "label": "",
    "id": "7603"
  },
  {
    "raw_code": "def stub_full_request(url, ip_address: IP_ADDRESS_STUB, port: 80, method: :get)\n    stub_dns(url, ip_address: ip_address, port: port)\n\n    url = stubbed_hostname(url, hostname: ip_address)\n    WebMock.stub_request(method, url)\n  end",
    "comment": "Fully stubs a request using WebMock class. This class also stubs the IP address the URL is translated to (DNS lookup).  It expects the final request to go to the `ip_address` instead the given url. That's primarily a DNS rebind attack prevention of Gitlab::HTTP (see: Gitlab::HTTP_V2::UrlBlocker). ",
    "label": "",
    "id": "7604"
  },
  {
    "raw_code": "def use_fake_application_settings\n    # We stub this way because we can't stub on\n    # `current_application_settings` due to `method_missing` is\n    # depending on current_application_settings...\n    allow(Gitlab::Database::Migration::V1_0::MigrationRecord.connection)\n      .to receive(:active?)\n      .and_return(false)\n    allow(Gitlab::Runtime)\n      .to receive(:rake?)\n      .and_return(true)\n\n    stub_env('IN_MEMORY_APPLICATION_SETTINGS', 'false')\n  end",
    "comment": "In some migration tests, we're using factories to create records, however those models might be depending on a schema version which doesn't have the columns we want in application_settings. In these cases, we'll need to use the fake application settings as if we have migrations pending",
    "label": "",
    "id": "7605"
  },
  {
    "raw_code": "def with_db_config(&block)\n    yield\n  end",
    "comment": "Overridden in EE to use custom connections",
    "label": "",
    "id": "7606"
  },
  {
    "raw_code": "def generate_user_from_membership(method, target, membership)\n    case membership\n    when :anonymous\n      nil\n    when :non_member\n      FactoryBot.send(method, :user, name: membership)\n    when :admin\n      FactoryBot.send(method, :user, :admin, name: 'admin')\n    else\n      # `.tap` can only be used with `create`, and if we want to `build` a user,\n      # it is more performant than creating a `project_member` or `group_member`\n      # with a built user\n      create(:user, name: membership).tap { |u| target.add_member(u, membership) }\n    end",
    "comment": "@param method [Symbol] FactoryBot methods :create, :build, :build_stubbed @param target [Project, Group] membership target @param membership [Symbol] accepts the membership levels :guest, :reporter... and pseudo levels :non_member and :anonymous",
    "label": "",
    "id": "7607"
  },
  {
    "raw_code": "def expand_path(path)\n    File.expand_path(path, File.join(__dir__, '../../..'))\n  end",
    "comment": "Expands a path relative to rails root. This module is used in non-rails contexts and so Rails.root cannot be used.",
    "label": "",
    "id": "7608"
  },
  {
    "raw_code": "def ensure_gitlab_shell_secret!\n    secret_file = rails_gitlab_shell_secret\n    shell_link = gitlab_shell_secret_file\n\n    unless File.size?(secret_file)\n      File.write(secret_file, SecureRandom.hex(16))\n    end",
    "comment": "Taken from Gitlab::Shell.generate_and_link_secret_token",
    "label": "",
    "id": "7609"
  },
  {
    "raw_code": "def gitaly_dir\n    socket_path = gitaly_socket_path\n    socket_path = File.expand_path(gitaly_socket_path) if expand_path_for_socket?\n\n    File.dirname(socket_path)\n  end",
    "comment": "Extracts the gitaly install directory based on the gitaly socket configured in gitlab.yml. This allows the test gitaly to be temporarily overridden.",
    "label": "",
    "id": "7610"
  },
  {
    "raw_code": "def expand_path_for_socket?\n    !!ENV['CI']\n  end",
    "comment": "Linux fails with \"bind: invalid argument\" if a UNIX socket path exceeds 108 characters: https://github.com/golang/go/issues/6895. We use absolute paths in CI to ensure that changes in the current working directory don't affect GRPC reconnections.",
    "label": "",
    "id": "7611"
  },
  {
    "raw_code": "def process_details(pid)\n    output = `ps -p #{pid} -o pid,ppid,state,%cpu,%mem,etime,args`\n    LOGGER.debug output\n\n    stack_output = `cat /proc/#{pid}/stack 2>/dev/null || echo \"Could not read process stack\"`\n    LOGGER.debug stack_output\n\n    load_avg = `cat /proc/loadavg 2>/dev/null || echo \"unavailable\"`\n    LOGGER.debug \"Load average: #{load_avg.strip}\\n\"\n\n    io_wait = `vmstat 1 1 2>/dev/null | tail -1 | awk '{print $16}' || echo \"0\"`\n    LOGGER.debug \"I/O wait: #{io_wait.strip}%\\n\"\n\n    disk_usage = `df -h 2>/dev/null | grep overlay | awk '{print $5}' || echo \"unknown\"`\n    LOGGER.debug \"Disk usage: #{disk_usage.strip}\\n\"\n  end",
    "comment": "Logs the details of the process with the given pid.",
    "label": "",
    "id": "7612"
  },
  {
    "raw_code": "def issue(signed_by:, expires_in:, certificate_authority:, email_address: 'test@example.com', cn: nil)\n    key = OpenSSL::PKey::RSA.new(4096)\n    public_key = key.public_key\n\n    subject = if certificate_authority\n                OpenSSL::X509::Name.parse(\"/CN=#{cn}\")\n              else\n                OpenSSL::X509::Name.parse(\"/CN=#{email_address}\")\n              end",
    "comment": "returns a hash { key:, cert: } containing a generated key, cert pair",
    "label": "",
    "id": "7613"
  },
  {
    "raw_code": "def kube_pod(name: \"kube-pod\", container_name: \"container-0\", environment_slug: \"production\", namespace: \"project-namespace\", project_slug: \"project-path-slug\", status: \"Running\", track: nil)\n    {\n      \"metadata\" => {\n        \"name\" => name,\n        \"namespace\" => namespace,\n        \"generateName\" => \"generated-name-with-suffix\",\n        \"creationTimestamp\" => \"2016-11-25T19:55:19Z\",\n        \"annotations\" => {\n          \"app.gitlab.com/env\" => environment_slug,\n          \"app.gitlab.com/app\" => project_slug\n        },\n        \"labels\" => {\n          \"track\" => track\n        }.compact\n      },\n      \"spec\" => {\n        \"containers\" => [\n          { \"name\" => container_name.to_s },\n          { \"name\" => \"#{container_name}-1\" }\n        ]\n      },\n      \"status\" => { \"phase\" => status }\n    }\n  end",
    "comment": "This is a partial response, it will have many more elements in reality but these are the ones we care about at the moment",
    "label": "",
    "id": "7614"
  },
  {
    "raw_code": "def kube_node\n    {\n      \"metadata\" => {\n        \"name\" => NODE_NAME\n      },\n      \"status\" => {\n        \"capacity\" => {\n          \"cpu\" => \"2\",\n          \"memory\" => \"7657228Ki\"\n        },\n        \"allocatable\" => {\n          \"cpu\" => \"1930m\",\n          \"memory\" => \"5777164Ki\"\n        }\n      }\n    }\n  end",
    "comment": "This is a partial response, it will have many more elements in reality but these are the ones we care about at the moment",
    "label": "",
    "id": "7615"
  },
  {
    "raw_code": "def kube_node_metrics\n    {\n      \"metadata\" => {\n        \"name\" => NODE_NAME\n      },\n      \"usage\" => {\n        \"cpu\" => \"144208668n\",\n        \"memory\" => \"1789048Ki\"\n      }\n    }\n  end",
    "comment": "This is a partial response, it will have many more elements in reality but these are the ones we care about at the moment",
    "label": "",
    "id": "7616"
  },
  {
    "raw_code": "def kube_knative_pod(name: \"kube-pod\", namespace: \"default\", status: \"Running\")\n    {\n      \"metadata\" => {\n        \"name\" => name,\n        \"namespace\" => namespace,\n        \"generateName\" => \"generated-name-with-suffix\",\n        \"creationTimestamp\" => \"2016-11-25T19:55:19Z\",\n        \"labels\" => {\n          \"serving.knative.dev/service\" => name\n        }\n      },\n      \"spec\" => {\n        \"containers\" => [\n          { \"name\" => \"container-0\" },\n          { \"name\" => \"container-1\" }\n        ]\n      },\n      \"status\" => { \"phase\" => status }\n    }\n  end",
    "comment": "Similar to a kube_pod, but should contain a running service",
    "label": "",
    "id": "7617"
  },
  {
    "raw_code": "def secret_key2\n      <<~KEY.strip\n      -----BEGIN PGP PRIVATE KEY BLOCK-----\n\n      lQWGBGN931ABDADe6KRsn1d37PKH9QSZiDqyGu77Av3vPlAwRHypUEEAc47WNle7\n      87CmIaDPKQ8f5R7vu9hpVX+Lisoy23s7lM9nvZcjfR/t465oP5JimGSOiQ1Ilcgz\n      eCvOmbvVdiSQthqrQ5oUY0jmRtnEbpNC4LMV3+i3Npj4UcCeORFOWNf+I1AiTtLX\n      fRyw+ifGjqxe/0dVt4w65kZbpetYlxGoYCjAMPZT287chfJCYeNm8N+8T9BKx3ex\n      Z4bpAGY0hcZwH2Qo5Dg6MFGn2oQjvGmD3iVJ48IEN7HPtiHeOoD8rlUG2smiAk7u\n      ZuSiNhEKf43p1hSOhUyB1KBrFs8/npNyhOIXzrmE8cFuUgDeUZQX/c7BS0QmNdmr\n      nRn1CUzYYHSsQB8oGMueQPCmjZh68AqRfIYjZW4KbsqydjPmTRUomZfCB0RWBd/T\n      8Ycvdh8NFQRCCcHfcUVj/PnMyaUE1aAf4xApA694ceXK6GtMz+2ZGNAThsLDrzfw\n      VtR6WSOjq+E9Ab8AEQEAAf4HAwKu9Xm6kU0nTPsmXAtzd6ExPJlcvnfhKcC8EZHJ\n      0neZ5hOPbr7MFKI45yESxI1mWdaPs+Oz2NnGtfc3XsJOyZIP/IMZ+Z+nMAf+F98A\n      akmqAWL96Ku17XQxl2JWnmOVjFmBYlVIdTie71nf8OhUWdTuDs42Mh4u28tKiUwp\n      dJ9JxFL3DAyeBM7gKs6OaceUMyMs82eeMZhGB1KzzeW+BZgFYT1tGeR1d875IYq0\n      AiwIPZViJZtrCkm5Sc0gliGIZ1kXbbeRc8dR3+RDjOF8U8oqMUsNw9ah1zuv112V\n      Plxy7t2ku47GsNcc1quNB5ORdCkFo//vMhDB8X5fIDRXDNCDaPj8iN47TuV0jiEU\n      TfMDq+aHhtW8bIrgDXFAg9LKP8lQDwz/UfR1pLHAdRNRTVYayDaNs1FdOKM7+DD5\n      3HenmlVyzVjR31oHQHlQEfSQ47beu4vSNIJ9zjj4PQU8axBQt9sMDg+hfwFqSgkU\n      NjoSTphc1lQMoRGXF21Bud123RjWpVVbjCE3kU2lAFeuT6FSrWTnICLs0si1p31r\n      flgPHrjSNTA/LNFCgjCMUEr4Odu69gQQ839vvTIne1OGcuPf3qfXNNKo9hygBDWY\n      5nCAnHIFZIHp9lTvmsBZVzW1QvIES7gOsovqLcWUxN1rR/dO1ETpaukHAVyyxUMH\n      v/BhMjeNOCKmwK/gNoT2dyL1pauGuG5hDqteFpteMsxH2CuoWIoA+egK1pzP2CO2\n      0IUh36PX3HSPBKrgNPnJsSyycCZ/ONmZ5XDWct32zqeH2AMnMAQ5rbao4nupGVLR\n      G7UmBLu3vxzZKzkSaibf1DkG14wtINnew1UfZyW17JtDZKWC3LWRcRZ7Ryisz0YK\n      8z+pAZlpAvXZJV7QDgD1/94jgYPPqfO7bimDV8t8rAU1E3QlF9jnMtjieFsskiIq\n      /g5fCpfWg/OS9Epdbv1yWAUPDMU/ZsthutD2P/3rN0bkiX31uM6zDfhsDyyzVWC1\n      1aDnQ/ZR8DGxmjeT5vPFdqHRCS5sAJQjXh/UKM26OmJDES4idywI8TS8yREqoTB7\n      MZXXfkJS9UZkS64Fm6iQDWxKe+8ll3NnZ+YHh3bTFiKy0LLP1tZXK2WeD8v06dFC\n      4ltq8A6wQAt63qdLzAmSlzTtzRM82qN5CLjc4U+TIiz34iTeiovZudR4hWRxG/zB\n      kkRCoYw2c1kbAZz7TriyeMFxn1KkQMneVENS7TqJplVYWu60ZAycY9ZAKBG/A94I\n      zfjVM8FD0xCsqWgFOGyS3PSU23z6UmHnHBzlFhgWpB2KiskcTk9NfpegnAHSuG/C\n      0rFwFaYRoM/XUcs3+3pQxXIBXGmfdkoLPrQtTmFubmllIEJlcm5oYXJkIDxuYW5u\n      aWUuYmVybmhhcmRAZXhhbXBsZS5jb20+iQHRBBMBCAA7FiEE7BZIQjEcz58AXaGj\n      T9gwzrPobJcFAmN931ACGwMFCwkIBwICIgIGFQoJCAsCBBYCAwECHgcCF4AACgkQ\n      T9gwzrPobJf+NwwAmWUPuV34gNWmo3eseq5ZTuekokOBEhpe/wmbmjz73gtuTFYj\n      q6JY/Xz0kS0VJIHM9WvBNKMeDUfqtZFAWIo76ePZDjOC/nRb9rn12XMFdxGD5bD0\n      36qOQqj6mRe2qLJ8vHHooRixwPh1cCB7PkEMyh4L2wnUaZe2q8RL2hPxDWRc1uKZ\n      fX/lJlfnxlzeJyJ/qo9QVXcDfe1vFPYHi4ZTxaz4CkY7Hfjbwyjc+K0bqlvNzLdf\n      ev+n5ZclSD2biNDBjg9tqvlrqThSxxCVXps35Olp70PUMMUBq9F7tC557xNKZXA+\n      jAhwM/S0cbIA6ordcyaF6NX/1BR922GMoqOUEui2v2ResXKp4KhO/pMh+yhqICdR\n      nhaT6Nequ4Kt4rrXYBCKBdE9wU9w+TPRz+Y4iW+rLmCcomwObrGaaNJQJx0EwlLa\n      C2hQNfUOTeJi69IlivqlTiy6va0ZKX8JY53Xjjr3/WGnfe29vj1OZLWVRD0P+rsd\n      4nF1+3gPbidwrF1DnQWGBGN931ABDADPf7v9dZYkkB1fGdSNZweEPx/j4VlEs/8s\n      D5j3AIInds2JVKSTT6BuNv6A660JUAHHaQUFZeIF+x7Kk5L74Ajdnwr9r522ZWcu\n      mcGvGrz+/W7qny33g/rK+EUHzM9Xjx7iJbUhDJgX0CFqx3B3viJdIZ6pysYhuQYu\n      33shnogadANozK2GbUJe+boj4ju4ql84VrxTkHjD78yvFCeGRyoWsTsD420rV5Ob\n      F4dyNDUywTQx9atTt2JuDKPjRX4Uh1foMLMDD7O1B2/ZNN2law99hsAUShcXmhzT\n      SucBOb4KNu9QQG2Rg8cW/qvfMlB7tfymt2DsEtLYHXn5KOtb2J9mrV6oJaSKZkwD\n      uwyvO7Ay5XbjS4QoVOLmWbMzaoitQrjtBoCoe9qUAO1aS0uV6iVh/E3vk/3vep7g\n      d2qzICyvd2cVQyLQ+2RfIIFKm98Cp/ItpLbDbdSMkNEst8PwdwN23xnGIZf803bH\n      Unh84YBfJhUBCoG898CpgVsSZzXyQCUAEQEAAf4HAwI4Z9yzpF+23PvqnazwQPDL\n      YZuvMhimnQCV96XtzHgFm0tnjb8356SANp14H41q5G3Io514MtxhgUVi/r82IJlC\n      SJ8gaicZFYRSW1WTH26sNBbTz4+xbPik+sMCHflfRZ6SaXYP131hHbUwYhfCSjHN\n      WujC4VVRBtrpqgRLhAPzJulAV/HZzRvyU5i6gfDqvIDYqgohd+YkLTd8TOkpfsD9\n      VBAVlUWK/8ibMcPb/IOVwrBsh7tAPTScPlkWCQC57U9CQIelwFUsslfOwu77My5r\n      Q7+8Xc/YjZ2u5ieln6Wweu2S50J7BJwNGESiQaRGOUwCXK+CA0XHMeT9+ce4DchQ\n      +SV6+jb2VoJ45Z0EEa3VkOe2KGslbhBNCl4M//CYl9Wx8wFVU4twTcJQRG5Kcn6L\n      65p0/2u6O4zgoFIGK6KpiQIogZTA2x8secONHjNzsQ6WHPD8tdPzGa+2BLV/Zssp\n      AgHehj6NN3j5VBJHhuqlAOhssE7YjAAG4Q/nuUzbiN7/gYJdcNLzz9FkLH0n++/D\n      P0/sbOIdDQDcatZFimhFAF+KHhd5jk5PZJgiG17RBvJg/JjG1BiSl9vih7mLlu8N\n      Ba5asxGbxFz75Guf5v4poj3nBLG+MJOR+1X6+fQvCx2oU/YZUFr5PnMuzWewyDEj\n      NfdOHBADk/oyypicIuxf/uwVQAM+uGaA8ghF6WuP7FoWsoWU0vZ3dpJuM4XZweN1\n      V/yEIMDKeFgb7npWkKRLn6bFrmK2H1RkiHsQW8dcplGBxvRYZOsKCh39w8f3i+5s\n      QGOmiK5lAjxoknkwMLL+bHZzB8xuN5HOGLNRwQs8hsuSavYLtXTDti49qZSDjPm/\n      7Sjy5gd6yvhSZ43qg9cQBfRVr7lSBpBKImYEo4YbLybXo3JIsbLd2fuWfcq8bn0F\n      InJdxU6N4iBtc9ApbAm/18bUnhVAR816mHwJY7psg40jYEbkDf5OPq+gpxiXbzps\n      4IXKUNXTZRJtTu8/1G2zserWu047zqLjeerjeX5iNpZLocMdxL9IomlcaQeiywvC\n      fFRJfFtm+QA9sLcozjkemwoTWS/IRC/uQiz9/1MN6qj39M0efEubDDPs+N730ukz\n      010Jy+D/dIFsrlCzoFqxAK9/bfxUT2Kh3Jd+Kes3wrm2PY5S4OnFOFv8oQj/ZYCC\n      Jpt3UIPMiDjoZzHoGPVKTyVHp5OaZVL7ROXyqwmSNYHIV0BLPmDZLo2Nmwc49ty/\n      zSpUs0YlYsKJGjjl0Es4Rnb6nmxKOdlf7pzf3KNeqYj+FIysDEdpViQhLQnOsb1L\n      yrdlEPHJQlQu2QbKs23Kq8VS83E6HEr8QrURVoJ+WSwMjdaAuYkBtgQYAQgAIBYh\n      BOwWSEIxHM+fAF2ho0/YMM6z6GyXBQJjfd9QAhsMAAoJEE/YMM6z6GyX9mwMAJmg\n      94l2U/fpHACWN8wbNQ61qWM51SqBYeIDCg1fZO/HPogHgRyvjLmqsp/FMTeFcMhF\n      RskNMFy61uWuoP6GnHtWWtW9jf4kDZdgZbzbomDNqhkb32i3wIegy5pjF1Xf0FvP\n      9cnBBdWVkmGyQRz5c9bvhuVSWwkbGjMhhGSMFAnzNWvFtUOd7kgp0jo+7uTrtQWx\n      uNyhpdsXm7ADbi7V+1Qr5OSsI919J65K3LXNKu+r1R6wixhCyPI2jt0sJj/R4Q7G\n      H4y6q0rwE5Ogsa+MG2xrZSF/y1MWENBildNWVtB2jBm3y/AswR8k+1iBMVUioqXE\n      15nuWEYrNdg6SV/05EjOiji3lVXRHRPdyX3VuCTJ7E7EE5lk8dx8/kP4o7fbYs55\n      HxNPZguTSQiMyyS0ocD/ac5AoUUxHeSoVwr0Qk0YcYUMiFb5ZgXD3hr4S2y2TqyU\n      7TQPXRtKZDhTqxCFGMD6qTWh7ysn+Hu60VIDw8enikBOwZE1oTbrKiTywMbufg==\n      =VtBY\n      -----END PGP PRIVATE KEY BLOCK-----\n      KEY\n    end",
    "comment": "passphrase for secret key 2 is: qHGKKXBZ72VamRMUH6d7LsLWsHnJJx3p",
    "label": "",
    "id": "7618"
  },
  {
    "raw_code": "def secret_key\n      <<~SECRET\n        -----BEGIN PGP PRIVATE KEY BLOCK-----\n\n        lQdGBF2QYYMBEACrIy5plkXAGvh95RWhPhN2JVnBkcIuflojdsdOzqbrm4nMmkqc\n        qU8XHcz6+MhqBH370XIW3QxJ2RXQEz6ycHGNkM8PFPP8uFPE8O1zEUdIT5+j000V\n        MoNdKBhxwvloMCjaMrYBBCcpd5eJclSsg8d9TOxYDnjlVWzOQuFhQ7bFmTGKaD5X\n        RPG/73VN6HqfqtnIgzVTcOLYZc2pFAfxLpVMAg/igXdxgpp8ooovKzrHC4a6WkvJ\n        CsYUksu2P2CbRZcahvJnbf5x7dXdfzPrzu4ZidnLtIq694+bDuJ2nhxL/9775utW\n        4i1ExziEq8xdndz/EKlY1fowLEPA2wuB5VqtR2choY1FjwSMQiPL/dJEFIBwOJeN\n        morQNhsPRzdM7LUXmU7SblYQYJTuBnZbFUHCBRA2rAGFwWAJe9f8wDqDH2Y3oHpk\n        bPE1LAzIW2BF56otWfYYyL3IxxjyDJsEPbN9JPJ9LpGdco/UqLe6sZFaDGhRnOpq\n        v7tyMVKkX42sGHRp6CB+2g3SjQWGbx3ebNN8pFoIHsHYJXIu3FhygbAZi67jLUA7\n        O/uyLpy9jNaGwQTHJEcv/FwBA8pcKVwGniahgsDCfLNiQDQxagXEwILEie0pLROE\n        UtaXwe0Fv8KbbEFRY4NKuIKYzc2R5YknRsfoiG82ikPZA2P4wj/Gu+H7GQARAQAB\n        /gcDAhdgTwPLD6FW5xos1L485Zm6eFl7JRpKMiNDswBXHYyv73yZDcHdkYuOXBKN\n        p2LumGgPeIBhWo/Ui07KPrkOhYBqgGvn10qyKaCtqCvEkJClzG+mdIZQNUyi7FRy\n        37K/gmkx3rZJF6ftcCA6YU4YeZ8dbj9hdfmjXoQhZWXBz1CBD94J6wVgmscAYwJe\n        1K+KazcJo3IQdLTfZAuM3f9FGPGW3ZIFiMt0pRJZhWwvtPYmgAeiMqkuQTxev9sb\n        AUQ+WhRXcm+VJqbSyC3wjuW4wmRMqJdwT+Ahpz+qkRtluiHNqQZKvKl1cvaNZbxk\n        6tm0N4OcuT31TJfxn4CfXqlUeKkEgtR3pL71PWd1Zo+MaLFjXTbzvD9epZtvDD3r\n        f924H5qYM/DNpRXK5/ivF7PQHW5FMQ77o93StjIN0iYWx0XXDcCSJNQr2+ITFLJs\n        Qr66N8y5JgZ++h44WUAZA3SiyV7OqdIqhO9CPYGgB00wbJszgH02ZhtE5c2dCGL1\n        nbRuog0kUkp8YmPm4Psoq81AVY/hmGVj4cQSqEEVA5gih1mju0C2Q9i8K14IS0zm\n        /wA4woglp+w7uUwyN882d685943HheP/LpRd+uip4iIfoc2o+WU98egUB+6/OsfC\n        qgeE6bO5TS/nZuEqgai2NN/Dg03aMyjF2wouD/5b4+3Ngw8UdLszxVsmOWzOtxaB\n        aOm5oVp3Uc9qii8n079HhreSu1MdxV9rInIMRrrQCj7umS1mPvPZrHsFLpxX+Go1\n        gjz5g67CZB58YyZf/1iE9TmY5ptXhxrgkALc8YJWABkMCKCiTyVccILEUB0GQAMz\n        EhULDUO1u5vide25ob/CAHcuA5AtjY2hlZNNEocGNBS0hSVXhKc/hyjysf3QUacY\n        V0/MNUEK04ij+v1Pi6C634uapPIIE6bJmf3bjiCy58ioR3uA2JF1fEmQZ60M1Z6s\n        dSesdN3ZupxUtBH4gzWMmViA3dRCQKqGSfekMORC/pojtdXhkKvBbOqDJpja0zAU\n        IqCarX7HVMAYzwYqG+ElKOihgN1229bTi12+ahaapQvdu7JNnJfZrsk2wYRxcxuG\n        h2V5FhEk1/FDOHL4yNrqL3eFns0HKKTChR4h1rzZkXwm0xDG9dwleJzz5CVTJGvd\n        vI9u4Mkk1/OVnDs9DHkzsfeReNcdepinW4HK4oFCPYYB9NgSySxxMOIEFLhNl4q6\n        re2Js9TkqDrWtEdHIW+hty8uhzk2Xeh1c23L5XiivmEnD74PcQbpTQ3Dvp0hvTzo\n        qhUC6+1KezUeOJD8IRBti8pxgZkBPnhvzUmxEiRWmviUHzCppmOxUT7X/8jb5PNi\n        jOOxtBXgIhLiw2dtCGcTL1kBGn+6cjOcBujXGPkoJEi6w0vLF5Cq+3jt/3P5Gfta\n        nTnBs8bNBvIUpwrtyvBYy/k/6B9xIqEi7Afomaspi0Owz9+MtXWaQrnL6l8OXU5P\n        XLS95c5Yl95Crj949oNzuH2yuJ3eXGac7/i3KcJwtWltFP4yLcS24SZlFhVFQMUI\n        GUZrTxPDOkn28cBcQvDtbU9eA7O5Y93N52HwQVR2LyjdbND3tpGQObG/YpvqVSOe\n        q/9IAT8k5wZ4XYB5EBukWGNZ/z01OJpaPa73cXN2IiLZvkFEeRIEVmaxD7fSYfTy\n        ygjxz6Pum/3TrJ0CjubZfQRjwJrI5fAMoEbMGKE0oboucBPawLnNIecy07jo1ixM\n        4Q7kzrodMrRQ4YvsLdNB8J06a+FNQWeTWeNXJcczNJWDNhqUK2QWHnq0H0pvaG4g\n        RG9lIDxqb2huLmRvZUBleGFtcGxlLmNvbT6JAk4EEwEIADgWIQTCl0Q4xqj6zDNx\n        KR0oWVAhYzyEIQUCXZBhgwIbAwULCQgHAgYVCgkICwIEFgIDAQIeAQIXgAAKCRAo\n        WVAhYzyEIU+4D/9iAtghVlUS387RClem6BDDITBqR7yiBUmYNB69xTnif97Q2tww\n        vvC9WPfZfD43ZzZoqxs8ztIWplEIStO0IstpQUBOxR99x67LjoInBbc9yI9MIJrC\n        V1gLwdYctMU15+uJvyFjnN9JyhCg4ZOdRENaHtaWdpPP7Z1T9pcqm1WHhbodbpgm\n        Ni3o2uuPuRTCXgE2PPInLS4zshYXLB4V97soO4V/Q3doKXXJ7aPP5Uh3CKpRobyy\n        rtWhYdzGOl5ca7rgkktqLTW05W7OpA35jEQa7AgjErNZMoJUi7zIFRvSbn3pcr/x\n        raw5sraQTqyHTU2IYvSA33GtrHse2clN+WDKhHXJC8UHiq/szHj77ATwk5UQ3WpW\n        xH6Kk6ECidYz7e4VexSFmLGzttM6BXnAk4t7YkMIU115ghYFR70Uyv7AXWL5hFjI\n        Ccw9S9vahODi3adG/5QX20wE+wOL5ktGqcWxQ9p67avRY8sgiYtbqhaBoPzaCm4F\n        zk4I1VLwb4XnoIuTz2Dl2gLT/A5/rdIqQiub7+PG4aSeH/2Qe9vZtur8CDSNndFE\n        tSLszjzPKylP3xp37Y+SgB0QATZ3Ww7OUYLEefXbI8v2GDEyc/VRYzHuTYt2aQ53\n        7vb9wyJhHQy9oQIBfYaPwsMlCjynZL7fl5J3pSozb/NllP5h7A/jHmUZSp0HRgRd\n        kGGDARAAzjsHpSEsJYZn8Pye1a15gKzM2kV+BfK+dJdRFviRLfeX6e54eWVnrB75\n        uWtQIfXawzIaeNjTaRgUOW6JrdEsDoEmTnP2bqgoDHZPfocm7VoQHTODspQJW28L\n        O0TeNHEFXQb2STfzOAe3COqJab6XhQ79/q60mNZiZNQcwh9MvCmcDzNQqE7V2V/z\n        M2VOu9DF70YaGhE+mayGxrTzsssx/Sdo2YjaE3FuVb68/+Wk8HEoBupQmJYMLsFf\n        ccWDc19H2WyBZp5iTtN21UrEfacfOWGxBmpcPmHP5D0Nb/tvWmTVbktoW/0MRCH+\n        VAanm5uyLL1QeuEuxNcIqTnscg7KIoXLaZMbeAPY/+DjJrjcIojsriopC/f7UMDP\n        NIurbzEztO/RvQzKCr3Asr3RJW6Ql5L1zaW0o+JCz7h06+vYSn6scpKG55yrA5av\n        RqKQP+7i8AxCDWORINZ2Z8ckpvh0HsOlIXTwLUKwuvjvoRul3RH/zqpLOc/t8hil\n        wpH75Zg73S9tIGGfQwW3kzlCR7iNUloB9QfDN1T1aQ/kuOeFjmkG8fjh+UVkwxzh\n        cHZwg5pT7kS1g7iwTkF4+EBPdIEGpB5EsvupJbnZkPOUS8eTU8hVSCLFk4x1v6W9\n        pWFHyBMcspU0IU9eq0mLte4LKfZHK4cCGuMi3vyMShg4qDOVye0AEQEAAf4HAwKr\n        FFKdQyXE9OdLh7w0owYE1piq8TicrWLjclS9DoEkM4jpPJZ6nhNjJlLxgB9LxyWQ\n        vKT50xd6Yhu2z/c4NUSbMFXaYLuYsx21mu5Bc7DASjr3cB06cBolaYRBjCOhhD6h\n        6ujnXxj/mxgkQBX7NN7CRSEtkFwMVaLGCaNo+r/BC17Ich0jzBxPKTkEnjjB9oNU\n        bYPJqmzKnr/PRynZrCg6wQ5n3LsPaiYisUydeuh6kLDJDpk4e+wzFBHslqpRDDoD\n        diVuMrWCWcWpzHx908YCql+0zf/0KZZ61Q0EuoUz6610AFWTmfXj0qLI9fG/JfYD\n        s1I4al44PEia6lO+impf5FaZqktFZfivz+G+d244xpck06kkyKH6OVHL84+eA/oi\n        x8aJ4vGdAKzEbYix52nMzWVbWqHEgTTNuShsErwpMI8FGx0rWufTc+E8DPnKnsoH\n        CVCbF1TooGZBW0cYfzbi0VhAIVAg/h+6KyxaA6RucUCvmiynvSwvKXKlk9uwpEoo\n        F3zKB6/EMd3pxlAM9TSqii2Tm63WPz2SWkUvV0WYqftuND9K907ZSTsQf+W4vfKy\n        VKsuwbKzfrg4ggDwwuj6T5IQVKT4eTVBE/5XhLlkNkhMAXjwgugnJGnsJ+M+Un/I\n        j8D5nOsTiYS6QIyobMSa/gNavcOMbZR8N4h9wY68doMztSEvvCCHyIJGdDtp15ZP\n        ebYvHlgyxkVCgb/1HmuniGvSw8zEOcJdTx/fO8Fb+Je4Ih/yjWYkFj+3BmvylaoG\n        fQUaFG6rf1x1WzCKAnHZlratN3Hq7vLDLXDbjXRRdY2FsCJpX2a+ptLZHNPdSa/y\n        sy98ut2TdZUE4b1wzLMNCxqUf19sapJ7ZEIk+IFBxvsL/u+EYoMpICOiz0JOb+1w\n        bzfAmj3SOaZTdZaUk0EZPoXxA54QVovvONYlhC4tuJnoKsNJ6IStMNd4ojrYSZAo\n        xzFU12Y1dXcW/EjKpUdc4q6t80/1VQsqIFvTwDeKRq4J9xaW8xTfecPCVYOacNRZ\n        6363vaoQuLSIhrvrL/lPTx4veERaDj6UkBuH6EIyE1q/sTEbs9e63CcSTw7Gmqq8\n        ZVIQT4bl/Zg7/77Jg/Cw5hU7m6rWaBQDHlJjxkt8mA+q4E0uOQkA9MoE/6tbUssc\n        cwwesfwiID898lQJPtsE1Ne4I9UrKjR0tzf+/m+p9vJjRLHClMosmCgIRPoy4tk9\n        vTLi+uYPggtxIibblWQem4knWP1q2w7Xu5ycGIiMXRxoYIQVkefBHWZSVSPJ6WkF\n        Cwb/K+gE5ep4/irzD1OvwQFmz1HwzoarAsIm9kzhQg5ftis+n1DbrLyf2CGdXd61\n        FGo/NKzA+3ONsK1gyKPiOFJzzccT05OcF3boD18QpiOnwyjjntipNg7e/fAmCHB/\n        IqO2cXWhAa//3Cxvjj7c9erY2cuPA2CRcjpSr6mht2hP4jkDaAK7UMXrXvlJQJ2O\n        TMGJRmAspsVa36CoIhUTJ+bVwd0M4H3ty34QmCKjEc+efPj7X21h9zJcNLsrTNSk\n        XIuYelWoanmW9d9//OBAXdYUy3AqH/tspROFLTaWYWpN1VIHvn60dJSYn397MlcZ\n        oM7x6Zilu41sSHDQzCi0oPiGReDlDfdNZjbCqBFHhNVO6SDq/A/XX6jVPlcjaOKJ\n        tvYU2X9cH6YBag4+KJqBq1CdEIjEKgzB6QKwY51fVOh5asAbNC2pqEKeZKfej8RQ\n        uwyGLvrZOLPTY39I2x+OULGh5HfUZUo+K5/G063XZn8peH+FiQI2BBgBCAAgFiEE\n        wpdEOMao+swzcSkdKFlQIWM8hCEFAl2QYYMCGwwACgkQKFlQIWM8hCFSoQ/7BYzi\n        pVu8Z4rScxlDnJo1iFHckHW7rmMngE7norQiq3Hqzbs36maOL48/sOfdSBfwUDT/\n        RsUuW6H8sOXV9cuFZKwRWlgcFqt7g1BUCpc+D2RCKMQaEhUGmtmZQFU807sCYGqL\n        j6EHWpMJoFgkmtbac19ZqH0OvtqGlnRXnNgETCVc8iCPiQEYful2PpNt9mdHDr6L\n        EjL7DQoepdgKMQC6r1QeNeHwuLEdNAIqTzPqdM+UlGxTtIbDSiBRa8WD1PIPwEDq\n        eHFFcR3LK5yY9WLzbOyo6cAwa/dPHYL3HpizgmY+MrxiopHzNxcFIqF9bU2j0Tia\n        7h/N58sdGkr3ml67NDOl9pjgEPEj2rEGVApyOI46l9Mf9v/TrO1Kg9N/FNKUzArr\n        +FWK1L5JK0WkceupsiYBm9pbgWUu97sg+t7JBPXH7Gn5gNNqZRDiam0YUm5WAOpF\n        ONn4FrVwzZcv6Rs29qiadvCuS9sPdaDJgakliQbGL6f5vvEaRQ/F7Vmc91aqFUDx\n        TXzfRujIBKLOKoBN1xgh4ElGTLgRU+E9TuQC+Wk6m+dsuG9EtFsjMgUfBriJjw69\n        H/5nv5yFiIuQSVX9fvCl3QO1fiXvMyY9FXfRy+nRi6fuAYuqmKcNiRLXnCYwWJDp\n        /UgXnDqgSSXAe3f4552KPbxVtRGLCdk1dztjxbCdB0YEXZBkDQEQALOViZavAJFG\n        pwEb8dN85K1g8zHuHYh9kbok0iP5q9KlMf/NltbejuR7+T5LLIBk4dZsHzeFTIBf\n        rXYpbmV8QmS76ULdEyGsU4SssrDzHgLRmsLVA7xpoWKomo4imsSoS/gBlQya1yGu\n        gDCCU4hyk7zY4xbQOkuTPWCeVQdhyXcIe6QAGGj15Wplg81rfyZgK7Nudf+FunRu\n        livqEpWXmdoamvdlax0Y/4a4xvadtcVRxMPH3bk4FzYjXKheQczTv02PyKv/iOt0\n        qDbETbfUf/OEfEk/4MNE++ukLxhNF80HaHg/Wf0NW5MCrT150/dPGTHCvpIr9MJH\n        PssgaSIr9K8K0vTJoDVsKTv+PdFJ5ELBdDqlUsk/AHajGp1cnXZVlWyGSaUdkz19\n        8xxSm1C5agGRGgO+TAvILRWHEYpxoT26lb1jJ7m7qEuU8+b1k8Bjh48tMwKQNkwx\n        f3j+TQW3b9lGo/ikV/E6NtkVpBBEmuX9LkNNzOErffFLG/bM7CsdBhTZbQaZqueR\n        I6RG3QzT4jqGfmN2SCnHOlz4TRPTvOo7casdyjU4FrqZni8k6IQNVcwhl48mDUht\n        Brg0sJlNhE/SXFHz6TzDHo3hw4iTjfpaNDm3icc/C/XYIccmsvazs0OOP+PtXx99\n        Ph9B+symlnxuDGx7yM8ewF3t05Ye2kAnABEBAAH+BwMC2s4YdfzUovfnEr95gTA9\n        GyCSMONbAdWew20r6OPdVDUNOHfsHSdtkCgLI+RUcgO02sFyNygdozGj4SYowQSY\n        YzOgUoUI4w3NKDojS7f1LMP+PQL/1+oG/koo2FNay131KnaZauNmYYzubUUJxPGG\n        sVH9Var4D2DPCzWcpRlhxXC7mIX3YPB51hjlHSE9QiJPGNwqri6LTaq31Ypo68K9\n        YqWU57WKxJR3YnJ6yrHFAhuPEubjy/lIAfcgClYncaHEbH19Sq+Nbe/LFq5nhbvm\n        UcVA4MjoZalF7wjLnQRZW6oC+NY6yTx52bIjNCcreP4pD/Q6bP+xEDBe4ISIAlNp\n        tYXFq4o+EVP2gTa1yoWlAgXHg9b4ML9Gc+7NBEzrZlQMDUDzjG30FlzBA3Gi8Dmc\n        rSo3a7MOqzg+QcuZvHfS9FH0s8NlglHqC28t/5lM9IfWaxQxv8u4+5up8GbVYvKx\n        HXl9Qd450ibBwKhOpLlGHVZMTe1ju925XCMwCFoOPxO/QhAaCFnkPjda4KFOD3rB\n        PuToKyGfJ6Fs0lO2OjgNxXCYLTWpkxietVh1Ff8GFAZ9QsGl8nJ2gAJzqFDnPZBU\n        4hXI/3mav5aNo3b/avRuzIFxuZbAUFAHbwcTwOXxYt1Xc1mivlyt/P27sCWGaOV5\n        4W/vJ6OFdoYtzNsGzIo3VqQVSUwcQtMC7cqLlLk6zcyxSdIO+1bsg/x50MyNCZYh\n        q3/KVfYHu13Jry2bNQSwCZE88U+KEXDSsWcRS53El6P+gsy+XHCtOEst+qJE30kK\n        /LdVaKU8Xlpfy3CuiWyPM5nELWQ4wqUke5XwhUOl+o/CCG0A8q+Fj3y2Vw9/f1bx\n        yZ0axxaFa8+LdQScfw406g9hF9tY4qHx1xlO783dhI/MBhszfyUVnNXytLTaX7SX\n        YtvOzYV+2GxIAQRt9NC332yWAw4tOCIuAMibv1ZWrGaMWdNIuk3z3zXkZ/YyR3nx\n        7mdiOivzD8Eba2eg6pLP6lLTDfGFTSdUaUc1LHMz3kl0dZNLqFwJcLRrpEfpQo+C\n        XbbKxjd1nQzVtNA0BdG2bvGyfNDhwlgWkROIpmu/KX7ZTTz55ahl2qSBvH9XCaKl\n        8sU8E7QD13MAqvUgVX8v9vA5Q80bQNsXjpDbBr+pN54UiXrGRyIQuektQLBM0KRQ\n        ozfmAFzk9PHw6udqRlr5nOgkWYd/kMTWCuK1l5yKhWexMO1WvZ4iC0u5I0a0PYK/\n        zSVhsME8FGeJ+74+lJ0zxa7klo40R6F/zW3jVvnVjG/evKcztPXd2lVm2h7sdQd2\n        SgOBAzLTMOCw8bU9wIDAJ2QwsGJyVG/rBzeoFySPvw/IxrGh4QM7hiuFKg36hmPP\n        2SAKyLCUgXLT+hZ+Shr0NT7BXnfN0LaEUKvmsTSQ4QYL05SbTh1p2YF9ZrEQDvmu\n        o5sIw9YdSqRpw0rH4+TtQ+9waZ7K3+CrwFnANG4oVMi+zkcC4OYmOSa7UWjxTRtl\n        IMbNJL9ef4zhg01BFckGDRqSSzs1Z7brHsNhsd2Al69CYc8nEbPVKhuo7X9MGeAJ\n        R7tTKZwhXm6t4YAJKs41F6H+tIsF2UM5uKdigUpd5h5nMva17eqps80uuusxJVsy\n        EIiVV7UpO5mlIJc7n/urbizNhI5UDcMkji16htc3iOzzgu77dCkf3Kl4VMWx4c0z\n        1WNodx3UuVrph0xgaHg/CX5Ss+f2CuLGwbJ+XyGKex2rRWpdq3r/lgm/ihAXG+j7\n        eOk61duFlb6nk/G4mVSnRodKGHHvYokEbAQYAQgAIBYhBMKXRDjGqPrMM3EpHShZ\n        UCFjPIQhBQJdkGQNAhsCAkAJEChZUCFjPIQhwXQgBBkBCAAdFiEEOtBpdPeN0WA9\n        XkYX0JVdIvLDJOIFAl2QZA0ACgkQ0JVdIvLDJOJHCQ//dYB0OuwIgtSLLMuBnxjB\n        uwEA27Fj5iL+gBr22vpbYtWgUvfkAa9s/qm/Z2tL2xJE/HN+eRO9AXBu3dWQeMPE\n        Wamd00eQktTFFpjqAOeeOCFRbCbRlWreAy4ES480IakHeOk9fNEnOCPqgtC/UdWI\n        G+HqhDCPOVaaa3zyqrGLx/+6ihdL9ztIuNoYW1l1pXfNSPJxp9HGiQvGhuL/+34D\n        Q7Hv5W0SYKS6jTqreFAOkQdsfNenzQk6YFm7TbhycAVH7IkjsX0s5EqLZpCZssHh\n        5hmm14GLkbGUy3fGFLTSg5uI92ZDdjJQYpUixIis/mPbDbQ7p3rOQayxVdDpzyCf\n        PNks5kAfHFj0lGOxOb3eEZUXxKcbP5QgqjTOWmNaQRnLN0ZUfZICt4m/t3VvuGmo\n        O66IH7b2F6iMpzO+BH9RqDGKkBkfgdeqkVSrIXW/og3dsccfAewcCjij3hRyMnr4\n        wwEBL4CJAFYoagtPdEOp9IAVZaAXh98nQL2X1y1/MSbzZT9hH7vDjYh8wpAYxbR5\n        nxZWILDI9ZP2JE2C9PWpDzupwYLgkJ/L0d/kpW93c5raSXInOB4hVUGUaGtqpJB1\n        aeuzTrs6Rb+2Py8ECbFn4SH7c0OJrlPoRhWTvu/g7z4Lhe+mNHoNSfJc3hkzmTRr\n        spknQfIRKmvOqc5BMGQAg8g1Wg//YVPK997C73rPEL2/syAQtn3oINtHLLsgx0fn\n        +pGV+BDl1aEdaTlF5kBprYAOC6oZQxnEbH+FlZ7ad6GanvRwY0/HiAo5WmIH0u5N\n        AcezTnppwX150Zs5rRBH0fbT/+I6CNYoEyGfX5yBmMhfc1gbS+il+Z68yiXoJ6Tk\n        ht/8PqHePj5u7ySJiiG1fEn7X8f1WjmHKuyUqauX6jOUGsIaO3q5NshUiIhhydbu\n        zgrnAS0JXdjc4rrGUxscI5QU0Hqj3hkvczeDeMrNEwBTbDaB9aNvHbYP+L3y7BUo\n        pgLJkP7ObZzcS0uFcv+STDKKAgKFoqoStZN35/3jXuar+d68aVObKDr5tbKGT6w7\n        EeUPkqqLolKxUyDDHO5YXchMJw8SpVc+GenC4kx1JPqnPbJSsvRgZPP6t3AJFh4G\n        cTCV9wIPO/YIBrFZHGgKB58u9XWjIJ0tdOvvdGrrugazh2Zn8ymOL80ENTSe75Ds\n        IJlZSj4AAI/vDB16voU9nQi/XoOvfizw6tm1frkReFyr9/gC3qqnc52u6I6VMsGk\n        hSkRYIoLAwpF+XysGp8ho707Gf38RnHOxemSjelz6JZDx65wI+B1KJDNCFHxQ3ky\n        3sah9AQbzUVN3Mi1kKkB7jXOqMMlZs0hf22Yembi+Bu7rmDr/adQN4wqXHHNIYEd\n        2/h4D2Y=\n        =NmAP\n        -----END PGP PRIVATE KEY BLOCK-----\n      SECRET\n    end",
    "comment": "passphrase for secret key is: 4a45718624c9939a043471d83d1eda7c",
    "label": "",
    "id": "7619"
  },
  {
    "raw_code": "def workhorse_post_with_file(url, file_key:, params:)\n    workhorse_form_with_file(url, method: :post, file_key: file_key, params: params)\n  end",
    "comment": "workhorse_post_with_file will transform file_key inside params as if it was disk accelerated by workhorse",
    "label": "",
    "id": "7620"
  },
  {
    "raw_code": "def workhorse_form_with_file(url, file_key:, params:, method: :post)\n    workhorse_request_with_file(\n      method, url,\n      file_key: file_key,\n      params: params,\n      env: { 'CONTENT_TYPE' => 'multipart/form-data' },\n      send_rewritten_field: true\n    )\n  end",
    "comment": "workhorse_form_with_file will transform file_key inside params as if it was disk accelerated by workhorse",
    "label": "",
    "id": "7621"
  },
  {
    "raw_code": "def workhorse_finalize(url, file_key:, params:, method: :post, headers: {}, send_rewritten_field: false)\n    workhorse_finalize_with_multiple_files(\n      url,\n      method: method,\n      file_keys: file_key,\n      params: params,\n      headers: headers,\n      send_rewritten_field: send_rewritten_field\n    )\n  end",
    "comment": "workhorse_finalize will transform file_key inside params as if it was the finalize call of an inline object storage upload. note that based on the content of the params it can simulate a disc acceleration or an object storage upload",
    "label": "",
    "id": "7622"
  },
  {
    "raw_code": "def prepare_exclusive_lease_stub\n    return if @exclusive_lease_allowed_to_call_original\n\n    allow(Gitlab::ExclusiveLease)\n      .to receive(:new).and_call_original\n\n    @exclusive_lease_allowed_to_call_original = true\n  end",
    "comment": "This prepares the stub to be able to stub specific lease keys while allowing unstubbed lease keys to behave as original.  allow(Gitlab::ExclusiveLease).to receive(:new).and_call_original can only be called once to prevent resetting stubs when `stub_exclusive_lease` is called multiple times.",
    "label": "",
    "id": "7623"
  },
  {
    "raw_code": "def drag_to(list_from_index: 0, from_index: 0, to_index: 0, list_to_index: 0, selector: '', scrollable: 'body', duration: 1000, perform_drop: true, extra_height: 0)\n    js = <<~JS\n      simulateDrag({\n        scrollable: document.querySelector('#{scrollable}'),\n        duration: #{duration},\n        from: {\n          el: document.querySelectorAll('#{selector}')[#{list_from_index}],\n          index: #{from_index}\n        },\n        to: {\n          el: document.querySelectorAll('#{selector}')[#{list_to_index}],\n          index: #{to_index}\n        },\n        performDrop: #{perform_drop},\n        extraHeight: #{extra_height}\n      });\n    JS\n    evaluate_script(js)\n\n    Timeout.timeout(Capybara.default_max_wait_time) do\n      loop while drag_active?\n    end",
    "comment": "rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "7624"
  },
  {
    "raw_code": "def drag_active?\n    page.evaluate_script('window.SIMULATE_DRAG_ACTIVE').nonzero?\n  end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "7625"
  },
  {
    "raw_code": "def n_months_ago(count)\n    count += 1 if Date.today.day == 1\n    count.months.ago\n  end",
    "comment": "On the 1st day of any month, the metrics table intentionally uses the previous month in the `Current month` column. To handle that case, we need to shift the generated test metric dates by 1 month.",
    "label": "",
    "id": "7626"
  },
  {
    "raw_code": "def find_asset(asset_name)\n    if ENV['CI']\n      Sprockets::Railtie.build_environment(Rails.application, true)[asset_name]\n    else\n      Rails.application.assets.find_asset(asset_name)\n    end",
    "comment": "In a CI environment the assets are not compiled, as there is a CI job `compile-assets` that compiles them in the prepare stage for all following specs. Locally the assets are precompiled dynamically.  Sprockets doesn't provide one method to access an asset for both cases.",
    "label": "",
    "id": "7627"
  },
  {
    "raw_code": "def within_viewport?(element)\n    evaluate_script(\"(function(el) {\n      var rect = el.getBoundingClientRect();\n      return (\n        rect.bottom >= 0 &&\n        rect.right >= 0 &&\n        rect.top <= (window.innerHeight || document.documentElement.clientHeight) &&\n        rect.left <= (window.innerWidth || document.documentElement.clientWidth)\n      );\n    })(arguments[0]);\", element.native)\n  end",
    "comment": "we're not using Capybara's .obscured here because it also checks if the element is clickable",
    "label": "",
    "id": "7628"
  },
  {
    "raw_code": "def stub_client(messages = {})\n    client = double('client', messages).as_null_object\n    allow(controller).to receive(:client).and_return(client)\n\n    client\n  end",
    "comment": "Stub `controller` to return a null object double with the provided messages when `client` is called  Examples:  stub_client(foo: %w(foo))  controller.client.foo         # => [\"foo\"] controller.client.bar.baz.foo # => [\"foo\"]  Returns the client double",
    "label": "",
    "id": "7629"
  },
  {
    "raw_code": "def filter(html, context = {}, result = nil)\n    if defined?(project)\n      context.reverse_merge!(project: project)\n    end",
    "comment": "Perform `call` on the described class  Automatically passes the current `project` value, if defined, to the context if none is provided.  html     - HTML String to pass to the filter's `call` method. context - Hash context for the filter. (default: {project: project})  Returns a Nokogiri::XML::DocumentFragment",
    "label": "",
    "id": "7630"
  },
  {
    "raw_code": "def filter_instance\n    context = { project: project, current_user: current_user, render_context: render_context }\n\n    described_class.new(input_text, context)\n  end",
    "comment": "Get an instance of the Filter class  Use this for testing instance methods, but remember to test the result of the full pipeline by calling #call using the other methods in this helper.",
    "label": "",
    "id": "7631"
  },
  {
    "raw_code": "def pipeline_result(body, context = {})\n    context.reverse_merge!(project: project) if defined?(project)\n\n    pipeline = Banzai::PipelineBase.new([described_class], context)\n    pipeline.call(body)\n  end",
    "comment": "Run text through HTML::Pipeline with the current filter and return the result Hash  body     - String text to run through the pipeline context - Hash context for the filter. (default: {project: project})  Returns the Hash",
    "label": "",
    "id": "7632"
  },
  {
    "raw_code": "def null_filter(text, context = {})\n    reference_pipeline(filter: nil, **context).to_document(text)\n  end",
    "comment": "Use to test no-ops",
    "label": "",
    "id": "7633"
  },
  {
    "raw_code": "def invalidate_reference(reference)\n    case reference\n    when /\\A(.+)?[^\\d]\\d+\\z/\n      # Integer-based reference with optional project prefix\n      reference.gsub(/\\d+\\z/) { |i| i.to_i + 10_000 }\n    when /\\A\\[\\w+:\\d+\\]\\z/\n      # Integer-based reference with [type:number] syntax\n      reference.gsub(/\\d+\\]\\z/) { |i| \"#{i[0..-2].to_i + 10_000}]\" }\n    when /\\A(.+@)?(#{Gitlab::Git::Commit::RAW_SHA_PATTERN}\\z)/o\n      # SHA-based reference with optional prefix\n      reference.gsub(/#{Gitlab::Git::Commit::RAW_SHA_PATTERN}\\z/o) { |v| v.reverse }\n    else\n      reference.gsub(/\\w+\\z/) { |v| v.reverse }\n    end",
    "comment": "Modify a String reference to make it invalid  Commit SHAs get reversed, IDs get incremented by 1, all other Strings get their word characters reversed.  reference - String reference to modify  Returns a String",
    "label": "",
    "id": "7634"
  },
  {
    "raw_code": "def urls\n    Gitlab::Routing.url_helpers\n  end",
    "comment": "Shortcut to Rails' auto-generated routes helpers, to avoid including the module",
    "label": "",
    "id": "7635"
  },
  {
    "raw_code": "def input_filtered_search(search_term, submit: true, extra_space: true)\n    search = search_term\n    if extra_space\n      # Add an extra space to engage visual tokens\n      search = \"#{search_term} \"\n    end",
    "comment": "Enables input to be set (similar to copy and paste)",
    "label": "",
    "id": "7636"
  },
  {
    "raw_code": "def select_label_on_dropdown(label_title)\n    input_filtered_search(\"label:=\", submit: false)\n\n    within('#js-dropdown-label') do\n      wait_for_requests\n\n      find('li', text: label_title).click\n    end",
    "comment": "Select a label clicking in the search dropdown instead of entering label names on the input.",
    "label": "",
    "id": "7637"
  },
  {
    "raw_code": "def input_filtered_search_keys(search_term)\n    # Add an extra space to engage visual tokens\n    filtered_search.send_keys(\"#{search_term} \")\n    filtered_search.send_keys(:enter)\n  end",
    "comment": "Enables input to be added character by character",
    "label": "",
    "id": "7638"
  },
  {
    "raw_code": "def expect_tokens(tokens)\n    page.within '.filtered-search-box .tokens-container' do\n      token_elements = page.all(:css, 'li.filtered-search-token')\n\n      tokens.each_with_index do |token, index|\n        el = token_elements[index]\n\n        expect(el.find('.name')).to have_content(token[:name])\n        expect(el.find('.operator')).to have_content(token[:operator]) if token[:operator].present?\n        expect(el.find('.value')).to have_content(token[:value]) if token[:value].present?\n\n        # gl-emoji content is blank when the emoji unicode is not supported\n        if token[:emoji_name].present?\n          selector = %(gl-emoji[data-name=\"#{token[:emoji_name]}\"])\n          expect(el.find('.value')).to have_css(selector)\n        end",
    "comment": "Iterates through each visual token inside .tokens-container to make sure the correct names and values are rendered",
    "label": "",
    "id": "7639"
  },
  {
    "raw_code": "def expect_vue_tokens(tokens)\n    page.within '.gl-search-box-by-click .gl-filtered-search-scrollable' do\n      token_elements = page.all(:css, '.gl-filtered-search-token')\n\n      tokens.each_with_index do |token, index|\n        el = token_elements[index]\n\n        expect(el.find('.gl-filtered-search-token-type')).to have_content(token[:name])\n        expect(el.find('.gl-filtered-search-token-operator')).to have_content(token[:operator]) if token[:operator].present?\n        expect(el.find('.gl-filtered-search-token-data')).to have_content(token[:value]) if token[:value].present?\n\n        # gl-emoji content is blank when the emoji unicode is not supported\n        if token[:emoji_name].present?\n          selector = %(gl-emoji[data-name=\"#{token[:emoji_name]}\"])\n          expect(el.find('.gl-filtered-search-token-data-content')).to have_css(selector)\n        end",
    "comment": "Same as `expect_tokens` but works with GlFilteredSearch",
    "label": "",
    "id": "7640"
  },
  {
    "raw_code": "def select_tokens(*args, submit: false, search_token: false, input_text: 'Search')\n    within '[data-testid=\"filtered-search-input\"]' do\n      find_field(input_text).click\n\n      args.each do |token|\n        # Move mouse away to prevent invoking tooltips on usernames, which blocks the search input\n        find_button('Search').hover\n\n        if search_token\n          find_by_testid('filtered-search-token-segment-input').send_keys token.to_s\n        end",
    "comment": " For use with gl-filtered-search",
    "label": "",
    "id": "7641"
  },
  {
    "raw_code": "def api(path, user = nil, version: API::API.version, personal_access_token: nil, oauth_access_token: nil, job_token: nil, access_token: nil, admin_mode: false)\n    full_path = \"/api/#{version}#{path}\"\n\n    if oauth_access_token\n      query_string = \"access_token=#{oauth_access_token.plaintext_token}\"\n    elsif personal_access_token\n      query_string = \"private_token=#{personal_access_token.token}\"\n    elsif job_token\n      query_string = \"job_token=#{job_token}\"\n    elsif access_token\n      query_string = \"access_token=#{access_token.token}\"\n    elsif user\n      organization = user.organization || FactoryBot.build(:common_organization)\n\n      personal_access_token = if admin_mode && user.admin?\n                                create(:personal_access_token, :admin_mode, user: user, organization: organization)\n                              else\n                                create(:personal_access_token, user: user, organization: organization)\n                              end",
    "comment": "Public: Prepend a request path with the path to the API  path - Path to append user - User object - If provided, automatically appends private_token query string for authenticated requests  Examples  >> api('/issues') => \"/api/v2/issues\"  >> api('/issues', User.last) => \"/api/v2/issues?private_token=...\"  >> api('/issues?foo=bar', User.last) => \"/api/v2/issues?foo=bar&private_token=...\"  Returns the relative path to the requested API resource",
    "label": "",
    "id": "7642"
  },
  {
    "raw_code": "def dropzone_file(files, max_file_size = 0, wait_for_queuecomplete = true)\n    # Generate a fake file input that Capybara can attach to\n    page.execute_script <<-JS.strip_heredoc\n      $('#fakeFileInput').remove();\n      var fakeFileInput = window.$('<input/>').attr(\n        {id: 'fakeFileInput', type: 'file', multiple: true}\n      ).appendTo('body');\n\n      window._dropzoneComplete = false;\n    JS\n\n    # Attach files to the fake input selector with Capybara\n    attach_file('fakeFileInput', files)\n\n    first('.div-dropzone')\n\n    # Manually trigger a Dropzone \"drop\" event with the fake input's file list\n    page.execute_script <<-JS.strip_heredoc\n      var dropzone = $('.div-dropzone')[0].dropzone;\n      dropzone.options.autoProcessQueue = false;\n\n      if (#{max_file_size} > 0) {\n        dropzone.options.maxFilesize = #{max_file_size};\n      }\n\n      dropzone.on('queuecomplete', function() {\n        window._dropzoneComplete = true;\n      });\n\n      var fileList = [$('#fakeFileInput')[0].files];\n\n      $.map(fileList, function(file){\n        var e = jQuery.Event('drop', { dataTransfer : { files : file } });\n\n        dropzone.listeners[0].events.drop(e);\n      });\n\n      dropzone.processQueue();\n    JS\n\n    if wait_for_queuecomplete\n      # Wait until Dropzone's fired `queuecomplete`\n      loop until page.evaluate_script('window._dropzoneComplete === true')\n    end",
    "comment": "Provides a way to perform `attach_file` for a Dropzone-based file input  This is accomplished by creating a standard HTML file input on the page, performing `attach_file` on that field, and then triggering the appropriate Dropzone events to perform the actual upload.  This method waits for the upload to complete before returning. max_file_size is an optional parameter. If it's not 0, then it used in dropzone.maxFilesize parameter. wait_for_queuecomplete is an optional parameter. If it's 'false', then the helper will NOT wait for backend response It lets to test behaviors while AJAX is processing.",
    "label": "",
    "id": "7643"
  },
  {
    "raw_code": "def stub=(stub)\n      @stub = stub\n    end",
    "comment": "Turn stubbed feature flags on or off.",
    "label": "",
    "id": "7644"
  },
  {
    "raw_code": "def reset_flipper\n      @flipper = nil\n    end",
    "comment": "Wipe any previously set feature flags.",
    "label": "",
    "id": "7645"
  },
  {
    "raw_code": "def flipper\n      if stub?\n        @flipper ||= Flipper.new(Flipper::Adapters::Memory.new)\n      else\n        super\n      end",
    "comment": "Replace #flipper method with the optional stubbed/unstubbed version.",
    "label": "",
    "id": "7646"
  },
  {
    "raw_code": "def enabled?(*args, **kwargs)\n      feature_flag = super\n      return feature_flag unless stub?\n\n      # If feature flag is not persisted we mark the feature flag as enabled\n      # We do `m.call` as we want to validate the execution of method arguments\n      # and a feature flag state if it is not persisted\n      unless Feature.persisted_name?(args.first)\n        feature_flag = true\n      end",
    "comment": "Replace #enabled? method with the optional stubbed/unstubbed version.",
    "label": "",
    "id": "7647"
  },
  {
    "raw_code": "def find_csp_directive(key, header: nil)\n    csp = header || response.headers['Content-Security-Policy']\n\n    # Transform \"default-src foo bar; connect-src foo bar; script-src ...\"\n    # into array of values for a single directive based on the given key\n    csp.split(';')\n      .map(&:strip)\n      .find { |entry| entry.starts_with?(key) }\n      .split(' ')\n      .drop(1)\n  end",
    "comment": "Finds the given csp directive values as an array  Example: ``` find_csp_directive('connect-src') ```",
    "label": "",
    "id": "7648"
  },
  {
    "raw_code": "def project\n    @project ||= create(:project, :repository, group: group).tap do |project|\n      project.add_maintainer(user)\n    end",
    "comment": "Direct references ----------------------------------------------------------",
    "label": "",
    "id": "7649"
  },
  {
    "raw_code": "def xproject\n    @xproject ||= begin\n      group = create(:group, :nested)\n      create(:project, :repository, namespace: group) do |project|\n        project.add_developer(user)\n      end",
    "comment": "Cross-references -----------------------------------------------------------",
    "label": "",
    "id": "7650"
  },
  {
    "raw_code": "def setup_methods\n    SETUP_METHODS\n  end",
    "comment": "Can be overriden",
    "label": "",
    "id": "7651"
  },
  {
    "raw_code": "def setup_go_projects\n    setup_gitaly\n    setup_gitlab_shell\n    setup_workhorse\n  end",
    "comment": "Can be overriden The Go build cache is not safe for concurrent builds: https://github.com/golang/go/issues/43645",
    "label": "",
    "id": "7652"
  },
  {
    "raw_code": "def init\n    unless Rails.env.test?\n      puts \"\\nTestEnv.init can only be run if `RAILS_ENV` is set to 'test' not '#{Rails.env}'!\\n\"\n      exit 1\n    end",
    "comment": "Test environment  See gitlab.yml.example test section for paths ",
    "label": "",
    "id": "7653"
  },
  {
    "raw_code": "def post_init\n    start_gitaly\n  end",
    "comment": "Can be overriden",
    "label": "",
    "id": "7654"
  },
  {
    "raw_code": "def clean_test_path\n    Dir[File.join(TMP_TEST_PATH, '**')].each do |entry|\n      unless test_dirs.include?(File.basename(entry))\n        FileUtils.rm_rf(entry)\n      end",
    "comment": "Clean /tmp/tests  Keeps gitlab-shell and gitlab-test",
    "label": "",
    "id": "7655"
  },
  {
    "raw_code": "def setup_workhorse\n    measure_setup_duration('GitLab Workhorse') do\n      # Always rebuild the config file\n      if skip_compile_workhorse?\n        Gitlab::SetupHelper::Workhorse.create_configuration(workhorse_dir, nil, force: true)\n      else\n        FileUtils.rm_rf(workhorse_dir)\n        Gitlab::SetupHelper::Workhorse.compile_into(workhorse_dir)\n        Gitlab::SetupHelper::Workhorse.create_configuration(workhorse_dir, nil)\n\n        File.write(workhorse_tree_file, workhorse_tree) if workhorse_source_clean?\n      end",
    "comment": "Feature specs are run through Workhorse",
    "label": "",
    "id": "7656"
  },
  {
    "raw_code": "def setup_factory_repo\n    setup_repo(factory_repo_path, factory_repo_bundle_path, factory_repo_name, BRANCH_SHA)\n  end",
    "comment": "Create repository for FactoryBot.create(:project)",
    "label": "",
    "id": "7657"
  },
  {
    "raw_code": "def setup_forked_repo\n    setup_repo(forked_repo_path, forked_repo_bundle_path, forked_repo_name, FORKED_BRANCH_SHA)\n  end",
    "comment": "Create repository for FactoryBot.create(:forked_project_with_submodules) This repo has a submodule commit that is not present in the main test repository.",
    "label": "",
    "id": "7658"
  },
  {
    "raw_code": "def eager_load_driver_server\n    return unless defined?(Capybara)\n\n    puts \"Starting the Capybara driver server...\"\n    Capybara.current_session.visit '/'\n  end",
    "comment": "When no cached assets exist, manually hit the root path to create them  Otherwise they'd be created by the first test, often timing out and causing a transient test failure",
    "label": "",
    "id": "7659"
  },
  {
    "raw_code": "def test_dirs\n    @test_dirs ||= [\n      'frontend',\n      'gitaly',\n      'gitlab-shell',\n      'gitlab-test',\n      'gitlab-test.bundle',\n      'gitlab-test-fork',\n      'gitlab-test-fork.bundle',\n      'gitlab-workhorse',\n      'gitlab_workhorse_secret',\n      File.basename(GitalySetup.storage_path),\n      File.basename(GitalySetup.second_storage_path)\n    ]\n  end",
    "comment": "These are directories that should be preserved at cleanup time",
    "label": "",
    "id": "7660"
  },
  {
    "raw_code": "def git_env\n    { 'GIT_TEMPLATE_DIR' => '' }\n  end",
    "comment": "Prevent developer git configurations from being persisted to test repositories",
    "label": "",
    "id": "7661"
  },
  {
    "raw_code": "def get_spec_file\n      caller.find do |line|\n        match = line.match(%r{^(.+_spec\\.rb|.+/frontend/fixtures/.+\\.rb):\\d+:in})\n        match[1] if match\n      end",
    "comment": "Determine the spec filename from the current backtrace.",
    "label": "",
    "id": "7662"
  },
  {
    "raw_code": "def run_parallel(blocks, task_wait_time: 20.seconds, max_concurrency: Concurrent.processor_count - 1)\n    thread_pool = Concurrent::FixedThreadPool.new(\n      [2, max_concurrency].max, { max_queue: blocks.size }\n    )\n    opts = { executor: thread_pool }\n\n    error = Concurrent::MVar.new\n\n    blocks.map { |block| Concurrent::Future.execute(opts, &block) }.each do |future|\n      future.wait(task_wait_time)\n\n      if future.complete?\n        error.put(future.reason) if future.reason && error.empty?\n      else\n        future.cancel\n        error.put(Cancelled.new) if error.empty?\n      end",
    "comment": "To test for contention, we may need to run some actions in parallel. This helper takes an array of blocks and schedules them all on different threads in a fixed-size thread pool.  @param [Array[Proc]] blocks @param [Integer] task_wait_time: time to wait for each task (upper bound on reasonable task execution time) @param [Integer] max_concurrency: maximum number of tasks to run at once ",
    "label": "",
    "id": "7663"
  },
  {
    "raw_code": "def block_and_wait_for_requests_complete\n    block_requests { wait_for_all_requests }\n  end",
    "comment": "This is inspired by http://www.salsify.com/blog/engineering/tearing-capybara-ajax-tests",
    "label": "",
    "id": "7664"
  },
  {
    "raw_code": "def block_requests\n    Gitlab::Testing::RequestBlockerMiddleware.block_requests!\n    yield\n  ensure\n    Gitlab::Testing::RequestBlockerMiddleware.allow_requests!\n  end",
    "comment": "Block all requests inside block with 503 response",
    "label": "",
    "id": "7665"
  },
  {
    "raw_code": "def slow_requests\n    Gitlab::Testing::RequestBlockerMiddleware.slow_requests!\n    yield\n  ensure\n    Gitlab::Testing::RequestBlockerMiddleware.allow_requests!\n  end",
    "comment": "Slow down requests inside block by injecting `sleep 1` before each response",
    "label": "",
    "id": "7666"
  },
  {
    "raw_code": "def wait_for_requests(max_wait_time: 2 * Capybara.default_max_wait_time)\n    wait_for('JS requests complete', max_wait_time: max_wait_time) do\n      finished_all_js_requests?\n    end",
    "comment": "Wait for client-side AJAX requests",
    "label": "",
    "id": "7667"
  },
  {
    "raw_code": "def wait_for_all_requests\n    wait_for('pending requests complete') do\n      finished_all_rack_requests? &&\n        finished_all_js_requests?\n    end",
    "comment": "Wait for active Rack requests and client-side AJAX requests",
    "label": "",
    "id": "7668"
  },
  {
    "raw_code": "def match_html_escaped_tags(content)\n    match_data = %r{&lt;\\s*(?:/\\s*)?\\w+}.match(content)\n    return unless match_data\n\n    # Escaped HTML tags are allowed inside quoted attribute values like:\n    # `title=\"Press &lt;back&gt;\"`\n    return if %r{=\\s*[\"'][^>]*\\z}.match?(match_data.pre_match)\n\n    match_data\n  end",
    "comment": "Checks if +content+ contains HTML escaped tags and returns its match.  It matches escaped opening and closing tags `&lt;<name>` and `&lt;/<name>`. The match is discarded if the tag is inside a quoted attribute value. Foor example, `<div title=\"We allow # &lt;b&gt;bold&lt;/b&gt;\">`.  @return [MatchData, nil] Returns the match or +nil+ if no match was found.",
    "label": "",
    "id": "7669"
  },
  {
    "raw_code": "def ensure_no_html_escaped_tags!(content, example)\n    match_data = match_html_escaped_tags(content)\n    return unless match_data\n\n    # Truncate\n    pre_match = match_data.pre_match.last(50)\n    match = match_data[0]\n    post_match = match_data.post_match.first(50)\n\n    string = \"#{pre_match}#{match}#{post_match}\"\n\n    raise <<~MESSAGE\n      The following string contains HTML escaped tags:\n\n      #{string}\n\n      Please consider using `.html_safe`.\n\n      This check can be disabled via:\n\n        it #{example.description.inspect}, :skip_html_escaped_tags_check do\n          ...\n        end",
    "comment": "Checks if +content+ contains HTML escaped tags and raises an exception if it does.  See #match_html_escaped_tags for details.",
    "label": "",
    "id": "7670"
  },
  {
    "raw_code": "def sign_in(resource, scope: nil)\n    super\n\n    @current_user = resource\n  end",
    "comment": "Overriding Devise::Test::IntegrationHelpers#sign_in to store @current_user since we may need it in LiveDebugger#live_debug.",
    "label": "",
    "id": "7671"
  },
  {
    "raw_code": "def sign_out(resource_or_scope)\n    super\n\n    @current_user = nil\n  end",
    "comment": "Overriding Devise::Test::IntegrationHelpers#sign_out to clear @current_user.",
    "label": "",
    "id": "7672"
  },
  {
    "raw_code": "def gitlab_sign_in(user_or_role, **kwargs)\n    user =\n      if user_or_role.is_a?(User)\n        user_or_role\n      else\n        create(user_or_role) # rubocop:disable Rails/SaveBang\n      end",
    "comment": "Internal: Log in as a specific user or a new user of a specific role  user_or_role - User object, or a role to create (e.g., :admin, :user)  Examples:  # Create a user automatically gitlab_sign_in(:user)  # Create an admin automatically gitlab_sign_in(:admin)  # Provide an existing User record user = create(:user) gitlab_sign_in(user)",
    "label": "",
    "id": "7673"
  },
  {
    "raw_code": "def gitlab_sign_out(user = @current_user)\n    if has_testid?('super-sidebar')\n      click_on \"#{user.name} users menu\"\n    else\n      # This can be removed once https://gitlab.com/gitlab-org/gitlab/-/issues/420121 is complete.\n      find(\".header-user-dropdown-toggle\").click\n    end",
    "comment": "Requires Javascript driver.",
    "label": "",
    "id": "7674"
  },
  {
    "raw_code": "def gitlab_disable_admin_mode\n    find_by_testid('user-menu-toggle').click\n    click_on 'Leave Admin Mode'\n  end",
    "comment": "Requires Javascript driver.",
    "label": "",
    "id": "7675"
  },
  {
    "raw_code": "def gitlab_sign_in_with(user, remember: false, two_factor_auth: false, password: nil, visit: true)\n    visit new_user_session_path if visit\n\n    fill_in \"user_login\", with: user.email\n\n    # When JavaScript is enabled, wait for the password field, with class `.js-password`,\n    # to be replaced by the Vue passsword component,\n    # `app/assets/javascripts/authentication/password/components/password_input.vue`.\n    expect(page).not_to have_selector('.js-password') if javascript_test?\n\n    fill_in \"user_password\", with: (password || user.password)\n\n    check 'user_remember_me' if remember\n\n    wait_for_all_requests\n\n    find('[data-testid=\"sign-in-button\"]:enabled').click\n\n    if two_factor_auth\n      fill_in \"user_otp_attempt\", with: user.reload.current_otp\n      click_button \"Verify code\"\n    end",
    "comment": "Private: Login as the specified user  user - User instance to login with remember - Whether or not to check \"Remember me\" (default: false) two_factor_auth - If two-factor authentication is enabled (default: false) password - password to attempt to login with (default: user.password)",
    "label": "",
    "id": "7676"
  },
  {
    "raw_code": "def select_work_item_type(type)\n    select type.to_s.capitalize, from: 'Type'\n  end",
    "comment": "Listbox helpers",
    "label": "",
    "id": "7677"
  },
  {
    "raw_code": "def fill_work_item_title(title)\n    find_by_testid('work-item-title-input').send_keys(title)\n  end",
    "comment": "Textbox helpers",
    "label": "",
    "id": "7678"
  },
  {
    "raw_code": "def assign_work_item_to_yourself\n    within_testid 'work-item-assignees' do\n      click_button 'assign yourself'\n    end",
    "comment": "Work item widget helpers",
    "label": "",
    "id": "7679"
  },
  {
    "raw_code": "def create_work_item_with_type(type)\n    click_button \"Create #{type}\"\n  end",
    "comment": "Action helpers",
    "label": "",
    "id": "7680"
  },
  {
    "raw_code": "def expect_work_item_widgets(widget_names)\n    widget_names.each do |widget|\n      expect(page).to have_selector(\"[data-testid=\\\"#{widget}\\\"]\")\n    end",
    "comment": "Assertion helpers",
    "label": "",
    "id": "7681"
  },
  {
    "raw_code": "def close_drawer\n    find('[data-testid=\"work-item-drawer\"] .gl-drawer-close-button').click\n    wait_for_all_requests\n  end",
    "comment": "drawer helpers",
    "label": "",
    "id": "7682"
  },
  {
    "raw_code": "def add_estimate(estimate)\n    click_button 'estimate'\n    within_testid 'set-time-estimate-modal' do\n      fill_in 'Estimate', with: estimate\n      click_button 'Save'\n    end",
    "comment": "time tracking",
    "label": "",
    "id": "7683"
  },
  {
    "raw_code": "def get_graphql_query_as_string(query_path, ee: false, with_base_path: true)\n    base = (ee ? 'ee/' : '') + (with_base_path ? 'app/assets/javascripts' : '')\n    path = Rails.root / base / query_path\n    queries = Gitlab::Graphql::Queries.find(path)\n    if queries.length == 1\n      query = queries.first.text(mode: Gitlab.ee? ? :ee : :ce)\n      inflate_query_with_typenames(query)\n    else\n      raise \"Could not find query file at #{path}, please check your query_path\" % path\n    end",
    "comment": "Public: Reads a GraphQL query from the filesystem as a string  query_path - file path to the GraphQL query, relative to `app/assets/javascripts`. ee - boolean, when true `query_path` will be looked up in `/ee`.",
    "label": "",
    "id": "7684"
  },
  {
    "raw_code": "def inflate_query_with_typenames(query, doc: Graphlyte.parse(query))\n    typename_editor.edit(doc)\n\n    doc.to_s\n  end",
    "comment": "Private: Parse a GraphQL query and inflate the fields with a __typename  query - the GraqhQL query to parse",
    "label": "",
    "id": "7685"
  },
  {
    "raw_code": "def store_frontend_fixture(response, fixture_file_name)\n    full_fixture_path = File.expand_path(fixture_file_name, fixture_root_path)\n    fixture = response.respond_to?(:body) ? parse_response(response) : response\n\n    FileUtils.mkdir_p(File.dirname(full_fixture_path))\n    File.write(full_fixture_path, fixture)\n  end",
    "comment": "Private: Store a response object as fixture file  response - string or response object to store fixture_file_name - file name to store the fixture in (relative to .fixture_root_path) ",
    "label": "",
    "id": "7686"
  },
  {
    "raw_code": "def parse_response(response)\n    fixture = response.body\n    fixture.force_encoding(\"utf-8\")\n\n    response_mime_type = Mime::Type.lookup(response.media_type)\n    if response_mime_type.html?\n      doc = parse_html(fixture)\n\n      link_tags = doc.css('link')\n      link_tags.remove\n\n      scripts = doc.css(\"script:not([type='text/template']):not([type='text/x-template']):not([type='application/json'])\")\n      scripts.remove\n\n      fixture = doc.to_html\n\n      # replace relative links\n      test_host = ActionDispatch::TestRequest::DEFAULT_ENV['HTTP_HOST']\n      fixture.gsub!(%r{=\"/}, \"=\\\"http://#{test_host}/\")\n    end",
    "comment": "Private: Prepare a response object for use as a frontend fixture  response - response object to prepare ",
    "label": "",
    "id": "7687"
  },
  {
    "raw_code": "def stub_all_feature_flags\n    Feature.stub = true\n    Feature.reset_flipper\n  end",
    "comment": "Ensure feature flags are stubbed and reset.",
    "label": "",
    "id": "7688"
  },
  {
    "raw_code": "def stub_feature_flags(features)\n    features.each do |feature_name, actors|\n      warn(\"Invalid Feature Flag #{feature_name} stubbed\") unless Feature::Definition.get(feature_name)\n\n      # Remove feature flag overwrite\n      feature = Feature.get(feature_name) # rubocop:disable Gitlab/AvoidFeatureGet\n      feature.remove\n\n      Array(actors).each do |actor|\n        raise ArgumentError, \"actor cannot be Hash\" if actor.is_a?(Hash)\n\n        # Control a state of feature flag\n        if actor == true || actor.nil? || actor.respond_to?(:flipper_id)\n          feature.enable(actor)\n        elsif actor == false\n          feature.disable\n        else\n          raise ArgumentError, \"#stub_feature_flags accepts only `nil`, `bool`, an object responding to `#flipper_id` or including `FeatureGate`.\"\n        end",
    "comment": "Stub Feature flags with `flag_name: true/false`  @param [Hash] features where key is feature name and value is boolean whether enabled or not. Alternatively, you can specify Hash to enable the flag on a specific thing.  Examples - `stub_feature_flags(ci_live_trace: false)` ... Disable `ci_live_trace` feature flag globally. - `stub_feature_flags(ci_live_trace: project)` ... - `stub_feature_flags(ci_live_trace: [project1, project2])` ... Enable `ci_live_trace` feature flag only on the specified projects.",
    "label": "",
    "id": "7689"
  },
  {
    "raw_code": "def sign_in(resource, deprecated = nil, scope: nil)\n    super\n\n    scope ||= Devise::Mapping.find_scope!(resource)\n\n    @controller.instance_variable_set(:\"@current_#{scope}\", nil)\n  end",
    "comment": "It seems Devise::Test::ControllerHelpers#sign_in doesn't clear out the @current_user variable of the controller, hence it's not overwritten on retries. This should be fixed in Devise: - https://github.com/heartcombo/devise/issues/5190 - https://github.com/heartcombo/devise/pull/5191",
    "label": "",
    "id": "7690"
  },
  {
    "raw_code": "def define_helper_redis_store_class(store_name = \"Workhorse\")\n    Class.new(Gitlab::Redis::Wrapper) do\n      define_singleton_method(:name) { store_name }\n\n      def config_file_name\n        config_file_name = \"spec/fixtures/config/redis_new_format_host.yml\"\n        Rails.root.join(config_file_name).to_s\n      end\n    end",
    "comment": "Defines a class of wrapper that uses `resque.yml` regardless of `config/redis.yml.example` this allows us to test against a standalone Redis even if Cache and SharedState are using Redis Cluster. We do not use queue as it does not perform redis cluster validations.",
    "label": "",
    "id": "7691"
  },
  {
    "raw_code": "def add_another_asset_link\n      click_button('Add another link')\n    end",
    "comment": "Click \"Add another link\" and tab back to the beginning of the new row",
    "label": "",
    "id": "7692"
  },
  {
    "raw_code": "def fill_and_wait_for_mirror_url_javascript(input_identifier, url)\n      fill_in input_identifier, with: url\n      wait_for_mirror_field_javascript('url', url)\n    end",
    "comment": "input_identifier - identifier of the input field, passed to `fill_in` (can be an ID or a label). url - the URL to fill the input field with.",
    "label": "",
    "id": "7693"
  },
  {
    "raw_code": "def wait_for_mirror_field_javascript(attribute, expected_value)\n      expect(page).to have_css(\".js-mirror-#{attribute}-hidden[value=\\\"#{expected_value}\\\"]\", visible: :hidden)\n    end",
    "comment": "attribute - can be `url` or `protected`. It's used in the `.js-mirror-<field>-hidden` selector. expected_value - the expected value of the hidden field.",
    "label": "",
    "id": "7694"
  },
  {
    "raw_code": "def pajamas_sort_by(value, from: nil)\n      raise ArgumentError, 'The :from option must be given' if from.nil?\n\n      click_button from\n      find('[role=\"option\"]', text: value).click\n    end",
    "comment": "pajamas_sort_by is used to sort new pajamas dropdowns. When all of the dropdowns are converted, pajamas_sort_by can be renamed to sort_by https://gitlab.com/groups/gitlab-org/-/epics/7551",
    "label": "",
    "id": "7695"
  },
  {
    "raw_code": "def ide_visit(project)\n      visit project_path(project)\n\n      ide_visit_from_link\n    end",
    "comment": "Open the IDE from anywhere by first visiting the given project's page",
    "label": "",
    "id": "7696"
  },
  {
    "raw_code": "def ide_visit_from_link\n      new_tab = window_opened_by do\n        edit_in_web_ide\n      end",
    "comment": "Open the IDE from the current page by clicking the Web IDE link",
    "label": "",
    "id": "7697"
  },
  {
    "raw_code": "def otp_authenticator_registration(pin, password = nil)\n      click_button _('Register authenticator')\n      fill_in 'current_password', with: password if password\n      fill_in 'pin_code', with: pin\n      click_button _('Register with two-factor app')\n    end",
    "comment": "Register OTP authenticator via UI",
    "label": "",
    "id": "7698"
  },
  {
    "raw_code": "def webauthn_device_registration(webauthn_device: nil, name: 'My device', password: 'fake')\n      webauthn_device ||= FakeWebauthnDevice.new(page, name)\n      webauthn_device.respond_to_webauthn_registration\n      click_on _('Register device')\n      wait_for_requests\n      click_on _('Set up new device')\n      webauthn_fill_form_and_submit(name: name, password: password)\n      webauthn_device\n    end",
    "comment": "Registers webauthn device via UI",
    "label": "",
    "id": "7699"
  },
  {
    "raw_code": "def add_webauthn_device(app_id, user, fake_device = nil, name: 'My device')\n      fake_device ||= WebAuthn::FakeClient.new(app_id)\n\n      options_for_create = WebAuthn::Credential.options_for_create(\n        user: { id: user.webauthn_xid, name: user.username },\n        authenticator_selection: { user_verification: 'discouraged' },\n        rp: { name: 'GitLab' }\n      )\n      challenge = options_for_create.challenge\n\n      device_response = fake_device.create(challenge: challenge).to_json # rubocop:disable Rails/SaveBang\n      device_registration_params = { device_response: device_response,\n                                     name: name }\n\n      Webauthn::RegisterService.new(\n        user, device_registration_params, challenge).execute\n      FakeWebauthnDevice.new(page, name, fake_device)\n    end",
    "comment": "Adds webauthn device directly via database",
    "label": "",
    "id": "7700"
  },
  {
    "raw_code": "def active_access_tokens\n      find_by_testid('active-tokens')\n    end",
    "comment": "Remove when we migrate the legacy UI to use initSharedAccessTokenApp",
    "label": "",
    "id": "7701"
  },
  {
    "raw_code": "def new_access_token\n      find_by_testid('created-access-token-field').value\n    end",
    "comment": "Keep when we migrate the legacy UI to use initSharedAccessTokenApp",
    "label": "",
    "id": "7702"
  },
  {
    "raw_code": "def focus_filtered_search\n      page.within(search_bar_selector) do\n        page.find('.gl-filtered-search-term-token').click\n      end",
    "comment": "The filters must be clicked first to be able to receive events See: https://gitlab.com/gitlab-org/gitlab-services/design.gitlab.com/-/issues/2799",
    "label": "",
    "id": "7703"
  },
  {
    "raw_code": "def ensure_table_exists!\n      disable_migrations_output do\n        next if described_class.table_exists?(:metrics_users_starred_dashboards)\n\n        described_class.execute <<~SQL\n          CREATE TABLE metrics_users_starred_dashboards (\n            id bigint NOT NULL,\n            created_at timestamp with time zone NOT NULL,\n            updated_at timestamp with time zone NOT NULL,\n            project_id bigint NOT NULL,\n            user_id bigint NOT NULL,\n            dashboard_path text NOT NULL,\n            CONSTRAINT check_79a84a0f57 CHECK ((char_length(dashboard_path) <= 255))\n          );\n\n          CREATE SEQUENCE metrics_users_starred_dashboards_id_seq\n            START WITH 1\n            INCREMENT BY 1\n            NO MINVALUE\n            NO MAXVALUE\n            CACHE 1;\n\n          ALTER SEQUENCE metrics_users_starred_dashboards_id_seq OWNED BY metrics_users_starred_dashboards.id;\n\n          ALTER TABLE ONLY metrics_users_starred_dashboards\n            ALTER COLUMN id SET DEFAULT nextval('metrics_users_starred_dashboards_id_seq'::regclass);\n\n          ALTER TABLE ONLY metrics_users_starred_dashboards\n            ADD CONSTRAINT metrics_users_starred_dashboards_pkey PRIMARY KEY (id);\n\n          CREATE UNIQUE INDEX idx_metrics_users_starred_dashboard_on_user_project_dashboard\n            ON metrics_users_starred_dashboards USING btree (user_id, project_id, dashboard_path);\n\n          CREATE INDEX index_metrics_users_starred_dashboards_on_project_id\n            ON metrics_users_starred_dashboards USING btree (project_id);\n\n          ALTER TABLE ONLY metrics_users_starred_dashboards\n            ADD CONSTRAINT fk_bd6ae32fac FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE;\n\n          ALTER TABLE ONLY metrics_users_starred_dashboards\n            ADD CONSTRAINT fk_d76a2b9a8c FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE;\n        SQL\n      end",
    "comment": "Loading the schema from structure.sql doesn't account for change made for https://gitlab.com/gitlab-com/gl-infra/production-engineering/-/issues/26996 so we modify the db schema directly for testing",
    "label": "",
    "id": "7704"
  },
  {
    "raw_code": "def create_cluster_project_list(quantity)\n      group = namespaces_table.create!(name: 'gitlab-org', path: 'gitlab-org')\n\n      quantity.times do |id|\n        create_cluster_project(group, id)\n      end",
    "comment": "Creates a list of cluster projects.",
    "label": "",
    "id": "7705"
  },
  {
    "raw_code": "def create_cluster_project(group, id)\n      project = projects_table.create!(\n        name: \"project-#{id}\",\n        path: \"project-#{id}\",\n        namespace_id: group.id\n      )\n\n      cluster = clusters_table.create!(\n        name: 'test-cluster',\n        cluster_type: 3,\n        provider_type: :gcp,\n        platform_type: :kubernetes\n      )\n\n      cluster_projects_table.create!(project_id: project.id, cluster_id: cluster.id)\n\n      provider_gcp_table.create!(\n        gcp_project_id: \"test-gcp-project-#{id}\",\n        endpoint: '111.111.111.111',\n        cluster_id: cluster.id,\n        status: 3,\n        num_nodes: 1,\n        zone: 'us-central1-a'\n      )\n\n      platform_kubernetes_table.create!(\n        cluster_id: cluster.id,\n        api_url: 'https://kubernetes.example.com',\n        encrypted_token: 'a' * 40,\n        encrypted_token_iv: 'a' * 40\n      )\n    end",
    "comment": "Creates dependencies for a cluster project: - Group - Project - Cluster - Project - cluster relationship - GCP provider - Platform Kubernetes",
    "label": "",
    "id": "7706"
  },
  {
    "raw_code": "def create_kubernetes_namespace(clusters)\n      clusters.each do |cluster|\n        cluster_project = cluster_projects_table.find_by(cluster_id: cluster.id)\n        project = projects_table.find(cluster_project.project_id)\n        namespace = \"#{project.path}-#{project.id}\"\n\n        cluster_kubernetes_namespaces_table.create!(\n          cluster_project_id: cluster_project.id,\n          cluster_id: cluster.id,\n          project_id: cluster_project.project_id,\n          namespace: namespace,\n          service_account_name: \"#{namespace}-service-account\"\n        )\n      end",
    "comment": "Creates a Kubernetes namespace for a list of clusters",
    "label": "",
    "id": "7707"
  },
  {
    "raw_code": "def create_finding!(\n      project_id:, scanner_id:, primary_identifier_id:, vulnerability_id: nil,\n      name: \"test\", severity: 7, report_type: 0,\n      location_fingerprint: 'test',\n      metadata_version: 'test', raw_metadata: 'test', uuid: 'b1cee17e-3d7a-11ed-b878-0242ac120002')\n      table(:vulnerability_occurrences).create!(\n        vulnerability_id: vulnerability_id,\n        project_id: project_id,\n        name: name,\n        severity: severity,\n        report_type: report_type,\n        scanner_id: scanner_id,\n        primary_identifier_id: primary_identifier_id,\n        location_fingerprint: location_fingerprint,\n        metadata_version: metadata_version,\n        raw_metadata: raw_metadata,\n        uuid: uuid\n      )\n    end",
    "comment": "rubocop:disable Metrics/ParameterLists",
    "label": "",
    "id": "7708"
  },
  {
    "raw_code": "def create_vulnerability!(\n      project_id:, author_id:, finding_id:, title: 'test', severity: 7, report_type: 0)\n      table(:vulnerabilities).create!(\n        project_id: project_id,\n        author_id: author_id,\n        title: title,\n        severity: severity,\n        confidence: confidence,\n        report_type: report_type,\n        finding_id: finding_id\n      )\n    end",
    "comment": "rubocop:enable Metrics/ParameterLists",
    "label": "",
    "id": "7709"
  },
  {
    "raw_code": "def self.as_graphql_literal(value)\n      case value\n      when ::Graphql::Arguments then \"{#{value}}\"\n      when Array then \"[#{value.map { |v| as_graphql_literal(v) }.join(',')}]\"\n      when Hash then \"{#{new(value)}}\"\n      when Integer, Float, Symbol then value.to_s\n      when String, GlobalID then \"\\\"#{value.to_s.gsub(/\"/, '\\\\\"')}\\\"\"\n      when Time, Date then \"\\\"#{value.iso8601}\\\"\"\n      when NilClass then 'null'\n      when true then 'true'\n      when false then 'false'\n      else\n        value.to_graphql_value\n      end",
    "comment": "Transform values to GraphQL literal arguments. Use symbol for Enum values",
    "label": "",
    "id": "7710"
  },
  {
    "raw_code": "def with(value)\n      copy = Var.new(name, type)\n      copy.value = value\n      copy\n    end",
    "comment": "We return a new object so that running the same query twice with different values does not risk re-using the value  e.g.  x = var('Int') expect { post_graphql(query, variables: x) } .to issue_same_number_of_queries_as { post_graphql(query, variables: x.with(1)) }  Here we post the `x` variable once with the value set to 1, and once with the value set to `nil`.",
    "label": "",
    "id": "7711"
  },
  {
    "raw_code": "def create_dpop_proof(alg, typ, kid, public_key, private_key, htu: '', htm: '', **claims)\n      headers = create_headers(alg, typ, public_key, kid)\n      jti = SecureRandom.uuid\n\n      payload = create_payload(\n        htu: htu, htm: htm, jti: jti, **claims)\n\n      JWT.encode(payload, private_key, alg, headers)\n    end",
    "comment": "-- all params needed for edge cases",
    "label": "",
    "id": "7712"
  },
  {
    "raw_code": "def _bulk_insert_callback_allowed?(name, args)\n        super || (args.first == :after && args.second == :check_partition_cascade_value)\n      end",
    "comment": "Allowing partition callback to be used with BulkInsertSafe",
    "label": "",
    "id": "7713"
  },
  {
    "raw_code": "def placeholder_user_references(import_type, import_uid, limit = 100)\n      user_references = ::Import::PlaceholderReferences::Store.new(import_source: import_type, import_uid: import_uid)\n        .get(limit)\n\n      user_references.map do |item|\n        item = Import::SourceUserPlaceholderReference.from_serialized(item)\n        key = item.numeric_key || item.composite_key\n\n        [item.model, key, item.user_reference_column, item.source_user_id]\n      end",
    "comment": "Return references pushed to Redis as an array  Example:  [ \"Issue\", 1, \"author_id\", 1, \"Issue\", 1, \"last_edited_by_id\", 1, \"Note\", 2, \"author_id\", 1 ]  @param import_type [String] @param import_uid [Integer] @param limit [Integer] @return [Array]",
    "label": "",
    "id": "7714"
  },
  {
    "raw_code": "def generate_source_user(project, identifier, placeholder_user: nil)\n      create_properties = placeholder_user.present? ? { placeholder_user: placeholder_user } : {}\n\n      create(\n        :import_source_user,\n        source_user_identifier: identifier,\n        source_hostname: project.safe_import_url,\n        import_type: project.import_type,\n        namespace: project.root_ancestor,\n        **create_properties\n      )\n    end",
    "comment": "Generate a source user with a provided project and identifier  @param project [Project] @param identifier [String, Integer] @param placholder_user [User] Sets a specific placeholder when given. Otherwise, use factory default. @return [Import::SourceUser]",
    "label": "",
    "id": "7715"
  },
  {
    "raw_code": "def swapout_view_for_table(view, connection:, schema: nil)\n      table_name = [schema, \"_test_#{view}_copy\"].compact.join('.')\n\n      connection.execute(<<~SQL.squish)\n        CREATE TABLE #{table_name} (LIKE #{view});\n        DROP VIEW #{view};\n        ALTER TABLE #{table_name} RENAME TO #{view};\n      SQL\n    end",
    "comment": "In order to directly work with views using factories, we can swapout the view for a table of identical structure.",
    "label": "",
    "id": "7716"
  },
  {
    "raw_code": "def with_statement_timeout(timeout, connection:)\n      # Force a positive value and a minimum of 1ms for very small values.\n      timeout = (timeout * 1000).abs.ceil\n\n      raise ArgumentError, 'Using a timeout of `0` means to disable statement timeout.' if timeout == 0\n\n      previous_timeout = connection.select_value('SHOW statement_timeout')\n\n      connection.execute(format(%(SET LOCAL statement_timeout = '%s'), timeout))\n\n      yield\n    ensure\n      begin\n        connection.execute(format(%(SET LOCAL statement_timeout = '%s'), previous_timeout))\n      rescue ActiveRecord::StatementInvalid\n        # After a transaction was canceled/aborted due to e.g. a statement\n        # timeout commands are ignored and will raise in PG::InFailedSqlTransaction.\n        # We can safely ignore this error because the statement timeout was set\n        # for the currrent transaction which will be closed anyway.\n      end",
    "comment": "Set statement timeout temporarily. Useful when testing query timeouts.  Note that this method cannot restore the timeout if a query was canceled due to e.g. a statement timeout. Refrain from using this transaction in these situations.  @param timeout - Statement timeout in seconds  Example:  with_statement_timeout(0.1) do model.select('pg_sleep(0.11)') end",
    "label": "",
    "id": "7717"
  },
  {
    "raw_code": "def with_reestablished_active_record_base(reconnect: true)\n      connection_classes = ActiveRecord::Base\n        .connection_handler\n        .connection_pool_names\n        .map(&:constantize)\n\n      connection_classes.delete(ActiveRecord::PendingMigrationConnection)\n\n      connection_class_to_config = connection_classes.index_with(&:connection_db_config)\n\n      original_handler = ActiveRecord::Base.connection_handler\n      new_handler = ActiveRecord::ConnectionAdapters::ConnectionHandler.new\n      ActiveRecord::Base.connection_handler = new_handler\n\n      if reconnect\n        # Schema validation requires all connections to be established so we skip this validator\n        # while we re-establish each connection class.\n        Gitlab::Database::QueryAnalyzers::GitlabSchemasValidateConnection.with_suppressed do\n          connection_class_to_config.each { |klass, db_config| klass.establish_connection(db_config) }\n        end",
    "comment": "The usage of this method switches temporarily used `connection_handler` allowing full manipulation of ActiveRecord::Base connections without having side effects like: - misaligned transactions since this is managed by `TestProf::BeforeAll::Adapters::ActiveRecord` - removal of primary connections  The execution within a block ensures safe cleanup of all allocated resources. ",
    "label": "",
    "id": "7718"
  },
  {
    "raw_code": "def fail_first_time\n      # We can't directly use a boolean here, as we need something that will be passed by-reference to the proc\n      fault_status = { faulted: false }\n      proc do |m, *args, **kwargs|\n        next m.call(*args, **kwargs) if fault_status[:faulted]\n\n        fault_status[:faulted] = true\n        raise 'fault!'\n      end",
    "comment": "These methods are used by specs that inject faults into the migration procedure and then ensure that it migrates correctly when rerun",
    "label": "",
    "id": "7719"
  },
  {
    "raw_code": "def record_migration_call_counts(migrations)\n      call_counts = migrations.index_with { |_m| 0 }\n      migrations.each do |migration|\n        allow_next_instances_of(migration, nil) do |migration_instance|\n          allow(migration_instance).to receive(:perform).and_wrap_original do |method, *args, **kwargs|\n            call_counts[migration] += 1\n            method.call(*args, **kwargs)\n          end",
    "comment": "Returns a hash of migration_class -> number of times perform was called. Sets up instrumentation of the provided array of migrations, as instances of their perform methods are called, the values in the hash update to count the calls.",
    "label": "",
    "id": "7720"
  },
  {
    "raw_code": "def cloud_platform_cluster_body(options)\n      {\n        name: options[:name] || 'string',\n        description: options[:description] || 'string',\n        initialNodeCount: options[:initialNodeCount] || 'number',\n        masterAuth: {\n          username: options[:username] || 'string',\n          password: options[:password] || 'string',\n          clusterCaCertificate: options[:clusterCaCertificate] || load_sample_cert,\n          clientCertificate: options[:clientCertificate] || 'string',\n          clientKey: options[:clientKey] || 'string'\n        },\n        loggingService: options[:loggingService] || 'string',\n        monitoringService: options[:monitoringService] || 'string',\n        network: options[:network] || 'string',\n        clusterIpv4Cidr: options[:clusterIpv4Cidr] || 'string',\n        subnetwork: options[:subnetwork] || 'string',\n        enableKubernetesAlpha: options[:enableKubernetesAlpha] || 'boolean',\n        labelFingerprint: options[:labelFingerprint] || 'string',\n        selfLink: options[:selfLink] || 'string',\n        zone: options[:zone] || 'string',\n        endpoint: options[:endpoint] || 'string',\n        initialClusterVersion: options[:initialClusterVersion] || 'string',\n        currentMasterVersion: options[:currentMasterVersion] || 'string',\n        currentNodeVersion: options[:currentNodeVersion] || 'string',\n        createTime: options[:createTime] || 'string',\n        status: options[:status] || 'RUNNING',\n        statusMessage: options[:statusMessage] || 'string',\n        nodeIpv4CidrSize: options[:nodeIpv4CidrSize] || 'number',\n        servicesIpv4Cidr: options[:servicesIpv4Cidr] || 'string',\n        currentNodeCount: options[:currentNodeCount] || 'number',\n        expireTime: options[:expireTime] || 'string'\n      }\n    end",
    "comment": " gcloud container clusters create https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters/create rubocop:disable Metrics/CyclomaticComplexity rubocop:disable Metrics/PerceivedComplexity",
    "label": "",
    "id": "7721"
  },
  {
    "raw_code": "def cloud_platform_operation_body(options)\n      {\n        name: options[:name] || 'operation-1234567891234-1234567',\n        zone: options[:zone] || 'us-central1-a',\n        operationType: options[:operationType] || 'CREATE_CLUSTER',\n        status: options[:status] || 'PENDING',\n        detail: options[:detail] || 'detail',\n        statusMessage: options[:statusMessage] || '',\n        selfLink: options[:selfLink] || 'https://container.googleapis.com/v1/projects/123456789101/zones/us-central1-a/operations/operation-1234567891234-1234567',\n        targetLink: options[:targetLink] || 'https://container.googleapis.com/v1/projects/123456789101/zones/us-central1-a/clusters/test-cluster',\n        startTime: options[:startTime] || '2017-09-13T16:49:13.055601589Z',\n        endTime: options[:endTime] || ''\n      }\n    end",
    "comment": "rubocop:enable Metrics/CyclomaticComplexity rubocop:enable Metrics/PerceivedComplexity",
    "label": "",
    "id": "7722"
  },
  {
    "raw_code": "def simple_sanitize(str)\n    str\n  end",
    "comment": "Override simple_sanitize for our testing purposes",
    "label": "",
    "id": "7723"
  },
  {
    "raw_code": "def item_ids\n        group? ? group_ids_for(group) : []\n      end",
    "comment": "normally an array of item ids would be returned, however for this spec just return the group ids",
    "label": "",
    "id": "7724"
  },
  {
    "raw_code": "def dynamic_segment\n    File.join(model.class.underscore, model.id.to_s)\n  end",
    "comment": "user/:id",
    "label": "",
    "id": "7725"
  },
  {
    "raw_code": "def dynamic_segment\n        File.join(model.class.underscore, model.id.to_s)\n      end",
    "comment": "user/:id",
    "label": "",
    "id": "7726"
  },
  {
    "raw_code": "def expect_command(cmd)\n      expect(runner).to receive(:exec).with(*cmd)\n    end",
    "comment": "rubocop:enable Gitlab/Json",
    "label": "",
    "id": "7727"
  }
]